<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[‰∏äÊµ∑ÊüêÂÖ¨Âè∏ÁöÑÁîü‰∫ßMySQLÁÅæÈöæÊÄßÊåΩÊïë]]></title>
    <url>%2F2019%2F05%2F30%2F%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91%2F</url>
    <content type="text"><![CDATA[1.ËÉåÊôØÊú¨‰∫∫(Ëã•Ê≥ΩÊï∞ÊçÆJÂì•)ÁöÑÂ™≥Â¶áÔºåÊòØ‰∏™ÊºÇ‰∫ÆÁöÑÂ¶πÂ≠êÔºåÂêåÊó∂‰πüÊòØ‰∏ÄÊûöÁà¨Ëô´&amp;SparkÂºÄÂèëÂ∑•Á®ãÂ∏à„ÄÇÂâçÂ§©ÔºåÂ•πÁöÑÂÖ¨Âè∏MySQL(ÈòøÈáå‰∫ëECSÊúçÂä°Âô®)ÔºåÁî±‰∫éÁ£ÅÁõòÁàÜ‰∫ÜÂä†‰∏ä‰∫∫‰∏∫ÁöÑ‰øÆÂ§çÔºåÂØºËá¥ÂêÑÁßçÈóÆÈ¢òÔºåÁÑ∂ÂêéÁªèËøá2Â§©ÁöÑÊäòËÖæÔºåÁªà‰∫éÂÖ¨Âè∏ÁöÑÂ§ßÁ•û‰øÆÂ§ç‰∏ç‰∫Ü‰∫Ü„ÄÇ‰∫éÊòØÂ∞±‰∏¢ÁªôÂ•π‰∫ÜÔºåÈ°∫ÁêÜÊàêÁ´†ÁöÑÂ∞±‰∏¢ÁªôÊàë‰∫Ü„ÄÇÊàëÊÉ≥ËØ¥ÔºåÈöæÈÅìJÂì•Ëøô‰πàÂá∫ÂêçÂêóÔºüÈÇ£‰∏∫‰∫ÜÂú®Â¶πÂ≠êÈù¢Ââç‰∏çËÉΩ‰∏¢Êàë‰ª¨ÁúüÊ≠£Â§ß‰Ω¨ÁöÑÁ•ûÊäÄÔºå‰∫éÊòØ‰πéÊàëÂ∞±ÂæàÁàΩÂø´Êé•‰∫ÜËøô‰∏™MySQLÊïÖÈöúÊÅ¢Â§çÔºåÊ≠§Ê¨°ÊïÖÈöúÁöÑÊòØ‰∏Ä‰∏™Êï∞ÊçÆÁõòÔºå1T„ÄÇËøôÊó∂ÁöÑÊàëÔºåËØ¥ÁúüÁöÑÂπ∂Ê≤°ÊúâÊÑèËØÜÂà∞ÔºåÊ≠§‰∫ãÊòØÂ¶ÇÊ≠§ÁöÑÁπÅÊùÇÔºåÁâπÊ≠§ÂÜôÊ≠§ÂçöÊñáËÆ∞ÂΩï‰∏Ä‰∏ãÔºåÊØïÁ´üJÂì•ÊàëÂπ¥Á∫™‰πüÂ§ß‰∫Ü„ÄÇPS:ËøôÈáåÂêêÊßΩ‰∏Ä‰∏ãÔºåÂπ∂Ê≤°ÊúâÂë®Êó•ÂÖ®Â§á+Âë®1~Âë®6Â¢ûÈáèÂ§á‰ªΩÊú∫Âà∂ÂìüÔºå‰∏çÁÑ∂ÊÅ¢Â§çÂ∞±ÁàΩÊ≠™Ê≠™‰∫Ü„ÄÇ2.ÊïÖÈöúÁé∞Ë±°12Êü•ÁúãË°®ÁªìÊûÑ„ÄÅÊü•ËØ¢Ë°®Êï∞ÊçÆÈÉΩÂ¶Ç‰∏ãÊäõÈîô:ERROR 1030 (HY000): Got error 122 from storage engine3.Â∞ùËØï‰øÆÂ§çÁ¨¨‰∏ÄÊ¨°ÔºåÂ§±Ë¥•3.1 ‰ΩøÁî®repairÂëΩ‰ª§‰øÆÂ§çË°®123mysql&gt; repair table wenshu.wenshu2018; ÈîôËØØ‰æùÊóß:ERROR 1030 (HY000): Got error 122 from storage engine3.2 Ë∞∑Ê≠å‰∏ÄÁØáÊúâÊåáÂØºÊÑè‰πâÁöÑhttps://stackoverflow.com/questions/68029/got-error-122-from-storage-engine3.2.1 ËÆ©ÂÖ∂Êâ©ÂÆπÊï∞ÊçÆÁ£ÅÁõò‰∏∫1.5TÔºåËØïËØïÔºå‰æùÊóßËøô‰∏™ÈîôËØØÔºõ3.2.2 ‰∏¥Êó∂ÁõÆÂΩï‰øÆÊîπ‰∏∫Â§ßÁöÑÁ£ÅÁõòÁ©∫Èó¥ÔºåËØïËØïÔºå‰æùÊóßËøô‰∏™ÈîôËØØÔºõ3.2.3 ÂèñÊ∂àÁ£ÅÁõòÈôêÈ¢ùÔºåËØïËØïÔºå‰æùÊóßËøô‰∏™ÈîôËØØÔºõ3.2.4 Â∞±ÊòØ‰∏ÄÂºÄÂßãÁöÑrepairÂëΩ‰ª§‰øÆÂ§çÔºåËØïËØïÔºå‰æùÊóßËøô‰∏™ÈîôËØØÔºõËøôÊó∂ÁöÑÊàëÔºå‰πüÊó†ËØ≠‰∫ÜÔºå‰ªÄ‰πàÈ¨ºÔºÅË∞∑Ê≠å‰∏ÄÈ°µÈ°µÊêúÁ¥¢È™åËØÅÔºåÊ≤°ÊúâÁî®ÔºÅ4.ÂÖàÈÉ®ÁΩ≤Áõ∏ÂêåÁ≥ªÁªüÁöÑÁõ∏ÂêåÁâàÊú¨ÁöÑÊú∫Âô®ÂíåMySQL‰∫éÊòØJÂì•ÔºåÂø´ÈÄüÂú®„ÄêËã•Ê≥ΩÊï∞ÊçÆ„ÄëÁöÑÈòøÈáå‰∫ëË¥¶Âè∑‰∏ä‰π∞‰∫Ü1Âè∞Ubuntu 16.04.6ÁöÑÊåâÈáè‰ªòË¥πÊú∫Âô®ËøÖÈÄüÈÉ®ÁΩ≤MySQL5.7.26„ÄÇ4.1 Ë¥≠‰π∞ÊåâÈáè‰ªòË¥πÊú∫Âô®(ÂÅáÂ¶Ç‰∏ç‰ºöË¥≠‰π∞ÔºåÊâæJÂì•)4.2 ÈÉ®ÁΩ≤MySQL123456789101112131415161718a.Êõ¥Êñ∞apt-get$ apt-get updateb.ÂÆâË£ÖMySQL-Server$ apt-get install mysql-server‰πãÂêé‰ºöÈóÆ‰Ω†ÔºåÊòØÂê¶Ë¶Å‰∏ãËΩΩÊñá‰ª∂Ôºå ËæìÂÖ• y Â∞±Â•Ω‰∫ÜÁÑ∂Âêé‰ºöÂá∫Áé∞ËÆ©‰Ω†ËÆæÁΩÆ root ÂØÜÁ†ÅÁöÑÁïåÈù¢ËæìÂÖ•ÂØÜÁ†Å: ruozedata123ÁÑ∂ÂêéÂÜçÈáçÂ§ç‰∏Ä‰∏ãÔºåÂÜçÊ¨°ËæìÂÖ•ÂØÜÁ†Å: ruozedata123c.ÂÆâË£ÖMySQL-Client$ apt install mysql-clientd.Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî®$ mysql -uroot -pruozedata123Êù•ËøûÊé•ÊúçÂä°Âô®Êú¨Âú∞ÁöÑ MySQL5.Â∞ùËØïÂÖàÈÄöËøáfrmÊñá‰ª∂ÊÅ¢Â§çË°®ÁªìÊûÑÔºåÂ§±Ë¥•123456789101112131415161718192021222324252627a. Âª∫Á´ã‰∏Ä‰∏™Êï∞ÊçÆÂ∫ìÔºåÊØîÂ¶Çwenshu.b. Âú®ruozedataÊï∞ÊçÆÂ∫ì‰∏ãÂª∫Á´ãÂêåÂêçÁöÑÊï∞ÊçÆË°®wenshu2018ÔºåË°®ÁªìÊûÑÈöèÊÑèÔºåËøôÈáåÂè™Êúâ‰∏Ä‰∏™idÂ≠óÊÆµÔºåÊìç‰ΩúËøáÁ®ãÁâáÊÆµÂ¶Ç‰∏ãÔºömysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;mysql&gt; show tables;+--------------+| Tables_in_aa |+--------------+| wenshu2018 |+--------------+1 rows in set (0.00 sec)mysql&gt; desc wenshu2018;+-------+------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+------------+------+-----+---------+-------+| id | bigint(20) | NO | | NULL | |+-------+------------+------+-----+---------+-------+1 row in set (0.00 sec)c.ÂÅúÊ≠¢mysqlÊúçÂä°Âô®ÔºåÂ∞Üwenshu2018.frmÊñá‰ª∂scpËøúÁ®ãÊã∑Ë¥ùÂà∞Êñ∞ÁöÑÊ≠£Â∏∏Êï∞ÊçÆÂ∫ìÁöÑÊï∞ÊçÆÁõÆÂΩïwenshu‰∏ãÔºåË¶ÜÁõñÊéâ‰∏ãËæπÂêåÂêçÁöÑfrmÊñá‰ª∂Ôºöd.ÈáçÊñ∞ÂêØÂä®MYSQLÊúçÂä°e.ÊµãËØï‰∏ãÊòØÂê¶ÊÅ¢Â§çÊàêÂäüÔºåËøõÂÖ•wenshuÊï∞ÊçÆÂ∫ìÔºåÁî®descÂëΩ‰ª§ÊµãËØï‰∏ãÔºåÈîôËØØ‰∏∫:mysql Tablespace is missing for table `wenshu`.`wenshu2018`.6.Â∞ùËØïÊúâÊ≤°ÊúâÂ§á‰ªΩÁöÑË°®ÁªìÊûÑÊÅ¢Â§çÊï∞ÊçÆÔºåÂ§±Ë¥•Â™≥Â¶áÂÖ¨Âè∏ÁªôÂá∫‰∏Ä‰∏™Ë°®ÁªìÊûÑ,Â¶Ç‰∏ãÔºåÁªèËøáÊµãËØïÊó†Ê≥ïÊÅ¢Â§çÔºåÂéüÂõ†Â∞±ÊòØÊó†Ê≥ïÂíåibdÊñá‰ª∂ÂåπÈÖç„ÄÇ123456789101112DROP TABLE IF EXISTS cpws_batch;CREATE TABLE cpws_batch ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, crawl_time datetime NULL DEFAULT NULL, status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 ÊàêÂäü/Â§±Ë¥•&apos;, PRIMARY KEY (id) USING BTREE, INDEX ix_status(status) USING BTREE, INDEX ix_doc_id(doc_id) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;7.Â¶Ç‰ΩïËé∑ÂèñÊ≠£Á°ÆÁöÑË°®ÁªìÊûÑÔºåËøôÊòØ„ÄêÊàêÂäüÁöÑÁ¨¨‰∏ÄÊ≠•„Äë1234567891011121314151617181920212223242526272829303132$ curl -s get.dbsake.net &gt; /tmp/dbsake$ chmod u+x /tmp/dbsake$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm ---- Table structure for table wenshu_0_1000-- Created with MySQL Version 5.7.25--CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 ÊàêÂäü/Â§±Ë¥•&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;ÂØπÊØîStep6ÁöÑË°®ÁªìÊûÑÔºåÊÑüËßâÂ∞±Â∑ÆÂàÜÂå∫ËÆæÁΩÆËÄåÂ∑≤ÔºåÂùëÔºÅËøôÊó∂ÔºåJÂì•ÊúâÁßç‰ø°ÂøÉÔºåÊÅ¢Â§çÂ∫îËØ•Â∞èËèú‰∫Ü„ÄÇ8.Áî±‰∫éÊÅ¢Â§çECSÊú∫Âô®ÊòØËã•Ê≥ΩÊï∞ÊçÆË¥¶Âè∑Ë¥≠‰π∞ÔºåËøôÊó∂ÈúÄË¶Å‰ªéÂ™≥Â¶áÂÖ¨Âè∏Ë¥¶Âè∑ÁöÑÊú∫Âô®‰º†ËæìËøôÂº†Ë°®ibdÊñá‰ª∂ÔºåÂ∑Æ‰∏çÂ§ö300GÔºåÂ∞ΩÁÆ°Êàë‰ª¨ÊòØÈòøÈáå‰∫ëÁöÑÂêå‰∏Ä‰∏™Âå∫ÂüüÂêå‰∏Ä‰∏™ÂèØÁî®Âå∫ÔºåÂä†‰∏äË∞ÉÂ§ßÂ§ñÁΩëÂ∏¶ÂÆΩ‰º†ËæìÔºå‰æùÁÑ∂‰∏çËÉΩÁ≠âÂæÖËøô‰πà‰πÖ‰º†ËæìÔºÅ9.Ë¶ÅÊ±ÇÂ™≥Â¶áÂÖ¨Âè∏Ë¥≠‰π∞ÂêåË¥¶Êà∑‰∏ãÂêåÂå∫ÂüüÁöÑÂèØÁî®Âå∫ÂüüÁöÑ‰∫ë‰∏ªÊú∫ÔºåÁ≥ªÁªüÁõò300GÔºåÊ≤°Êúâ‰π∞Êï∞ÊçÆÁõòÔºåÂÖàÂ∞ùËØïÂÅöÊÅ¢Â§çÁúãÁúãÔºåËÉΩ‰∏çËÉΩÊàêÂäüÊÅ¢Â§çÁ¨¨‰∏Ä‰∏™Ë°®ÂìüÔºü„ÄêÊàêÂäüÁöÑÁ¨¨‰∫åÊ≠•„Äë1234567891011121314151617181920212223242526272829303132333435363738394041429.1È¶ñÂÖàÈúÄË¶Å‰∏Ä‰∏™Ë∑üË¶ÅÊÅ¢Â§çÁöÑË°®ÁªìÊûÑÂÆåÂÖ®‰∏ÄËá¥ÁöÑË°®ÔºåËá≥ÂÖ≥ÈáçË¶Åmysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;USE wenshu;CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 ÊàêÂäü/Â§±Ë¥•&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;9.2ÁÑ∂ÂêéDISCARD TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;9.3ÊääË¶ÅÊÅ¢Â§çÁöÑibdÊñá‰ª∂Â§çÂà∂Âà∞mysqlÁöÑdataÊñá‰ª∂Â§π‰∏ãÔºå‰øÆÊîπÁî®Êà∑ÂíåÁî®Êà∑ÁªÑ‰∏∫mysql$ scp wenshu2018#P#p*.ibd Êñ∞Âª∫Êú∫Âô®IP:/mnt/mysql_data/wenshu/$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd9.4ÁÑ∂ÂêéÊâßË°åIMPORT TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;9.5Á≠âÂæÖÔºåÊúâÊàèÔºåËÄóÊó∂3hÔºåËøôÊó∂ÊàëÁõ∏‰ø°Â∫îËØ•‰πàÈóÆÈ¢òÁöÑ9.6Êü•ËØ¢Êï∞ÊçÆÔºåÊûúÁÑ∂ÊÅ¢Â§çÊúâÁªìÊûúÔºåÂøÉÈáåÊöóÊöóËá™Âñúmysql&gt; select * from wenshu.wenshu2018 limit 1\G;10.ÁªôÂ™≥Â¶áÂÖ¨Âè∏‰∏§‰∏™ÈÄâÊã©ÔºåËøô‰∏™ÂæàÈáçË¶ÅÔºåÂú®Ëá™Â∑±ÂÖ¨Âè∏ÁªôÈ¢ÜÂØºÂÅöÈÄâÊã©Êó∂Ôºå‰πüË¶ÅÂ∫îËØ•ËøôÊ†∑ÔºåÂ§öÈ°πÈÄâÊã©ÔºåÂà©ÂºäËØ¥ÊòéÔºå‰æõÂØπÊñπÈÄâÊã©10.1 ÈáçÊñ∞Ë¥≠‰π∞‰∏ÄÂè∞Êñ∞ÁöÑÊúçÂä°Âô®ÔºåÂú®ÂàùÂßãÂåñÈÖçÁΩÆÊó∂ÔºåÂ∞±Âä†‰∏ä1Âùó1.5TÁöÑÂ§ßÁ£ÅÁõò„ÄÇÂ•ΩÂ§ÑÊòØÊó†ÈúÄÊåÇÁõòÊìç‰ΩúÔºåÂùèÂ§ÑÊòØÈúÄË¶ÅÈáçÊñ∞ÂÅöÁ¨¨‰∏Ä‰∏™Ë°®ÔºåÊµ™Ë¥π3hÔºõ10.2 Ë¥≠‰π∞1.5TÁöÑÂ§ßÁ£ÅÁõòÔºåÊåÇËΩΩËøô‰∏™Êú∫Âô®‰∏ä„ÄÇÂ•ΩÂ§ÑÊòØÊó†ÈúÄÂÜçÂÅö‰∏ÄÊ¨°Á¨¨‰∏Ä‰∏™Ë°®ÔºåÂùèÂ§ÑÊòØÈúÄË¶Å‰øÆÊîπmysqlÁöÑÊï∞ÊçÆÁõÆÂΩïÊåáÂêë‰∏∫Ëøô‰∏™Â§ßÁ£ÅÁõò„ÄÇÁ≥ªÁªüÁõòÊâ©ÂÆπÊúÄÂ§ß‰πüÂ∞±500GÔºåÊâÄ‰ª•ÂøÖÈ°ªÂ§ñÂä†‰∏Ä‰∏™Êï∞ÊçÆÁõò1.5TÂÆπÈáè„ÄÇÊâÄ‰ª•JÂì•ÊòØËÅåÂú∫ËÄÅÊâã‰∫ÜÔºÅË¥ºÁ¨ëÔºÅ11.ÊúçÂä°Âô®Âä†Êï∞ÊçÆÁ£ÅÁõòÔºå1.5TÔºåË¥≠‰π∞„ÄÅÊåÇËΩΩ„ÄÅÊ†ºÂºèÂåñÊé•‰∏ãÊù•ÁöÑÊìç‰ΩúÊòØÊàëÂ™≥Â¶áÁã¨Á´ãÂÆåÊàêÁöÑÔºåËøôÈáåË°®Êâ¨‰∏Ä‰∏ã:11.1 ÂÖà‰π∞‰∫ëÁõò https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n811.2 ÂÜçÊåÇËΩΩ‰∫ëÁõò Âà∞ÂØπÂ∫îÊú∫Âô® https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB11.3 ÊúÄÂêéLinuxÊ†ºÂºèÂåñÊï∞ÊçÆÁõò https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9LrÂõæ2ÊâÄÁ§∫Ôºådf -hÂëΩ‰ª§Êü•ÁúãÔºåÂ§ßÁ£ÅÁõò/dev/vdb112.MySQL‰øÆÊîπÊï∞ÊçÆÁõÆÂΩï‰∏∫Â§ßÁ£ÅÁõòÔºåÈáçÊñ∞ÂêØÂä®Â§±Ë¥•ÔºåËß£ÂÜ≥12345678910111213141516171819202122232425262728293031323312.1 ‰øÆÊîπÊï∞ÊçÆÁõÆÂΩï‰∏∫Â§ßÁ£ÅÁõò$ mkdir -p /mnt/mysql_data$ chown mysql:mysql /mnt/mysql_data$ vi /etc/mysql/mysql.conf.d/mysqld.cnfdatadir = /mnt/mysql_data12.2 Êó†Ê≥ïÂêØÂä®mysql$ service mysql restartÊó†Ê≥ïÂêØÂä®ÊàêÂäüÔºåÊü•ÁúãÊó•Âøó2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error12.3 ÁôæÊÄù‰∏çÂæóÂÖ∂Ëß£ÔºåCentOS‰πüÊ≤°ÊúâËøô‰πàÈ∫ªÁÉ¶ÔºåUbuntuÈöæÈÅìËøô‰πàÊêû‰∫ãÂêóÔºü12.4 Êñ∞Â¢ûmysqldÂÜÖÂÆπ$ vi /etc/apparmor.d/local/usr.sbin.mysqld# Site-specific additions and overrides for usr.sbin.mysqld.# For more details, please see /etc/apparmor.d/local/README./mnt/mysql_data/ r,/mnt/mysql_data/** rwk,12.5 reload apparmorÁöÑÈÖçÁΩÆÂπ∂ÈáçÂêØ$ service apparmor reload $ service apparmor restart 12.6 ÈáçÂêØmysql$ service mysql restartÂ¶ÇÊûúÂêØÂä®‰∏ç‰∫ÜÔºåÊü•Áúã/var/log/mysql/error.logÂ¶ÇÊûúÂá∫Áé∞ÔºöInnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable ‰ªîÁªÜÊ†∏ÂØπÁõÆÂΩïÊùÉÈôê12.7 ËøõmysqlÊü•ËØ¢Êï∞ÊçÆÈ™åËØÅÔºåÊàêÂäüselect * from wenshu.wenshu2018 limit 1\G;13.ÂºÄÂßãÊåáÂØºÊàëÂ™≥Â¶áÂÅöÁ¨¨‰∫å‰∏™„ÄÅÁ¨¨‰∏â‰∏™Ë°®ÔºåÊâπÈáèÊÅ¢Â§çÔºåËÄóÊó∂ÂÖ±ËÆ°16Â∞èÊó∂ÔºåÂÖ®ÈÉ®ÊÅ¢Â§çÂÆåÊàê„ÄÇÊúÄÂêé@Ëã•Ê≥ΩÊï∞ÊçÆJÂì•ÊÄªÁªì‰∏Ä‰∏ã:Ë°®ÁªìÊûÑÊ≠£Á°ÆÁöÑËé∑ÂèñÔºõÊú∫Âô®Á£ÅÁõòËßÑÂàíÊèêÂâçÊÄùËÄÉÔºõibdÊï∞ÊçÆÊñá‰ª∂ÊÅ¢Â§çÔºõÊúÄÂêéÂä†‰∏ä‰∏Ä‰∏™ËÅ™ÊòéÁöÑÂ™≥Â¶áÔºÅ(PS:ËÄÅÊùø‰ºöÁªôÂ™≥Â¶áÊ∂®Ëñ™Ê∞¥‰∏çüôÖ‚Äç‚ôÇÔ∏è)]]></content>
      <categories>
        <category>ÂÖ∂‰ªñÁªÑ‰ª∂</category>
      </categories>
      <tags>
        <tag>Êû∂ÊûÑ</tag>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Ê°à‰æã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ëã•Ê≥ΩÊï∞ÊçÆ-CDH5.16.1ÈõÜÁæ§‰ºÅ‰∏öÁúüÊ≠£Á¶ªÁ∫øÈÉ®ÁΩ≤(ÂÖ®ÁΩëÊúÄÁªÜÔºåÈÖçÂ•óËßÜÈ¢ëÔºåÁîü‰∫ßÂèØÂÆûË∑µ)]]></title>
    <url>%2F2019%2F05%2F13%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÊï∞ÊçÆCDH5.16.1ÈõÜÁæ§‰ºÅ‰∏öÁúüÊ≠£Á¶ªÁ∫øÈÉ®ÁΩ≤(ÂÖ®ÁΩëÊúÄÁªÜÔºåÈÖçÂ•óËßÜÈ¢ëÔºåÁîü‰∫ßÂèØÂÆûË∑µ)ËßÜÈ¢ë:https://www.bilibili.com/video/av52167219PS:Âª∫ËÆÆÂÖàÁúãËØæÁ®ãËßÜÈ¢ë1-2ÁØáÔºåÂÜçÊ†πÊçÆËßÜÈ¢ëÊàñÊñáÊ°£ÈÉ®ÁΩ≤ÔºåÂ¶ÇÊúâÈóÆÈ¢òÔºåÂèäÊó∂‰∏é@Ëã•Ê≥ΩÊï∞ÊçÆJÂì•ËÅîÁ≥ª„ÄÇ‰∏Ä.ÂáÜÂ§áÂ∑•‰Ωú1.Á¶ªÁ∫øÈÉ®ÁΩ≤‰∏ªË¶ÅÂàÜ‰∏∫‰∏âÂùó:a.MySQLÁ¶ªÁ∫øÈÉ®ÁΩ≤b.CMÁ¶ªÁ∫øÈÉ®ÁΩ≤c.ParcelÊñá‰ª∂Á¶ªÁ∫øÊ∫êÈÉ®ÁΩ≤2.ËßÑÂàí:ËäÇÁÇπMySQLÈÉ®ÁΩ≤ÁªÑ‰ª∂ParcelÊñá‰ª∂Á¶ªÁ∫øÊ∫êCMÊúçÂä°ËøõÁ®ãÂ§ßÊï∞ÊçÆÁªÑ‰ª∂hadoop001MySQLParcelActivity MonitorNN RM DN NMhadoop002Alert PublisherEvent ServerDN NMhadoop003Host MonitorService MonitorDN NM3.‰∏ãËΩΩÊ∫ê:CMcloudera-manager-centos7-cm5.16.1_x86_64.tar.gzParcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1manifest.jsonJDKhttps://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html‰∏ãËΩΩjdk-8u202-linux-x64.tar.gzMySQLhttps://dev.mysql.com/downloads/mysql/5.7.html#downloads‰∏ãËΩΩmysql-5.7.26-el7-x86_64.tar.gzMySQL jdbc jarmysql-connector-java-5.1.47.jar‰∏ãËΩΩÂÆåÊàêÂêéË¶ÅÈáçÂëΩÂêçÂéªÊéâÁâàÊú¨Âè∑Ôºåmv mysql-connector-java-5.1.47.jar mysql-connector-java.jar###ÂáÜÂ§áÂ•ΩÁôæÂ∫¶‰∫ë,‰∏ãËΩΩÂÆâË£ÖÂåÖ:ÈìæÊé•:https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA ÂØÜÁ†Å:viqp‰∫å.ÈõÜÁæ§ËäÇÁÇπÂàùÂßãÂåñ1.ÈòøÈáå‰∫ë‰∏äÊµ∑Âå∫Ë¥≠‰π∞3Âè∞ÔºåÊåâÈáè‰ªòË¥πËôöÊãüÊú∫CentOS7.2Êìç‰ΩúÁ≥ªÁªüÔºå2Ê†∏8GÊúÄ‰ΩéÈÖçÁΩÆ2.ÂΩìÂâçÁ¨îËÆ∞Êú¨ÊàñÂè∞ÂºèÊú∫ÈÖçÁΩÆhostsÊñá‰ª∂MAC: /etc/hostsWindow: C:\windows\system32\drivers\etc\hosts1234ÂÖ¨ÁΩëÂú∞ÂùÄ: 106.15.234.222 hadoop001 106.15.235.200 hadoop002 106.15.234.239 hadoop0033.ËÆæÁΩÆÊâÄÊúâËäÇÁÇπÁöÑhostsÊñá‰ª∂1234ÁßÅÊúâÂú∞ÈìÅ„ÄÅÂÜÖÁΩëÂú∞ÂùÄ:echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts4.ÂÖ≥Èó≠ÊâÄÊúâËäÇÁÇπÁöÑÈò≤ÁÅ´Â¢ôÂèäÊ∏ÖÁ©∫ËßÑÂàô123systemctl stop firewalld systemctl disable firewalldiptables -F5.ÂÖ≥Èó≠ÊâÄÊúâËäÇÁÇπÁöÑselinux123vi /etc/selinux/configÂ∞ÜSELINUX=enforcingÊîπ‰∏∫SELINUX=disabled ËÆæÁΩÆÂêéÈúÄË¶ÅÈáçÂêØÊâçËÉΩÁîüÊïà6.ËÆæÁΩÆÊâÄÊúâËäÇÁÇπÁöÑÊó∂Âå∫‰∏ÄËá¥ÂèäÊó∂ÈíüÂêåÊ≠•1234567891011121314151617181920212223242526272829303132333435363738394041424344454647486.1.Êó∂Âå∫[root@hadoop001 ~]# dateSat May 11 10:07:53 CST 2019[root@hadoop001 ~]# timedatectl Local time: Sat 2019-05-11 10:10:31 CST Universal time: Sat 2019-05-11 02:10:31 UTC RTC time: Sat 2019-05-11 10:10:29 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: yes DST active: n/a#Êü•ÁúãÂëΩ‰ª§Â∏ÆÂä©ÔºåÂ≠¶‰π†Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÊó†ÈúÄÁôæÂ∫¶ÔºåÂ§™üëé[root@hadoop001 ~]# timedatectl --helptimedatectl [OPTIONS...] COMMAND ...Query or change system time and date settings. -h --help Show this help message --version Show package version --no-pager Do not pipe output into a pager --no-ask-password Do not prompt for password -H --host=[USER@]HOST Operate on remote host -M --machine=CONTAINER Operate on local container --adjust-system-clock Adjust system clock when changing local RTC modeCommands: status Show current time settings set-time TIME Set system time set-timezone ZONE Set system time zone list-timezones Show known time zones set-local-rtc BOOL Control whether RTC is in local time set-ntp BOOL Control whether NTP is enabled#Êü•ÁúãÂì™‰∫õÊó∂Âå∫[root@hadoop001 ~]# timedatectl list-timezonesAfrica/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/AlgiersAfrica/AsmaraAfrica/Bamako#ÊâÄÊúâËäÇÁÇπËÆæÁΩÆ‰∫öÊ¥≤‰∏äÊµ∑Êó∂Âå∫ [root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455566.2.Êó∂Èó¥#ÊâÄÊúâËäÇÁÇπÂÆâË£Öntp[root@hadoop001 ~]# yum install -y ntp#ÈÄâÂèñhadoop001‰∏∫ntpÁöÑ‰∏ªËäÇÁÇπ[root@hadoop001 ~]# vi /etc/ntp.conf #timeserver 0.asia.pool.ntp.orgserver 1.asia.pool.ntp.orgserver 2.asia.pool.ntp.orgserver 3.asia.pool.ntp.org#ÂΩìÂ§ñÈÉ®Êó∂Èó¥‰∏çÂèØÁî®Êó∂ÔºåÂèØ‰ΩøÁî®Êú¨Âú∞Á°¨‰ª∂Êó∂Èó¥server 127.127.1.0 iburst local clock #ÂÖÅËÆ∏Âì™‰∫õÁΩëÊÆµÁöÑÊú∫Âô®Êù•ÂêåÊ≠•Êó∂Èó¥restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap#ÂºÄÂêØntpdÂèäÊü•ÁúãÁä∂ÊÄÅ[root@hadoop001 ~]# systemctl start ntpd[root@hadoop001 ~]# systemctl status ntpd ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago Main PID: 18518 (ntpd) CGroup: /system.slice/ntpd.service ‚îî‚îÄ18518 /usr/sbin/ntpd -u ntp:ntp -gMay 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usecMay 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabledMay 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.#È™åËØÅ[root@hadoop001 ~]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== LOCAL(0) .LOCL. 10 l 726 64 0 0.000 0.000 0.000#ÂÖ∂‰ªñ‰ªéËäÇÁÇπÂÅúÊ≠¢Á¶ÅÁî®ntpdÊúçÂä° [root@hadoop002 ~]# systemctl stop ntpd[root@hadoop002 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#ÊØèÂ§©ÂáåÊô®ÂêåÊ≠•hadoop001ËäÇÁÇπÊó∂Èó¥[root@hadoop002 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop001 [root@hadoop003 ~]# systemctl stop ntpd[root@hadoop004 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#ÊØèÂ§©ÂáåÊô®ÂêåÊ≠•hadoop001ËäÇÁÇπÊó∂Èó¥[root@hadoop003 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop0017.ÈÉ®ÁΩ≤ÈõÜÁæ§ÁöÑJDK123456789mkdir /usr/javatar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/#ÂàáËÆ∞ÂøÖÈ°ª‰øÆÊ≠£ÊâÄÂ±ûÁî®Êà∑ÂèäÁî®Êà∑ÁªÑchown -R root:root /usr/java/jdk1.8.0_45echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profileecho &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profilesource /etc/profilewhich java8.hadoop001ËäÇÁÇπÁ¶ªÁ∫øÈÉ®ÁΩ≤MySQL5.7(ÂÅáÂ¶ÇËßâÂæóÂõ∞ÈöæÂìüÔºåÂ∞±Ëá™Ë°åÁôæÂ∫¶RPMÈÉ®ÁΩ≤ÔºåÂõ†‰∏∫ËØ•ÈÉ®ÁΩ≤ÊñáÊ°£ÊòØÊàëÂè∏Áîü‰∫ßÊñáÊ°£)ÊñáÊ°£ÈìæÊé•:https://github.com/Hackeruncle/MySQLËßÜÈ¢ëÈìæÊé•:https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ ÂØÜÁ†Å:whic9.ÂàõÂª∫CDHÁöÑÂÖÉÊï∞ÊçÆÂ∫ìÂíåÁî®Êà∑„ÄÅamonÊúçÂä°ÁöÑÊï∞ÊçÆÂ∫ìÂèäÁî®Êà∑12345create database cmf DEFAULT CHARACTER SET utf8;create database amon DEFAULT CHARACTER SET utf8;grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;flush privileges;10.hadoop001ËäÇÁÇπÈÉ®ÁΩ≤mysql jdbc jar12mkdir -p /usr/share/java/cp mysql-connector-java.jar /usr/share/java/‰∏â.CDHÈÉ®ÁΩ≤1.Á¶ªÁ∫øÈÉ®ÁΩ≤cm serverÂèäagent1234567891011121314151617181920211.1.ÊâÄÊúâËäÇÁÇπÂàõÂª∫ÁõÆÂΩïÂèäËß£Âéãmkdir /opt/cloudera-managertar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/1.2.ÊâÄÊúâËäÇÁÇπ‰øÆÊîπagentÁöÑÈÖçÁΩÆÔºåÊåáÂêëserverÁöÑËäÇÁÇπhadoop001sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini1.3.‰∏ªËäÇÁÇπ‰øÆÊîπserverÁöÑÈÖçÁΩÆ:vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=hadoop001com.cloudera.cmf.db.name=cmfcom.cloudera.cmf.db.user=cmfcom.cloudera.cmf.db.password=Ruozedata123456!com.cloudera.cmf.db.setupType=EXTERNAL1.4.ÊâÄÊúâËäÇÁÇπÂàõÂª∫Áî®Êà∑useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm1.5.ÁõÆÂΩï‰øÆÊîπÁî®Êà∑ÂèäÁî®Êà∑ÁªÑchown -R cloudera-scm:cloudera-scm /opt/cloudera-manager2.hadoop001ËäÇÁÇπÈÉ®ÁΩ≤Á¶ªÁ∫øparcelÊ∫ê1234567891011121314151617182.1.ÈÉ®ÁΩ≤Á¶ªÁ∫øparcelÊ∫ê$ mkdir -p /opt/cloudera/parcel-repo$ lltotal 3081664-rw-r--r-- 1 root root 2127506677 May 9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel-rw-r--r-- 1 root root 41 May 9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1-rw-r--r-- 1 root root 841524318 May 9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz-rw-r--r-- 1 root root 185515842 Aug 10 2017 jdk-8u144-linux-x64.tar.gz-rw-r--r-- 1 root root 66538 May 9 18:03 manifest.json-rw-r--r-- 1 root root 989495 May 25 2017 mysql-connector-java.jar$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/#ÂàáËÆ∞cpÊó∂ÔºåÈáçÂëΩÂêçÂéªÊéâ1Ôºå‰∏çÁÑ∂Âú®ÈÉ®ÁΩ≤ËøáÁ®ãCMËÆ§‰∏∫Â¶Ç‰∏äÊñá‰ª∂‰∏ãËΩΩÊú™ÂÆåÊï¥Ôºå‰ºöÊåÅÁª≠‰∏ãËΩΩ$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha$ cp manifest.json /opt/cloudera/parcel-repo/2.2.ÁõÆÂΩï‰øÆÊîπÁî®Êà∑ÂèäÁî®Êà∑ÁªÑ$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/3.ÊâÄÊúâËäÇÁÇπÂàõÂª∫ËΩØ‰ª∂ÂÆâË£ÖÁõÆÂΩï„ÄÅÁî®Êà∑ÂèäÁî®Êà∑ÁªÑÊùÉÈôêmkdir -p /opt/cloudera/parcelschown -R cloudera-scm:cloudera-scm /opt/cloudera/4.hadoop001ËäÇÁÇπÂêØÂä®Server1234564.1.ÂêØÂä®server/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start4.2.ÈòøÈáå‰∫ëwebÁïåÈù¢ÔºåËÆæÁΩÆËØ•hadoop001ËäÇÁÇπÈò≤ÁÅ´Â¢ôÊîæÂºÄ7180Á´ØÂè£4.3.Á≠âÂæÖ1minÔºåÊâìÂºÄ http://hadoop001:7180 Ë¥¶Âè∑ÂØÜÁ†Å:admin/admin4.4.ÂÅáÂ¶ÇÊâì‰∏çÂºÄÔºåÂéªÁúãserverÁöÑlogÔºåÊ†πÊçÆÈîôËØØ‰ªîÁªÜÊéíÊü•ÈîôËØØ5.ÊâÄÊúâËäÇÁÇπÂêØÂä®Agent1/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start6.Êé•‰∏ãÊù•ÔºåÂÖ®ÈÉ®WebÁïåÈù¢Êìç‰Ωúhttp://hadoop001:7180/Ë¥¶Âè∑ÂØÜÁ†Å:admin/admin7.Ê¨¢Ëøé‰ΩøÁî®Cloudera Manager‚ÄìÊúÄÁªàÁî®Êà∑ËÆ∏ÂèØÊù°Ê¨æ‰∏éÊù°‰ª∂„ÄÇÂãæÈÄâ8.Ê¨¢Ëøé‰ΩøÁî®Cloudera Manager‚ÄìÊÇ®ÊÉ≥Ë¶ÅÈÉ®ÁΩ≤Âì™‰∏™ÁâàÊú¨ÔºüÈÄâÊã©Cloudera ExpressÂÖçË¥πÁâàÊú¨9.ÊÑüË∞¢ÊÇ®ÈÄâÊã©Cloudera ManagerÂíåCDH10.‰∏∫CDHÈõÜÁæ§ÂÆâË£ÖÊåáÂØº‰∏ªÊú∫„ÄÇÈÄâÊã©[ÂΩìÂâçÁÆ°ÁêÜÁöÑ‰∏ªÊú∫]ÔºåÂÖ®ÈÉ®ÂãæÈÄâ11.ÈÄâÊã©Â≠òÂÇ®Â∫ì12.ÈõÜÁæ§ÂÆâË£Ö‚ÄìÊ≠£Âú®ÂÆâË£ÖÈÄâÂÆöParcelÂÅáÂ¶ÇÊú¨Âú∞parcelÁ¶ªÁ∫øÊ∫êÈÖçÁΩÆÊ≠£Á°ÆÔºåÂàô‚Äù‰∏ãËΩΩ‚ÄùÈò∂ÊÆµÁû¨Èó¥ÂÆåÊàêÔºåÂÖ∂‰ΩôÈò∂ÊÆµËßÜËäÇÁÇπÊï∞‰∏éÂÜÖÈÉ®ÁΩëÁªúÊÉÖÂÜµÂÜ≥ÂÆö„ÄÇ13.Ê£ÄÊü•‰∏ªÊú∫Ê≠£Á°ÆÊÄß123456789101112131415161718192021222324252627282913.1.Âª∫ËÆÆÂ∞Ü/proc/sys/vm/swappinessËÆæÁΩÆ‰∏∫ÊúÄÂ§ßÂÄº10„ÄÇswappinessÂÄºÊéßÂà∂Êìç‰ΩúÁ≥ªÁªüÂ∞ùËØï‰∫§Êç¢ÂÜÖÂ≠òÁöÑÁßØÊûÅÔºõswappiness=0ÔºöË°®Á§∫ÊúÄÂ§ßÈôêÂ∫¶‰ΩøÁî®Áâ©ÁêÜÂÜÖÂ≠òÔºå‰πãÂêéÊâçÊòØswapÁ©∫Èó¥Ôºõswappiness=100ÔºöË°®Á§∫ÁßØÊûÅ‰ΩøÁî®swapÂàÜÂå∫ÔºåÂπ∂‰∏îÊääÂÜÖÂ≠ò‰∏äÁöÑÊï∞ÊçÆÂèäÊó∂Êê¨ËøÅÂà∞swapÁ©∫Èó¥ÔºõÂ¶ÇÊûúÊòØÊ∑∑ÂêàÊúçÂä°Âô®Ôºå‰∏çÂª∫ËÆÆÂÆåÂÖ®Á¶ÅÁî®swapÔºåÂèØ‰ª•Â∞ùËØïÈôç‰Ωéswappiness„ÄÇ‰∏¥Êó∂Ë∞ÉÊï¥Ôºösysctl vm.swappiness=10Ê∞∏‰πÖË∞ÉÊï¥Ôºöcat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf# Adjust swappiness valuevm.swappiness=10EOF13.2.Â∑≤ÂêØÁî®ÈÄèÊòéÂ§ßÈ°µÈù¢ÂéãÁº©ÔºåÂèØËÉΩ‰ºöÂØºËá¥ÈáçÂ§ßÊÄßËÉΩÈóÆÈ¢òÔºåÂª∫ËÆÆÁ¶ÅÁî®Ê≠§ËÆæÁΩÆ„ÄÇ‰∏¥Êó∂Ë∞ÉÊï¥Ôºöecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledÊ∞∏‰πÖË∞ÉÊï¥Ôºöcat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local# Disable transparent_hugepageecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledEOF# centos7.xÁ≥ªÁªüÔºåÈúÄË¶Å‰∏∫&quot;/etc/rc.d/rc.local&quot;Êñá‰ª∂Ëµã‰∫àÊâßË°åÊùÉÈôêchmod +x /etc/rc.d/rc.local14.Ëá™ÂÆö‰πâÊúçÂä°ÔºåÈÄâÊã©ÈÉ®ÁΩ≤Zookeeper„ÄÅHDFS„ÄÅYarnÊúçÂä°15.Ëá™ÂÆö‰πâËßíËâ≤ÂàÜÈÖç16.Êï∞ÊçÆÂ∫ìËÆæÁΩÆ17.ÂÆ°ÊîπËÆæÁΩÆÔºåÈªòËÆ§Âç≥ÂèØ18.È¶ñÊ¨°ËøêË°å19.ÊÅ≠ÂñúÊÇ®!20.‰∏ªÈ°µCDHÂÖ®Â•óËØæÁ®ãÁõÆÂΩïÔºåÂ¶ÇÊúâbuyÔºåÂä†ÂæÆ‰ø°(ruoze_star)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061620.Èùí‰∫ëÁéØÂ¢É‰ªãÁªçÂíå‰ΩøÁî® 1.Preparation Ë∞àË∞àÊÄéÊ†∑ÂÖ•Èó®Â§ßÊï∞ÊçÆ Ë∞àË∞àÊÄéÊ†∑ÂÅöÂ•Ω‰∏Ä‰∏™Â§ßÊï∞ÊçÆÂπ≥Âè∞ÁöÑËøêËê•Â∑•‰Ωú LinuxÊú∫Âô®,ÂêÑËΩØ‰ª∂ÁâàÊú¨‰ªãÁªçÂèäÂÆâË£Ö(ÂΩïÊí≠) 2.Introduction Cloudera„ÄÅCMÂèäCDH‰ªãÁªç CDHÁâàÊú¨ÈÄâÊã© CDHÂÆâË£ÖÂá†ÁßçÊñπÂºèËß£ËØª 3.Install&amp;UnInstall ÈõÜÁæ§ËäÇÁÇπËßÑÂàí,ÁéØÂ¢ÉÂáÜÂ§á(NTP,Jdk and etc) MySQLÁºñËØëÂÆâË£ÖÂèäÂ∏∏Áî®ÂëΩ‰ª§ Êé®Ëçê:CDHÁ¶ªÁ∫øÂÆâË£Ö(Ë∏©ÂùëÂøÉÂæó,ÂÖ®Èù¢ÂâñÊûê) Ëß£ËØªÊö¥ÂäõÂç∏ËΩΩËÑöÊú¨ 4.CDH Management CDH‰ΩìÁ≥ªÊû∂ÊûÑÂâñÊûê CDHÈÖçÁΩÆÊñá‰ª∂Ê∑±Â∫¶Ëß£Êûê CMÁöÑÂ∏∏Áî®ÂëΩ‰ª§ CDHÈõÜÁæ§Ê≠£Á°ÆÂêØÂä®ÂíåÂÅúÊ≠¢È°∫Â∫è CDH Tsquery Language CDHÂ∏∏ËßÑÁÆ°ÁêÜ(ÁõëÊéß/È¢ÑË≠¶/ÈÖçÁΩÆ/ËµÑÊ∫ê/Êó•Âøó/ÂÆâÂÖ®) 5.Maintenance Experiment HDFS HA ÈÖçÁΩÆ Âèähadoop/hdfsÂ∏∏ËßÑÂëΩ‰ª§ Yarn HA ÈÖçÁΩÆ ÂèäyarnÂ∏∏ËßÑÂëΩ‰ª§ Other CDH Components HA ÈÖçÁΩÆ CDHÂä®ÊÄÅÊ∑ªÂä†Âà†Èô§ÊúçÂä°(hive/spark/hbase) CDHÂä®ÊÄÅÊ∑ªÂä†Âà†Èô§Êú∫Âô® CDHÂä®ÊÄÅÊ∑ªÂä†Âà†Èô§ÂèäËøÅÁßªDataNodeËøõÁ®ãÁ≠â CDHÂçáÁ∫ß(5.10.0--&gt;5.12.0) 6.Resource Management Linux Cgroups ÈùôÊÄÅËµÑÊ∫êÊ±† Âä®ÊÄÅËµÑÊ∫êÊ±† Â§öÁßüÊà∑Ê°à‰æã 7.Performance Tunning Memory/CPU/Network/DiskÂèäÈõÜÁæ§ËßÑÂàí LinuxÂèÇÊï∞ HDFSÂèÇÊï∞ MapReduceÂèäYarnÂèÇÊï∞ ÂÖ∂‰ªñÊúçÂä°ÂèÇÊï∞ 8.Cases Share CDH4&amp;5‰πãAlternativesÂëΩ‰ª§ ÁöÑÁ†îÁ©∂ CDH5.8.2ÂÆâË£Ö‰πãHash verification failed ËÆ∞ÂΩï‰∏ÄÊ¨°CDH4.8.6 ÈÖçÁΩÆHDFS HA Âùë CDH5.0ÈõÜÁæ§IPÊõ¥Êîπ CDHÁöÑactive namenode exit(GC)ÂíåÂΩ©ËõãÂàÜ‰∫´ 9. Kerberos KerberosÁÆÄ‰ªã Kerberos‰ΩìÁ≥ªÁªìÊûÑ KerberosÂ∑•‰ΩúÊú∫Âà∂ KerberosÂÆâË£ÖÈÉ®ÁΩ≤ CDHÂêØÁî®kerberos KerberosÂºÄÂèë‰ΩøÁî®(ÁúüÂÆû‰ª£Á†Å)10.Summary ÊÄªÁªìJoin us if you have a dream.Ëã•Ê≥ΩÊï∞ÊçÆÂÆòÁΩë: http://ruozedata.comËÖæËÆØËØæÂ†ÇÔºåÊêúËã•Ê≥ΩÊï∞ÊçÆ: http://ruoze.ke.qq.comBilibiliÁΩëÁ´ô,ÊêúËã•Ê≥ΩÊï∞ÊçÆ: https://space.bilibili.com/356836323Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆ‚ÄìÂÆòÊñπÂçöÂÆ¢Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆ‚ÄìÂçöÂÆ¢‰∏ÄËßàËã•Ê≥ΩÂ§ßÊï∞ÊçÆ‚ÄìÂÜÖÈÉ®Â≠¶ÂëòÈù¢ËØïÈ¢òÊâ´‰∏ÄÊâ´ÔºåÂ≠¶‰∏ÄÂ≠¶:]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerÂ∏∏Áî®ÂëΩ‰ª§‰ª•ÂèäÂÆâË£Ömysql]]></title>
    <url>%2F2019%2F05%2F08%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.ÁÆÄ‰ªãDockerÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ∫îÁî®ÂÆπÂô®ÂºïÊìéÔºõÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÂÆπÂô®ÊäÄÊúØÔºõDockerÊîØÊåÅÂ∞ÜËΩØ‰ª∂ÁºñËØëÊàê‰∏Ä‰∏™ÈïúÂÉèÔºõÁÑ∂ÂêéÂú®ÈïúÂÉè‰∏≠ÂêÑÁßçËΩØ‰ª∂ÂÅöÂ•ΩÈÖçÁΩÆÔºåÂ∞ÜÈïúÂÉèÂèëÂ∏ÉÂá∫ÂéªÔºåÂÖ∂‰ªñ‰ΩøÁî®ËÄÖÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®Ëøô‰∏™ÈïúÂÉèÔºõËøêË°å‰∏≠ÁöÑËøô‰∏™ÈïúÂÉèÁß∞‰∏∫ÂÆπÂô®ÔºåÂÆπÂô®ÂêØÂä®ÊòØÈùûÂ∏∏Âø´ÈÄüÁöÑ„ÄÇ2.Ê†∏ÂøÉÊ¶ÇÂøµdocker‰∏ªÊú∫(Host)ÔºöÂÆâË£Ö‰∫ÜDockerÁ®ãÂ∫èÁöÑÊú∫Âô®ÔºàDockerÁõ¥Êé•ÂÆâË£ÖÂú®Êìç‰ΩúÁ≥ªÁªü‰πã‰∏äÔºâÔºõdockerÂÆ¢Êà∑Á´Ø(Client)ÔºöËøûÊé•docker‰∏ªÊú∫ËøõË°åÊìç‰ΩúÔºõdocker‰ªìÂ∫ì(Registry)ÔºöÁî®Êù•‰øùÂ≠òÂêÑÁßçÊâìÂåÖÂ•ΩÁöÑËΩØ‰ª∂ÈïúÂÉèÔºõdockerÈïúÂÉè(Images)ÔºöËΩØ‰ª∂ÊâìÂåÖÂ•ΩÁöÑÈïúÂÉèÔºõÊîæÂú®docker‰ªìÂ∫ì‰∏≠ÔºõdockerÂÆπÂô®(Container)ÔºöÈïúÂÉèÂêØÂä®ÂêéÁöÑÂÆû‰æãÁß∞‰∏∫‰∏Ä‰∏™ÂÆπÂô®ÔºõÂÆπÂô®ÊòØÁã¨Á´ãËøêË°åÁöÑ‰∏Ä‰∏™Êàñ‰∏ÄÁªÑÂ∫îÁî®3.ÂÆâË£ÖÁéØÂ¢É1234VM ware Workstation10CentOS-7-x86_64-DVD-1804.isouname -r3.10.0-862.el7.x86_64Ê£ÄÊü•ÂÜÖÊ†∏ÁâàÊú¨ÔºåÂøÖÈ°ªÊòØ3.10Âèä‰ª•‰∏ä Êü•ÁúãÂëΩ‰ª§Ôºöuname -r4.Âú®linuxËôöÊãüÊú∫‰∏äÂÆâË£ÖdockerÊ≠•È™§Ôºö1„ÄÅÊ£ÄÊü•ÂÜÖÊ†∏ÁâàÊú¨ÔºåÂøÖÈ°ªÊòØ3.10Âèä‰ª•‰∏äuname -r2„ÄÅÂÆâË£Ödockeryum install docker3„ÄÅËæìÂÖ•yÁ°ÆËÆ§ÂÆâË£ÖDependency Updated:audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1Complete!(ÊàêÂäüÊ†áÂøó)4„ÄÅÂêØÂä®docker123[root@hadoop000 ~]# systemctl start docker[root@hadoop000 ~]# docker -vDocker version 1.13.1, build 8633870/1.13.15„ÄÅÂºÄÊú∫ÂêØÂä®docker12[root@hadoop000 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6„ÄÅÂÅúÊ≠¢docker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@hadoop000 ~]# systemctl stop docker``` ### 5.Â∏∏Áî®ÂëΩ‰ª§ÈïúÂÉèÊìç‰Ωú|Êìç‰Ωú|ÂëΩ‰ª§|ËØ¥Êòé||---|---|---|Ê£ÄÁ¥¢ |docker search ÂÖ≥ÈîÆÂ≠ó egÔºödocker search redis| Êàë‰ª¨ÁªèÂ∏∏Âéªdocker hub‰∏äÊ£ÄÁ¥¢ÈïúÂÉèÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºåÂ¶ÇÈïúÂÉèÁöÑTAG„ÄÇ|ÊãâÂèñ |docker pull ÈïúÂÉèÂêç:tag| :tagÊòØÂèØÈÄâÁöÑÔºåtagË°®Á§∫Ê†áÁ≠æÔºåÂ§ö‰∏∫ËΩØ‰ª∂ÁöÑÁâàÊú¨ÔºåÈªòËÆ§ÊòØlatestÂàóË°®| docker images |Êü•ÁúãÊâÄÊúâÊú¨Âú∞ÈïúÂÉèÂà†Èô§|docker rmi image-id |Âà†Èô§ÊåáÂÆöÁöÑÊú¨Âú∞ÈïúÂÉèÂΩìÁÑ∂Â§ßÂÆ∂‰πüÂèØ‰ª•Âú®ÂÆòÁΩëÊü•ÊâæÔºöhttps://hub.docker.com/ÂÆπÂô®Êìç‰ΩúËΩØ‰ª∂ÈïúÂÉèÔºàQQÂÆâË£ÖÁ®ãÂ∫èÔºâ----ËøêË°åÈïúÂÉè----‰∫ßÁîü‰∏Ä‰∏™ÂÆπÂô®ÔºàÊ≠£Âú®ËøêË°åÁöÑËΩØ‰ª∂ÔºåËøêË°åÁöÑQQÔºâÔºõÊ≠•È™§Ôºö- 1„ÄÅÊêúÁ¥¢ÈïúÂÉè[root@localhost ~]# docker search tomcat- 2„ÄÅÊãâÂèñÈïúÂÉè[root@localhost ~]# docker pull tomcat- 3„ÄÅÊ†πÊçÆÈïúÂÉèÂêØÂä®ÂÆπÂô®docker run --name mytomcat -d tomcat:latest- 4„ÄÅdocker ps Êü•ÁúãËøêË°å‰∏≠ÁöÑÂÆπÂô®- 5„ÄÅ ÂÅúÊ≠¢ËøêË°å‰∏≠ÁöÑÂÆπÂô®docker stop ÂÆπÂô®ÁöÑid- 6„ÄÅÊü•ÁúãÊâÄÊúâÁöÑÂÆπÂô®docker ps -a- 7„ÄÅÂêØÂä®ÂÆπÂô®docker start ÂÆπÂô®id- 8„ÄÅÂà†Èô§‰∏Ä‰∏™ÂÆπÂô® docker rm ÂÆπÂô®id- 9„ÄÅÂêØÂä®‰∏Ä‰∏™ÂÅö‰∫ÜÁ´ØÂè£Êò†Â∞ÑÁöÑtomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-dÔºöÂêéÂè∞ËøêË°å-p: Â∞Ü‰∏ªÊú∫ÁöÑÁ´ØÂè£Êò†Â∞ÑÂà∞ÂÆπÂô®ÁöÑ‰∏Ä‰∏™Á´ØÂè£ ‰∏ªÊú∫Á´ØÂè£:ÂÆπÂô®ÂÜÖÈÉ®ÁöÑÁ´ØÂè£- 10„ÄÅ‰∏∫‰∫ÜÊºîÁ§∫ÁÆÄÂçïÂÖ≥Èó≠‰∫ÜlinuxÁöÑÈò≤ÁÅ´Â¢ôservice firewalld status ÔºõÊü•ÁúãÈò≤ÁÅ´Â¢ôÁä∂ÊÄÅservice firewalld stopÔºöÂÖ≥Èó≠Èò≤ÁÅ´Â¢ôsystemctl disable firewalld.service #Á¶ÅÊ≠¢firewallÂºÄÊú∫ÂêØÂä®- 11„ÄÅÊü•ÁúãÂÆπÂô®ÁöÑÊó•Âøódocker logs container-name/container-idÊõ¥Â§öÂëΩ‰ª§ÂèÇÁúãhttps://docs.docker.com/engine/reference/commandline/docker/ÂèØ‰ª•ÂèÇËÄÉÈïúÂÉèÊñáÊ°£### 6.‰ΩøÁî®dockerÂÆâË£Ömysql- docker pull mysqldocker pull mysqlUsing default tag: latestTrying to pull repository docker.io/library/mysql ‚Ä¶latest: Pulling from docker.io/library/mysqla5a6f2f73cd8: Pull complete936836019e67: Pull complete283fa4c95fb4: Pull complete1f212fb371f9: Pull completee2ae0d063e89: Pull complete5ed0ae805b65: Pull complete0283dc49ef4e: Pull completea7e1170b4fdb: Pull complete88918a9e4742: Pull complete241282fa67c2: Pull completeb0fecf619210: Pull completebebf9f901dcc: Pull completeDigest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8Status: Downloaded newer image for docker.io/mysql:latest[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB1- ÂêØÂä®[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB[root@hadoop000 ~]# docker run ‚Äìname mysql01 -d mysql756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a[root@hadoop000 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES756620c8e583 mysql ‚Äúdocker-entrypoint‚Ä¶‚Äù 9 seconds ago Exited (1) 7 seconds ago mysql01[root@hadoop000 ~]# docker logs 756620c8e583error: database is uninitialized and password option is not specifiedYou need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD1ÂèØ‰ª•ÁúãÂà∞‰∏äÈù¢ÂêØÂä®ÁöÑÊñπÂºèÊòØÈîôËØØÁöÑÔºåÊèêÁ§∫Êàë‰ª¨Ë¶ÅÂ∏¶‰∏äÂÖ∑‰ΩìÁöÑÂØÜÁ†Å[root@hadoop000 ~]# docker run -p 3306:3306 ‚Äìname mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqleae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc0612345```‚ÄìnameÔºöÁªôÊñ∞ÂàõÂª∫ÁöÑÂÆπÂô®ÂëΩÂêçÔºåÊ≠§Â§ÑÂëΩÂêç‰∏∫pwc-mysql-eÔºöÈÖçÁΩÆ‰ø°ÊÅØÔºåÊ≠§Â§ÑÈÖçÁΩÆmysqlÁöÑrootÁî®Êà∑ÁöÑÁôªÈôÜÂØÜÁ†Å-pÔºöÁ´ØÂè£Êò†Â∞ÑÔºåÊ≠§Â§ÑÊò†Â∞Ñ‰∏ªÊú∫3306Á´ØÂè£Âà∞ÂÆπÂô®pwc-mysqlÁöÑ3306Á´ØÂè£-dÔºöÊàêÂäüÂêØÂä®ÂÆπÂô®ÂêéËæìÂá∫ÂÆπÂô®ÁöÑÂÆåÊï¥IDÔºå‰æãÂ¶Ç‰∏äÂõæ 73f8811f669ee...Êü•ÁúãÊòØÂê¶ÂêØÂä®ÊàêÂäü123[root@hadoop000 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeae86796e132 mysql &quot;docker-entrypoint...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql02ÁôªÈôÜMySQL12345678910111213141516docker exec -it mysql04 /bin/bashroot@e34aba02c0c3:/# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 80Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;ÂÖ∂‰ªñÁöÑÈ´òÁ∫ßÊìç‰Ωú123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tagÊää‰∏ªÊú∫ÁöÑ/conf/mysqlÊñá‰ª∂Â§πÊåÇËΩΩÂà∞ mysqldockerÂÆπÂô®ÁöÑ/etc/mysql/conf.dÊñá‰ª∂Â§πÈáåÈù¢ÊîπmysqlÁöÑÈÖçÁΩÆÊñá‰ª∂Â∞±Âè™ÈúÄË¶ÅÊäämysqlÈÖçÁΩÆÊñá‰ª∂ÊîæÂú®Ëá™ÂÆö‰πâÁöÑÊñá‰ª∂Â§π‰∏ãÔºà/conf/mysqlÔºâdocker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ciÊåáÂÆömysqlÁöÑ‰∏Ä‰∫õÈÖçÁΩÆÂèÇÊï∞]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ëã•Ê≥ΩÊï∞ÊçÆËØæÁ®ã‰∏ÄËßà]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÊï∞ÊçÆËØæÁ®ãÁ≥ªÂàóÂü∫Á°ÄÁè≠LiunxVMËôöÊãüÊú∫ÂÆâË£ÖLiunxÂ∏∏Áî®ÂëΩ‰ª§ÔºàÈáçÁÇπÔºâÂºÄÂèëÁéØÂ¢ÉÊê≠MySQLÊ∫êÁ†ÅÂÆâË£Ö&amp;yumÂÆâË£ÖCRUDÁºñÂÜôÊùÉÈôêÊéßÂà∂HadoopÊû∂ÊûÑ‰ªãÁªç&amp;&amp;Ê∫êÁ†ÅÁºñËØë‰º™ÂàÜÂ∏ÉÂºèÂÆâË£Ö&amp;&amp;‰ºÅ‰∏öÂ∫îÁî®HDFSÔºàÈáçÁÇπÔºâÊû∂ÊûÑËÆæËÆ°ÂâØÊú¨ÊîæÁΩÆÁ≠ñÁï•ËØªÂÜôÊµÅÁ®ãYARNÔºàÈáçÁÇπÔºâÊû∂ÊûÑËÆæËÆ°Â∑•‰ΩúÊµÅÁ®ãË∞ÉÂ∫¶ÁÆ°ÁêÜ&amp;&amp;Â∏∏ËßÅÂèÇÊï∞ÈÖçÁΩÆÔºàË∞É‰ºòÔºâMapReduceÊû∂ÊûÑËÆæËÆ°wordcountÂéüÁêÜ&amp;&amp;joinÂéüÁêÜÂíåÊ°à‰æãHiveÊû∂ÊûÑËÆæËÆ°Hive DDL&amp;DMLjoinÂú®Â§ßÊï∞ÊçÆ‰∏≠ÁöÑ‰ΩøÁî®‰ΩøÁî®Ëá™Â∏¶UDFÂíåÂºÄÂèëËá™ÂÆö‰πâUDFSqoopÊû∂ÊûÑËÆæËÆ°RDBMSÂØºÂÖ•ÂØºÂá∫Êï¥ÂêàÈ°πÁõÆÂ∞ÜÊâÄÊúâÁªÑ‰ª∂Âêà‰Ωú‰ΩøÁî®„ÄÇ‰∫∫Â∑•Êô∫ËÉΩÂü∫Á°ÄpythonÂü∫Á°ÄÂ∏∏Áî®Â∫ì‚Äî‚Äîpandas„ÄÅnumpy„ÄÅsklearn„ÄÅkerasÈ´òÁ∫ßÁè≠scalaÁºñÁ®ãÔºàÈáçÁÇπÔºâSparkÔºà‰∫îÊòüÈáçÁÇπÔºâHadoopÈ´òÁ∫ßHiveÈ´òÁ∫ßFlumeKafkaHBaseFlinkCDHÂÆπÂô®Ë∞ÉÂ∫¶Âπ≥Âè∞Á∫ø‰∏ãÁè≠]]></content>
      <categories>
        <category>ËØæÁ®ã</category>
      </categories>
      <tags>
        <tag>ËØæÁ®ã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2ËØ¶ÁªÜ‰ªãÁªç]]></title>
    <url>%2F2019%2F04%2F23%2Fspark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[SparkÂèëÂ∏É‰∫ÜÊúÄÊñ∞ÁöÑÁâàÊú¨spark-2.4.2Ê†πÊçÆÂÆòÁΩë‰ªãÁªçÔºåÊ≠§ÁâàÊú¨ÂØπ‰∫é‰ΩøÁî®spark2.4ÁöÑÁî®Êà∑Êù•ËØ¥Â∏ÆÂä©ÊòØÂ∑®Â§ßÁöÑÁâàÊú¨‰ªãÁªçSpark2.4.2ÊòØ‰∏Ä‰∏™ÂåÖÂê´Á®≥ÂÆöÊÄß‰øÆÂ§çÁöÑÁª¥Êä§ÁâàÊú¨„ÄÇ Ê≠§ÁâàÊú¨Âü∫‰∫éSpark2.4Áª¥Êä§ÂàÜÊîØ„ÄÇ Êàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÊâÄÊúâ2.4Áî®Êà∑ÂçáÁ∫ßÂà∞Ê≠§Á®≥ÂÆöÁâàÊú¨„ÄÇÊòæËëóÁöÑÂèòÂåñSPARK-27419ÔºöÂú®spark2.4‰∏≠Â∞Üspark.executor.heartbeatIntervalËÆæÁΩÆ‰∏∫Â∞è‰∫é1ÁßíÁöÑÂÄºÊó∂ÔºåÂÆÉÂ∞ÜÂßãÁªàÂ§±Ë¥•„ÄÇ Âõ†‰∏∫ËØ•ÂÄºÂ∞ÜËΩ¨Êç¢‰∏∫0ÔºåÂøÉË∑≥Â∞ÜÂßãÁªàË∂ÖÊó∂ÔºåÂπ∂ÊúÄÁªàÁªàÊ≠¢ÊâßË°åÁ®ãÂ∫è„ÄÇËøòÂéüSPARK-25250ÔºöÂèØËÉΩÂØºËá¥‰Ωú‰∏öÊ∞∏‰πÖÊåÇËµ∑ÔºåÂú®2.4.2‰∏≠ËøòÂéü„ÄÇËØ¶ÁªÜÊõ¥ÊîπBUGissuesÂÜÖÂÆπÊëòË¶Å[ SPARK-26961 ]Âú®Spark Driver‰∏≠ÂèëÁé∞JavaÊ≠ªÈîÅ[ SPARK-26998 ]Âú®StandaloneÊ®°Âºè‰∏ãÊâßË°å‚Äôps -ef‚ÄôÁ®ãÂ∫èËøõÁ®ã,ËæìÂá∫spark.ssl.keyStorePasswordÁöÑÊòéÊñá[ SPARK-27216 ]Â∞ÜRoaringBitmapÂçáÁ∫ßÂà∞0.7.45‰ª•‰øÆÂ§çKryo‰∏çÂÆâÂÖ®ÁöÑser / dserÈóÆÈ¢ò[ SPARK-27244 ]‰ΩøÁî®ÈÄâÈ°πlogConf = trueÊó∂ÂØÜÁ†ÅÂ∞Ü‰ª•confÁöÑÊòéÊñáÂΩ¢ÂºèËÆ∞ÂΩï[ SPARK-27267 ]Áî®Snappy 1.1.7.1Ëß£Âéã„ÄÅÂéãÁº©Á©∫Â∫èÂàóÂåñÊï∞ÊçÆÊó∂Â§±Ë¥•[ SPARK-27275 ]EncryptedMessage.transferTo‰∏≠ÁöÑÊΩúÂú®ÊçüÂùè[ SPARK-27301 ]DStreamCheckpointDataÂõ†Êñá‰ª∂Á≥ªÁªüÂ∑≤ÁºìÂ≠òËÄåÊó†Ê≥ïÊ∏ÖÁêÜ[ SPARK-27338 ]TaskMemoryManagerÂíåUnsafeExternalSorter $ SpillableIterator‰πãÈó¥ÁöÑÊ≠ªÈîÅ[ SPARK-27351 ]Âú®‰ªÖ‰ΩøÁî®Á©∫ÂÄºÂàóÁöÑAggregateEstimation‰πãÂêéÁöÑÈîôËØØoutputRows‰º∞ËÆ°[ SPARK-27390 ]‰øÆÂ§çÂåÖÂêçÁß∞‰∏çÂåπÈÖç[ SPARK-27394 ]ÂΩìÊ≤°Êúâ‰ªªÂä°ÂºÄÂßãÊàñÁªìÊùüÊó∂ÔºåUI ÁöÑÈôàÊóßÊÄßÂèØËÉΩÊåÅÁª≠Êï∞ÂàÜÈíüÊàñÊï∞Â∞èÊó∂[ SPARK-27403 ]‰øÆÂ§çupdateTableStats‰ª•‰ΩøÁî®Êñ∞ÁªüËÆ°‰ø°ÊÅØÊàñÊó†Êõ¥Êñ∞Ë°®ÁªüËÆ°‰ø°ÊÅØ[ SPARK-27406 ]ÂΩì‰∏§Âè∞Êú∫Âô®ÂÖ∑Êúâ‰∏çÂêåÁöÑOopsÂ§ßÂ∞èÊó∂ÔºåUnsafeArrayDataÂ∫èÂàóÂåñ‰ºö‰∏≠Êñ≠[ SPARK-27419 ]Â∞Üspark.executor.heartbeatIntervalËÆæÁΩÆ‰∏∫Â∞è‰∫é1ÁßíÁöÑÂÄºÊó∂ÔºåÂÆÉÂ∞ÜÂßãÁªàÂ§±Ë¥•[ SPARK-27453 ]DSV1ÈùôÈªòÂà†Èô§DataFrameWriter.partitionByÊîπËøõissuesÂÜÖÂÆπÊëòË¶Å[ SPARK-27346 ]ÊùæÂºÄÂú®ExpressionInfoÁöÑ‚Äôexamples‚ÄôÂ≠óÊÆµ‰∏≠Êç¢Ë°åÊñ≠Ë®ÄÊù°‰ª∂[ SPARK-27358 ]Â∞ÜjqueryÊõ¥Êñ∞‰∏∫1.12.x‰ª•Ëé∑ÂèñÂÆâÂÖ®‰øÆÂ§çÁ®ãÂ∫è[ SPARK-27479 ]ÈöêËóè‚Äúorg.apache.spark.util.kvstore‚ÄùÁöÑAPIÊñáÊ°£Â∑•‰ΩúissuesÂÜÖÂÆπÊëòË¶Å[ SPARK-27382 ]Âú®HiveExternalCatalogVersionsSuite‰∏≠Êõ¥Êñ∞Spark 2.4.xÊµãËØï]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊàëÂè∏Kafka+Flink+MySQLÁîü‰∫ßÂÆåÊï¥Ê°à‰æã‰ª£Á†Å]]></title>
    <url>%2F2018%2F12%2F20%2F%E6%88%91%E5%8F%B8Kafka%2BFlink%2BMySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1.ÁâàÊú¨‰ø°ÊÅØÔºöFlink Version:1.6.2Kafka Version:0.9.0.0MySQL Version:5.6.212.Kafka Ê∂àÊÅØÊ†∑‰æãÂèäÊ†ºÂºèÔºö[IP TIME URL STATU_CODE REFERER]11.74.103.143 2018-12-20 18:12:00 &quot;GET /class/130.html HTTP/1.1&quot; 404 https://search.yahoo.com/search?p=FlinkÂÆûÊàò3.Â∑•Á®ãpom.xml12345678910111213141516171819202122232425262728&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;flink.version&gt;1.6.2&lt;/flink.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Flink-Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt;4.sConfÁ±ª ÂÆö‰πâ‰∏éMySQLËøûÊé•ÁöÑJDBCÁöÑÂèÇÊï∞1234567891011package com.soul.conf;/** * @author Ëã•Ê≥ΩÊï∞ÊçÆsoulChun * @create 2018-12-20-15:11 */public class sConf &#123; public static final String USERNAME = &quot;root&quot;; public static final String PASSWORD = &quot;www.ruozedata.com&quot;; public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;&#125;5.MySQLSlinkÁ±ª123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.soul.kafka;import com.soul.conf.sConf;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * @author Ëã•Ê≥ΩÊï∞ÊçÆsoulChun * @create 2018-12-20-15:09 */public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123; try &#123; if (connection == null) &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setString(3, value.f2); preparedStatement.setString(4, value.f3); preparedStatement.setString(5, value.f4); System.out.println(&quot;Start insert&quot;); preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void open(Configuration parms) throws Exception &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; public void close() throws Exception &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;6.Êï∞ÊçÆÊ∏ÖÊ¥óÊó•ÊúüÂ∑•ÂÖ∑Á±ª1234567891011121314151617181920212223package com.soul.utils;import org.apache.commons.lang3.time.FastDateFormat;import java.util.Date;/** * @author soulChun * @create 2018-12-19-18:44 */public class DateUtils &#123; private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;); private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;); public static Long getTime(String time) throws Exception&#123; return SOURCE_FORMAT.parse(time).getTime(); &#125; public static String parseMinute(String time) throws Exception&#123; return TARGET_FORMAT.format(new Date(getTime(time))); &#125; //ÊµãËØï‰∏Ä‰∏ã public static void main(String[] args) throws Exception&#123; String time = &quot;2018-12-19 18:55:00&quot;; System.out.println(parseMinute(time)); &#125;&#125;7.MySQLÂª∫Ë°®123456789create table log_info(ID INT NOT NULL AUTO_INCREMENT,IP VARCHAR(50),TIME VARCHAR(50),CourseID VARCHAR(10),Status_Code VARCHAR(10),Referer VARCHAR(100),PRIMARY KEY ( ID ))ENGINE=InnoDB DEFAULT CHARSET=utf8;8.‰∏ªÁ®ãÂ∫èÔºö‰∏ªË¶ÅÊòØÂ∞ÜtimeÁöÑÊ†ºÂºèËΩ¨ÊàêyyyyMMddHHmmss,ËøòÊúâÂèñURL‰∏≠ÁöÑËØæÁ®ãIDÔºåÂ∞Ü‰∏çÊòØ/classÂºÄÂ§¥ÁöÑËøáÊª§Êéâ„ÄÇ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.soul.kafka;import com.soul.utils.DateUtils;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;import java.util.Properties;/** * @author soulChun * @create 2018-12-19-17:23 */public class FlinkCleanKafka &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafkaÁöÑËäÇÁÇπÁöÑIPÊàñËÄÖhostNameÔºåÂ§ö‰∏™‰ΩøÁî®ÈÄóÂè∑ÂàÜÈöî properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeperÁöÑËäÇÁÇπÁöÑIPÊàñËÄÖhostNameÔºåÂ§ö‰∏™‰ΩøÁî®ÈÄóÂè∑ËøõË°åÂàÜÈöî properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flinkÁöÑÊ∂àË¥πËÄÖÁöÑgroup.id FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties); DataStream&lt;String&gt; stream = env.addSource(myConsumer);// stream.print().setParallelism(2); DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123; String[] data = value.split(&quot;\\\t&quot;); String CourseID = null; String url = data[2].split(&quot;\\ &quot;)[2]; if (url.startsWith(&quot;/class&quot;)) &#123; String CourseHTML = url.split(&quot;\\/&quot;)[2]; CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));// System.out.println(CourseID); &#125; return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]); &#125; &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123; return value.f2 != null; &#125; &#125;); CleanData.addSink(new MySQLSink()); env.execute(&quot;Flink kafka&quot;); &#125;&#125;9.ÂêØÂä®‰∏ªÁ®ãÂ∫èÔºåÊü•ÁúãMySQLË°®Êï∞ÊçÆÂú®ÈÄíÂ¢û123456mysql&gt; select count(*) from log_info;+----------+| count(*) |+----------+| 15137 |+----------+KafkaËøáÊù•ÁöÑÊ∂àÊÅØÊòØÊàëÊ®°ÊãüÁöÑÔºå‰∏ÄÂàÜÈíü‰∫ßÁîü100Êù°„ÄÇ‰ª•‰∏äÊòØÊàëÂè∏Áîü‰∫ßÈ°πÁõÆ‰ª£Á†ÅÁöÑÊäΩÂèñÂá∫Êù•ÁöÑÊ°à‰æã‰ª£Á†ÅV1„ÄÇÁ®çÂêéËøòÊúâWaterMark‰πãÁ±ª‰ºöÂÅöÂàÜ‰∫´„ÄÇ]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkÂú®Êê∫Á®ãÁöÑÂÆûË∑µÔºà‰∫åÔºâ]]></title>
    <url>%2F2018%2F12%2F16%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[‰ª•‰∏ãÂÜÖÂÆπÊù•Ëá™Á¨¨‰∏âÂ±äÊê∫Á®ãÂ§ßÊï∞ÊçÆÊ≤ôÈæô‰∏É„ÄÅÈÅáÂà∞ÁöÑÈóÆÈ¢òorc splitSparkËØªÂèñHiveË°®Áî®ÁöÑÂêÑ‰∏™Êñá‰ª∂Ê†ºÂºèÁöÑInuptFormatÔºåËÆ°ÁÆóËØªÂèñË°®ÈúÄË¶ÅÁöÑtaskÊï∞Èáè‰æùËµñ‰∫éInputFormat#getSplitsÁî±‰∫éÂ§ßÈÉ®ÂàÜË°®ÁöÑÂ≠òÂÇ®Ê†ºÂºè‰∏ªË¶Å‰ΩøÁî®ÁöÑÊòØorcÔºåÂΩì‰∏Ä‰∏™orcÊñá‰ª∂Ë∂ÖËøá256MBÔºåsplitÁÆóÊ≥ïÂπ∂Ë°åÂéªËØªÂèñorcÂÖÉÊï∞ÊçÆÔºåÊúâÊó∂ÂÄôDriverÂÜÖÂ≠òÈ£ôÂçáÔºåOOM crashÔºåFull GCÂØºËá¥network timeoutÔºåspark context stopHiveËØªËøô‰∫õÂ§ßË°®‰∏∫‰ΩïÊ≤°ÊúâÈóÆÈ¢òÔºüÂõ†‰∏∫HiveÈªòËÆ§‰ΩøÁî®ÁöÑÊòØCombineHiveInputFormatÔºåsplitÊòØÂü∫‰∫éÊñá‰ª∂Â§ßÂ∞èÁöÑ„ÄÇSpark‰πüÈúÄË¶ÅÂÆûÁé∞Á±ª‰ºº‰∫éHiveÁöÑCombineInputFormatÔºåËøòËÉΩËß£ÂÜ≥Â∞èÊñá‰ª∂ËøáÂ§öÂØºËá¥Êèê‰∫§taskÊï∞ÈáèËøáÂ§öÁöÑÈóÆÈ¢ò„ÄÇExecutor Container killedExecutor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverheadÂéüÂõ†Ôºö1.Shuffle ReadÊó∂nettyÂ†ÜÂ§ñÂÜÖÂ≠òÁöÑ‰ΩøÁî®2.Window function spill thresholdËøáÂ∞èÔºåÂØºËá¥ÊØè4096Êù°ÊàñËÄÖ64MB‰∏∫‰∏Ä‰∏™Êñá‰ª∂ÂÜôÂà∞Á£ÅÁõòÂ§ñÈÉ®ÊéíÂ∫èÂêåÊó∂ÊâìÂºÄÊØè‰∏™Êñá‰ª∂ÔºåÊØè‰∏™Êñá‰ª∂Âç†Áî®1MBÁöÑÂ†ÜÂ§ñÂÜÖÂ≠òÔºåÂØºËá¥container‰ΩøÁî®ÁöÑÂÜÖÂ≠òËøúË∂ÖËøáÁî≥ËØ∑ÁöÑÂÜÖÂ≠òÔºåÈÅÇË¢´yarn kill„ÄÇËß£ÂÜ≥ÔºöPatchÔºö[SPARK-19659] Fetch big blocks to disk when shuffle-read[SPARK-21369][CORE] Don‚Äôt use Scala Tuple2 in common/network-ÂèÇÊï∞Ôºöspark.reducer.maxReqSizeShuffleToMem=209715200PatchÔºö[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArrayÂèÇÊï∞Ôºöspark.sql.windowExec.buffer.in.memory.threshold=4096spark.sql.windowExec.buffer.spill.threshold= 1024 1024 * 1024 / 2Â∞èÊñá‰ª∂ÈóÆÈ¢òSparkÂÜôÊï∞ÊçÆÊó∂ÁîüÊàêÂæàÂ§öÂ∞èÊñá‰ª∂ÔºåÂØπNameNode‰∫ßÁîüÂ∑®Â§ßÁöÑÂéãÂäõÔºåÂú®‰∏ÄÂºÄÂßãSparkÁÅ∞Â∫¶‰∏äÁ∫øÁöÑÊó∂ÂÄôÔºåÊñá‰ª∂Êï∞ÂíåBlockÊï∞È£ôÂçáÔºåÊñá‰ª∂ÂèòÂ∞èÂØºËá¥ÂéãÁº©ÁéáÈôç‰ΩéÔºåÂÆπÈáè‰πüË∑üÁùÄ‰∏äÂéª„ÄÇÁßªÊ§çHive MergeFileTaskÁöÑÂÆûÁé∞Âú®SparkÊúÄÂêéÂÜôÁõÆÊ†áË°®ÁöÑÈò∂ÊÆµËøΩÂä†ÂÖ•‰∫Ü‰∏Ä‰∏™MergeFileTaskÔºåÂèÇËÄÉ‰∫ÜHiveÁöÑÂÆûÁé∞org.apache.hadoop.hive.ql.io.merge.MergeFileTaskorg.apache.hadoop.hive.ql.exec.OrcFileMergeOperatorÊó†Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ã‰∏çÂàõÂª∫Á©∫Êñá‰ª∂[SPARK-21435][SQL]Empty files should be skipped while write to fileÂÖ´„ÄÅ‰ºòÂåñ1.Êü•ËØ¢ÂàÜÂå∫Ë°®Êó∂ÊîØÊåÅbroadcast joinÔºåÂä†ÈÄüÊü•ËØ¢2.ÂáèÂ∞ëBroadcast joinÁöÑÂÜÖÂ≠òÂéãÂäõ SPARK-221703.FetchÂ§±Ë¥•ÂêéËÉΩÂø´ÈÄüÂ§±Ë¥•Ôºå‰ª•ÂÖç‰Ωú‰∏öÂç°Âá†‰∏™Â∞èÊó∂ SPARK-197534.Spark Thrift ServerÁ®≥ÂÆöÊÄßÁªèÂ∏∏ÊåÇÊéâÔºåÊó•ÂøóÈáåÂºÇÂ∏∏Ôºåmore than one active taskSet for stageApply SPARK-23433‰ªçÊúâÂ∞ëÊï∞ÊåÇÊéâÁöÑÊÉÖÂÜµÔºåÊèê‰∫§SPARK-24677Âà∞Á§æÂå∫Ôºå‰øÆÂ§ç‰πã5.‰Ωú‰∏öhang‰Ωè SPARK-21834 SPARK-19326 SPARK-11334‰πù„ÄÅÊú™Êù•ËÆ°ÂàíËá™Âä®Ë∞É‰ºòÂÜÖÂ≠òÊâãÊú∫spark driverÂíåexecutorÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµÊ†πÊçÆ‰Ωú‰∏öÂéÜÂè≤ÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµÔºåÂú®Ë∞ÉÂ∫¶Á≥ªÁªüÁ´ØËá™Âä®ËÆæÁΩÆÂêàÈÄÇÁöÑÂÜÖÂ≠òhttps://github.com/uber-common/jvm-profilerspark adaptiveÂä®ÊÄÅË∞ÉÊï¥ÊâßË°åËÆ°Âàí SortMergeJoinËΩ¨Âåñ‰∏∫BroadcastHashJoinÂä®ÊÄÅÂ§ÑÁêÜÊï∞ÊçÆÂÄæÊñúhttps://issues.apache.org/jira/browse/SPARK-23128https://github.com/Intel-bigdata/spark-adaptive]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkÂú®Êê∫Á®ãÁöÑÂÆûË∑µÔºà‰∏ÄÔºâ]]></title>
    <url>%2F2018%2F12%2F09%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅSparkÂú®Êê∫Á®ãÂ∫îÁî®ÁöÑÁé∞Áä∂ÈõÜÁæ§ËßÑÊ®°ÔºöÂπ≥ÂùáÊØèÂ§©MR‰ªªÂä°Êï∞Ôºö30W+ÂºÄÂèëÂπ≥Âè∞ÔºöË∞ÉÂ∫¶Á≥ªÁªüËøêË°åÁöÑ‰ªªÂä°Êï∞Ôºö10W+ÊØèÂ§©ËøêË°å‰ªªÂä°ÂÆû‰æãÊï∞Ôºö23W+ETL/ËÆ°ÁÆó‰ªªÂä°Ôºö~58%Êü•ËØ¢Âπ≥Âè∞:adhocÊü•ËØ¢Ôºö2W+ÊîØÊåÅSpark/Hive/Presto‰∫å„ÄÅHive‰∏éSparkÁöÑÂå∫Âà´HiveÔºö‰ºòÁÇπÔºöËøêË°åÁ®≥ÂÆöÔºåÂÆ¢Êà∑Á´ØÂÜÖÂ≠òÊ∂àËÄóÂ∞è„ÄÇÂ≠òÂú®ÈóÆÈ¢òÔºöÁîüÊàêÂ§ö‰∏™MapReduce‰Ωú‰∏öÔºõ‰∏≠Èó¥ÁªìÊûúËêΩÂú∞ÔºåIOÂºÄÈîÄÂ§ßÔºõÈ¢ëÁπÅÁî≥ËØ∑ÂíåÈáäÊîæcontainerÔºåËµÑÊ∫êÊ≤°ÊúâÂêàÁêÜÂÖÖÂàÜÂà©Áî®SparkÔºöÂø´ÔºöÈ´òÊïàÁöÑDAGÊâßË°åÂºïÊìéÔºåÂèØ‰ª•Âü∫‰∫éÂÜÖÂ≠òÊù•È´òÊïàÁöÑÂ§ÑÁêÜÊï∞ÊçÆÊµÅÔºåËäÇÁúÅÂ§ßÈáèIOÂºÄÈîÄÈÄöÁî®ÊÄßÔºöSparkSQLËÉΩÁõ¥Êé•‰ΩøÁî®HiveQLËØ≠Ê≥ïÔºåHive MetastoreÔºåSerdesÔºåUDFs‰∏â„ÄÅËøÅÁßªSparkSQLÁöÑÊåëÊàòÂÖºÂÆπÊÄßÔºöHiveÂéüÂÖàÁöÑÊùÉÈôêÊéßÂà∂SQLËØ≠Ê≥ïÔºåUDFÂíåHiveÁöÑÂÖºÂÆπÊÄßÁ®≥ÂÆöÊÄßÔºöËøÅÁßªÈÄèÊòéÔºå‰Ωé‰ºòÂÖàÁ∫ßÁî®Êà∑Êó†ÊÑüÁü•ÁõëÊéß‰Ωú‰∏öËøÅÁßªÂêéÊàêÂäüÁéáÂèäËøêË°åÊó∂ÈïøÂØπÊØîÂáÜÁ°ÆÊÄßÔºöÊï∞ÊçÆ‰∏ÄËá¥ÂäüËÉΩÂ¢ûÂº∫ÔºöÁî®Êà∑‰ΩìÈ™åÔºåÊòØÂê¶ÊòìÁî®ÔºåÊä•Èîô‰ø°ÊÅØÊòØÂê¶ÂèØËØªÊΩúÂú®BugÂë®ËæπÁ≥ªÁªüÈÖçÂêàÊîπÈÄ†Ë°ÄÁºòÊî∂ÈõÜÂõõ„ÄÅÂÖºÂÆπÊÄßÊîπÈÄ†ÁßªÊ§çhiveÊùÉÈôêSparkÊ≤°ÊúâÊùÉÈôêËÆ§ËØÅÊ®°ÂùóÔºåÂèØÂØπ‰ªªÊÑèË°®ËøõË°åÊü•ËØ¢ÔºåÊúâÂÆâÂÖ®ÈöêÊÇ£ÈúÄË¶Å‰∏éHiveÂÖ±‰∫´Âêå‰∏ÄÂ•óÊùÉÈôêÊñπÊ°àÔºöÊâßË°åSQLÊó∂ÔºåÂØπSQLËß£ÊûêÂæóÂà∞LogicalPlanÔºåÂØπLogicalPlanËøõË°åÈÅçÂéÜÔºåÊèêÂèñËØªÂèñÁöÑË°®ÂèäÂÜôÂÖ•ÁöÑË°®ÔºåË∞ÉÁî®HvieÁöÑËÆ§ËØÅÊñπÊ≥ïËøõË°åÊ£ÄÊü•ÔºåÂ¶ÇÊûúÊúâÊùÉÈôêÂàôÁªßÁª≠ÊâßË°åÔºåÂê¶ÂàôÊãíÁªùËØ•Áî®Êà∑ÁöÑÊìç‰Ωú„ÄÇSQLËØ≠Ê≥ïÂíåhiveÂÖºÂÆπSparkÂàõÂª∫ÁöÑÊüê‰∫õËßÜÂõæÔºåÂú®HiveÊü•ËØ¢Êó∂Êä•ÈîôÔºåSparkÂàõÂª∫ÁöÑËßÜÂõæ‰∏ç‰ºöÂØπSQLËøõË°åÂ±ïÂºÄÔºåËßÜÂõæÂÆö‰πâÊ≤°ÊúâÂΩìÂâçÁöÑDB‰ø°ÊÅØÔºåHive‰∏çÂÖºÂÆπËØªÂèñËøôÊ†∑ÁöÑËßÜÂõæÊñπÊ°àÔºö„ÄÅ‰øùÊåÅ‰∏éHive‰∏ÄËá¥ÔºåÂú®SparkÂàõÂª∫Âíå‰øÆÊîπËßÜÂõæÊó∂Ôºå‰ΩøÁî®hive cli driverÂéªÊâßË°åcreate/alter view sqlUDF‰∏éhiveÂÖºÂÆπUDFËÆ°ÁÆóÁªìÊûú‰∏ç‰∏ÄÊ†∑ÔºåÂç≥‰ΩøÊòØÊ≠£Â∏∏Êï∞ÊçÆÔºåSparkËøîÂõûnullÔºåHiveÁªìÊûúÊ≠£Á°ÆÔºõÂºÇÂ∏∏Êï∞ÊçÆÔºåSparkÊäõexceptionÂØºËá¥‰Ωú‰∏öÂ§±Ë¥•ÔºåHiveËøîÂõûÁöÑnull„ÄÇÊñπÊ°àÔºöSparkÂáΩÊï∞‰øÆÂ§çÔºåÊØîÂ¶ÇroundÂáΩÊï∞Â∞Ühive‰∏Ä‰∫õÂáΩÊï∞ÁßªÊ§çÔºåÂπ∂Ê≥®ÂÜåÊàêÊ∞∏‰πÖÂáΩÊï∞Êï¥ÁêÜSparkÂíåHiveËØ≠Ê≥ïÂíåUDFÂ∑ÆÂºÇ‰∫î„ÄÅÁ®≥ÂÆöÊÄßÂíåÂáÜÁ°ÆÊÄßÁ®≥ÂÆöÊÄßÔºöËøÅÁßªÈÄèÊòéÔºöË∞ÉÂ∫¶Á≥ªÁªüÂØπ‰Ωé‰ºòÂÖàÁ∫ß‰Ωú‰∏öÔºåÊåâ‰Ωú‰∏öÁ≤íÂ∫¶ÂàáÊç¢ÊàêSparkÊâßË°åÔºåÂ§±Ë¥•ÂêéÂÜçÂàáÊç¢ÊàêhiveÁÅ∞Â∫¶ÂèòÊõ¥ÔºåÂ§öÁßçÂèòÊõ¥ËßÑÂàôÔºöÊîØÊåÅÂ§öÁâàÊú¨SparkÔºåËá™Âä®ÂàáÊç¢ÂºïÊìéÔºåSpark v2 -&gt; Spark v1 -&gt; HiveÔºõÁÅ∞Â∫¶Êé®ÈÄÅÂèÇÊï∞ÔºåË∞É‰ºòÂèÇÊï∞ÔºåÊüê‰∫õÂäüËÉΩÁõëÊéßÔºöÊØèÊó•ÁªüËÆ°sparkÂíåhiveËøêË°åÂØπÊØîÔºåÊØèÊó∂Êî∂ÈõÜ‰Ωú‰∏öÁ≤íÂ∫¶Â§±Ë¥•ÁöÑSpark‰Ωú‰∏öÔºåÂàÜÊûêÂ§±Ë¥•ÂéüÂõ†ÂáÜÁ°ÆÊÄßÔºöÊï∞ÊçÆË¥®ÈáèÁ≥ªÁªüÔºöÊ†°È™å‰ªªÂä°ÔºåÊ£ÄÊü•Êï∞ÊçÆÂáÜÁ°ÆÊÄßÂÖ≠„ÄÅÂäüËÉΩÂ¢ûÂº∫Spark Thrift ServerÔºö1.Âü∫‰∫édelegation tokenÁöÑimpersontionDriverÔºö‰∏∫‰∏çÂêåÁöÑÁî®Êà∑Êãødelegation tokenÔºåÂÜôÂà∞stagingÁõÆÂΩïÔºåËÆ∞ÂΩïUser-&gt;SQL-&gt;JobÊò†Â∞ÑÂÖ≥Á≥ªÔºåÂàÜÂèëtaskÂ∏¶‰∏äÂØπÂ∫îÁöÑusernameExecutorÔºöÊ†πÊçÆtask‰ø°ÊÅØÂ∏¶ÁöÑusernameÊâæÂà∞stagingÁõÆÂΩï‰∏ãÁöÑtokenÔºåÂä†Âà∞ÂΩìÂâçproxy userÁöÑugiÔºåÂÆûÁé∞impersonate2.Âü∫‰∫ézookeeperÁöÑÊúçÂä°ÂèëÁé∞ÔºåÊîØÊåÅÂ§öÂè∞serverËøô‰∏ÄÂùó‰∏ªË¶ÅÁßªÊ§ç‰∫ÜHive zookeeperÁöÑÂÆûÁé∞3.ÈôêÂà∂Â§ßÊü•ËØ¢‰Ωú‰∏öÔºåÈò≤Ê≠¢driver OOMÈôêÂà∂ÊØè‰∏™job‰∫ßÁîüÁöÑtaskÊúÄÂ§ßÊï∞ÈáèÈôêÂà∂Êü•ËØ¢SQLÁöÑÊúÄÂ§ßË°åÊï∞ÔºåÂÆ¢Êà∑Á´ØÊü•ËØ¢Â§ßÊâπÈáèÊï∞ÊçÆÔºåÊï∞ÊçÆÊå§ÂéãÂú®Thrift ServerÔºåÂ†ÜÂÜÖÂÜÖÂ≠òÈ£ôÂçáÔºåÂº∫Âà∂Âú®Âè™ÊúâÊü•ÁöÑSQLÂä†‰∏älimitÈôêÂà∂Êü•ËØ¢SQLÁöÑÁªìÊûúÈõÜÊï∞ÊçÆÂ§ßÂ∞è4.ÁõëÊéßÂØπÊØè‰∏™serverÂÆöÊó∂Êü•ËØ¢ÔºåÊ£ÄÊµãÊòØÂê¶ÂèØÁî®Â§öËøêË°åÊó∂ÈïøËæÉ‰πÖÁöÑ‰Ωú‰∏öÔºå‰∏ªÂä®killÁî®Êà∑‰ΩìÈ™åÁî®Êà∑ÁúãÂà∞ÁöÑÊòØÁ±ª‰ººHive MRËøõÂ∫¶ÁöÑÊó•ÂøóÔºåINFOÁ∫ßÂà´Êó•ÂøóÊî∂ÈõÜÂà∞ESÔºåÂèØ‰æõÊó•ÂøóÁöÑÂàÜÊûêÂíåÊéíÊü•ÈóÆÈ¢òÊî∂ÈõÜÁîüÊàêÁöÑË°®ÊàñËÄÖÂàÜÂå∫ÁöÑnumRows numFile totalSizeÔºåËæìÂá∫Âà∞Êó•ÂøóÂØπÁÆÄÂçïÁöÑËØ≠Âè•ÔºåÂ¶ÇDDLËØ≠Âè•ÔºåËá™Âä®‰ΩøÁî®‚Äìmaster=localÊñπÂºèÂêØÂä®Combine input FormatÂú®HadoopTableReader#makeRDDForTableÔºåÊãøÂà∞ÂØπÂ∫îtableÁöÑInputFormatClassÔºåËΩ¨Êç¢ÊàêÂØπÂ∫îÊ†ºÂºèÁöÑCombineInputFormatÈÄöËøáÂºÄÂÖ≥Êù•ÂÜ≥ÂÆöÊòØÂê¶ÂêØÁî®Ëøô‰∏™ÁâπÊÄßset spark.sql.combine.input.splits.enable=trueÈÄöËøáÂèÇÊï∞Êù•Ë∞ÉÊï¥ÊØè‰∏™splitÁöÑtotal input sizemapreduce.input.fileinputformat.split.maxsize=256MB 10241024‰πãÂâçdriverËØªÂ§ßË°®È´òÂ≥∞Êó∂ÊÆµsplitÈúÄË¶Å30ÂàÜÈíü‰∏çÊ≠¢ÔºåÊâçÊää‰ªªÂä°Êèê‰∫§‰∏äÔºåÁé∞Âú®Âè™Ë¶ÅÂá†ÂàÜÈíüÂ∞±ÁÆóÂ•ΩsplitÁöÑÊï∞ÈáèÂπ∂Êèê‰∫§‰ªªÂä°Ôºå‰πüËß£ÂÜ≥‰∫Ü‰∏Ä‰∫õË°®‰∏çÂ§ßÔºåÂ∞èÊñá‰ª∂Â§öÔºåËÉΩÂêàÂπ∂Âà∞Âêå‰∏Ä‰∏™taskËøõË°åËØªÂèñ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ª£Á†Å | SparkËØªÂèñmongoDBÊï∞ÊçÆÂÜôÂÖ•HiveÊôÆÈÄöË°®ÂíåÂàÜÂå∫Ë°®]]></title>
    <url>%2F2018%2F11%2F20%2FSpark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[ÁâàÊú¨Ôºöspark 2.2.0hive 1.1.0scala 2.11.8hadoop-2.6.0-cdh5.7.0jdk 1.8MongoDB 3.6.4‰∏Ä ÂéüÂßãÊï∞ÊçÆÂèäHiveË°®MongoDBÊï∞ÊçÆÊ†ºÂºè1234567&#123; &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;), &quot;id&quot; : &quot;1&quot;, &quot;name&quot; : &quot;lisi&quot;, &quot;age&quot; : &quot;18&quot;, &quot;deptno&quot; : &quot;01&quot;&#125;HiveÊôÆÈÄöË°®123456create table mg_hive_test(id string,name string,age string,deptno string)row format delimited fields terminated by &apos;\t&apos;;HiveÂàÜÂå∫Ë°®1234567create table mg_hive_external(id string,name string,age string)partitioned by (deptno string)row format delimited fields terminated by &apos;\t&apos;;‰∫å IDEA+Maven+Java‰æùËµñ1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt;‰ª£Á†Å123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/2 21:00 * */import com.mongodb.spark.MongoSpark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.RowFactory;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.hive.HiveContext;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructField;import org.apache.spark.sql.types.StructType;import org.bson.Document;import java.io.File;import java.util.ArrayList;import java.util.List;public class sparkreadmgtohive &#123; public static void main(String[] args) &#123; //spark 2.x String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath(); SparkSession spark = SparkSession.builder() .master(&quot;local[2]&quot;) .appName(&quot;SparkReadMgToHive&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;) .enableHiveSupport() .getOrCreate(); JavaSparkContext sc = new JavaSparkContext(spark.sparkContext()); //spark 1.x// JavaSparkContext sc = new JavaSparkContext(conf);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);// SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);// conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);// conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);// HiveContext sqlContext = new HiveContext(sc);// //create df from mongo// Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();// df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show(); String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;; String opType =&quot;P&quot;; SQLUtils sqlUtils = new SQLUtils(); List&lt;String&gt; column = sqlUtils.getColumns(querysql); //create rdd from mongo JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc); //Â∞ÜDocumentËΩ¨ÊàêObject JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123; public Object call(Document document)&#123; List list = new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; list.add(String.valueOf(document.get(column.get(i)))); &#125; return list;// return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;); &#125; &#125;); System.out.println(Ordd.first()); //ÈÄöËøáÁºñÁ®ãÊñπÂºèÂ∞ÜRDDËΩ¨ÊàêDF List ls= new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; ls.add(column.get(i)); &#125; String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;); System.out.println(schemaString); List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); for (String fieldName : schemaString.split(&quot;,&quot;)) &#123; StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true); fields.add(field); &#125; StructType schema = DataTypes.createStructType(fields); JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123; List fileds = (List) record;// String[] attributes = record.toString().split(&quot;,&quot;); return RowFactory.create(fileds.toArray()); &#125;); Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema); //Â∞ÜDFÂÜôÂÖ•Âà∞Hive‰∏≠ //ÈÄâÊã©HiveÊï∞ÊçÆÂ∫ì spark.sql(&quot;use datalake&quot;); //Ê≥®ÂÜå‰∏¥Êó∂Ë°® df.registerTempTable(&quot;mgtable&quot;); if (&quot;O&quot;.equals(opType.trim())) &#123; System.out.println(&quot;Êï∞ÊçÆÊèíÂÖ•Âà∞Hive ordinary table&quot;); Long t1 = System.currentTimeMillis(); spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;); Long t2 = System.currentTimeMillis(); System.out.println(&quot;ÂÖ±ËÄóÊó∂Ôºö&quot; + (t2 - t1) / 60000 + &quot;ÂàÜÈíü&quot;); &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123; System.out.println(&quot;Êï∞ÊçÆÊèíÂÖ•Âà∞Hive dynamic partition table&quot;); Long t3 = System.currentTimeMillis(); //ÂøÖÈ°ªËÆæÁΩÆ‰ª•‰∏ãÂèÇÊï∞ Âê¶ÂàôÊä•Èîô spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;); //depton‰∏∫ÂàÜÂå∫Â≠óÊÆµ selectËØ≠Âè•ÊúÄÂêé‰∏Ä‰∏™Â≠óÊÆµÂøÖÈ°ªÊòØdeptno spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;); Long t4 = System.currentTimeMillis(); System.out.println(&quot;ÂÖ±ËÄóÊó∂Ôºö&quot;+(t4 -t3)/60000+ &quot;ÂàÜÈíü&quot;); &#125; spark.stop(); &#125;&#125;Â∑•ÂÖ∑Á±ª1234567891011121314151617181920212223242526272829303132333435package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/3 23:20 * */import java.util.ArrayList;import java.util.List;public class SQLUtils &#123; public List&lt;String&gt; getColumns(String querysql)&#123; List&lt;String&gt; column = new ArrayList&lt;String&gt;(); String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6, querysql.indexOf(&quot;from&quot;)).trim(); if (tmp.indexOf(&quot;*&quot;) == -1)&#123; String cols[] = tmp.split(&quot;,&quot;); for (String c:cols)&#123; column.add(c); &#125; &#125; return column; &#125; public String getTBname(String querysql)&#123; String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim(); int sx = tmp.indexOf(&quot; &quot;); if(sx == -1)&#123; return tmp; &#125;else &#123; return tmp.substring(0,sx); &#125; &#125;&#125;‰∏â ÈîôËØØËß£ÂÜ≥ÂäûÊ≥ï1 IDEA‰ºöËé∑Âèñ‰∏çÂà∞HiveÁöÑÊï∞ÊçÆÂ∫ìÂíåË°®ÔºåÂ∞Ühive-site.xmlÊîæÂÖ•resourcesÊñá‰ª∂‰∏≠„ÄÇÂπ∂‰∏îÂ∞ÜresourcesËÆæÁΩÆÊàêÈÖçÁΩÆÊñá‰ª∂(ËÆæÁΩÆÊàêÂäüÊñá‰ª∂Â§πÊòØËìùËâ≤Âê¶ÂàôÊòØÁÅ∞Ëâ≤)file‚Äì&gt;Project Structure‚Äì&gt;Modules‚Äì&gt;Source2 ‰∏äÈù¢ÈîôËØØÂ§ÑÁêÜÂÆåÂêéÂ¶ÇÊûúÊä•JDOÁ±ªÂûãÁöÑÈîôËØØÔºåÈÇ£‰πàÊ£ÄÊü•HIVE_HOME/lib‰∏ãÊó∂ÂÄôÂê¶mysqlÈ©±Âä®ÔºåÂ¶ÇÊûúÁ°ÆÂÆöÊúâÔºåÈÇ£‰πàÂ∞±ÊòØIDEAËé∑Âèñ‰∏çÂà∞„ÄÇËß£ÂÜ≥ÊñπÊ≥ïÂ¶Ç‰∏ãÔºöÂ∞ÜmysqlÈ©±Âä®Êã∑Ë¥ùÂà∞jdk1.8.0_171.jdk/Contents/Home/jre/lib/extË∑ØÂæÑ‰∏ã(jdk/jre/lib/ext)Âú®IDEAÈ°πÁõÆExternal Libraries‰∏ãÁöÑ&lt;1.8&gt;ÈáåÈù¢Ê∑ªÂä†mysqlÈ©±Âä®Âõõ Ê≥®ÊÑèÁÇπÁî±‰∫éÂ∞ÜMongoDBÊï∞ÊçÆË°®Ê≥®ÂÜåÊàê‰∫Ü‰∏¥Êó∂Ë°®ÂíåHiveË°®ËøõË°å‰∫ÜÂÖ≥ËÅîÔºåÊâÄ‰ª•Ë¶ÅÂ∞ÜMongoDB‰∏≠ÁöÑidÂ≠óÊÆµËÆæÁΩÆÊàêÁ¥¢ÂºïÂ≠óÊÆµÔºåÂê¶ÂàôÊÄßËÉΩ‰ºöÂæàÊÖ¢„ÄÇMongoDBËÆæÁΩÆÁ¥¢ÂºïÊñπÊ≥ïÔºö1db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;Êü•ÁúãÁ¥¢ÂºïÔºö12db.getCollection(&apos;mgtest&apos;).getIndexes()MongoSparkÁΩëÂùÄÔºöhttps://docs.mongodb.com/spark-connector/current/java-api/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊúÄÂÖ®ÁöÑFlinkÈÉ®ÁΩ≤ÂèäÂºÄÂèëÊ°à‰æã(KafkaSource+SinkToMySQL)]]></title>
    <url>%2F2018%2F11%2F10%2F%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource%2BSinkToMySQL)%2F</url>
    <content type="text"><![CDATA[1.‰∏ãËΩΩFlinkÂÆâË£ÖÂåÖflink‰∏ãËΩΩÂú∞ÂùÄhttps://archive.apache.org/dist/flink/flink-1.5.0/Âõ†‰∏∫‰æãÂ≠ê‰∏çÈúÄË¶ÅhadoopÔºå‰∏ãËΩΩflink-1.5.0-bin-scala_2.11.tgzÂç≥ÂèØ‰∏ä‰º†Ëá≥Êú∫Âô®ÁöÑ/optÁõÆÂΩï‰∏ã2.Ëß£Âéãtar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/3.ÈÖçÁΩÆmasterËäÇÁÇπÈÄâÊã©‰∏Ä‰∏™ masterËäÇÁÇπ(JobManager)ÁÑ∂ÂêéÂú®conf/flink-conf.yaml‰∏≠ËÆæÁΩÆjobmanager.rpc.address ÈÖçÁΩÆÈ°π‰∏∫ËØ•ËäÇÁÇπÁöÑIP ÊàñËÄÖ‰∏ªÊú∫Âêç„ÄÇÁ°Æ‰øùÊâÄÊúâËäÇÁÇπÊúâÊúâ‰∏ÄÊ†∑ÁöÑjobmanager.rpc.address ÈÖçÁΩÆ„ÄÇjobmanager.rpc.address: node1(ÈÖçÁΩÆÁ´ØÂè£Â¶ÇÊûúË¢´Âç†Áî®‰πüË¶ÅÊîπ Â¶ÇÈªòËÆ§8080Â∑≤ÁªèË¢´sparkÂç†Áî®ÔºåÊîπÊàê‰∫Ü8088)rest.port: 8088Êú¨Ê¨°ÂÆâË£Ö masterËäÇÁÇπ‰∏∫node1ÔºåÂõ†‰∏∫ÂçïÊú∫ÔºåslaveËäÇÁÇπ‰πü‰∏∫node14.ÈÖçÁΩÆslavesÂ∞ÜÊâÄÊúâÁöÑ worker ËäÇÁÇπ ÔºàTaskManagerÔºâÁöÑIP ÊàñËÄÖ‰∏ªÊú∫ÂêçÔºà‰∏ÄË°å‰∏Ä‰∏™ÔºâÂ°´ÂÖ•conf/slaves Êñá‰ª∂‰∏≠„ÄÇ5.ÂêØÂä®flinkÈõÜÁæ§bin/start-cluster.shÊâìÂºÄ http://node1:8088 Êü•ÁúãwebÈ°µÈù¢Task Managers‰ª£Ë°®ÂΩìÂâçÁöÑflinkÂè™Êúâ‰∏Ä‰∏™ËäÇÁÇπÔºåÊØè‰∏™taskËøòÊúâ‰∏§‰∏™slots6.ÊµãËØï‰æùËµñ123456789101112131415161718192021222324252627&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;7.SocketÊµãËØï‰ª£Á†Å12345678910111213141516171819202122232425262728293031public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; final String hostName; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(&quot;port&quot;); hostName = params.get(&quot;hostname&quot;); &#125; catch (Exception e) &#123; System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + &quot; : &quot; + count; &#125; &#125;&#125;ÊâìÂåÖmvn clean install (Â¶ÇÊûúÊâìÂåÖËøáÁ®ã‰∏≠Êä•Èîôjava.lang.OutOfMemoryError)Âú®ÂëΩ‰ª§Ë°åset MAVEN_OPTS= -Xms128m -Xmx512mÁªßÁª≠ÊâßË°åmvn clean installÁîüÊàêFlinkTest.jarÊâæÂà∞ÊâìÊàêÁöÑjarÔºåÂπ∂uploadÔºåÂºÄÂßã‰∏ä‰º†ËøêË°åÂèÇÊï∞‰ªãÁªçÊèê‰∫§ÁªìÊùü‰πãÂêéÂéªoverviewÁïåÈù¢ÁúãÔºåÂèØ‰ª•ÁúãÂà∞ÔºåÂèØÁî®ÁöÑslotsÂèòÊàê‰∫Ü‰∏Ä‰∏™ÔºåÂõ†‰∏∫Êàë‰ª¨ÁöÑsocketÁ®ãÂ∫èÂç†Áî®‰∫Ü‰∏Ä‰∏™ÔºåÊ≠£Âú®runningÁöÑjobÂèòÊàê‰∫Ü‰∏Ä‰∏™ÂèëÈÄÅÊï∞ÊçÆ12345[root@hadoop000 flink-1.5.0]# nc -l 8099aaa bbbaaa cccaaa bbbbbb cccÁÇπÂºÄrunningÁöÑjobÔºå‰Ω†ÂèØ‰ª•ÁúãËßÅÊé•Êî∂ÁöÑÂ≠óËäÇÊï∞Á≠â‰ø°ÊÅØÂà∞logÁõÆÂΩï‰∏ãÂèØ‰ª•Ê∏ÖÊ•öÁöÑÁúãËßÅËæìÂá∫1234567891011[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.outaaa : 1ccc : 1ccc : 1bbb : 1ccc : 1bbb : 1bbb : 1ccc : 1bbb : 1ccc : 1Èô§‰∫ÜÂèØ‰ª•Âú®ÁïåÈù¢Êèê‰∫§ÔºåËøòÂèØ‰ª•Â∞Üjar‰∏ä‰º†ÁöÑlinux‰∏≠ËøõË°åÊèê‰∫§‰ªªÂä°ËøêË°åflink‰∏ä‰º†ÁöÑjar1bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1ÂÖ∂‰ªñÊ≠•È™§‰∏ÄËá¥„ÄÇ8.‰ΩøÁî®kafka‰Ωú‰∏∫sourceÂä†‰∏ä‰æùËµñ1234&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;1234567891011121314151617public class KakfaSource010 &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); //ÂèØ‰ª•ÈÄöËøáÊ≠£ÂàôË°®ËææÂºèÊù•ÂåπÈÖçÂêàÈÄÇÁöÑtopic FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties); //ÈÖçÁΩÆ‰ªéÊúÄÊñ∞ÁöÑÂú∞ÊñπÂºÄÂßãÊ∂àË¥π kafkaSource.setStartFromLatest(); //‰ΩøÁî®addsourceÔºåÂ∞ÜkafkaÁöÑËæìÂÖ•ËΩ¨Âèò‰∏∫datastream DataStream&lt;String&gt; consume = env.addSource(wordfre); ... //process and sink env.execute(&quot;KakfaSource010&quot;); &#125;&#125;9.‰ΩøÁî®mysql‰Ωú‰∏∫sinkflinkÊú¨Ë∫´Âπ∂Ê≤°ÊúâÊèê‰æõdatastreamËæìÂá∫Âà∞mysqlÔºåÈúÄË¶ÅÊàë‰ª¨Ëá™Â∑±ÂéªÂÆûÁé∞È¶ñÂÖàÔºåÂØºÂÖ•‰æùËµñ12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt;Ëá™ÂÆö‰πâsinkÔºåÈ¶ñÂÖàÊÉ≥Âà∞ÁöÑÊòØextends SinkFunctionÔºåÈõÜÊàêflinkËá™Â∏¶ÁöÑsinkfunctionÔºåÂÜçÂΩì‰∏≠ÂÆûÁé∞ÊñπÊ≥ïÔºåÂÆûÁé∞Â¶Ç‰∏ã12345678910111213141516171819202122public class MysqlSink implements SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; String username = &quot;mysql.user&quot;; String password = &quot;mysql.password&quot;; String drivername = &quot;mysql.driver&quot;; String dburl = &quot;mysql.url&quot;; @Override public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;ËøôÊ†∑ÂÆûÁé∞Êúâ‰∏™ÈóÆÈ¢òÔºåÊØè‰∏ÄÊù°Êï∞ÊçÆÔºåÈÉΩË¶ÅÊâìÂºÄmysqlËøûÊé•ÔºåÂÜçÂÖ≥Èó≠ÔºåÊØîËæÉËÄóÊó∂ÔºåËøô‰∏™ÂèØ‰ª•‰ΩøÁî®flink‰∏≠ÊØîËæÉÂ•ΩÁöÑRichÊñπÂºèÊù•ÂÆûÁé∞Ôºå‰ª£Á†ÅÂ¶Ç‰∏ã12345678910111213141516171819202122232425262728public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement preparedStatement = null; private String userName = null; private String password = null; private String driverName = null; private String DBUrl = null; public MysqlSink() &#123; userName = &quot;mysql.username&quot;; password = &quot;mysql.password&quot;; driverName = &quot;mysql.driverName&quot;; DBUrl = &quot;mysql.DBUrl&quot;; &#125; public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; if(connection==null)&#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; String sql =&quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,value.f0); preparedStatement.setString(2,value.f1); preparedStatement.executeUpdate();//ËøîÂõûÊàêÂäüÁöÑËØùÂ∞±ÊòØ‰∏Ä‰∏™ÔºåÂê¶ÂàôÂ∞±ÊòØ0 &#125; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; @Override public void close() throws Exception &#123; if(preparedStatement!=null)&#123; preparedStatement.close(); &#125; if(connection!=null)&#123; connection.close(); &#125; &#125;&#125;RichÊñπÂºèÁöÑ‰ºòÁÇπÂú®‰∫éÔºåÊúâ‰∏™openÂíåcloseÊñπÊ≥ïÔºåÂú®ÂàùÂßãÂåñÁöÑÊó∂ÂÄôÂª∫Á´ã‰∏ÄÊ¨°ËøûÊé•Ôºå‰πãÂêé‰∏ÄÁõ¥‰ΩøÁî®Ëøô‰∏™ËøûÊé•Âç≥ÂèØÔºåÁº©Áü≠Âª∫Á´ãÂíåÂÖ≥Èó≠ËøûÊé•ÁöÑÊó∂Èó¥Ôºå‰πüÂèØ‰ª•‰ΩøÁî®ËøûÊé•Ê±†ÂÆûÁé∞ÔºåËøôÈáåÂè™ÊòØÊèê‰æõËøôÊ†∑‰∏ÄÁßçÊÄùË∑Ø„ÄÇ‰ΩøÁî®Ëøô‰∏™mysqlsink‰πüÈùûÂ∏∏ÁÆÄÂçï1//Áõ¥Êé•addsinkÔºåÂç≥ÂèØËæìÂá∫Âà∞Ëá™ÂÆö‰πâÁöÑmysql‰∏≠Ôºå‰πüÂèØ‰ª•Â∞ÜmysqlÁöÑÂ≠óÊÆµÁ≠âÂÜôÊàêÂèØÈÖçÁΩÆÁöÑÔºåÊõ¥Âä†Êñπ‰æøÂíåÈÄöÁî®proceDataStream.addSink(new MysqlSink());10.ÊÄªÁªìÊú¨Ê¨°ÁöÑÁ¨îËÆ∞ÂÅö‰∫ÜÁÆÄÂçïÁöÑÈÉ®ÁΩ≤„ÄÅÊµãËØï„ÄÅkafkademoÔºå‰ª•ÂèäËá™ÂÆö‰πâÂÆûÁé∞mysqlsinkÁöÑ‰∏Ä‰∫õÂÜÖÂÆπÔºåÂÖ∂‰∏≠ÊØîËæÉÈáçË¶ÅÁöÑÊòØRichÁöÑ‰ΩøÁî®ÔºåÂ∏åÊúõÂ§ßÂÆ∂ËÉΩÊúâÊâÄÊî∂Ëé∑„ÄÇ]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãÂü∫‰∫éSpark Streaming+Saprk SQLÂºÄÂèëOnLineLogAanlysis2]]></title>
    <url>%2F2018%2F09%2F18%2F15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%2BSaprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2%2F</url>
    <content type="text"><![CDATA[1.influxdbÂàõÂª∫database[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0create database online_log_analysis2.ÂØºÂÖ•Ê∫ê‰ª£Á†ÅÈ°πÁõÆ‰∏≠ÂéüÊú¨ÊÉ≥Â∞Ü influxdb-java https://github.com/influxdata/influxdb-javaÁöÑInfluxDBTest.java Êñá‰ª∂ÁöÑÂä†Âà∞È°πÁõÆ‰∏≠ÔºåÊâÄ‰ª•ÂøÖÈ°ªË¶ÅÂºïÂÖ• influxdb-java ÁöÑÂåÖÔºõ‰ΩÜÊòØÁî±‰∫éGitHubÁöÑ‰∏äÁöÑclassÊñá‰ª∂ÁöÑÊüê‰∫õÊñπÊ≥ïÔºåÊòØÁâàÊú¨ÊòØ2.6ÔºåËÄåmaven‰∏≠ÁöÑÊúÄÈ´ò‰πüÂ∞±2.5ÁâàÊú¨ÔºåÊâÄ‰ª•Â∞ÜGithubÁöÑÊ∫ê‰ª£Á†Å‰∏ãËΩΩÂØºÂÖ•Âà∞idea‰∏≠ÔºåÁºñËØëÂØºÂá∫2.6.jarÂåÖÔºõÂèØÊòØ ÂºïÂÖ•2.6jarÂåÖÔºåÂÖ∂Âú®InfluxDBTest.classÊñá‰ª∂ÁöÑ Êó†Ê≥ïimport org.influxdbÔºàÁôæÂ∫¶Ë∞∑Ê≠åÂæàÈïøÊó∂Èó¥ÔºåÂ∞ùËØïÂæàÂ§öÊñπÊ≥ï‰∏çË°åÔºâ„ÄÇÊúÄÂêéÁ¥¢ÊÄßÂ∞Ü influx-javaÁöÑÊ∫ê‰ª£Á†ÅÂÖ®ÈÉ®Ê∑ªÂä†Âà∞È°πÁõÆ‰∏≠Âç≥ÂèØÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫„ÄÇ3.ËøêË°åOnLineLogAanlysis2.javahttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.javaÊØîÂ¶Ç logtype_count,host_service_logtype=hadoopnn-01_namenode_WARN count=12logtype_count ÊòØË°®host_service_logtype=hadoopnn-01_namenode_WARN ÊòØ tag‚ÄìÊ†áÁ≠æÔºåÂú®InfluxDB‰∏≠ÔºåtagÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÈáçË¶ÅÁöÑÈÉ®ÂàÜÔºåË°®Âêç+tag‰∏ÄËµ∑‰Ωú‰∏∫Êï∞ÊçÆÂ∫ìÁöÑÁ¥¢ÂºïÔºåÊòØ‚Äúkey-value‚ÄùÁöÑÂΩ¢Âºè„ÄÇcount=12 ÊòØ field‚ÄìÊï∞ÊçÆÔºåfield‰∏ªË¶ÅÊòØÁî®Êù•Â≠òÊîæÊï∞ÊçÆÁöÑÈÉ®ÂàÜÔºå‰πüÊòØ‚Äúkey-value‚ÄùÁöÑÂΩ¢Âºè„ÄÇtag„ÄÅfield ‰∏≠Èó¥ÊòØË¶ÅÊúâÁ©∫Ê†ºÁöÑ4.influxdbÊü•ËØ¢Êï∞ÊçÆ]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãinfluxdb-1.2.0 InstallÂíåÊ¶ÇÂøµÔºåËØ≠Ê≥ïÁ≠âÂ≠¶‰π†]]></title>
    <url>%2F2018%2F09%2F17%2F14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.‰∏ãËΩΩrpmhttps://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpmÊàëÈÄâÊã©Áî®window7 ÊµèËßàÂô®‰∏ãËΩΩÔºåÁÑ∂Âêérz‰∏ä‰º†Âà∞linuxÊú∫Âô®‰∏ä2.ÂÆâË£Öyum install influxdb-1.2.0.x86_64.rpm3.ÂêØÂä®service influxdb startÂèÇËÄÉ:https://docs.influxdata.com/influxdb/v1.2/introduction/installation/ÁºñËØëÂÆâË£Ö:https://anomaly.io/compile-influxdb/4.ËøõÂÖ•123[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0ËØ≠Ê≥ïÂèÇËÄÉ:https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/Â≠¶‰π†url:http://www.linuxdaxue.com/influxdb-study-series-manual.html]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãËàçÂºÉRedis+echarts3,ÈÄâÊã©InfluxDB+Grafana]]></title>
    <url>%2F2018%2F09%2F17%2F13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis%2Becharts3%2C%E9%80%89%E6%8B%A9InfluxDB%2BGrafana%2F</url>
    <content type="text"><![CDATA[1.ÊúÄÂàùÈÄâÊã©Redis‰Ωú‰∏∫Â≠òÂÇ®ÔºåÊòØ‰∏ªË¶ÅÊúâ4‰∏™ÂéüÂõ†:a.redisÊòØ‰∏Ä‰∏™key-valueÁöÑÂ≠òÂÇ®Á≥ªÁªüÔºåÊï∞ÊçÆÊòØÂ≠òÂÇ®Âú®ÂÜÖÂ≠ò‰∏≠ÔºåËØªÂÜôÊÄßËÉΩÂæàÈ´òÔºõb.ÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÁ±ªÂûãÔºåÂ¶Çset,zset,list,hash,stringÔºõc.keyËøáÊúüÁ≠ñÁï•Ôºõd.ÊúÄ‰∏ªË¶ÅÊòØÁΩë‰∏äÁöÑÂçöÂÆ¢ÂÖ®ÊòØsparkstreaming+redisÔºåÈÉΩ‰∫íÁõ∏Ê®°‰ªøÔºõËá≥‰∫éÁº∫ÁÇπÔºåÂΩìÊó∂ËøòÊ≤°ËÄÉËôëÂà∞„ÄÇ2.ÁÑ∂ÂêéÂºÄÂßãÊ∑ªÂä†CDHRolelog.classÁ±ªÂíåÂ∞ÜredisÊ®°ÂùóÂä†ÂÖ•‰ª£Á†Å‰∏≠Ôºå‰ΩøËÆ°ÁÆóÁªìÊûúÔºàÊú¨Ê¨°‰ΩøÁî®spark streaming+spark sqlÔºå‰πãÂâç‰ªÖ‰ªÖÊòØspark streamingÔºåÂÖ∑‰ΩìÁúã‰ª£Á†ÅÔºâÂ≠òÂÇ®Âà∞redis‰∏≠ÔºåÂΩìÁÑ∂Â≠òÂÇ®Âà∞redis‰∏≠ÔºåÊúâ‰∏§ÁßçÂ≠òÂÇ®Ê†ºÂºè„ÄÇ2.1 key‰∏∫Êú∫Âô®ÂêçÁß∞,ÊúçÂä°ÂêçÁß∞,Êó•ÂøóÁ∫ßÂà´ÊãºÊé•ÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÂ¶Çhadoopnn-01_namenode_WARNÔºåvalue‰∏∫Êï∞ÊçÆÁ±ªÂûãlistÔºåÂÖ∂Â≠òÂÇ®‰∏∫jsonÊ†ºÂºèÁöÑ [{‚ÄútimeStamp‚Äù: ‚Äú2017-02-09 17:16:14.249‚Äù,‚ÄùhostName‚Äù: ‚Äúhadoopnn-01‚Äù,‚ÄùserviceName‚Äù: ‚Äúnamenode‚Äù,‚ÄùlogType‚Äù:‚ÄùWARN‚Äù,‚Äùcount‚Äù:‚Äù12‚Äù }]‰ª£Á†Åurl,‰∏ãËΩΩÂØºÂÖ•idea,ËøêË°åÂç≥ÂèØ:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java2.2 key‰∏∫timestampÂ¶Ç 2017-02-09 18:09:02.462,value ‰∏∫ [ {‚Äúhost_service_logtype‚Äù: ‚Äúhadoopnn-01_namenode_INFO‚Äù,‚Äùcount‚Äù:‚Äù110‚Äù }, {‚Äúhost_service_logtype‚Äù: ‚Äúhadoopnn-01_namenode_DEBUG‚Äù,‚Äùcount‚Äù:‚Äù678‚Äù }, {‚Äúhost_service_logtype‚Äù: ‚Äúhadoopnn-01_namenode_WARN‚Äù,‚Äùcount‚Äù:‚Äù12‚Äù }]‰ª£Á†Åurl,‰∏ãËΩΩÂØºÂÖ•idea,ËøêË°åÂç≥ÂèØ:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java3.ÂÅöÂèØËßÜÂåñËøôÂùóÔºåÊàë‰ª¨ÈÄâÊã©adminLTE+flask+echarts3, ËÆ°ÂàíÂíåÁºñÁ®ãÂºÄÂèëÂ∞ùËØïÂéª‰ªéredisÂÆûÊó∂ËØªÂèñÊï∞ÊçÆÔºåÂä®ÊÄÅÁªòÂà∂ÂõæË°®ÔºõÂêéÊù•ÂºÄÂèëË∞ÉÁ†îÂ§ßÊ¶Ç1Âë®ÔºåÊúÄÁªà2.1 Âíå2.2ÊñπÊ≥ïÁöÑÂ≠òÂÇ®Ê†ºÂºèÈÉΩ‰∏çËÉΩÊúâÊïàÈÄÇÂêàÊàë‰ª¨ÔºåËøõË°åÂºÄÂèëÂèØËßÜÂåñDashboardÔºåÊâÄ‰ª•Êàë‰ª¨ÊúÄÁªàË∞ÉÁ†îÈááÂèñInfluxDB+GrafanaÊù•ÂÅöÂ≠òÂÇ®ÂíåÂèØËßÜÂåñÂ±ïÁ§∫ÂèäÈ¢ÑË≠¶„ÄÇ4.InfluxDBÊòØÊó∂Â∫èÊï∞ÊçÆÂ∫ìhttps://docs.influxdata.com/influxdb/v1.2/5.GrafanaÊòØÂèØËßÜÂåñÁªÑ‰ª∂http://grafana.org/https://github.com/grafana/grafana]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãRedisLiveÁõëÊéßÂ∑•ÂÖ∑ÁöÑËØ¶ÁªÜÂÆâË£Ö]]></title>
    <url>%2F2018%2F09%2F14%2F12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1GitHub: https://github.com/nkrode/RedisLive1.ÂÆâË£Öpython2.7.5 Âíåpip1http://blog.itpub.net/30089851/viewspace-2132450/2.‰∏ãËΩΩRedisLive123456789101112[root@sht-sgmhadoopdn-04 app]# wget https://github.com/nkrode/RedisLive/archive/master.zip[root@sht-sgmhadoopdn-04 app]# unzip master [root@sht-sgmhadoopdn-04 app]# mv RedisLive-master RedisLive[root@sht-sgmhadoopdn-04 app]# cd RedisLive/[root@sht-sgmhadoopdn-04 RedisLive]# lltotal 20drwxr-xr-x 2 root root 4096 Aug 20 2015 design-rw-r--r-- 1 root root 1067 Aug 20 2015 MIT-LICENSE.txt-rw-r--r-- 1 root root 902 Aug 20 2015 README.md-rw-r--r-- 1 root root 58 Aug 20 2015 requirements.txtdrwxr-xr-x 7 root root 4096 Aug 20 2015 src[root@sht-sgmhadoopdn-04 RedisLive]#3.Êü•ÁúãÁâàÊú¨Ë¶ÅÊ±Ç(ÂàöÂºÄÂßãÂÆâË£ÖÊ≤°Ê≥®ÊÑèÁâàÊú¨ÔºåÁõ¥Êé•pipÂØºËá¥ÂêéÈù¢ÂêÑÁßçÈóÆÈ¢òÔºåÊâÄ‰ª•ËØ∑‰ªîÁªÜÁúã‰∏ãÈù¢ËøáÁ®ã)123456[root@sht-sgmhadoopdn-04 RedisLive]# cat requirements.txtargparse==1.2.1python-dateutil==1.5redistornado==2.1.1[root@sht-sgmhadoopdn-04 RedisLive]# cd ../4.pipÂÆâË£ÖÁéØÂ¢ÉË¶ÅÊ±Ç1234[root@sht-sgmhadoopdn-04 app]# pip install tornado[root@sht-sgmhadoopdn-04 app]# pip install redis[root@sht-sgmhadoopdn-04 app]# pip install python-dateutil[root@sht-sgmhadoopdn-04 app]# pip install argparse5.ËøõÂÖ• /root/learnproject/app/RedisLive/srcÁõÆÂΩï,ÈÖçÁΩÆredis-live.confÊñá‰ª∂12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopdn-04 app]# cd -/root/learnproject/app/RedisLive[root@sht-sgmhadoopdn-04 RedisLive]# cd src[root@sht-sgmhadoopdn-04 src]# lltotal 40drwxr-xr-x 4 root root 4096 Aug 20 2015 apidrwxr-xr-x 2 root root 4096 Aug 20 2015 dataproviderdrwxr-xr-x 2 root root 4096 Aug 20 2015 db-rw-r--r-- 1 root root 0 Aug 20 2015 __init__.py-rw-r--r-- 1 root root 381 Aug 20 2015 redis-live.conf.example-rwxr-xr-x 1 root root 1343 Aug 20 2015 redis-live.py-rwxr-xr-x 1 root root 9800 Aug 20 2015 redis-monitor.pydrwxr-xr-x 2 root root 4096 Aug 20 2015 utildrwxr-xr-x 4 root root 4096 Aug 20 2015 wwwYou have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp redis-live.conf.example redis-live.conf[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# vi redis-live.conf&#123; &quot;RedisServers&quot;: [ &#123; &quot;server&quot;: &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125; ], &quot;DataStoreType&quot; : &quot;redis&quot;, &quot;RedisStatsServer&quot;: &#123; &quot;server&quot; : &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125;&#125;6.Á¨¨‰∏ÄÊ¨°Â∞ùËØïÂêØÂä®redis-monitor.pyÊäõÈîô _sqlite312345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named _sqlite3[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite-devel[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite[root@sht-sgmhadoopdn-04 ~]# find / -name _sqlite3.so/usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so/usr/local/Python-2.7.5/build/lib.linux-x86_64-2.7/_sqlite3.so/usr/lib64/python2.6/lib-dynload/_sqlite3.so[root@sht-sgmhadoopdn-04 ~]# cp /usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 ~]# pythonPython 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sqlite3&gt;&gt;&gt;ÂèÇËÄÉ: http://ju.outofmemory.cn/entry/976587.Á¨¨‰∫åÊ¨°Â∞ùËØïÂêØÂä®redis-monitor.pyÊäõÈîô redis12345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named redis[root@sht-sgmhadoopdn-04 src]# find / -name redis/etc/rc.d/init.d/redis/root/learnproject/app/redis/root/learnproject/app/redis-monitor/src/main/java/sun/redis/root/learnproject/app/redis-monitor/src/test/java/sun/redis/usr/local/redis/usr/local/python27/lib/python2.7/site-packages/redis[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/redis /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 src]# python Python 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import redis8.Á¨¨‰∏âÊ¨°Â∞ùËØïÂêØÂä®redis-monitor.pyÔºåÊàêÂäüÔºõÊåâctrl+c‰∏≠Êñ≠Êéâ1234[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ^Cshutting down...You have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]#9.Â∞ùËØïÁ¨¨‰∏ÄÊ¨°ÂêØÂä®redis-live.py Ôºåtornado.ioloop12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 3, in &lt;module&gt; import tornado.ioloopImportError: No module named tornado.ioloop[root@sht-sgmhadoopdn-04 src]# find / -name tornado/usr/local/python27/lib/python2.7/site-packages/tornado[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/tornado /usr/local/lib/python2.7/lib-dynload/10.Â∞ùËØïÁ¨¨‰∫åÊ¨°ÂêØÂä®redis-live.py Ôºåsingledispatch123456789[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 6, in &lt;module&gt; import tornado.web File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/web.py&quot;, line 84, in &lt;module&gt; from tornado import gen File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/gen.py&quot;, line 98, in &lt;module&gt; from singledispatch import singledispatch # backportImportError: No module named singledispatchËøô‰∏™ singledispatch ÈîôËØØÔºåÂÖ∂ÂÆûÂ∞±ÊòØÂú®tornadoÈáåÁöÑÔºåË∞∑Ê≠åÂíåÊÄùËÄÉËøáÂêéÔºåÊÄÄÁñëÊòØÁâàÊú¨ÈóÆÈ¢òÔºå‰∫éÊòØÊûúÊñ≠Âç∏ËΩΩtornado12345[root@sht-sgmhadoopdn-04 src]# pip uninstall tornado[root@sht-sgmhadoopdn-04 src]# rm -rf /usr/local/lib/python2.7/lib-dynload/tornado[root@sht-sgmhadoopdn-04 src]# find / -name tornado[root@sht-sgmhadoopdn-04 src]# ÂÅáÂ¶ÇfindÊúâÁöÑËØù ÔºåÂ∞±Ë¶ÅÊâãÂ∑•Âà†Èô§Êéâ11.‰∫éÊòØÊÉ≥ÊÉ≥ÂÖ∂‰ªñ‰πüÊòØË¶ÅÂç∏ËΩΩÊéâ12345[root@sht-sgmhadoopdn-04 src]# pip uninstall argparse[root@sht-sgmhadoopdn-04 src]# pip uninstall python-dateutil[root@sht-sgmhadoopdn-04 src]# find / -name argparse[root@sht-sgmhadoopdn-04 src]# find / -name python-dateutilÂÅáÂ¶ÇfindÊúâÁöÑËØù ÔºåÂ∞±Ë¶ÅÊâãÂ∑•Âà†Èô§Êéâ12.ÂÖ≥ÈîÆ‰∏ÄÊ≠•: Ê†πÊçÆstep3ÁöÑÊåáÂÆöÁâàÊú¨Êù•ÂÆâË£Ö123[root@sht-sgmhadoopdn-04 src]# pip install -v tornado==2.1.1[root@sht-sgmhadoopdn-04 src]# pip install -v argparse==1.2.1[root@sht-sgmhadoopdn-04 src]# pip install -v python-dateutil==1.513.ÂÜçÊ¨°Â∞ùËØïÂêØÂä®redis-live.py ÔºåÊäõÈîôdateutil.parser1234567891011121314151617[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 10, in &lt;module&gt; from api.controller.ServerListController import ServerListController File &quot;/root/learnproject/app/RedisLive/src/api/controller/ServerListController.py&quot;, line 1, in &lt;module&gt; from BaseController import BaseController File &quot;/root/learnproject/app/RedisLive/src/api/controller/BaseController.py&quot;, line 4, in &lt;module&gt; import dateutil.parserImportError: No module named dateutil.parser[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# find / -name dateutil/usr/local/python27/lib/python2.7/site-packages/dateutil[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/dateutil /usr/local/lib/python2.7/lib-dynload/You have mail in /var/spool/mail/root14.ÂÜçÂú®Â∞ùËØïÂêØÂä®redis-live.py ÔºåÊàêÂäü‰∫ÜÔºåÁÑ∂ÂêéÊåâctrl+c‰∏≠Êñ≠Êéâ12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py ^CTraceback (most recent call last): File &quot;./redis-live.py&quot;, line 36, in &lt;module&gt; tornado.ioloop.IOLoop.instance().start() File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/ioloop.py&quot;, line 283, in start event_pairs = self._impl.poll(poll_timeout)KeyboardInterrupt[root@sht-sgmhadoopdn-04 src]#15.ÂêØÂä®12[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 &amp;[root@sht-sgmhadoopdn-04 src]# ./redis-live.py &amp;ÊâìÂºÄwebÁïåÈù¢http://172.16.101.66:8888/index.html16.ÊÄªÁªìa.ÂÆâË£Ö python2.7+pipb.pipÊåáÂÆöÁâàÊú¨ÂéªÂÆâË£ÖÈÇ£Âá†‰∏™ÁªÑ‰ª∂17.ËØ¥Êòé:redis live ÂÆûÊó∂redisÁõëÊéßÈù¢ÊùøÂèØ‰ª•ÂêåÊó∂ÁõëÊéßÂ§ö‰∏™redisÂÆû‰æã , ÂåÖÊã¨ ÂÜÖÂ≠ò‰ΩøÁî® „ÄÅÂàÜdbÊòæÁ§∫ÁöÑkeyÊï∞„ÄÅÂÆ¢Êà∑Á´ØËøûÊé•Êï∞„ÄÅ ÂëΩ‰ª§Â§ÑÁêÜÊï∞„ÄÅ Á≥ªÁªüËøêË°åÊó∂Èó¥ , ‰ª•ÂèäÂêÑÁßçÁõ¥ËßÇÁöÑÊäòÁ∫øÂõæÊü±Áä∂Âõæ.Áº∫ÁÇπÊòØ‰ΩøÁî®‰∫Ümonitor ÂëΩ‰ª§ÁõëÊéß , ÂØπÊÄßËÉΩÊúâÂΩ±Âìç ,ÊúÄÂ•Ω‰∏çË¶ÅÈïøÊó∂Èó¥ÂêØÂä® .redis-monitor.py:Áî®Êù•Ë∞ÉÁî®redisÁöÑmonitorÂëΩ‰ª§Êù•Êî∂ÈõÜredisÁöÑÂëΩ‰ª§Êù•ËøõË°åÁªüËÆ°redis-live.py:ÂêØÂä®webÊúçÂä°]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãredis-3.2.5 install(ÂçïËäÇÁÇπ)]]></title>
    <url>%2F2018%2F09%2F12%2F11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)%2F</url>
    <content type="text"><![CDATA[1.ÂÆâË£Öjdk1.8123456789101112[root@sht-sgmhadoopdn-04 ~]# cd /usr/java/[root@sht-sgmhadoopdn-04 java]# wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# tar -zxvf jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.8.0_111export path=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-04 java]# source /etc/profile[root@sht-sgmhadoopdn-04 java]# java -versionjava version &quot;1.8.0_111&quot;Java(TM) SE Runtime Environment (build 1.8.0_111-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)[root@sht-sgmhadoopdn-04 java]#2.ÂÆâË£Ö redis 3.2.52.1 ÂÆâË£ÖÁºñÁªéÊâÄÈúÄÂåÖgcc,tcl12[root@sht-sgmhadoopdn-04 local]# yum install gcc[root@sht-sgmhadoopdn-04 local]# yum install tcl2.2 ‰∏ãËΩΩredis-3.2.5123456789[root@sht-sgmhadoopdn-04 local]# wget http://download.redis.io/releases/redis-3.2.5.tar.gz--2016-11-12 20:16:40-- http://download.redis.io/releases/redis-3.2.5.tar.gzResolving download.redis.io (download.redis.io)... 109.74.203.151Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1544040 (1.5M) [application/x-gzip]Saving to: ‚Äòredis-3.2.5.tar.gz‚Äô100%[==========================================================================================================================&gt;] 1,544,040 221KB/s in 6.8s 2016-11-12 20:16:47 (221 KB/s) - ‚Äòredis-3.2.5.tar.gz‚Äô saved [1544040/1544040]2.3 ÂÆâË£Öredis12345678910111213[root@sht-sgmhadoopdn-04 local]# mkdir /usr/local/redis[root@sht-sgmhadoopdn-04 local]# tar xzvf redis-3.2.5.tar.gz[root@sht-sgmhadoopdn-04 local]# cd redis-3.2.5[root@sht-sgmhadoopdn-04 redis-3.2.5]# make PREFIX=/usr/local/redis install[root@sht-sgmhadoopdn-04 redis-3.2.5]# cd ../[root@sht-sgmhadoopdn-04 redis-3.2.5]# ll /usr/local/redis/bin/total 15056-rwxr-xr-x 1 root root 2431728 Nov 12 20:45 redis-benchmark-rwxr-xr-x 1 root root 25165 Nov 12 20:45 redis-check-aof-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-check-rdb-rwxr-xr-x 1 root root 2584443 Nov 12 20:45 redis-clilrwxrwxrwx 1 root root 12 Nov 12 20:45 redis-sentinel -&gt; redis-server-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-server2.4 ÈÖçÁΩÆredis‰∏∫ÊúçÂä°1234567891011121314[root@server redis-3.2.5]# cp utils/redis_init_script /etc/rc.d/init.d/redis[root@server redis-3.2.5]# vi /etc/rc.d/init.d/redis Âú®Á¨¨‰∫åË°åÊ∑ªÂä†Ôºö#chkconfig: 2345 80 90EXEC=/usr/local/bin/redis-server ‰øÆÊîπÊàê EXEC=/usr/local/redis/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cli ‰øÆÊîπÊàê CLIEXEC=/usr/local/redis/bin/redis-cliCONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot; ‰øÆÊîπÊàê CONF=&quot;/usr/local/redis/conf/$&#123;REDISPORT&#125;.conf&quot;$EXEC $CONF ‰øÆÊîπÊàê $EXEC $CONF &amp;[root@server redis-3.2.5]# mkdir /usr/local/redis/conf/[root@server redis-3.2.5]# chkconfig --add redis[root@server redis-3.2.5]# cp redis.conf /usr/local/redis/conf/6379.conf [root@server redis-3.2.5]# vi /usr/local/redis/conf/6379.conf daemonize yespidfile /var/run/redis_6379.pidbind 172.16.101.662.5 ÂêØÂä®redis123456[root@server redis-3.2.5]# cd ../redis[root@sht-sgmhadoopdn-04 redis]# service redis startStarting Redis server...[root@sht-sgmhadoopdn-04 redis]# netstat -tnlp|grep redistcp 0 0 172.16.100.79:6379 0.0.0.0:* LISTEN 30032/redis-server [root@sht-sgmhadoopdn-04 redis]#2.6 Ê∑ªÂä†ÁéØÂ¢ÉÂèòÈáè123456[root@sht-sgmhadoopdn-04 redis]# vi /etc/profileexport REDIS_HOME=/usr/local/redisexport PATH=$REDIS_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@sht-sgmhadoopdn-04 redis]# source /etc/profile[root@sht-sgmhadoopdn-04 redis]# which redis-cli/usr/local/redis/bin/redis-cli2.7 ÊµãËØï Âíå ËÆæÁΩÆÂØÜÁ†Å(Êú¨Ê¨°ÂÆûÈ™åÊú™ËÆæÁΩÆÂØÜÁ†Å)1234567891011121314151617181920212223242526[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; sht-sgmhadoopdn-04:6379&gt; set testkey testvalue OKsht-sgmhadoopdn-04:6379&gt; get test(nil)sht-sgmhadoopdn-04:6379&gt; get testkey&quot;testvalue&quot;sht-sgmhadoopdn-04:6379&gt;[root@sht-sgmhadoopdn-04 redis]# vi /usr/local/redis/conf/6379.conf /*Ê∑ªÂä†‰∏Ä‰∏™È™åËØÅÂØÜÁ†Å*/requirepass 123456[root@sht-sgmhadoopdn-04 redis]# service redis stop[root@sht-sgmhadoopdn-04 redis]# service redis start[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; set key ss(error) NOAUTH Authentication required. [root@server redis-3.2.5]# redis-cli -h sht-sgmhadoopdn-04 -a 123456sht-sgmhadoopdn-04:6379&gt; set a bOKsht-sgmhadoopdn-04:6379&gt; get a&quot;b&quot;sht-sgmhadoopdn-04:6379&gt; exit;[root@sht-sgmhadoopdn-04 redis]#]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãÂü∫‰∫éSpark StreamingÂºÄÂèëOnLineLogAanlysis1]]></title>
    <url>%2F2018%2F09%2F11%2F10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1%2F</url>
    <content type="text"><![CDATA[1.GitHubhttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java2.‰ΩøÁî®IDEA Êú¨Âú∞ËøêË°åÊµãËØïÔºàÊú™ÊâìjarÂåÖÔºâ]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãÂü∫‰∫éSpark Streaming DirectÊñπÂºèÁöÑWordCountÊúÄËØ¶ÁªÜÊ°à‰æã(javaÁâà)]]></title>
    <url>%2F2018%2F09%2F10%2F09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)%2F</url>
    <content type="text"><![CDATA[1.ÂâçÊèêa. flume Êî∂ÈõÜ‚Äì„Äãflume ËÅöÂêà‚Äì„Äãkafka ÔºåÂêØÂä®ËøõÁ®ãÂíåÂêØÂä®kafka managerÁõëÊéßÔºåÂèÇËÄÉ08„ÄêÂú®Á∫øÊó•ÂøóÂàÜÊûê„Äë‰πãFlume Agent(ËÅöÂêàËäÇÁÇπ) sink to kafka clusterb.window7 ÂÆâË£Öjdk1.7 ÊàñËÄÖ1.8(Êú¨Ê¨°ÁéØÂ¢ÉÊòØ1.8)c.window7 ÂÆâË£ÖIDEAÂºÄÂèëÂ∑•ÂÖ∑(‰ª•‰∏ã‰ªÖ‰æõÂèÇËÄÉ)‰ΩøÁî®IntelliJ IDEA ÈÖçÁΩÆMavenÔºàÂÖ•Èó®Ôºâ:http://blog.csdn.net/qq_32588349/article/details/51461182IDEA Java/ScalaÊ∑∑ÂêàÈ°πÁõÆMavenÊâìÂåÖ:http://blog.csdn.net/rongyongfeikai2/article/details/51404611Intellij idea‰ΩøÁî®javaÁºñÂÜôÂπ∂ÊâßË°åsparkÁ®ãÂ∫è:http://blog.csdn.net/yhao2014/article/details/442390212.Ê∫ê‰ª£Á†ÅÔºàÂèØ‰∏ãËΩΩÂçï‰∏™javaÊñá‰ª∂ÔºåÂä†ÂÖ•projet ÊàñËÄÖ Êï¥‰∏™Â∑•Á®ã‰∏ãËΩΩÔºåIDEAÈÄâÊã©open Âç≥ÂèØÔºâGitHub: https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java3.‰ΩøÁî®IDEA Êú¨Âú∞ËøêË°åÊµãËØïÔºàÊú™ÊâìjarÂåÖÔºâÊµ∑Â∫∑Â®ÅËßÜÊ†°ÊãõÁîµËØùÈù¢ËØïÔºö1.Êï∞ÊçÆÂÄæÊñúÁöÑËß£ÂÜ≥ÔºåÊÄé‰πàÁü•ÈÅìÂì™ÈáåÂÄæÊñú2.Ëá™ÂÆö‰πâÁ±ªÁöÑÂπøÊí≠3.cacheÊú∫Âà∂ÔºårddÂíådfÁöÑcache‰ªÄ‰πàÂå∫Âà´4.sparkÂä®ÊÄÅÂÜÖÂ≠òÔºåÂ†ÜÂÜÖÂíåÂ†ÜÂ§ñ5.rddÁÆóÂ≠êÔºåmap,mappartitions,foreachÔºåunion6.ÂÆΩ‰æùËµñÔºåÁ™Ñ‰æùËµñ7.spark DAGËøáÁ®ãÔºådoOnreciveÔºåeventloopÊâßË°åËøáÁ®ã8.stageÂíåtaskÊÄé‰πàÂàÜÁ±ª9.sparkË∞É‰ºò10.Ê¶ÇÂøµÔºåexecutorÔºåworkerÔºåjob,taskÂíåpartitionÁöÑÂÖ≥Á≥ª11.Áî®Ê≤°Áî®Ëøáspark‰ªÄ‰πàlogÔºåÊ≤°ËÆ∞‰Ωè12.ËÆ≤ËÆ≤sparkSQLÊï∞ÊçÆÊ∏ÖÊ¥óËøáÁ®ã13.ÊçéÂ∏¶‰∏ÄÁÇπÈ°πÁõÆ]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãFlume Agent(ËÅöÂêàËäÇÁÇπ) sink to kafka cluster]]></title>
    <url>%2F2018%2F09%2F07%2F08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster%2F</url>
    <content type="text"><![CDATA[1.ÂàõÂª∫logtopic1[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic logtopic2.ÂàõÂª∫avro_memory_kafka.properties (kafka sink)12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# cp avro_memory_hdfs.properties avro_memory_kafka.properties[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_kafka.properties #Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = logtopica1.sinks.k1.kafka.bootstrap.servers = 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092a1.sinks.k1.kafka.flumeBatchSize = 6000a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1a1.sinks.ki.kafka.producer.compression.type = snappy#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90a1.channels.c1.capacity = 2000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c13.ÂêéÂè∞ÂêØÂä® flume-ng agent(ËÅöÂêàËäÇÁÇπ)ÂíåÊü•Áúãnohup.out123456789[root@sht-sgmhadoopcm-01 ~]# source /etc/profile[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/[root@sht-sgmhadoopcm-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_kafka.properties -n a1 -Dflume.root.logger=INFO,console &amp;[1] 4971[root@sht-sgmhadoopcm-01 flume-ng]# nohup: ignoring input and appending output to `nohup.out&apos;[root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# cat nohup.out4.Ê£ÄÊü•logÊî∂ÈõÜÁöÑ‰∏âÂè∞(Êî∂ÈõÜËäÇÁÇπ)ÂºÄÂêØÊ≤°12345678[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;5.ÊâìÂºÄkafka managerÁõëÊéßhttp://172.16.101.55:9999]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãkafka-managerÁõëÊéßÂ∑•ÂÖ∑ÁöÑÊê≠Âª∫(sbtÂÆâË£Ö‰∏éÁºñËØë)]]></title>
    <url>%2F2018%2F09%2F06%2F07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)%2F</url>
    <content type="text"><![CDATA[1.‰∏ãËΩΩsbt123456http://www.scala-sbt.org/download.html[root@sht-sgmhadoopnn-01 app]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring sbt-0.13.13.tgz... 100% 1025 KB 1025 KB/sec 00:00:01 0 Errors2.Ëß£Âéã1234567891011[root@sht-sgmhadoopnn-01 app]# tar -zxvf sbt-0.13.13.tgzsbt-launcher-packaging-0.13.13/sbt-launcher-packaging-0.13.13/conf/sbt-launcher-packaging-0.13.13/conf/sbtconfig.txtsbt-launcher-packaging-0.13.13/conf/sbtoptssbt-launcher-packaging-0.13.13/bin/sbt-launcher-packaging-0.13.13/bin/sbt.batsbt-launcher-packaging-0.13.13/bin/sbtsbt-launcher-packaging-0.13.13/bin/sbt-launch.jarsbt-launcher-packaging-0.13.13/bin/sbt-launch-lib.bash[root@sht-sgmhadoopnn-01 app]# mv sbt-launcher-packaging-0.13.13 sbt3.Ê∑ªÂä†ËÑöÊú¨Êñá‰ª∂12345[root@sht-sgmhadoopnn-01 bin]# vi sbt#!/usr/bin/env bashBT_OPTS=&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;java $SBT_OPTS -jar /root/learnproject/app/sbt/bin/sbt-launch.jar &quot;$@&quot;4.‰øÆÊîπÊùÉÈôêÂíåÁéØÂ¢ÉÂèòÈáè123456[root@sht-sgmhadoopnn-01 bin]# chmod u+x sbt[root@sht-sgmhadoopnn-01 bin]# vi /etc/profileexport SBT_HOME=/root/learnproject/app/sbtexport PATH=$SBT_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&quot;/etc/profile&quot; 94L, 2265C written[root@sht-sgmhadoopnn-01 bin]# source /etc/profile5.ÊµãËØï12345678/*Á¨¨‰∏ÄÊ¨°ÊâßË°åÊó∂Ôºå‰ºö‰∏ãËΩΩ‰∏Ä‰∫õÊñá‰ª∂ÂåÖÔºåÁÑ∂ÂêéÊâçËÉΩÊ≠£Â∏∏‰ΩøÁî®ÔºåË¶ÅÁ°Æ‰øùËÅîÁΩë‰∫ÜÔºåÂÆâË£ÖÊàêÂäüÂêéÊòæÁ§∫Â¶Ç‰∏ã*/[root@sht-sgmhadoopnn-01 bin]# sbt sbt-version[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[root@sht-sgmhadoopnn-01 bin]#]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πã KafkaOffsetMonitorÁõëÊéßÂ∑•ÂÖ∑ÁöÑÊê≠Âª∫]]></title>
    <url>%2F2018%2F09%2F05%2F06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.‰∏ãËΩΩÂú®window7 ÊâãÂ∑•‰∏ãËΩΩÂ•Ω‰∏ãÈù¢ÁöÑÈìæÊé•1https://github.com/quantifind/KafkaOffsetMonitor/releases/tag/v0.2.112345678910[root@sht-sgmhadoopnn-01 app]# mkdir kafkaoffsetmonitor[root@sht-sgmhadoopnn-01 app]# cd kafkaoffsetmonitor#‰ΩøÁî®rzÂëΩ‰ª§‰∏ä‰º†[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring KafkaOffsetMonitor-assembly-0.2.1.jar... 100% 51696 KB 12924 KB/sec 00:00:04 0 Errors You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#2.Êñ∞Âª∫‰∏Ä‰∏™kafkaMonitor.shÊñá‰ª∂ÔºåÊñá‰ª∂ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö12345678910[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# vi kafkaoffsetmonitor.sh! /bin/bashjava -cp KafkaOffsetMonitor-assembly-0.2.1.jar \com.quantifind.kafka.offsetapp.OffsetGetterWeb \--zk 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka \--port 8089 \--refresh 5.seconds \--retain 7.days[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# chmod +x *.sh[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#ÂèÇÊï∞ËØ¥ÊòéÔºö‚Äìzk ËøôÈáåÂÜôÁöÑÂú∞ÂùÄÂíåÁ´ØÂè£ÔºåÊòØzookeeperÈõÜÁæ§ÁöÑÂêÑ‰∏™Âú∞ÂùÄÂíåÁ´ØÂè£„ÄÇÂ∫îÂíåkafka/binÊñá‰ª∂Â§π‰∏≠ÁöÑzookeeper.properties‰∏≠ÁöÑhost.nameÂíåclientPort‰∏ÄËá¥„ÄÇ‚Äìport Ëøô‰∏™ÊòØÊú¨ËΩØ‰ª∂KafkaOffsetMonitorÁöÑÁ´ØÂè£„ÄÇÊ≥®ÊÑè‰∏çË¶Å‰ΩøÁî®ÈÇ£‰∫õËëóÂêçÁöÑÁ´ØÂè£Âè∑Ôºå‰æãÂ¶Ç80,8080Á≠â„ÄÇÊàëÈááÁî®‰∫Ü8089.‚Äìrefresh Ëøô‰∏™ÊòØËΩØ‰ª∂Âà∑Êñ∞Èó¥ÈöîÊó∂Èó¥Ôºå‰∏çË¶ÅÂ§™Áü≠‰πü‰∏çË¶ÅÂ§™Èïø„ÄÇ‚Äìretain Ëøô‰∏™ÊòØÊï∞ÊçÆÂú®Êï∞ÊçÆÂ∫ì‰∏≠‰øùÂ≠òÁöÑÊó∂Èó¥„ÄÇ3.ÂêéÂè∞ÂêØÂä®1234567891011121314 1[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# nohup ./kafkaoffsetmonitor.sh &amp; 2serving resources from: jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp 3SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;. 4SLF4J: Defaulting to no-operation (NOP) logger implementation 5SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. 6log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection). 7log4j:WARN Please initialize the log4j system properly. 8log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 9log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).10log4j:WARN Please initialize the log4j system properly.11log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.122016-12-25 22:00:24.252:INFO:oejs.Server:jetty-7.x.y-SNAPSHOT132016-12-25 22:00:24.319:INFO:oejsh.ContextHandler:started o.e.j.s.ServletContextHandler&#123;/,jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp&#125;142016-12-25 22:00:24.328:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:80894.IEÊµèËßàÂô®ÊâìÂºÄ1http://172.16.101.55:8089]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãKafka 0.10.1.0 ClusterÁöÑÊê≠Âª∫ÂíåTopicÁÆÄÂçïÊìç‰ΩúÂÆûÈ™å]]></title>
    <url>%2F2018%2F09%2F04%2F05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[„Äêkafka clusterÊú∫Âô®„Äë:Êú∫Âô®ÂêçÁß∞ Áî®Êà∑ÂêçÁß∞sht-sgmhadoopdn-01/02/03 root„ÄêÂÆâË£ÖÁõÆÂΩï„Äë: /root/learnproject/app1.Â∞ÜscalaÊñá‰ª∂Â§πÂêåÊ≠•Âà∞ÈõÜÁæ§ÂÖ∂‰ªñÊú∫Âô®(scala 2.11ÁâàÊú¨ÔºåÂèØÂçïÁã¨‰∏ãËΩΩËß£Âéã)123456789101112131415161718192021[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-01:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-02:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-03:/root/learnproject/app/#ÁéØÂ¢ÉÂèòÈáè[root@sht-sgmhadoopdn-01 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 app]# source /etc/profile[root@sht-sgmhadoopdn-02 app]# source /etc/profile[root@sht-sgmhadoopdn-03 app]# source /etc/profile2.‰∏ãËΩΩÂü∫‰∫éScala 2.11ÁöÑkafkaÁâàÊú¨‰∏∫0.10.1.012345[root@sht-sgmhadoopdn-01 app]# pwd/root/learnproject/app[root@sht-sgmhadoopdn-01 app]# wget http://www-eu.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz[root@sht-sgmhadoopdn-01 app]# tar xzvf kafka_2.11-0.10.1.0.tgz [root@sht-sgmhadoopdn-01 app]# mv kafka_2.11-0.10.1.0 kafka3.ÂàõÂª∫logsÁõÆÂΩïÂíå‰øÆÊîπserver.properties(ÂâçÊèêzookeeper clusterÈÉ®ÁΩ≤Â•ΩÔºåËßÅ‚Äú03„ÄêÂú®Á∫øÊó•ÂøóÂàÜÊûê„Äë‰πãhadoop-2.7.3ÁºñËØëÂíåÊê≠Âª∫ÈõÜÁæ§ÁéØÂ¢É(HDFS HA,Yarn HA)‚Äù )123456789[root@sht-sgmhadoopdn-01 app]# cd kafka[root@sht-sgmhadoopdn-01 kafka]# mkdir logs[root@sht-sgmhadoopdn-01 kafka]# cd config/[root@sht-sgmhadoopdn-01 config]# vi server.propertiesbroker.id=1port=9092host.name=172.16.101.58log.dirs=/root/learnproject/app/kafka/logszookeeper.connect=172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka4.ÂêåÊ≠•Âà∞02/03ÊúçÂä°Âô®ÔºåÊõ¥Êîπbroker.id Âèähost.name123456789101112[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-02 config]# vi server.properties broker.id=2port=9092host.name=172.16.101.59[root@sht-sgmhadoopdn-03 config]# vi server.properties broker.id=3port=9092host.name=172.16.101.605.ÁéØÂ¢ÉÂèòÈáè1234567891011[root@sht-sgmhadoopdn-01 kafka]# vi /etc/profileexport KAFKA_HOME=/root/learnproject/app/kafkaexport PATH=$KAFKA_HOME/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-02:/etc/profile[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-03:/etc/profile[root@sht-sgmhadoopdn-01 kafka]#[root@sht-sgmhadoopdn-01 kafka]# source /etc/profile[root@sht-sgmhadoopdn-02 kafka]# source /etc/profile[root@sht-sgmhadoopdn-03 kafka]# source /etc/profile6.ÂêØÂä®/ÂÅúÊ≠¢123456[root@sht-sgmhadoopdn-01 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-02 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-03 kafka]# nohup kafka-server-start.sh config/server.properties &amp;###ÂÅúÊ≠¢bin/kafka-server-stop.sh7.topicÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú12345678910111213141516171819202122232425262728293031323334353637a.ÂàõÂª∫topicÔºåÂ¶ÇËÉΩÊàêÂäüÂàõÂª∫topicÂàôË°®Á§∫ÈõÜÁæ§ÂÆâË£ÖÂÆåÊàêÔºå‰πüÂèØ‰ª•Áî®jpsÂëΩ‰ª§Êü•ÁúãkafkaËøõÁ®ãÊòØÂê¶Â≠òÂú®„ÄÇ[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic testb.ÈÄöËøálistÂëΩ‰ª§Êü•ÁúãÂàõÂª∫ÁöÑtopic:[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --list --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafkac.Êü•ÁúãÂàõÂª∫ÁöÑTopic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:1 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2[root@sht-sgmhadoopdn-01 kafka]# Á¨¨‰∏ÄË°åÂàóÂá∫‰∫ÜËøô‰∏™topicÁöÑÊÄª‰ΩìÊÉÖÂÜµÔºåÂ¶ÇtopicÂêçÁß∞ÔºåÂàÜÂå∫Êï∞ÈáèÔºåÂâØÊú¨Êï∞ÈáèÁ≠â„ÄÇÁ¨¨‰∫åË°åÂºÄÂßãÔºåÊØè‰∏ÄË°åÂàóÂá∫‰∫Ü‰∏Ä‰∏™ÂàÜÂå∫ÁöÑ‰ø°ÊÅØÔºåÂ¶ÇÂÆÉÊòØÁ¨¨Âá†‰∏™ÂàÜÂå∫ÔºåËøô‰∏™ÂàÜÂå∫ÁöÑleaderÊòØÂì™‰∏™brokerÔºåÂâØÊú¨‰Ωç‰∫éÂì™‰∫õbrokerÔºåÊúâÂì™‰∫õÂâØÊú¨Â§ÑÁêÜÂêåÊ≠•Áä∂ÊÄÅ„ÄÇPartitionÔºö ÂàÜÂå∫Leader Ôºö Ë¥üË¥£ËØªÂÜôÊåáÂÆöÂàÜÂå∫ÁöÑËäÇÁÇπReplicas Ôºö Â§çÂà∂ËØ•ÂàÜÂå∫logÁöÑËäÇÁÇπÂàóË°®Isr Ôºö ‚Äúin-sync‚Äù replicasÔºåÂΩìÂâçÊ¥ªË∑ÉÁöÑÂâØÊú¨ÂàóË°®ÔºàÊòØ‰∏Ä‰∏™Â≠êÈõÜÔºâÔºåÂπ∂‰∏îÂèØËÉΩÊàê‰∏∫LeaderÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáKafkaËá™Â∏¶ÁöÑbin/kafka-console-producer.shÂíåbin/kafka-console-consumer.shËÑöÊú¨ÔºåÊù•È™åËØÅÊºîÁ§∫Â¶ÇÊûúÂèëÂ∏ÉÊ∂àÊÅØ„ÄÅÊ∂àË¥πÊ∂àÊÅØ„ÄÇd.Âà†Èô§topic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --delete --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic teste.‰øÆÊîπtopic‰ΩøÁî®‚Äî-alertÂéüÂàô‰∏äÂèØ‰ª•‰øÆÊîπ‰ªª‰ΩïÈÖçÁΩÆÔºå‰ª•‰∏ãÂàóÂá∫‰∫Ü‰∏Ä‰∫õÂ∏∏Áî®ÁöÑ‰øÆÊîπÈÄâÈ°πÔºöÔºà1ÔºâÊîπÂèòÂàÜÂå∫Êï∞Èáè[root@sht-sgmhadoopdn-02 kafka]#bin/kafka-topics.sh --alter --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test --partitions 3[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:3 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: test Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: test Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1[root@sht-sgmhadoopdn-02 kafka]#Ôºà2ÔºâÂ¢ûÂä†„ÄÅ‰øÆÊîπÊàñËÄÖÂà†Èô§‰∏Ä‰∏™ÈÖçÁΩÆÂèÇÊï∞ bin/kafka-topics.sh ‚Äîalter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --config key=value bin/kafka-topics.sh ‚Äîalter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --deleteConfig key8.Ê®°ÊãüÂÆûÈ™å11234567Âú®‰∏Ä‰∏™ÁªàÁ´ØÔºåÂêØÂä®ProducerÔºåÂπ∂ÂêëÊàë‰ª¨‰∏äÈù¢ÂàõÂª∫ÁöÑÂêçÁß∞‰∏∫my-replicated-topic5ÁöÑTopic‰∏≠Áîü‰∫ßÊ∂àÊÅØÔºåÊâßË°åÂ¶Ç‰∏ãËÑöÊú¨Ôºö[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-console-producer.sh --broker-list 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092 --topic testÂú®Âè¶‰∏Ä‰∏™ÁªàÁ´ØÔºåÂêØÂä®ConsumerÔºåÂπ∂ËÆ¢ÈòÖÊàë‰ª¨‰∏äÈù¢ÂàõÂª∫ÁöÑÂêçÁß∞‰∏∫my-replicated-topic5ÁöÑTopic‰∏≠Áîü‰∫ßÁöÑÊ∂àÊÅØÔºåÊâßË°åÂ¶Ç‰∏ãËÑöÊú¨Ôºö[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --from-beginning --topic testÂèØ‰ª•Âú®ProducerÁªàÁ´Ø‰∏äËæìÂÖ•Â≠óÁ¨¶‰∏≤Ê∂àÊÅØË°åÔºåÂ∞±ÂèØ‰ª•Âú®ConsumerÁªàÁ´Ø‰∏äÁúãÂà∞Ê∂àË¥πËÄÖÊ∂àË¥πÁöÑÊ∂àÊÅØÂÜÖÂÆπ„ÄÇ]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãhadoop-2.7.3ÁºñËØëÂíåÊê≠Âª∫ÈõÜÁæ§ÁéØÂ¢É(HDFS HA,Yarn HA)]]></title>
    <url>%2F2018%2F09%2F03%2F03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA%2CYarn%20HA)%2F</url>
    <content type="text"><![CDATA[1.‰∏ãËΩΩhadoop2.7.3ÊúÄÊñ∞Ê∫êÁ†Å123456789101112131415161718192021222324252627282930313233343536373839404142[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt Build instructions for Hadoop----------------------------------------------------------------------------------Requirements:* Unix System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac* Zlib devel (if compiling native code)* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )* Internet connection for first build (to fetch all Maven and Hadoop dependencies)----------------------------------------------------------------------------------Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:* Oracle JDK 1.7 (preferred) $ sudo apt-get purge openjdk* $ sudo apt-get install software-properties-common $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-java7-installer* Maven $ sudo apt-get -y install maven* Native libraries $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev* ProtocolBuffer 2.5.0 (required) $ sudo apt-get -y install libprotobuf-dev protobuf-compilerOptional packages:* Snappy compression $ sudo apt-get install snappy libsnappy-dev* Bzip2 $ sudo apt-get install bzip2 libbz2-dev* Jansson (C Library for JSON) $ sudo apt-get install libjansson-dev* Linux FUSE $ sudo apt-get install fuse libfuse-dev2.ÂÆâË£Ö‰æùËµñÂåÖ1[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*3.ÂÆâË£Öjdk12345678910[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport PATH=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# java -versionjava version &quot;1.7.0_67&quot;Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 compilesoft]#4.ÂÆâË£Ömaven123456789101112131415161718[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9#Âú®ÁºñËØëËøáÁ®ã‰∏≠‰∏∫‰∫ÜÈò≤Ê≠¢JavaÂÜÖÂ≠òÊ∫¢Âá∫ÔºåÈúÄË¶ÅÂä†ÂÖ•‰ª•‰∏ãÁéØÂ¢ÉÂèòÈáèexport MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# mvn -versionApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9Java version: 1.7.0_67, vendor: Oracle CorporationJava home: /usr/java/jdk1.7.0_67-cloudera/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;You have new mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#5.ÁºñËØëÂÆâË£Öprotobuf12345678910111213[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/[root@hadoop-01 protobuf-2.5.0]# ./configure [root@hadoop-01 protobuf-2.5.0]# make[root@hadoop-01 protobuf-2.5.0]# make install#Êü•ÁúãprotobufÁâàÊú¨‰ª•ÊµãËØïÊòØÂê¶ÂÆâË£ÖÊàêÂäü[root@hadoop-01 protobuf-2.5.0]# protoc --versionprotoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib[root@hadoop-01 protobuf-2.5.0]# protoc --versionlibprotoc 2.5.0[root@hadoop-01 protobuf-2.5.0]#6.ÂÆâË£Ösnappy123456789101112131415[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz#Áî®rootÁî®Êà∑ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install#Êü•ÁúãsnappyÂ∫ìÊñá‰ª∂[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy-rw-r--r-- 1 root root 229K Jun 21 15:46 libsnappy.a-rwxr-xr-x 1 root root 953 Jun 21 15:46 libsnappy.lalrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0lrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0-rwxr-xr-x 1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0[root@sht-sgmhadoopnn-01 snappy-1.1.1]#7.ÁºñËØë123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-srcmvn clean package -Pdist,native -DskipTests -DtarÊàñmvn package -Pdist,native -DskipTests -Dtar[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package ‚ÄìPdist,native ‚ÄìDskipTests ‚ÄìDtar[INFO] Executing tasksmain: [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3 [exec] $ gzip -f hadoop-2.7.3.tar [exec] [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz [exec] [INFO] Executed tasks[INFO] [INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s][INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 6.832 s][INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s][INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s][INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.411 s][INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 4.814 s][INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s][INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min][INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s][INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s][INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min][INFO] Apache Hadoop NFS .................................. SUCCESS [ 9.285 s][INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s][INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.049 s][INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min][INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min][INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s][INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 4.599 s][INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.044 s][INFO] hadoop-yarn ........................................ SUCCESS [ 0.043 s][INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min][INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s][INFO] hadoop-yarn-server ................................. SUCCESS [ 0.041 s][INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s][INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s][INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 6.415 s][INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s][INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s][INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 7.751 s][INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s][INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [ 5.612 s][INFO] hadoop-yarn-applications ........................... SUCCESS [ 0.038 s][INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [ 4.029 s][INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 2.611 s][INFO] hadoop-yarn-site ................................... SUCCESS [ 0.077 s][INFO] hadoop-yarn-registry ............................... SUCCESS [ 8.045 s][INFO] hadoop-yarn-project ................................ SUCCESS [ 5.456 s][INFO] hadoop-mapreduce-client ............................ SUCCESS [ 0.226 s][INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s][INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s][INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 6.697 s][INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s][INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 9.328 s][INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s][INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [ 2.412 s][INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 8.876 s][INFO] hadoop-mapreduce ................................... SUCCESS [ 4.237 s][INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s][INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s][INFO] Apache Hadoop Archives ............................. SUCCESS [ 3.069 s][INFO] Apache Hadoop Rumen ................................ SUCCESS [ 7.446 s][INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 5.765 s][INFO] Apache Hadoop Data Join ............................ SUCCESS [ 3.752 s][INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 2.771 s][INFO] Apache Hadoop Extras ............................... SUCCESS [ 5.612 s][INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s][INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 7.131 s][INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min][INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s][INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s][INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 1.142 s][INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 7.354 s][INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s][INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.035 s][INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 26:29 min[INFO] Finished at: 2016-12-24T21:07:09+08:00[INFO] Final Memory: 214M/740M[INFO] ------------------------------------------------------------------------You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# [root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../[root@sht-sgmhadoopnn-01 learnproject]# lltotal 193152drwxr-xr-x 5 root root 4096 Dec 24 20:24 compilesoft-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz[root@sht-sgmhadoopnn-01 learnproject]#8.Êê≠Âª∫HDFS HA,YARN HAÈõÜÁæ§Ôºà5‰∏™ËäÇÁÇπÔºâÂèÇËÄÉ:http://blog.itpub.net/30089851/viewspace-1994585/https://github.com/Hackeruncle/Hadoop9.Êê≠Âª∫ÈõÜÁæ§,È™åËØÅÁâàÊú¨ÂíåÊîØÊåÅÁöÑÂéãÁº©‰ø°ÊÅØ1234567891011121314151617181920[root@sht-sgmhadoopnn-01 app]# hadoop versionHadoop 2.7.3Subversion Unknown -r UnknownCompiled by root on 2016-12-24T12:45ZCompiled with protoc 2.5.0From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar[root@sht-sgmhadoopnn-01 app]# hadoop checknative16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0zlib: true /lib64/libz.so.1snappy: true /usr/local/lib/libsnappy.so.1lz4: true revision:99bzip2: true /lib64/libbz2.so.1openssl: true /usr/lib64/libcrypto.so[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped[root@sht-sgmhadoopnn-01 app]#[ÂèÇËÄÉ]http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.htmlhttp://blog.csdn.net/haohaixingyun/article/details/52800048]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãFlume AgentÁöÑ3Âè∞Êî∂ÈõÜ+1Âè∞ËÅöÂêàÂà∞hdfsÁöÑÊê≠Âª∫]]></title>
    <url>%2F2018%2F09%2F03%2F04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86%2B1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[„ÄêlogÊî∂ÈõÜ„Äë:Êú∫Âô®ÂêçÁß∞ ÊúçÂä°ÂêçÁß∞ Áî®Êà∑flume-agent-01: namenode hdfsflume-agent-02: datanode hdfsflume-agent-03: datanode hdfs„ÄêlogËÅöÂêà„Äë:Êú∫Âô®ÂêçÁß∞ Áî®Êà∑sht-sgmhadoopcm-01(172.16.101.54) root„ÄêsinkÂà∞hdfs„Äë:hdfs://172.16.101.56:8020/testwjp/1.‰∏ãËΩΩapache-flume-1.7.0-bin.tar.gz1234567891011[hdfs@flume-agent-01 tmp]$ wget http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz--2017-01-04 20:40:10-- http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gzResolving www-eu.apache.org... 88.198.26.2, 2a01:4f8:130:2192::2Connecting to www-eu.apache.org|88.198.26.2|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 55711670 (53M) [application/x-gzip]Saving to: ‚Äúapache-flume-1.7.0-bin.tar.gz‚Äù100%[===============================================================================================================================================================================================&gt;] 55,711,670 473K/s in 74s 2017-01-04 20:41:25 (733 KB/s) - ‚Äúapache-flume-1.7.0-bin.tar.gz‚Äù saved [55711670/55711670]2.Ëß£ÂéãÈáçÂëΩÂêç1234[hdfs@flume-agent-01 tmp]$ [hdfs@flume-agent-01 tmp]$ tar -xzvf apache-flume-1.7.0-bin.tar.gz [hdfs@flume-agent-01 tmp]$ mv apache-flume-1.7.0-bin flume-ng[hdfs@flume-agent-01 tmp]$ cd flume-ng/conf3.Êã∑Ë¥ùflumeÁéØÂ¢ÉÈÖçÁΩÆÂíåagentÈÖçÁΩÆÊñá‰ª∂12[hdfs@flume-agent-01 tmp]$ cp flume-env.sh.template flume-env.sh[hdfs@flume-agent-01 tmp]$ cp flume-conf.properties.template exec_memory_avro.properties4.Ê∑ªÂä†hdfsÁî®Êà∑ÁöÑÁéØÂ¢ÉÂèòÈáèÊñá‰ª∂123456789101112131415161718192021[hdfs@flume-agent-01 tmp]$ cd[hdfs@flume-agent-01 ~]$ ls -latotal 24drwxr-xr-x 3 hdfs hadoop 4096 Jul 8 14:05 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_historydrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo[hdfs@flume-agent-01 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-01 ~]$ ls -latotal 36drwxr-xr-x 3 hdfs hadoop 4096 Jan 4 20:49 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_history-rw-r--r-- 1 hdfs hdfs 18 Jan 4 20:49 .bash_logout-rw-r--r-- 1 hdfs hdfs 176 Jan 4 20:49 .bash_profile-rw-r--r-- 1 hdfs hdfs 124 Jan 4 20:49 .bashrcdrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo5.Ê∑ªÂä†flumeÁöÑÁéØÂ¢ÉÂèòÈáè123456[hdfs@flume-agent-01 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-01 ~]$ . .bash_profile6.‰øÆÊîπflumeÁéØÂ¢ÉÈÖçÁΩÆÊñá‰ª∂12[hdfs@flume-agent-01 conf]$ vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_257.Â∞ÜÂü∫‰∫éFlume-ng Exec SourceÂºÄÂèëËá™ÂÆö‰πâÊèí‰ª∂AdvancedExecSourceÁöÑAdvancedExecSource.jarÂåÖ‰∏ä‰º†Âà∞$FLUME_HOME/lib/1http://blog.itpub.net/30089851/viewspace-2131995/12345[hdfs@LogshedNameNodeLogcollector lib]$ pwd/tmp/flume-ng/lib[hdfs@LogshedNameNodeLogcollector lib]$ ll AdvancedExecSource.jar -rw-r--r-- 1 hdfs hdfs 10618 Jan 5 23:50 AdvancedExecSource.jar[hdfs@LogshedNameNodeLogcollector lib]$8.‰øÆÊîπflumeÁöÑagentÈÖçÁΩÆÊñá‰ª∂1234567891011121314151617181920212223242526[hdfs@flume-agent-01 conf]$ vi exec_memory_avro.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the custom exec sourcea1.sources.r1.type = com.onlinelog.analysis.AdvancedExecSourcea1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-NAMENODE-flume-agent-01.log.outa1.sources.r1.hostname = flume-agent-01a1.sources.r1.servicename = namenode#Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 172.16.101.54a1.sinks.k1.port = 4545#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 60 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 2000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c19.Â∞Üflume-agent-01ÁöÑflume-ngÊâìÂåÖ,scpÂà∞flume-agent-02/03 Âíå sht-sgmhadoopcm-01(172.16.101.54)12345[hdfs@flume-agent-01 tmp]$ zip -r flume-ng.zip flume-ng/*[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-02:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-03:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip sht-sgmhadoopcm-01:/tmp/10.Âú®flume-agent-02ÈÖçÁΩÆhdfsÁî®Êà∑ÁéØÂ¢ÉÂèòÈáèÂíåËß£ÂéãÔºå‰øÆÊîπagentÈÖçÁΩÆÊñá‰ª∂12345678910111213141516171819[hdfs@flume-agent-02 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-02 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-02 ~]$ . .bash_profile[hdfs@flume-agent-02 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-02 tmp]$ cd flume-ng/conf##‰øÆÊîπ‰ª•‰∏ãÂèÇÊï∞Âç≥ÂèØ[hdfs@flume-agent-02 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-02.log.outa1.sources.r1.hostname = flume-agent-02a1.sources.r1.servicename = datanode###Ë¶ÅÊ£ÄÊü•flume-env.shÁöÑJAVA_HOMEÁõÆÂΩïÊòØÂê¶Â≠òÂú®11.Âú®flume-agent-03ÈÖçÁΩÆhdfsÁî®Êà∑ÁéØÂ¢ÉÂèòÈáèÂíåËß£ÂéãÔºå‰øÆÊîπagentÈÖçÁΩÆÊñá‰ª∂12345678910111213141516171819[hdfs@flume-agent-03 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-03 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-03 ~]$ . .bash_profile[hdfs@flume-agent-03 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-03 tmp]$ cd flume-ng/conf##‰øÆÊîπ‰ª•‰∏ãÂèÇÊï∞Âç≥ÂèØ[hdfs@flume-agent-03 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-03.log.outa1.sources.r1.hostname = flume-agent-03a1.sources.r1.servicename = datanode###Ë¶ÅÊ£ÄÊü•flume-env.shÁöÑJAVA_HOMEÁõÆÂΩïÊòØÂê¶Â≠òÂú®12.ËÅöÂêàÁ´Ø sht-sgmhadoopcm-01ÔºåÈÖçÁΩÆrootÁî®Êà∑ÁéØÂ¢ÉÂèòÈáèÂíåËß£ÂéãÔºå‰øÆÊîπagentÈÖçÁΩÆÊñá‰ª∂1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@sht-sgmhadoopcm-01 tmp]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$FLUME_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopcm-01 tmp]# source /etc/profile[root@sht-sgmhadoopcm-01 tmp]#[root@sht-sgmhadoopcm-01 tmp]# unzip flume-ng.zip[root@sht-sgmhadoopcm-01 tmp]# cd flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera ###ÊµãËØï: ÂÖàËÅöÂêà, sinkÂà∞hdfsÁ´Ø[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_hdfs.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://172.16.101.56:8020/testwjp/a1.sinks.k1.hdfs.filePrefix = logsa1.sinks.k1.hdfs.inUsePrefix = .a1.sinks.k1.hdfs.rollInterval = 0###roll 16 m = 16777216 bytesa1.sinks.k1.hdfs.rollSize = 1048576a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.batchSize = 6000a1.sinks.k1.hdfs.writeFormat = texta1.sinks.k1.hdfs.fileType = DataStream#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c113.ÂêéÂè∞ÂêØÂä®123456789101112[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[root@sht-sgmhadoopnn-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_hdfs.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;14.Ê†°È™åÔºöÂ∞ÜÈõÜÁæ§ÁöÑÊó•Âøó‰∏ãËΩΩÂà∞Êú¨Âú∞ÔºåÊâìÂºÄÊü•ÁúãÂç≥ÂèØ(Áï•)12345678910111213141516171819202122232425262728293031------------------------------------------------------------------------------------------------------------------------------------------------„ÄêÂ§áÊ≥®„Äë: 1.ÈîôËØØ1 flume-ngÂÆâË£ÖÁöÑÊú∫Âô®‰∏äÊ≤°ÊúâhadoopÁéØÂ¢ÉÔºåÊâÄ‰ª•ÂÅáÂ¶ÇsinkÂà∞hdfsËØùÔºåÈúÄË¶ÅÁî®Âà∞hdfsÁöÑjarÂåÖ[ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:146)] Failed to start agent because dependencies were not found in classpath. Error follows.java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionTypeÂè™ÈúÄÂú®ÂÖ∂‰ªñÂÆâË£ÖhadoopÊú∫Âô®‰∏äÊêúÁ¥¢‰ª•‰∏ã5‰∏™jarÂåÖÔºåÊã∑Ë¥ùÂà∞$FLUME_HOME/libÁõÆÂΩïÂç≥ÂèØ„ÄÇÊêúÁ¥¢ÊñπÊ≥ï: find $HADOOP_HOME/ -name commons-configuration*.jarcommons-configuration-1.6.jarhadoop-auth-2.7.3.jarhadoop-common-2.7.3.jarhadoop-hdfs-2.7.3.jarhadoop-mapreduce-client-core-2.7.3.jarprotobuf-java-2.5.0.jarhtrace-core-3.1.0-incubating.jarcommons-io-2.4.jar2.ÈîôËØØ2 Êó†Ê≥ïÂä†ËΩΩËá™ÂÆö‰πâÊèí‰ª∂ÁöÑÁ±ª Unable to load source type: com.onlinelog.analysis.AdvancedExecSource2017-01-06 21:10:48,278 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:142)] Failed to load configuration data. Exception follows.org.apache.flume.FlumeException: Unable to load source type: com.onlinelog.analysis.AdvancedExecSource, class: com.onlinelog.analysis.AdvancedExecSourceÊâßË°åhdfsÊàñËÄÖrootÁî®Êà∑ÁöÑÁéØÂ¢ÉÂèòÈáèÂç≥ÂèØ[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãFlume-1.7.0Ê∫êÁ†ÅÁºñËØëÂØºÂÖ•eclipse]]></title>
    <url>%2F2018%2F08%2F28%2F02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse%2F</url>
    <content type="text"><![CDATA[„ÄêÂâçÊèê„Äë:1.windows 7 ÂÆâË£Ömaven-3.3.9ÂÖ∂‰∏≠Âú®conf/setting.xmlÊñá‰ª∂Ê∑ªÂä†D:\software\apache-maven-3.3.9\repositoryhttp://blog.csdn.net/defonds/article/details/419572872.windows 7 ÂÆâË£Öeclipse 64‰Ωç(ÁôæÂ∫¶‰∏ãËΩΩÔºåËß£ÂéãÂç≥ÂèØ)3.eclipseÂÆâË£ÖmavenÊèí‰ª∂ÔºåÈÄâÊã©Á¨¨‰∫åÁßçÊñπÂºèlinkhttp://blog.csdn.net/lfsfxy9/article/details/9397937ÂÖ∂‰∏≠ eclipse-maven3-plugin.7z Ëøô‰∏™ÂåÖÂèØ‰ª•Âä†Áæ§258669058ÊâæÊàëÔºåÂàÜ‰∫´Áªô‰Ω†„Äêflume-ng 1.7.0Ê∫êÁ†ÅÁöÑÁºñËØëÂØºÂÖ•eclipse„Äë:1.‰∏ãËΩΩÂÆòÁΩëÁöÑÊ∫êÁ†Å(‰∏çË¶Å‰∏ãËΩΩGitHub‰∏äÊ∫êÁ†ÅÔºåÂõ†‰∏∫ËøôÊó∂pomÊñá‰ª∂‰∏≠ÁâàÊú¨‰∏∫1.8.0ÔºåÁºñËØë‰ºöÊúâÈóÆÈ¢ò)http://archive.apache.org/dist/flume/1.7.0/a.‰∏ãËΩΩapache-flume-1.7.0-src.tar.gzb.Ëß£ÂéãÈáçÂëΩÂêç‰∏∫flume-1.7.02.‰øÆÊîπpom.xml (Â§ßÊ¶ÇÂú®621Ë°åÔºåÂ∞ÜËá™Â∏¶ÁöÑrepositoryÊ≥®ÈáäÊéâÔºåÊ∑ªÂä†‰ª•‰∏ãÁöÑ)1234&lt;repository&gt; &lt;id&gt;maven.tempo-db.com&lt;/id&gt; &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt; &lt;/repository&gt;3.ÊâìÂºÄcmd,ÁºñËØëcd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0mvn compile4.ÊâìÂºÄeclipse,ÂçïÂáªWindow‚Äì&gt;Perferences‚Äì&gt;Â∑¶‰æßÁöÑMaven‚Äì&gt;User SettingsÁÑ∂ÂêéËÆæÁΩÆËá™Â∑±ÁöÑmvnÁöÑsetting.xmlË∑ØÂæÑÂíåLocal Repository(ÊúÄÂ•Ω‰ΩøÁî®Maven3.3.xÁâàÊú¨‰ª•‰∏äÔºåÊàëÊòØ3.3.9)5.ÂÖ≥Èó≠eclipseÁöÑ Project‚Äì&gt;Buid Automatically6.ÂÖ≥Èó≠eclipseÁöÑDownload repository index updates on startup7.ÂØºÂÖ•flume1.7.0Ê∫êÁ†Åa.File‚Äì&gt;Import‚Äì&gt;Maven‚Äì&gt;Existing Maven Projects‚Äì&gt;Nextb.ÈÄâÊã©ÁõÆÂΩï‚Äì&gt; Finish8.Ê£ÄÊü•Ê∫êÁ†ÅÔºåÊ≤°ÊúâÊäõ‰ªª‰ΩïÈîôËØØ]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ‰πãÈ°πÁõÆÊ¶ÇËø∞]]></title>
    <url>%2F2018%2F08%2F27%2F01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1.ÂâçÊúüÂü∫Êú¨Êû∂ÊûÑÂõæ2.ÊúÄÁªàÂü∫Êú¨Êû∂ÊûÑÂõæ3.ÁâàÊú¨ÁªÑ‰ª∂ÁâàÊú¨Flume:1.7Hadoop:2.7.3Scala:2.11Kafka:0.10.1.0Spark:2.0.2InfluxDB:1.2.0Grafana:4.1.1maven:3.3.94.‰∏ªË¶ÅÁõÆÁöÑ‰∏ªË¶ÅÊòØÊÉ≥Âü∫‰∫éExec SourceÂºÄÂèëËá™ÂÆö‰πâÊèí‰ª∂AdvancedExecSourceÔºåÂ∞ÜÊú∫Âô®ÂêçÁß∞ Âíå ÊúçÂä°ÂêçÁß∞ Ê∑ªÂä†Âà∞cdh ÊúçÂä°ÁöÑËßíËâ≤logÊï∞ÊçÆÁöÑÊØè‰∏ÄË°åÂâçÈù¢ÔºåÂàôÊ†ºÂºè‰∏∫ÔºöÊú∫Âô®ÂêçÁß∞ ÊúçÂä°ÂêçÁß∞ Âπ¥ÊúàÊó• Êó∂ÂàÜÁßí.ÊØ´Áßí Êó•ÂøóÁ∫ßÂà´ Êó•Âøó‰ø°ÊÅØ ÔºõÁÑ∂ÂêéÂú®ÂêéÈù¢ÁöÑspark streaming ÂÆûÊó∂ËÆ°ÁÆóÊàë‰ª¨ÊâÄÈúÄÊ±ÇÔºöÊØîÂ¶ÇÁªüËÆ°ÊØèÂè∞Êú∫Âô®ÁöÑÊúçÂä°ÁöÑÊØèÁßíÂá∫Áé∞ÁöÑerrorÊ¨°Êï∞ „ÄÅÁªüËÆ°ÊØè5ÁßíÁöÑwarnÔºåerrorÊ¨°Êï∞Á≠âÁ≠âÔºõÊù•ÂÆûÊó∂ÂèØËßÜÂåñÂ±ïÁ§∫ÂíåÈÇÆ‰ª∂Áü≠‰ø°„ÄÅÂæÆ‰ø°‰ºÅ‰∏öÂè∑ÈÄöÁü•„ÄÇÂÖ∂ÂÆû‰∏ªË¶ÅÊàë‰ª¨Áé∞Âú®ÁöÑÂæàÂ§öÁõëÊéßÊúçÂä°Âü∫Êú¨Ëææ‰∏çÂà∞ÁßíÁ∫ßÁöÑÈÄöÁü•ÔºåÈÉΩ‰∏∫5ÂàÜÈíüÁ≠âÁ≠âÔºå‰∏∫‰∫ÜÊñπ‰æøÊàë‰ª¨Ëá™Â∑±ÁöÑÁª¥Êä§ÔºõÂÖ∂ÂÆûÂØπ‰∏Ä‰∫õÂç≥Â∞ÜÂá∫Áé∞ÁöÑÈóÆÈ¢òÂèØ‰ª•ÊèêÂâçÈ¢ÑÁü•ÔºõÂÖ∂ÂÆûÊúÄ‰∏ªË¶ÅÂèØ‰ª•ÊúâÊïàÊâ©Â±ïÂà∞ÂÆûÊó∂ËÆ°ÁÆóÊï∞ÊçÆÂ∫ìÁ∫ßÂà´Êó•ÂøóÔºåÊØîÂ¶ÇMySQLÊÖ¢Êü•ËØ¢Êó•ÂøóÔºånginxÔºåtomcatÔºålinuxÁöÑÁ≥ªÁªüÁ∫ßÂà´Êó•ÂøóÁ≠âÁ≠â„ÄÇ5.Â§ßÊ¶ÇÊµÅÁ®ã1.Êê≠Âª∫hadoop cluster2.eclipse ÂØºÂÖ•flumeÊ∫ê‰ª£Á†ÅÔºàwindow7 ÂÆâË£ÖmavenÔºåeclipseÔºåeclipse‰∏émavenÈõÜÊàêÔºâ3.ÂºÄÂèëflume-ng Ëá™ÂÆö‰πâÊèí‰ª∂4.flume Êî∂ÈõÜÔºåÊ±áËÅöÂà∞hdfs(‰∏ªË¶ÅÊµãËØïÊòØÂê¶Ê±áËÅöÊàêÂäüÔºåÂêéÊúü‰πüÂèØ‰ª•ÂÅöÁ¶ªÁ∫øÂ§ÑÁêÜ)5.flume Êî∂ÈõÜÔºåÊ±áËÅöÂà∞kafka6.Êê≠Âª∫kafka monitor7.Êê≠Âª∫ spark client8.window7Ë£ÖiedaÂºÄÂèëÂ∑•ÂÖ∑9.ideaÂºÄÂèë spark streaming ÁöÑwc10.ËØªÂèñkafkaÊó•ÂøóÔºåÂºÄÂèëspark streamingÁöÑËøôÂùóÊó•ÂøóÂàÜÊûê11.ÂÜôÂÖ•influxdb12.grafanaÂèØËßÜÂåñÂ±ïÁ§∫13.ÈõÜÊàêÈÇÆ‰ª∂###ËØ¥ÊòéÔºöÈíàÂØπËá™Ë∫´ÊÉÖÂÜµÔºåËá™Ë°åÈÄâÊã©ÔºåÊ≠•È™§Â¶Ç‰∏äÔºå‰ΩÜ‰∏çÊòØÂõ∫ÂÆöÁöÑÔºåÊúâ‰∫õÈ°∫Â∫èÊòØÂèØ‰ª•Êâì‰π±ÁöÑÔºå‰æãÂ¶ÇÂºÄÂèëÂ∑•ÂÖ∑ÁöÑÂÆâË£ÖÔºåÂèØ‰ª•‰∏ÄËµ∑Êìç‰ΩúÁöÑÔºåÂÜçÂ¶ÇËøôÂá†‰∏™ÁªÑ‰ª∂ÁöÑ‰∏ãËΩΩÁºñËØëÔºåÂ¶ÇÊûú‰∏çÊÉ≥ÁºñËØëÂèØ‰ª•Áõ¥Êé•‰∏ãtarÂåÖÁöÑÔºåËá™Ë°åÈÄâÊã©Â∞±Â•ΩÔºå‰ΩÜÊòØÂª∫ËÆÆËøòÊòØËá™Â∑±ÁºñËØëÔºåÈÅáÂà∞ÂùëÊâçËÉΩÊõ¥Â•ΩÁöÑËÆ∞‰ΩèËøô‰∏™‰∏úË•øÔºåÊú¨Ë∫´Ëøô‰∏™È°πÁõÆÂ∞±ÊòØÂ≠¶‰π†ÊèêÂçáÁöÑËøáÁ®ãÔºåË¶ÅÊòØ‰ªÄ‰πàÈÉΩÊòØÁé∞ÊàêÁöÑÔºåÈÇ£Â∞±Ê≤°‰ªÄ‰πàÊÑè‰πâ‰∫Ü]]></content>
      <categories>
        <category>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Áîü‰∫ßÈ¢ÑË≠¶Âπ≥Âè∞È°πÁõÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËøôÊòØ‰∏ÄÁØáÁÉ≠ËÖæËÖæÁöÑÈù¢Áªè]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[‰ºüÊ¢¶Ôºö1.‰∏ªË¶ÅËøòÊòØÈ°πÁõÆÔºüÂü∫Êú¨‰∏äÊ≤°ÈóÆ‰ªÄ‰πàÊäÄÊúØÔºåÊàëÂ∞±ËØ¥‰∫Ü‰∏ÄÈÅçÈ°πÁõÆÊµÅÁ®ãÔºåÁÑ∂ÂêéËØ¥Âá†‰∏™‰ºòÂåñÁÇπÔºåÊØîÂ¶Ç‰∏äÊ¨°ËÆ≤ÁöÑË°ÄÊ°àÔºåÊàë‰πüÈ°∫Â∏¶Êèê‰∫Ü‰∏Ä‰∏ã„ÄÇ2.Âú®Â§ßÊï∞ÊçÆ‰∏≠ÔºåÊúâÊ≤°Êúâ‰ªÄ‰πàÊòØ‰∏çË∂≥ÁöÑÔºåÈÅáÂà∞Ëøá‰ªÄ‰πàÈóÆÈ¢òÔºüÂæÆÁõüÔºö1.SparkStreamingÂ§ÑÁêÜÂÆå‰∏ÄÊâπÊ¨°ÁöÑÊï∞ÊçÆÔºåÂÜôÂÅèÁßªÈáè‰πãÂâçÊåÇ‰∫ÜÔºåÊï∞ÊçÆÊÄé‰πà‰øùËØÅ‰∏çÈáçÔºü2.MaxwellÁöÑÂ∫ïÂ±ÇÂéüÁêÜÔºü3.ÊâãÂÜôSpringÔºü4.ÈÅçÂéÜ‰∫åÂèâÊ†ëÔºü5.Áî®Ëøá‰ªÄ‰πàÁÆóÊ≥ïÔºü6.Â§öÁ∫øÁ®ãÊñπÈù¢ÔºåÊÄé‰πàÂÆûÁé∞‰∏Ä‰∏™‰∏ªÁ∫øÁ®ãÔºåÁ≠âÂæÖÂÖ∂‰ªñÂ≠êÁ∫øÁ®ãÂÆåÊàêÂêéÂÜçËøêË°åÔºü7.MaxwellÂíåCannalÁöÑÊØîËæÉÔºü8.directÊØîËæÉreceiverÁöÑ‰ºòÂäøÔºü9.ÂéüÊù•ÊòØÊääÊï∞ÊçÆ‰º†ÂÖ•Âà∞HiveÔºå‰πãÂêéÊîπ‰∫ÜÊû∂ÊûÑÔºåÊÄé‰πàÊääHiveÁöÑÊï∞ÊçÆÂØºÂÖ•Âà∞HbaseÔºü10.‰∏∫‰ªÄ‰πàÁî®KafkaËá™Â∑±Â≠òÂÇ®offsetÊù•Êõø‰ª£checkpointÔºåÊÄé‰πàÈò≤Ê≠¢‰∫ÜÊï∞ÊçÆÂèå‰ªΩËêΩÂú∞ÔºåÊï∞ÊçÆÂèå‰ªΩÊòØÊåá‰ªÄ‰πàÔºü11.Âçï‰æãÁî®ËøáÂêóÔºüÂπ≥ÂÆâÔºö1.ÈóÆÈ°πÁõÆÔºåÊµÅÁ®ãÔºå‰∏öÂä°Ôºü2.Êï∞ÊçÆÈáèÔºåÂ¢ûÈáèÔºü3.Âá†‰∏™‰∫∫ÂºÄÂèëÁöÑÔºå‰ª£Á†ÅÈáèÂ§öÂ∞ëÔºü4.‰Ω†‰∏ªË¶ÅÂÅö‰ªÄ‰πàÁöÑÔºü5.‰ªÄ‰πàÂú∫ÊôØÔºåÁî®SparkSqlÂàÜÊûê‰ªÄ‰πà‰∏úË•øÔºüÊÄªÁªìÔºöÂü∫Êú¨‰∏äÈÉΩÊòØÂõ¥ÁªïÈ°πÁõÆÊù•Èù¢ÔºåÁ¨¨‰∏ÄÂÆ∂ÈóÆÁöÑÊØîËæÉÂ∞ëÔºåËÄå‰∏îÈÉΩÊòØÂÖ≥‰∫éÈ°πÁõÆÔºõÂæÆÁõüÁöÑÈù¢ËØïÂÆòÂÅöÁöÑÈ°πÁõÆÔºåË∑üÁÆÄÂéÜ‰∏äÁöÑÈ°πÁõÆÔºåÊû∂ÊûÑ‰∏äÂü∫Êú¨‰∏ÄÊ†∑ÔºåÊâÄ‰ª•ÈóÆÁöÑÊØîËæÉÊ∑±ÔºåÈóÆÊàëMaxwellÁöÑÂ∫ïÂ±ÇÂéüÁêÜÔºåÂØπÊØîCannalÊúâ‰ªÄ‰πà‰ºòÂäøÔºå‰∏∫‰ªÄ‰πàÈÄâÊã©ÂÆÉÔºåËøô‰∏™ÊàëÊ≤°ÂõûÁ≠î‰∏äÊù•ÔºåÂêéÊù•ËÆ©ÊâãÂÜôSpringÔºåÁÆóÊ≥ïÔºåÂêéÊù•Â∞±ËÆ©ÊàëËµ∞‰∫ÜÔºõÂπ≥ÂÆâ‰πüÊòØÂü∫Êú¨Âõ¥ÁªïÈ°πÁõÆÔºå‰∏öÂä°ÔºåÊï∞ÊçÆÈáèÔºåÊ≤°ÈóÆ‰ªÄ‰πàÊäÄÊúØÔºåËÄå‰∏îÊàëËØ¥‰∫ÜÂÖ≥‰∫é‰ºòÂåñÁöÑÁÇπ(Èù¢ËØïÂÆòËØ¥‰∏çË¶ÅËØ¥ÁΩë‰∏äÈÉΩÊúâÁöÑ‰∏úË•ø)„ÄÇ]]></content>
      <categories>
        <category>Èù¢ËØïÈ¢ò</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark‰∏≠ÈÖçÁΩÆÂêØÁî®LZOÂéãÁº©]]></title>
    <url>%2F2018%2F08%2F20%2Fspark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Spark‰∏≠ÈÖçÁΩÆÂêØÁî®LZOÂéãÁº©ÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö‰∏Ä„ÄÅspark-env.shÈÖçÁΩÆ123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*‰∫å„ÄÅspark-defaults.confÈÖçÁΩÆ12spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarÊ≥®ÔºöÊåáÂêëÁºñËØëÁîüÊàêlzoÁöÑjarÂåÖ‰∏â„ÄÅÊµãËØï1„ÄÅËØªÂèñLzoÊñá‰ª∂123spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)2„ÄÅÂÜôÂá∫lzoÊñá‰ª∂1234spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val lzoTest = sc.parallelize(1 to 10)scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])ÁªìÊûúÔºö12345[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzoFound 3 items-rw-r--r-- 1 hadoop supergroup 0 2018-03-16 23:24 /input/test_lzo/_SUCCESS-rw-r--r-- 1 hadoop supergroup 60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo-rw-r--r-- 1 hadoop supergroup 61 2018-03-16 23:24 /input/test_lzo/part-00001.lzoËá≥Ê≠§ÈÖçÁΩÆ‰∏éÊµãËØïÂÆåÊàê„ÄÇÂõõ„ÄÅÈÖçÁΩÆ‰∏éÊµãËØï‰∏≠Â≠òÈóÆÈ¢ò1„ÄÅÂºïÁî®nativeÔºåÁº∫Â∞ëLD_LIBRARY_PATH1.1„ÄÅÈîôËØØÊèêÁ§∫Ôºö1234567891011121314151617181920Caused by: java.lang.RuntimeException: native-lzo library not available at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120) at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178) at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246) at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)1.2„ÄÅËß£ÂÜ≥ÂäûÊ≥ïÔºöÂú®sparkÁöÑconf‰∏≠ÈÖçÁΩÆspark-evn.shÔºåÂ¢ûÂä†‰ª•‰∏ãÂÜÖÂÆπÔºö123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*2„ÄÅÊó†Ê≥ïÊâæÂà∞LzopCodecÁ±ª2.1„ÄÅÈîôËØØÊèêÁ§∫Ôºö1234567Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found. at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135) at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175) at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980) at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)2.2„ÄÅËß£ÂÜ≥ÂäûÊ≥ïÔºöÂú®sparkÁöÑconf‰∏≠ÈÖçÁΩÆspark-defaults.confÔºåÂ¢ûÂä†‰ª•‰∏ãÂÜÖÂÆπÔºö123spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS‰πãÂûÉÂúæÂõûÊî∂ÁÆ±ÈÖçÁΩÆÂèä‰ΩøÁî®]]></title>
    <url>%2F2018%2F07%2F18%2FHDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HDFS‰∏∫ÊØè‰∏™Áî®Êà∑ÂàõÂª∫‰∏Ä‰∏™ÂõûÊî∂Á´ô:ÁõÆÂΩï:/user/Áî®Êà∑/.Trash/Current, Á≥ªÁªüÂõûÊî∂Á´ôÈÉΩÊúâ‰∏Ä‰∏™Âë®Êúü,Âë®ÊúüËøáÂêéhdfs‰ºöÂΩªÂ∫ïÂà†Èô§Ê∏ÖÁ©∫,Âë®ÊúüÂÜÖÂèØ‰ª•ÊÅ¢Â§ç„ÄÇ‰∏Ä„ÄÅHDFSÂà†Èô§Êñá‰ª∂,Êó†Ê≥ïÊÅ¢Â§ç12[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.logDeleted /123.log‰∫å„ÄÅ ÂêØÁî®ÂõûÊî∂Á´ôÂäüËÉΩ12345678910111213[hadoop@hadoop001 hadoop]$ vim core-site.xml&lt;property&gt;&lt;!--Â§öÈïøÊó∂Èó¥ÂàõÂª∫CheckPoint NameNodeËäÇÁÇπ‰∏äËøêË°åÁöÑCheckPointer ‰ªéCurrentÊñá‰ª∂Â§πÂàõÂª∫CheckPoint; ÈªòËÆ§: 0 Áî±fs.trash.intervalÈ°πÊåáÂÆö --&gt;&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--Â§öÂ∞ëÂàÜÈíü.Trash‰∏ãÁöÑCheckPointÁõÆÂΩï‰ºöË¢´Âà†Èô§,ËØ•ÈÖçÁΩÆÊúçÂä°Âô®ËÆæÁΩÆ‰ºòÂÖàÁ∫ßÂ§ß‰∫éÂÆ¢Êà∑Á´ØÔºåÈªòËÆ§:‰∏çÂêØÁî® --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; -- Ê∏ÖÈô§Âë®ÊúüÂàÜÈíü(24Â∞èÊó∂)&lt;/property&gt;1„ÄÅÈáçÂêØhdfsÊúçÂä°12[hadoop@hadoop001 sbin]$ ./stop-dfs.sh[hadoop@hadoop001 sbin]$ ./start-dfs.sh2„ÄÅÊµãËØïÂõûÊî∂Á´ôÂäüËÉΩ123[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /[hadoop@hadoop001 opt]$ hdfs dfs -ls /-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /123.logÊñá‰ª∂Âà†Èô§ÊàêÂäüÂ≠òÊîæÂõûÊî∂Á´ôË∑ØÂæÑ‰∏ã12345[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log[hadoop@hadoop001 opt]$ hdfs dfs -ls /Found 1 itemsdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /userÊÅ¢Â§çÊñá‰ª∂12345[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log[hadoop@hadoop001 ~]$ hdfs dfs -ls /Found 2 items-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /456.logdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /userÂà†Èô§Êñá‰ª∂Ë∑≥ËøáÂõûÊî∂Á´ô123[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.logDeleted /456.logÊ∫êÁ†ÅÂèÇËÄÉÔºöhttps://blog.csdn.net/tracymkgld/article/details/17557655]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkÂ∫èÂàóÂåñÔºå‰Ω†‰∫ÜËß£Âêó]]></title>
    <url>%2F2018%2F07%2F16%2FSpark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[Â∫èÂàóÂåñÂú®ÂàÜÂ∏ÉÂºèÂ∫îÁî®ÁöÑÊÄßËÉΩ‰∏≠ÊâÆÊºîÁùÄÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÊ†ºÂºèÂåñÂØπË±°ÁºìÊÖ¢ÔºåÊàñËÄÖÊ∂àËÄóÂ§ßÈáèÁöÑÂ≠óËäÇÊ†ºÂºèÂåñÔºå‰ºöÂ§ßÂ§ßÈôç‰ΩéËÆ°ÁÆóÊÄßËÉΩ„ÄÇÈÄöÂ∏∏ËøôÊòØÂú®sparkÂ∫îÁî®‰∏≠Á¨¨‰∏Ä‰ª∂ÈúÄË¶Å‰ºòÂåñÁöÑ‰∫ãÊÉÖ„ÄÇSparkÁöÑÁõÆÊ†áÊòØÂú®‰æøÂà©‰∏éÊÄßËÉΩ‰∏≠ÂèñÂæóÂπ≥Ë°°ÔºåÊâÄ‰ª•Êèê‰æõ2ÁßçÂ∫èÂàóÂåñÁöÑÈÄâÊã©„ÄÇJava serializationÂú®ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåSpark‰ºö‰ΩøÁî®JavaÁöÑObjectOutputStreamÊ°ÜÊû∂ÂØπÂØπË±°ËøõË°åÂ∫èÂàóÂåñÔºåÂπ∂‰∏îÂèØ‰ª•‰∏é‰ªª‰ΩïÂÆûÁé∞java.io.SerializableÁöÑÁ±ª‰∏ÄËµ∑Â∑•‰Ωú„ÄÇÊÇ®ËøòÂèØ‰ª•ÈÄöËøáÊâ©Â±ïjava.io.ExternalizableÊù•Êõ¥Á¥ßÂØÜÂú∞ÊéßÂà∂Â∫èÂàóÂåñÁöÑÊÄßËÉΩ„ÄÇJavaÂ∫èÂàóÂåñÊòØÁÅµÊ¥ªÁöÑÔºå‰ΩÜÈÄöÂ∏∏Áõ∏ÂΩìÊÖ¢ÔºåÂπ∂‰∏î‰ºöÂØºËá¥ËÆ∏Â§öÁ±ªÁöÑÂ§ßÂûãÂ∫èÂàóÂåñÊ†ºÂºè„ÄÇÊµãËØï‰ª£Á†ÅÔºö12345678910111213141516171819202122232425262728package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;ÊµãËØïÁªìÊûúÔºöKryo serializationSparkËøòÂèØ‰ª•‰ΩøÁî®KryoÂ∫ìÔºàÁâàÊú¨2ÔºâÊù•Êõ¥Âø´Âú∞Â∫èÂàóÂåñÂØπË±°„ÄÇKryoÊØîJava‰∏≤Ë°åÂåñÔºàÈÄöÂ∏∏Â§öËææ10ÂÄçÔºâË¶ÅÂø´ÂæóÂ§öÔºå‰πüÊõ¥Á¥ßÂáëÔºå‰ΩÜÊòØ‰∏çÊîØÊåÅÊâÄÊúâÂèØ‰∏≤Ë°åÂåñÁ±ªÂûãÔºåÂπ∂‰∏îË¶ÅÊ±ÇÊÇ®ÊèêÂâçÊ≥®ÂÜåÊÇ®Â∞ÜÂú®Á®ãÂ∫è‰∏≠‰ΩøÁî®ÁöÑÁ±ªÔºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÊµãËØï‰ª£Á†ÅÔºö12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;ÊµãËØïÁªìÊûú‰∏≠ÂèëÁé∞Ôºå‰ΩøÁî® Kryo serialization ÁöÑÂ∫èÂàóÂåñÂØπË±° ÊØî‰ΩøÁî® Java serializationÁöÑÂ∫èÂàóÂåñÂØπË±°Ë¶ÅÂ§ßÔºå‰∏éÊèèËø∞ÁöÑ‰∏ç‰∏ÄÊ†∑ÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢ÔºüÊü•ÊâæÂÆòÁΩëÔºåÂèëÁé∞Ëøô‰πà‰∏ÄÂè•ËØù Finally, if you don‚Äôt register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.„ÄÇ‰øÆÊîπ‰ª£Á†ÅÂêéÂú®ÊµãËØï‰∏ÄÊ¨°12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .registerKryoClasses(Array(classOf[Student])) // Â∞ÜËá™ÂÆö‰πâÁöÑÁ±ªÊ≥®ÂÜåÂà∞Kryo val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125;ÊµãËØïÁªìÊûúÔºöÊÄªÁªìÔºöKryo serialization ÊÄßËÉΩÂíåÂ∫èÂàóÂåñÂ§ßÂ∞èÈÉΩÊØîÈªòËÆ§Êèê‰æõÁöÑ Java serialization Ë¶ÅÂ•ΩÔºå‰ΩÜÊòØ‰ΩøÁî®KryoÈúÄË¶ÅÂ∞ÜËá™ÂÆö‰πâÁöÑÁ±ªÂÖàÊ≥®ÂÜåËøõÂéªÔºå‰ΩøÁî®Ëµ∑Êù•ÊØîJava serializationÈ∫ªÁÉ¶„ÄÇËá™‰ªéSpark 2.0.0‰ª•Êù•ÔºåÊàë‰ª¨Âú®‰ΩøÁî®ÁÆÄÂçïÁ±ªÂûã„ÄÅÁÆÄÂçïÁ±ªÂûãÊï∞ÁªÑÊàñÂ≠óÁ¨¶‰∏≤Á±ªÂûãÁöÑÁÆÄÂçïÁ±ªÂûãÊù•Ë∞ÉÊï¥RDDsÊó∂ÔºåÂú®ÂÜÖÈÉ®‰ΩøÁî®KryoÂ∫èÂàóÂåñÂô®„ÄÇÈÄöËøáÊü•ÊâæsparkcontextÂàùÂßãÂåñÁöÑÊ∫êÁ†ÅÔºåÂèØ‰ª•ÂèëÁé∞Êüê‰∫õÁ±ªÂûãÂ∑≤ÁªèÂú®sparkcontextÂàùÂßãÂåñÁöÑÊó∂ÂÄôË¢´Ê≥®ÂÜåËøõÂéª„ÄÇ12345678910111213141516171819202122232425262728 /** * Component which configures serialization, compression and encryption for various Spark * components, including automatic selection of which [[Serializer]] to use for shuffles. */private[spark] class SerializerManager( defaultSerializer: Serializer, conf: SparkConf, encryptionKey: Option[Array[Byte]]) &#123; def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None) private[this] val kryoSerializer = new KryoSerializer(conf) private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]] private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123; val primitiveClassTags = Set[ClassTag[_]]( ClassTag.Boolean, ClassTag.Byte, ClassTag.Char, ClassTag.Double, ClassTag.Float, ClassTag.Int, ClassTag.Long, ClassTag.Null, ClassTag.Short ) val arrayClassTags = primitiveClassTags.map(_.wrap) primitiveClassTags ++ arrayClassTags]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming Áä∂ÊÄÅÁÆ°ÁêÜÂáΩÊï∞Ôºå‰Ω†‰∫ÜËß£Âêó]]></title>
    <url>%2F2018%2F06%2F25%2FSpark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[Spark Streaming Áä∂ÊÄÅÁÆ°ÁêÜÂáΩÊï∞ÂåÖÊã¨updateStateByKeyÂíåmapWithState‰∏Ä„ÄÅupdateStateByKeyÂÆòÁΩëÂéüËØùÔºöIn every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.ÁªüËÆ°ÂÖ®Â±ÄÁöÑkeyÁöÑÁä∂ÊÄÅÔºå‰ΩÜÊòØÂ∞±ÁÆóÊ≤°ÊúâÊï∞ÊçÆËæìÂÖ•Ôºå‰ªñ‰πü‰ºöÂú®ÊØè‰∏Ä‰∏™ÊâπÊ¨°ÁöÑÊó∂ÂÄôËøîÂõû‰πãÂâçÁöÑkeyÁöÑÁä∂ÊÄÅ„ÄÇËøôÊ†∑ÁöÑÁº∫ÁÇπÔºöÂ¶ÇÊûúÊï∞ÊçÆÈáèÂ§™Â§ßÁöÑËØùÔºåÊàë‰ª¨ÈúÄË¶ÅcheckpointÊï∞ÊçÆ‰ºöÂç†Áî®ËæÉÂ§ßÁöÑÂ≠òÂÇ®„ÄÇËÄå‰∏îÊïàÁéá‰πü‰∏çÈ´ò123456789101112131415161718192021222324252627282930//[root@bda3 ~]# nc -lk 9999 object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //Ê≥®ÊÑèÔºöupdateStateByKeyÂøÖÈ°ªËÆæÁΩÆcheckpointÁõÆÂΩï ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() // ‰∏ÄÂÆöË¶ÅÂÜô ssc.awaitTermination() &#125; /*Áä∂ÊÄÅÊõ¥Êñ∞ÂáΩÊï∞ * @param currentValues keyÁõ∏ÂêåvalueÂΩ¢ÊàêÁöÑÂàóË°® * @param preValues keyÂØπÂ∫îÁöÑvalueÔºåÂâç‰∏ÄÁä∂ÊÄÅ * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seqÂàóË°®‰∏≠ÊâÄÊúâvalueÊ±ÇÂíå val pre = preValues.getOrElse(0) //Ëé∑Âèñ‰∏ä‰∏ÄÁä∂ÊÄÅÂÄº Some(curr + pre) &#125; &#125;‰∫å„ÄÅmapWithState (ÊïàÁéáÊõ¥È´òÔºåÁîü‰∫ß‰∏≠Âª∫ËÆÆ‰ΩøÁî®)mapWithStateÔºö‰πüÊòØÁî®‰∫éÂÖ®Â±ÄÁªüËÆ°keyÁöÑÁä∂ÊÄÅÔºå‰ΩÜÊòØÂÆÉÂ¶ÇÊûúÊ≤°ÊúâÊï∞ÊçÆËæìÂÖ•Ôºå‰æø‰∏ç‰ºöËøîÂõû‰πãÂâçÁöÑkeyÁöÑÁä∂ÊÄÅÔºåÊúâ‰∏ÄÁÇπÂ¢ûÈáèÁöÑÊÑüËßâ„ÄÇËøôÊ†∑ÂÅöÁöÑÂ•ΩÂ§ÑÊòØÔºåÊàë‰ª¨ÂèØ‰ª•Âè™ÊòØÂÖ≥ÂøÉÈÇ£‰∫õÂ∑≤ÁªèÂèëÁîüÁöÑÂèòÂåñÁöÑkeyÔºåÂØπ‰∫éÊ≤°ÊúâÊï∞ÊçÆËæìÂÖ•ÔºåÂàô‰∏ç‰ºöËøîÂõûÈÇ£‰∫õÊ≤°ÊúâÂèòÂåñÁöÑkeyÁöÑÊï∞ÊçÆ„ÄÇËøôÊ†∑ÁöÑËØùÔºåÂç≥‰ΩøÊï∞ÊçÆÈáèÂæàÂ§ßÔºåcheckpoint‰πü‰∏ç‰ºöÂÉèupdateStateByKeyÈÇ£Ê†∑ÔºåÂç†Áî®Â§™Â§öÁöÑÂ≠òÂÇ®„ÄÇÂÆòÊñπ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every * second starting with initial value of word count. * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive * data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example * org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999` */ object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;) // Create the context with a 1 second batch size val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) // Initial state RDD for mapWithState operation val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1))) // Create a ReceiverInputDStream on target ip:port and count the // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) // Update the cumulative count using mapWithState // This will give a DStream made of state (which is the cumulative count of the words) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache SparkÂíåDL/AIÁªìÂêàÔºåË∞Å‰∏é‰∫âÈîã? ÊúüÂæÖSpark3.0ÁöÑÂà∞Êù•ÔºÅ]]></title>
    <url>%2F2018%2F06%2F22%2FAI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20%2F</url>
    <content type="text"><![CDATA[‰∏çÁü•ÂêÑ‰ΩçÔºåÊòØÂê¶ÂÖ≥Ê≥®Á§æÂå∫ÁöÑÂèëÂ±ïÔºüÂÖ≥Ê≥®SparkÂë¢ÔºüÂÆòÁΩëÁöÑSparkÂõæÊ†áÂíåËß£ÈáäËØ≠Â∑≤ÁªèÂèëÁîüÂèòÂåñ‰∫Ü„ÄÇÁÑ∂ËÄåÂú®6-18Âè∑ÔºåÁ§æÂå∫ÊèêÂá∫Spark and DL/AIÁõ∏ÁªìÂêàÔºåËøôÊó†ÊØîÂÜç‰∏ÄÊ¨°ËØ¥ÊòéÔºåSparkÂú®Â§ßÊï∞ÊçÆÁöÑÂú∞‰ΩçÊòØÊó†Ê≥ïÊíºÂä®ÁöÑÔºÅÊúüÂæÖSpark3.0ÁöÑÂà∞Êù•ÔºÅÊé•‰∏ãÊù•ÂØπSPARK-24579ÁöÑÁøªËØë:Âú®Â§ßÊï∞ÊçÆÂíå‰∫∫Â∑•Êô∫ËÉΩÁöÑÂçÅÂ≠óË∑ØÂè£ÔºåÊàë‰ª¨ÁúãÂà∞‰∫ÜApache Spark‰Ωú‰∏∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÊûêÂºïÊìé‰ª•ÂèäAIÊ°ÜÊû∂Â¶ÇTensorFlowÂíåApache MXNet (Ê≠£Âú®Â≠µÂåñ‰∏≠)ÁöÑÂÖ¥Ëµ∑ÂèäËøô‰∏§Â§ßÂùóÁöÑÂ∑®Â§ßÊàêÂäü „ÄÇÂ§ßÊï∞ÊçÆÂíå‰∫∫Â∑•Êô∫ËÉΩÈÉΩÊòØÊé®Âä®‰ºÅ‰∏öÂàõÊñ∞ÁöÑ‰∏çÂèØÊàñÁº∫ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºå ‰∏§‰∏™Á§æÂå∫ÁöÑÂ§öÊ¨°Â∞ùËØïÔºå‰Ωø‰ªñ‰ª¨ÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇÊàë‰ª¨ÁúãÂà∞AIÁ§æÂå∫ÁöÑÂä™ÂäõÔºå‰∏∫AIÊ°ÜÊû∂ÂÆûÁé∞Êï∞ÊçÆËß£ÂÜ≥ÊñπÊ°àÔºåÂ¶ÇTF.DATAÂíåTF.Tror„ÄÇÁÑ∂ËÄåÔºå50+‰∏™Êï∞ÊçÆÊ∫êÂíåÂÜÖÁΩÆSQL„ÄÅÊï∞ÊçÆÊµÅÂíåÊµÅÁâπÂæÅÔºåSpark‰ªçÁÑ∂ÊòØÂØπ‰∫éÂ§ßÊï∞ÊçÆÁ§æÂå∫ÈÄâÊã©„ÄÇËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàÊàë‰ª¨ÁúãÂà∞ËÆ∏Â§öÂä™Âäõ,Â∞ÜDL/AIÊ°ÜÊû∂‰∏éSparkÁªìÂêàËµ∑Êù•Ôºå‰ª•Âà©Áî®ÂÆÉÁöÑÂäõÈáèÔºå‰æãÂ¶ÇÔºåSparkÊï∞ÊçÆÊ∫êTFRecords„ÄÅTensorFlowOnSpark, TensorFramesÁ≠â„ÄÇ‰Ωú‰∏∫È°πÁõÆHydrogenÁöÑ‰∏ÄÈÉ®ÂàÜÔºåËøô‰∏™SPIPÂ∞ÜSpark+AI‰ªé‰∏çÂêåÁöÑËßíÂ∫¶Áªü‰∏ÄËµ∑Êù•„ÄÇÊ≤°ÊúâÂú®SparkÂíåÂ§ñÈÉ®DL/AIÊ°ÜÊû∂‰πãÈó¥‰∫§Êç¢Êï∞ÊçÆÔºåËøô‰∫õÈõÜÊàêÈÉΩÊòØ‰∏çÂèØËÉΩÁöÑ,‰πüÊúâÊÄßËÉΩÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçËøòÊ≤°Êúâ‰∏ÄÁßçÊ†áÂáÜÁöÑÊñπÂºèÊù•‰∫§Êç¢Êï∞ÊçÆÔºåÂõ†Ê≠§ÂÆûÁé∞ÂíåÊÄßËÉΩ‰ºòÂåñÂ∞±Èô∑ÂÖ•‰∫ÜÂõ∞Â¢É„ÄÇ‰æãÂ¶ÇÔºåÂú®Python‰∏≠ÔºåTensorFlowOnSpark‰ΩøÁî®Hadoop InputFormat/OutputFormat‰Ωú‰∏∫TensorFlowÁöÑTFRecordsÔºåÊù•Âä†ËΩΩÂíå‰øùÂ≠òÊï∞ÊçÆÔºåÂπ∂Â∞ÜRDDÊï∞ÊçÆ‰º†ÈÄíÁªôTensorFlow„ÄÇTensorFrames‰ΩøÁî®TensorFlowÁöÑJava APIÔºåËΩ¨Êç¢‰∏∫ Spark DataFrames Rows to/from TensorFlow Tensors „ÄÇÊàë‰ª¨ÊÄéÊ†∑ÊâçËÉΩÈôç‰ΩéÂ§çÊùÇÊÄßÂë¢?ËøôÈáåÁöÑÂª∫ËÆÆÊòØÊ†áÂáÜÂåñSparkÂíåDL/AIÊ°ÜÊû∂‰πãÈó¥ÁöÑÊï∞ÊçÆ‰∫§Êç¢Êé•Âè£(ÊàñÊ†ºÂºè)ÔºåÂπ∂‰ºòÂåñ‰ªé/Âà∞Ëøô‰∏™Êé•Âè£ÁöÑÊï∞ÊçÆËΩ¨Êç¢„ÄÇÂõ†Ê≠§ÔºåDL/AIÊ°ÜÊû∂ÂèØ‰ª•Âà©Áî®Spark‰ªé‰ªª‰ΩïÂú∞ÊñπÂä†ËΩΩÊï∞ÊçÆÔºåËÄåÊó†ÈúÄËä±Ë¥πÈ¢ùÂ§ñÁöÑÁ≤æÂäõÊûÑÂª∫Â§çÊùÇÁöÑÊï∞ÊçÆËß£ÂÜ≥ÊñπÊ°àÔºåÊØîÂ¶Ç‰ªéÁîü‰∫ßÊï∞ÊçÆ‰ªìÂ∫ìËØªÂèñÁâπÊÄßÊàñÊµÅÊ®°ÂûãÊé®Êñ≠„ÄÇSparkÁî®Êà∑ÂèØ‰ª•‰ΩøÁî®DL/AIÊ°ÜÊû∂ÔºåËÄåÊó†ÈúÄÂ≠¶‰π†ÈÇ£ÈáåÂÆûÁé∞ÁöÑÁâπÂÆöÊï∞ÊçÆapi„ÄÇËÄå‰∏îÂèåÊñπÁöÑÂºÄÂèë‰∫∫ÂëòÈÉΩÂèØ‰ª•Áã¨Á´ãÂú∞ËøõË°åÊÄßËÉΩ‰ºòÂåñÔºåÂõ†‰∏∫Êé•Âè£Êú¨Ë∫´‰∏ç‰ºöÂ∏¶Êù•ÂæàÂ§ßÁöÑÂºÄÈîÄ„ÄÇISSUE: https://issues.apache.org/jira/browse/SPARK-24579Ëã•Ê≥ΩÊï∞ÊçÆÔºåÊòüÊòüÊú¨‰∫∫Ê∞¥Âπ≥ÊúâÈôêÔºåÁøªËØëÂ§öÂ§öÂåÖÊ∂µ„ÄÇÂØπ‰∫ÜÂøòËÆ∞ËØ¥‰∫ÜÔºåÊú¨ISSUEÊúâ‰∏™PDFÊñáÊ°£ÔºåËµ∂Âø´Âéª‰∏ãËΩΩÂêß„ÄÇhttps://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊúÄÂâçÊ≤øÔºÅÂ∏¶‰Ω†ËØªStructured StreamingÈáçÈáèÁ∫ßËÆ∫ÊñáÔºÅ]]></title>
    <url>%2F2018%2F06%2F14%2F%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81%2F</url>
    <content type="text"><![CDATA[1.ËÆ∫Êñá‰∏ãËΩΩÂú∞ÂùÄhttps://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf2.ÂâçË®ÄÂª∫ËÆÆÈ¶ñÂÖàÈòÖËØªStructured StreamingÂÆòÁΩëÔºöhttp://spark.apache.org/docs/latest/structured-streaming-programming-guide.html‰ª•ÂèäËøô‰∏§ÁØáDatabricksÂú®2016Âπ¥ÂÖ≥‰∫éStructured StreamingÁöÑÊñáÁ´†Ôºöhttps://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.htmlhttps://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.htmlË®ÄÂΩíÊ≠£‰º†ËØ•ËÆ∫ÊñáÊî∂ÂΩïËá™2018Âπ¥ACM SIGMOD‰ºöËÆÆÔºåÊòØÁî±ÁæéÂõΩËÆ°ÁÆóÊú∫Âçè‰ºöÔºàACMÔºâÂèëËµ∑ÁöÑ„ÄÅÂú®Êï∞ÊçÆÂ∫ìÈ¢ÜÂüüÂÖ∑ÊúâÊúÄÈ´òÂ≠¶ÊúØÂú∞‰ΩçÁöÑÂõΩÈôÖÊÄßÂ≠¶ÊúØ‰ºöËÆÆ„ÄÇËÆ∫ÊñáÁöÑ‰ΩúËÄÖ‰∏∫DatabricksÁöÑÂ∑•Á®ãÂ∏àÂèäSparkÁöÑÂºÄÂèëËÄÖÔºåÂÖ∂ÊùÉÂ®ÅÊÄß„ÄÅÈáçË¶ÅÁ®ãÂ∫¶‰∏çË®ÄËÄåÂñª„ÄÇÊñáÁ´†ÂºÄÂ§¥‰∏∫ËØ•ËÆ∫ÊñáÁöÑ‰∏ãËΩΩÂú∞ÂùÄÔºå‰æõËØªËÄÖÈòÖËØª‰∫§ÊµÅ„ÄÇÊú¨ÊñáÂØπËØ•ËÆ∫ÊñáËøõË°åÁÆÄË¶ÅÁöÑÊÄªÁªìÔºåÂ∏åÊúõÂ§ßÂÆ∂ËÉΩÂ§ü‰∏ãËΩΩÂéüÊñáÁªÜÁªÜÂìÅËØªÔºå‰∫ÜËß£ÊúÄÂâçÊ≤øÁöÑÂ§ßÊï∞ÊçÆÊäÄÊúØ„ÄÇ3.ËÆ∫ÊñáÁÆÄË¶ÅÊÄªÁªìÈ¢òÁõÆÔºöStructured Streaming: A Declarative API for Real-Time Applications in Apache Spark3.1 ÊëòË¶ÅÊëòË¶ÅÊòØ‰∏ÄÁØáËÆ∫ÊñáÁöÑÁ≤æÈ´ìÔºåËøôÈáåÁªôÂá∫ÊëòË¶ÅÂÆåÊï¥ÁöÑÁøªËØë„ÄÇÈöèÁùÄÂÆûÊó∂Êï∞ÊçÆÁöÑÊôÆÈÅçÂ≠òÂú®ÔºåÊàë‰ª¨ÈúÄË¶ÅÂèØÊâ©Â±ïÁöÑ„ÄÅÊòìÁî®ÁöÑ„ÄÅÊòì‰∫éÈõÜÊàêÁöÑÊµÅÂºèÂ§ÑÁêÜÁ≥ªÁªü„ÄÇÁªìÊûÑÂåñÊµÅÊòØÂü∫‰∫éÊàë‰ª¨ÂØπSpark StreamingÁöÑÁªèÈ™åÂºÄÂèëÂá∫Êù•ÁöÑÈ´òÁ∫ßÂà´ÁöÑSparkÊµÅÂºèAPI„ÄÇÁªìÊûÑÂåñÊµÅ‰∏éÂÖ∂‰ªñÁé∞ÊúâÁöÑÊµÅÂºèAPIÔºåÂ¶ÇË∞∑Ê≠åÁöÑDataflowÔºå‰∏ªË¶ÅÊúâ‰∏§ÁÇπ‰∏çÂêå„ÄÇÁ¨¨‰∏ÄÔºåÂÆÉÊòØ‰∏Ä‰∏™Âü∫‰∫éËá™Âä®Â¢ûÈáèÂåñÁöÑÂÖ≥Á≥ªÂûãÊü•ËØ¢APIÔºåÊó†ÈúÄÁî®Êà∑Ëá™Â∑±ÊûÑÂª∫DAGÔºõÁ¨¨‰∫åÔºåÁªìÊûÑÂåñÊµÅÊó®Âú®‰∫éÊîØÊåÅÁ´ØÂà∞Á´ØÁöÑÂÆûÊó∂Â∫îÁî®Âπ∂Êï¥ÂêàÊµÅ‰∏éÊâπÂ§ÑÁêÜÁöÑ‰∫§‰∫íÂàÜÊûê„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞ËøôÁßçÊï¥ÂêàÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑÊåëÊàò„ÄÇÁªìÊûÑÂåñÊµÅÈÄöËøáSpark SQLÁöÑ‰ª£Á†ÅÁîüÊàêÂºïÊìéÂÆûÁé∞‰∫ÜÂæàÈ´òÁöÑÊÄßËÉΩÔºåÊòØApache FlinkÁöÑ‰∏§ÂÄç‰ª•ÂèäApache KafkaÁöÑ90ÂÄç„ÄÇÂÆÉËøòÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑËøêË°åÁâπÊÄßÔºåÂ¶ÇÂõûÊªö„ÄÅ‰ª£Á†ÅÊõ¥Êñ∞‰ª•ÂèäÊµÅ/ÊâπÊ∑∑ÂêàÊâßË°å„ÄÇÊúÄÂêéÊàë‰ª¨ÊèèËø∞‰∫ÜÁ≥ªÁªüÁöÑËÆæËÆ°‰ª•ÂèäÈÉ®ÁΩ≤Âú®DatabricksÂá†Áôæ‰∏™Áîü‰∫ßËäÇÁÇπÁöÑ‰∏Ä‰∏™Áî®‰æã„ÄÇ3.2 ÊµÅÂºèÂ§ÑÁêÜÈù¢‰∏¥ÁöÑÊåëÊàò(1) Â§çÊùÇ„ÄÅ‰ΩéÁ∫ßÂà´ÁöÑAPI(2) Á´ØÂà∞Á´ØÂ∫îÁî®ÁöÑÈõÜÊàê(3) ËøêË°åÊó∂ÊåëÊàòÔºöÂÆπÁÅæÔºå‰ª£Á†ÅÊõ¥Êñ∞ÔºåÁõëÊéßÁ≠â(4) ÊàêÊú¨ÂíåÊÄßËÉΩÊåëÊàò3.3 ÁªìÊûÑÂåñÊµÅÂü∫Êú¨Ê¶ÇÂøµÂõæ1 ÁªìÊûÑÂåñÊµÅÁöÑÁªÑÊàêÈÉ®ÂàÜ(1) Input and OutputInput sources ÂøÖÈ°ªÊòØ replayable ÁöÑÔºåÊîØÊåÅËäÇÁÇπÂÆïÊú∫Âêé‰ªéÂΩìÂâçËæìÂÖ•ÁªßÁª≠ËØªÂèñ„ÄÇ‰æãÂ¶ÇÔºöApache KinesisÂíåApache Kafka„ÄÇOutput sinks ÂøÖÈ°ªÊîØÊåÅ idempotent ÔºàÂπÇÁ≠âÔºâÔºåÁ°Æ‰øùÂú®ËäÇÁÇπÂÆïÊú∫Êó∂ÂèØÈù†ÁöÑÊÅ¢Â§ç„ÄÇ(2) APIsÁºñÂÜôÁªìÊûÑÂåñÊµÅÁ®ãÂ∫èÊó∂ÔºåÂèØ‰ª•‰ΩøÁî®Spark SQLÁöÑAPIsÔºöDataFrameÂíåSQLÊù•Êü•ËØ¢streamsÂíåtablesÔºåËØ•Êü•ËØ¢ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™output tableÔºàËæìÂá∫Ë°®ÔºâÔºåÁî®Êù•Êé•Êî∂Êù•Ëá™steamÁöÑÊï∞ÊçÆ„ÄÇengineÂÜ≥ÂÆöÂ¶Ç‰ΩïËÆ°ÁÆóÂπ∂Â∞ÜËæìÂá∫Ë°® incrementallyÔºàÂ¢ûÈáèÂú∞ÔºâÂÜôÂÖ•sink„ÄÇ‰∏çÂêåÁöÑsinksÊîØÊåÅ‰∏çÂêåÁöÑoutput modesÔºàËæìÂá∫Ê®°ÂºèÔºåÂêéÈù¢‰ºöÊèêÂà∞Ôºâ„ÄÇ‰∏∫‰∫ÜÂ§ÑÁêÜÊµÅÂºèÊï∞ÊçÆÔºåÁªìÊûÑÂåñÊµÅËøòÂ¢ûÂä†‰∫Ü‰∏Ä‰∫õAPIs‰∏éÂ∑≤ÊúâÁöÑSpark SQL APIÁõ∏ÈÖçÂêàÔºöa. Triggers ÊéßÂà∂engineÂ§ö‰πÖÊâßË°å‰∏ÄÊ¨°ËÆ°ÁÆób. event time ÊòØÊï∞ÊçÆÊ∫êÁöÑÊó∂Èó¥Êà≥Ôºõwatermark Á≠ñÁï•Ôºå‰∏éevent time Áõ∏Â∑Æ‰∏ÄÊÆµÊó∂Èó¥Âêé‰∏çÂÜçÊé•Êî∂Êï∞ÊçÆ„ÄÇc.Stateful operatorÔºàÁä∂ÊÄÅÁÆóÂ≠êÔºâÔºåÁ±ª‰ºº‰∫éSpark Streaming ÁöÑupdateStateByKey„ÄÇ(3) ÊâßË°å‰∏ÄÊó¶Êé•Êî∂Âà∞‰∫ÜÊü•ËØ¢ÔºåÁªìÊûÑÂåñÊµÅÂ∞±‰ºöËøõË°å‰ºòÂåñÈÄíÂ¢ûÔºåÂπ∂ÂºÄÂßãÊâßË°å„ÄÇÁªìÊûÑÂåñÊµÅ‰ΩøÁî®‰∏§ÁßçÊåÅ‰πÖÂåñÂ≠òÂÇ®ÁöÑÊñπÂºèÂÆûÁé∞ÂÆπÈîôÔºöa.write-ahead log ÔºàWALÔºöÈ¢ÑÂÜôÊó•ÂøóÔºâÊåÅÁª≠ËøΩË∏™Âì™‰∫õÊï∞ÊçÆÂ∑≤Ë¢´ÊâßË°åÔºåÁ°Æ‰øùÊï∞ÊçÆÁöÑÂèØÈù†ÂÜôÂÖ•„ÄÇb.Á≥ªÁªüÈááÁî®Â§ßËßÑÊ®°ÁöÑ state storeÔºàÁä∂ÊÄÅÂ≠òÂÇ®ÔºâÊù•‰øùÂ≠òÈïøÊó∂Èó¥ËøêË°åÁöÑËÅöÂêàÁÆóÂ≠êÁöÑÁÆóÂ≠êÁä∂ÊÄÅÂø´ÁÖß„ÄÇ3.4 ÁºñÁ®ãÊ®°ÂûãÁªìÊûÑÂåñÊµÅÂ∞ÜË∞∑Ê≠åÁöÑDataflow„ÄÅÂ¢ûÈáèÊü•ËØ¢ÂíåSpark Streaming ÁªìÂêàËµ∑Êù•Ôºå‰ª•‰æøÂú®Spark SQL‰∏ãÂÆûÁé∞ÊµÅÂºèÂ§ÑÁêÜ„ÄÇa. A Short ExampleÈ¶ñÂÖà‰ªé‰∏Ä‰∏™ÊâπÂ§ÑÁêÜ‰Ωú‰∏öÂºÄÂßãÔºåÁªüËÆ°‰∏Ä‰∏™webÂ∫îÁî®Âú®‰∏çÂêåÂõΩÂÆ∂ÁöÑÁÇπÂáªÊï∞„ÄÇÂÅáËÆæËæìÂÖ•Êï∞ÊçÆÊòØ‰∏Ä‰∏™JSONÊñá‰ª∂ÔºåËæìÂá∫‰∏Ä‰∏™ParquetÊñá‰ª∂ÔºåËØ•‰Ωú‰∏öÂèØ‰ª•ÈÄöËøáDataFrameÊù•ÂÆåÊàêÔºö 1234561// Define a DataFrame to read from static data2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a static data sink6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)ÊääËØ•‰Ωú‰∏öÂèòÊàê‰ΩøÁî®ÁªìÊûÑÂåñÊµÅ‰ªÖ‰ªÖÈúÄË¶ÅÊîπÂèòËæìÂÖ•ÂíåËæìÂá∫Ê∫êÔºå‰æãÂ¶ÇÔºåÂ¶ÇÊûúÊñ∞ÁöÑJSONÊñá‰ª∂continuallyÔºàÊåÅÁª≠Âú∞Ôºâ‰∏ä‰º†ÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÊîπÂèòÁ¨¨‰∏ÄË°åÂíåÊúÄÂêé‰∏ÄË°å„ÄÇ12345671// Define a DataFrame to read streaming data2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a streaming data sink6counts . writeStream . format (&quot; parquet &quot;)7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)ÁªìÊûÑÂåñÊµÅ‰πüÊîØÊåÅ windowingÔºàÁ™óÂè£ÔºâÂíåÈÄöËøáSpark SQLÂ∑≤Â≠òÂú®ÁöÑËÅöÂêàÁÆóÂ≠êÂ§ÑÁêÜevent time„ÄÇ‰æãÂ¶ÇÔºöÊàë‰ª¨ÂèØ‰ª•ÈÄöËøá‰øÆÊîπ‰∏≠Èó¥ÁöÑ‰ª£Á†ÅÔºåËÆ°ÁÆó1Â∞èÊó∂ÁöÑÊªëÂä®Á™óÂè£ÔºåÊØè‰∫îÂàÜÈíüÂâçËøõ‰∏ÄÊ¨°Ôºö121// Count events by windows on the &quot; time &quot; field2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()b. ÁºñÁ®ãÊ®°ÂûãËØ≠‰πâÂõæ 2 ‰∏§ÁßçËæìÂá∫Ê®°Âºèi. ÊØè‰∏Ä‰∏™ËæìÂÖ•Ê∫êÊèê‰æõ‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÊó∂Èó¥ÁöÑÈÉ®ÂàÜÊúâÂ∫èÁöÑËÆ∞ÂΩïÈõÜÔºàset of recordsÔºâÔºå‰æãÂ¶ÇÔºåKafkaÂ∞ÜÊµÅÂºèÊï∞ÊçÆÂàÜ‰∏∫ÂêÑËá™ÊúâÂ∫èÁöÑpartitions„ÄÇii. Áî®Êà∑Êèê‰æõË∑®ËæìÂÖ•Êï∞ÊçÆÊâßË°åÁöÑÊü•ËØ¢ÔºåËØ•ËæìÂÖ•Êï∞ÊçÆÂèØ‰ª•Âú®‰ªªÊÑèÁªôÂÆöÁöÑÂ§ÑÁêÜÊó∂Èó¥ÁÇπËæìÂá∫‰∏Ä‰∏™ result tableÔºàÁªìÊûúË°®Ôºâ„ÄÇÁªìÊûÑÂåñÊµÅÊÄª‰ºö‰∫ßÁîü‰∏éÊâÄÊúâËæìÂÖ•Ê∫êÁöÑÊï∞ÊçÆÁöÑÂâçÁºÄ‰∏äÔºàprefix of the data in all input sourcesÔºâÊü•ËØ¢Áõ∏‰∏ÄËá¥ÁöÑÁªìÊûú„ÄÇiii. Triggers ÂëäËØâÁ≥ªÁªü‰ΩïÊó∂ÂéªËøêË°å‰∏Ä‰∏™Êñ∞ÁöÑÂ¢ûÈáèËÆ°ÁÆóÔºå‰ΩïÊó∂Êõ¥Êñ∞result table„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂæÆÊâπÂ§ÑÁêÜÊ®°ÂºèÔºåÁî®Êà∑Â∏åÊúõ‰ºöÊØèÂàÜÈíüËß¶Âèë‰∏ÄÊ¨°Â¢ûÈáèËÆ°ÁÆó„ÄÇiiii. engineÊîØÊåÅ‰∏âÁßçoutput modeÔºö CompleteÔºöengine‰∏ÄÊ¨°ÂÜôÊâÄÊúâresult table„ÄÇ AppendÔºöengine‰ªÖ‰ªÖÂêësinkÂ¢ûÂä†ËÆ∞ÂΩï„ÄÇ UpdateÔºöengineÂü∫‰∫ékeyÊõ¥Êñ∞ÊØè‰∏Ä‰∏™recordÔºåÊõ¥Êñ∞ÂÄºÊîπÂèòÁöÑkeys„ÄÇ ËØ•Ê®°ÂûãÊúâ‰∏§‰∏™ÁâπÊÄßÔºöÁ¨¨‰∏ÄÔºåÁªìÊûúË°®ÁöÑÂÜÖÂÆπÁã¨Á´ã‰∫éËæìÂá∫Ê®°Âºè„ÄÇÁ¨¨‰∫åÔºåËØ•Ê®°ÂûãÂÖ∑ÊúâÂæàÂº∫ÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄßÔºåË¢´Áß∞‰∏∫prefix consistency„ÄÇ c.ÊµÅÂºèÁÆóÂ≠êÂä†ÂÖ•‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÁÆóÂ≠êÔºöwatermarkingÁÆóÂ≠êÂëäËØâÁ≥ªÁªü‰ΩïÊó∂ÂÖ≥Èó≠event time windowÂíåËæìÂá∫ÁªìÊûúÔºõÁªìÊûÑÂåñÊµÅÂÖÅËÆ∏Áî®Êà∑ÈÄöËøáwithWatermarkÁÆóÂ≠êÊù•ËÆæÁΩÆ‰∏Ä‰∏™watermarkÔºåËØ•ÁÆóÂ≠êÁªôÁ≥ªÁªüËÆæÁΩÆ‰∏Ä‰∏™ÁªôÂÆöÊó∂Èó¥Êà≥CÁöÑÂª∂ËøüÈòàÂÄºtCÔºåÂú®‰ªªÊÑèÊó∂Èó¥ÁÇπÔºåCÁöÑwatermarkÊòØmaxÔºàCÔºâ-tC„ÄÇ stateful operatorsÂÖÅËÆ∏Áî®Êà∑ÁºñÂÜôËá™ÂÆö‰πâÈÄªËæëÊù•ÂÆûÁé∞Â§çÊùÇÁöÑÂäüËÉΩ„ÄÇ 1234567891011121314 1// Define an update function that simply tracks the 2// number of events for each key as its state , returns 3// that as its result , and times out keys after 30 min. 4def updateFunc (key: UserId , newValues : Iterator [ Event ], 5state : GroupState [Int ]): Int = &#123; 6val totalEvents = state .get () + newValues . size () 7state . update ( totalEvents ) 8state . setTimeoutDuration (&quot;30 min&quot;) 9return totalEvents10&#125;11// Use this update function on a stream , returning a12// new table lens that contains the session lengths .13lens = events . groupByKey ( event =&gt; event . userId )14. mapGroupsWithState ( updateFunc )Áî®mapGroupWithStateÁÆóÂ≠êÊù•ËøΩË∏™ÊØè‰∏™‰ºöËØùÁöÑ‰∫ã‰ª∂Êï∞ÈáèÔºå30ÂàÜÈíüÂêéÂÖ≥Èó≠‰ºöËØù„ÄÇ3.5 ËøêË°åÁâπÊÄß(1) ‰ª£Á†ÅÊõ¥Êñ∞Ôºàcode updateÔºâÂºÄÂèëËÄÖËÉΩÂ§üÂú®ÁºñÁ®ãËøáÁ®ã‰∏≠Êõ¥Êñ∞UDFÔºåÂπ∂‰∏îÂèØ‰ª•ÁÆÄÂçïÁöÑÈáçÂêØ‰ª•‰ΩøÁî®Êñ∞ÁâàÊú¨ÁöÑ‰ª£Á†Å„ÄÇ(2) ÊâãÂä®ÂõûÊªöÔºàmanual rollbackÔºâÊúâÊó∂Âú®Áî®Êà∑ÂèëÁé∞‰πãÂâçÔºåÁ®ãÂ∫è‰ºöËæìÂá∫ÈîôËØØÁöÑÁªìÊûúÔºåÂõ†Ê≠§ÂõûÊªöËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁªìÊûÑÂåñÊµÅÂæàÂÆπÊòìÂÆö‰ΩçÈóÆÈ¢òÊâÄÂú®„ÄÇÂêåÊó∂ÊâãÂä®ÂõûÊªö‰∏éÂâçÈù¢ÊèêÂà∞ÁöÑprefix consistencyÊúâÂæàÂ•ΩÁöÑ‰∫§‰∫í„ÄÇ(3) ÊµÅÂºèÂíåÊâπÊ¨°Ê∑∑ÂêàÂ§ÑÁêÜËøôÊòØÁªìÊûÑÂåñÊµÅÊúÄÊòæËÄåÊòìËßÅÁöÑÂ•ΩÂ§ÑÔºåÁî®Êà∑ËÉΩÂ§üÂÖ±Áî®ÊµÅÂºèÂ§ÑÁêÜÂíåÊâπÂ§ÑÁêÜ‰Ωú‰∏öÁöÑ‰ª£Á†Å„ÄÇ(4) ÁõëÊéßÁªìÊûÑÂåñÊµÅ‰ΩøÁî®SparkÂ∑≤ÊúâÁöÑAPIÂíåÁªìÊûÑÂåñÊó•ÂøóÊù•Êä•Âëä‰ø°ÊÅØÔºå‰æãÂ¶ÇÂ§ÑÁêÜËøáÁöÑËÆ∞ÂΩïÊï∞ÈáèÔºåË∑®ÁΩëÁªúÁöÑÂ≠óËäÇÊï∞Á≠â„ÄÇËøô‰∫õÊé•Âè£Ë¢´SparkÂºÄÂèëËÄÖÊâÄÁÜüÁü•ÔºåÂπ∂Êòì‰∫éËøûÊé•Âà∞‰∏çÂêåÁöÑUIÂ∑•ÂÖ∑„ÄÇ3.6 Áîü‰∫ßÁî®‰æã‰∏éÊÄªÁªìÁªôÂá∫ÁÆÄË¶ÅÊû∂ÊûÑÂõæÔºåÁØáÂπÖÂéüÂõ†‰∏çÂÜçËµòËø∞ÔºåÂ∏åÊúõËØ¶ÁªÜ‰∫ÜËß£ÁöÑ‰∏ãËΩΩËÆ∫ÊñáËá™Ë°åÈòÖËØª„ÄÇÊú¨ÊñáÂè™ÊåëÈÄâ‰∫ÜÈÉ®ÂàÜÂÖ≥ÈîÆÁÇπËøõË°å‰∫ÜÊµÖÂ±ÇÊ¨°ÁöÑÂèôËø∞ÔºåÂ∏åÊúõËØªËÄÖËÉΩÂ§üÂ∞ÜËÆ∫Êñá‰∏ãËΩΩ‰∏ãÊù•ËÆ§ÁúüÂìÅËØªÔºåÊêûÊáÇÂºÄÂèëËÄÖÁöÑÂºÄÂèëÊÄùË∑ØÔºåË∑ü‰∏äÂ§ßÊï∞ÊçÆÁöÑÂâçÊ≤øÊ≠•‰ºê„ÄÇ]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áîü‰∫ßÂºÄÂèëÂøÖÁî®-Spark RDDËΩ¨DataFrameÁöÑ‰∏§ÁßçÊñπÊ≥ï]]></title>
    <url>%2F2018%2F06%2F14%2F%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Êú¨ÁØáÊñáÁ´†Â∞Ü‰ªãÁªçSpark SQL‰∏≠ÁöÑDataFrameÔºåÂÖ≥‰∫éDataFrameÁöÑ‰ªãÁªçÂèØ‰ª•ÂèÇËÄÉ:https://blog.csdn.net/lemonzhaotao/article/details/80211231Âú®Êú¨ÁØáÊñáÁ´†‰∏≠ÔºåÂ∞Ü‰ªãÁªçRDDËΩ¨Êç¢‰∏∫DataFrameÁöÑ2ÁßçÊñπÂºèÂÆòÁΩë‰πãRDDËΩ¨DF:http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rddsDataFrame ‰∏é RDD ÁöÑ‰∫§‰∫íSpark SQLÂÆÉÊîØÊåÅ‰∏§Áßç‰∏çÂêåÁöÑÊñπÂºèËΩ¨Êç¢Â∑≤ÁªèÂ≠òÂú®ÁöÑRDDÂà∞DataFrameÊñπÊ≥ï‰∏ÄÁ¨¨‰∏ÄÁßçÊñπÂºèÊòØ‰ΩøÁî®ÂèçÂ∞ÑÁöÑÊñπÂºèÔºåÁî®ÂèçÂ∞ÑÂéªÊé®ÂÄíÂá∫Êù•RDDÈáåÈù¢ÁöÑschema„ÄÇËøô‰∏™ÊñπÂºèÁÆÄÂçïÔºå‰ΩÜÊòØ‰∏çÂª∫ËÆÆ‰ΩøÁî®ÔºåÂõ†‰∏∫Âú®Â∑•‰ΩúÂΩì‰∏≠Ôºå‰ΩøÁî®ËøôÁßçÊñπÂºèÊòØÊúâÈôêÂà∂ÁöÑ„ÄÇÂØπ‰∫é‰ª•ÂâçÁöÑÁâàÊú¨Êù•ËØ¥Ôºåcase classÊúÄÂ§öÊîØÊåÅ22‰∏™Â≠óÊÆµÂ¶ÇÊûúË∂ÖËøá‰∫Ü22‰∏™Â≠óÊÆµÔºåÊàë‰ª¨Â∞±ÂøÖÈ°ªË¶ÅËá™Â∑±ÂºÄÂèë‰∏Ä‰∏™Á±ªÔºåÂÆûÁé∞productÊé•Âè£ÊâçË°å„ÄÇÂõ†Ê≠§ËøôÁßçÊñπÂºèËôΩÁÑ∂ÁÆÄÂçïÔºå‰ΩÜÊòØ‰∏çÈÄöÁî®ÔºõÂõ†‰∏∫Áîü‰∫ß‰∏≠ÁöÑÂ≠óÊÆµÊòØÈùûÂ∏∏ÈùûÂ∏∏Â§öÁöÑÔºåÊòØ‰∏çÂèØËÉΩÂè™Êúâ20Êù•‰∏™Â≠óÊÆµÁöÑ„ÄÇÁ§∫‰æãÔºö12345678910111213141516171819202122/** * convert rdd to dataframe 1 * @param spark */private def runInferSchemaExample(spark:SparkSession): Unit =&#123; import spark.implicits._ val rdd = spark.sparkContext.textFile(&quot;E:/Â§ßÊï∞ÊçÆ/data/people.txt&quot;) val df = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; People(x(0), x(1).trim.toInt)) //Â∞ÜrddÁöÑÊØè‰∏ÄË°åÈÉΩËΩ¨Êç¢Êàê‰∫Ü‰∏Ä‰∏™people .toDF //ÂøÖÈ°ªÂÖàÂØºÂÖ•import spark.implicits._ ‰∏çÁÑ∂Ëøô‰∏™ÊñπÊ≥ï‰ºöÊä•Èîô df.show() df.createOrReplaceTempView(&quot;people&quot;) // Ëøô‰∏™DFÂåÖÂê´‰∫Ü‰∏§‰∏™Â≠óÊÆµnameÂíåage val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;) // teenager(0)‰ª£Ë°®Á¨¨‰∏Ä‰∏™Â≠óÊÆµ // ÂèñÂÄºÁöÑÁ¨¨‰∏ÄÁßçÊñπÂºèÔºöindex from zero teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show() // ÂèñÂÄºÁöÑÁ¨¨‰∫åÁßçÊñπÂºèÔºöbyName teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()&#125;// Ê≥®ÊÑèÔºöcase classÂøÖÈ°ªÂÆö‰πâÂú®mainÊñπÊ≥ï‰πãÂ§ñÔºõÂê¶Âàô‰ºöÊä•Èîôcase class People(name:String, age:Int)ÊñπÊ≥ï‰∫åÂàõÂª∫‰∏Ä‰∏™DataFrameÔºå‰ΩøÁî®ÁºñÁ®ãÁöÑÊñπÂºè Ëøô‰∏™ÊñπÂºèÁî®ÁöÑÈùûÂ∏∏Â§ö„ÄÇÈÄöËøáÁºñÁ®ãÊñπÂºèÊåáÂÆöschema ÔºåÂØπ‰∫éÁ¨¨‰∏ÄÁßçÊñπÂºèÁöÑschemaÂÖ∂ÂÆûÂÆö‰πâÂú®‰∫Ücase classÈáåÈù¢‰∫Ü„ÄÇÂÆòÁΩëËß£ËØªÔºöÂΩìÊàë‰ª¨ÁöÑcase class‰∏çËÉΩÊèêÂâçÂÆö‰πâ(Âõ†‰∏∫‰∏öÂä°Â§ÑÁêÜÁöÑËøáÁ®ãÂΩì‰∏≠Ôºå‰Ω†ÁöÑÂ≠óÊÆµÂèØËÉΩÊòØÂú®ÂèòÂåñÁöÑ),Âõ†Ê≠§‰ΩøÁî®case classÂæàÈöæÂéªÊèêÂâçÂÆö‰πâ„ÄÇ‰ΩøÁî®ËØ•ÊñπÂºèÂàõÂª∫DFÁöÑ‰∏âÂ§ßÊ≠•È™§ÔºöCreate an RDD of Rows from the original RDD;Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.Á§∫‰æãÔºö1234567891011121314151617181920212223242526/** * convert rdd to dataframe 2 * @param spark */private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123; // 1.ËΩ¨ÊàêRDD val rdd = spark.sparkContext.textFile(&quot;E:/Â§ßÊï∞ÊçÆ/data/people.txt&quot;) // 2.ÂÆö‰πâschemaÔºåÂ∏¶ÊúâStructTypeÁöÑ // ÂÆö‰πâschema‰ø°ÊÅØ val schemaString = &quot;name age&quot; // ÂØπschema‰ø°ÊÅØÊåâÁ©∫Ê†ºËøõË°åÂàÜÂâ≤ // ÊúÄÁªàfiledsÈáåÂåÖÂê´‰∫Ü2‰∏™StructField val fields = schemaString.split(&quot; &quot;) // Â≠óÊÆµÁ±ªÂûãÔºåÂ≠óÊÆµÂêçÁß∞Âà§Êñ≠ÊòØ‰∏çÊòØ‰∏∫Á©∫ .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // 3.ÊääÊàë‰ª¨ÁöÑschema‰ø°ÊÅØ‰ΩúÁî®Âà∞RDD‰∏ä // Ëøô‰∏™RDDÈáåÈù¢ÂåÖÂê´‰∫Ü‰∏Ä‰∫õË°å // ÂΩ¢ÊàêRowÁ±ªÂûãÁöÑRDD val rowRDD = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; Row(x(0), x(1).trim)) // ÈÄöËøáSparkSessionÂàõÂª∫‰∏Ä‰∏™DataFrame // ‰º†ËøõÊù•‰∏Ä‰∏™rowRDDÂíåschemaÔºåÂ∞Üschema‰ΩúÁî®Âà∞rowRDD‰∏ä val peopleDF = spark.createDataFrame(rowRDD, schema) peopleDF.show()&#125;[Êâ©Â±ï]Áîü‰∫ß‰∏äÂàõÂª∫DataFrameÁöÑ‰ª£Á†Å‰∏æ‰æãÂú®ÂÆûÈôÖÁîü‰∫ßÁéØÂ¢É‰∏≠ÔºåÊàë‰ª¨ÂÖ∂ÂÆûÈÄâÊã©ÁöÑÊòØÊñπÂºè‰∫åËøôÁßçËøõË°åÂàõÂª∫DataFrameÁöÑÔºåËøôÈáåÂ∞ÜÂ±ïÁ§∫ÈÉ®ÂàÜ‰ª£Á†ÅÔºöSchemaÁöÑÂÆö‰πâ1234567891011121314151617181920212223242526272829303132333435363738394041object AccessConvertUtil &#123; val struct = StructType( Array( StructField(&quot;url&quot;,StringType), StructField(&quot;cmsType&quot;,StringType), StructField(&quot;cmsId&quot;,LongType), StructField(&quot;traffic&quot;,LongType), StructField(&quot;ip&quot;,StringType), StructField(&quot;city&quot;,StringType), StructField(&quot;time&quot;,StringType), StructField(&quot;day&quot;,StringType) ) ) /** * Ê†πÊçÆËæìÂÖ•ÁöÑÊØè‰∏ÄË°å‰ø°ÊÅØËΩ¨Êç¢ÊàêËæìÂá∫ÁöÑÊ†∑Âºè */ def parseLog(log:String) = &#123; try &#123; val splits = log.split(&quot;\t&quot;) val url = splits(1) val traffic = splits(2).toLong val ip = splits(3) val domain = &quot;http://www.imooc.com/&quot; val cms = url.substring(url.indexOf(domain) + domain.length) val cmsTypeId = cms.split(&quot;/&quot;) var cmsType = &quot;&quot; var cmsId = 0l if (cmsTypeId.length &gt; 1) &#123; cmsType = cmsTypeId(0) cmsId = cmsTypeId(1).toLong &#125; val city = IpUtils.getCity(ip) val time = splits(0) val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;) //Ëøô‰∏™RowÈáåÈù¢ÁöÑÂ≠óÊÆµË¶ÅÂíåstruct‰∏≠ÁöÑÂ≠óÊÆµÂØπÂ∫î‰∏ä Row(url, cmsType, cmsId, traffic, ip, city, time, day) &#125; catch &#123; case e: Exception =&gt; Row(0) &#125; &#125;&#125;ÂàõÂª∫DataFrame1234567891011121314object SparkStatCleanJob &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;) .master(&quot;local[2]&quot;).getOrCreate() val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;) //accessRDD.take(10).foreach(println) //RDD ==&gt; DFÔºåÂàõÂª∫ÁîüÊàêDataFrame val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct) accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite) .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;) spark.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaÂèØÊâ©Â±ïÁ∫øÁ®ãÊ±†‰πãThreadPoolExecutor]]></title>
    <url>%2F2018%2F06%2F13%2FJava%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[1„ÄÅThreadPoolExecutorÊàë‰ª¨Áü•ÈÅìThreadPoolExecutorÊòØÂèØÊâ©Â±ïÁöÑ,ÂÆÉÊèê‰æõ‰∫ÜÂá†‰∏™ÂèØ‰ª•Âú®Â≠êÁ±ª‰∏≠ÊîπÂÜôÁöÑÁ©∫ÊñπÊ≥ïÂ¶Ç‰∏ãÔºö123protected void beforeExecute(Thread t, Runnable r) &#123; &#125;protected void beforeExecute(Thread t, Runnable r) &#123; &#125; protected void terminated() &#123; &#125;2„ÄÅ‰∏∫‰ªÄ‰πàË¶ÅËøõË°åÊâ©Â±ïÔºüÂõ†‰∏∫Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•ÂØπÁ∫øÁ®ãÊ±†ËøêË°åÁä∂ÊÄÅËøõË°åË∑üË∏™ÔºåËæìÂá∫‰∏Ä‰∫õÊúâÁî®ÁöÑË∞ÉËØï‰ø°ÊÅØÔºå‰ª•Â∏ÆÂä©ÊïÖÈöúËØäÊñ≠„ÄÇ3„ÄÅThreadPoolExecutor.WorkerÁöÑrunÊñπÊ≥ïÂÆûÁé∞ÈÄöËøáÁúãÊ∫êÁ†ÅÊàë‰ª¨ÂèëÁé∞ ThreadPoolExecutorÁöÑÂ∑•‰ΩúÁ∫øÁ®ãÂÖ∂ÂÆûÂ∞±ÊòØWorkerÂÆû‰æãÔºåWorker.runTask()‰ºöË¢´Á∫øÁ®ãÊ±†‰ª•Â§öÁ∫øÁ®ãÊ®°ÂºèÂºÇÊ≠•Ë∞ÉÁî®ÔºåÂàô‰ª•‰∏ä‰∏â‰∏™ÊñπÊ≥ï‰πüÂ∞ÜË¢´Â§öÁ∫øÁ®ãÂêåÊó∂ËÆøÈóÆ„ÄÇ1234567891011121314151617181920212223242526272829303132333435363738391// Âü∫‰∫éjdk1.8.0_161final void runWorker(Worker w) &#123; 2 Thread wt = Thread.currentThread(); 3 Runnable task = w.firstTask; 4 w.firstTask = null; 5 w.unlock(); // allow interrupts 6 boolean completedAbruptly = true; 7 try &#123; 8 while (task != null || (task = getTask()) != null) &#123; 9 w.lock(); 10 if ((runStateAtLeast(ctl.get(), STOP) ||11 (Thread.interrupted() &amp;&amp;12 runStateAtLeast(ctl.get(), STOP))) &amp;&amp;13 !wt.isInterrupted())14 wt.interrupt(); 15 try &#123;16 beforeExecute(wt, task);17 Throwable thrown = null; 18 try &#123;19 task.run();20 &#125; catch (RuntimeException x) &#123;21 thrown = x; throw x;22 &#125; catch (Error x) &#123;23 thrown = x; throw x;24 &#125; catch (Throwable x) &#123;25 thrown = x; throw new Error(x);26 &#125; finally &#123;27 afterExecute(task, thrown);28 &#125;29 &#125; finally &#123;30 task = null;31 w.completedTasks++;32 w.unlock();33 &#125;34 &#125;35 completedAbruptly = false;36 &#125; finally &#123;37 processWorkerExit(w, completedAbruptly);38 &#125;39 &#125;4„ÄÅÊâ©Â±ïÁ∫øÁ®ãÊ±†ÂÆûÁé∞1234567891011121314151617181920212223242526272829303132333435 1public class ExtThreadPool &#123; 2 public static class MyTask implements Runnable &#123; 3 public String name; 4 public MyTask(String name) &#123; 5 this.name = name; 6 &#125; 7 public void run() &#123; 8 System.out.println(&quot;Ê≠£Âú®ÊâßË°å:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name); try &#123; 9 Thread.sleep(100);10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;14 &#125; 15public static void main(String args[]) throws InterruptedException &#123;16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123; 18protected void beforeExecute(Thread t, Runnable r) &#123;19 System.out.println(&quot;ÂáÜÂ§áÊâßË°åÔºö&quot; + ((MyTask) r).name);20&#125; 21protected void afterExecute(Thread t, Runnable r) &#123;22 System.out.println(&quot;ÊâßË°åÂÆåÊàê&quot; + ((MyTask) r).name);23&#125; 24protected void terminated() &#123;25 System.out.println(&quot;Á∫øÁ®ãÊ±†ÈÄÄÂá∫ÔºÅ&quot;);26&#125;27&#125;; 28for (int i = 0; i &lt; 5; i++) &#123;29 MyTask task = new MyTask(&quot;TASK--&quot; + i);30 executorService.execute(task);31 Thread.sleep(10);32 &#125;33 executorService.shutdown();34 &#125;35&#125;ËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ãÔºö1234567891011ÂáÜÂ§áÊâßË°åÔºöTASK‚Äì0 Ê≠£Âú®ÊâßË°å:Thread ID:10,Task Name:TASK‚Äì0 ÂáÜÂ§áÊâßË°åÔºöTASK‚Äì1 Ê≠£Âú®ÊâßË°å:Thread ID:11,Task Name:TASK‚Äì1 ÂáÜÂ§áÊâßË°åÔºöTASK‚Äì2 Ê≠£Âú®ÊâßË°å:Thread ID:12,Task Name:TASK‚Äì2 ÂáÜÂ§áÊâßË°åÔºöTASK‚Äì3 Ê≠£Âú®ÊâßË°å:Thread ID:13,Task Name:TASK‚Äì3 ÂáÜÂ§áÊâßË°åÔºöTASK‚Äì4 Ê≠£Âú®ÊâßË°å:Thread ID:14,Task Name:TASK‚Äì4 Á∫øÁ®ãÊ±†ÈÄÄÂá∫ÔºÅËøôÊ†∑Â∞±ÂÆûÁé∞‰∫ÜÂú®ÊâßË°åÂâçÂêéËøõË°åÁöÑ‰∏Ä‰∫õÊéßÂà∂ÔºåÈô§Ê≠§‰πãÂ§ñÊàë‰ª¨ËøòÂèØ‰ª•ËæìÂá∫ÊØè‰∏™Á∫øÁ®ãÁöÑÊâßË°åÊó∂Èó¥ÔºåÊàñËÄÖ‰∏Ä‰∫õÂÖ∂‰ªñÂ¢ûÂº∫Êìç‰Ωú„ÄÇ5„ÄÅÊÄùËÄÉÔºüËØ∑ËØªËÄÖÊÄùËÄÉshutdownNowÂíåshutdownÊñπÊ≥ïÁöÑÂå∫Âà´ÔºüÂ¶Ç‰Ωï‰ºòÈõÖÁöÑÂÖ≥Èó≠Á∫øÁ®ãÊ±†Ôºü]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰Ω†Â§ßÁà∑Ê∞∏ËøúÊòØ‰Ω†Â§ßÁà∑ÔºåRDDË°ÄÁºòÂÖ≥Á≥ªÊ∫êÁ†ÅËØ¶Ëß£ÔºÅ]]></title>
    <url>%2F2018%2F06%2F13%2F%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅRDDÁöÑ‰æùËµñÂÖ≥Á≥ªRDDÁöÑ‰æùËµñÂÖ≥Á≥ªÂàÜ‰∏∫‰∏§Á±ªÔºöÂÆΩ‰æùËµñÂíåÁ™Ñ‰æùËµñ„ÄÇÊàë‰ª¨ÂèØ‰ª•ËøôÊ†∑ËÆ§‰∏∫ÔºöÔºà1ÔºâÁ™Ñ‰æùËµñÔºöÊØè‰∏™parent RDD ÁöÑ partition ÊúÄÂ§öË¢´ child RDD ÁöÑ‰∏Ä‰∏™partition ‰ΩøÁî®„ÄÇÔºà2ÔºâÂÆΩ‰æùËµñÔºöÊØè‰∏™parent RDD partition Ë¢´Â§ö‰∏™ child RDD ÁöÑpartition ‰ΩøÁî®„ÄÇÁ™Ñ‰æùËµñÊØè‰∏™ child RDD ÁöÑ partition ÁöÑÁîüÊàêÊìç‰ΩúÈÉΩÊòØÂèØ‰ª•Âπ∂Ë°åÁöÑÔºåËÄåÂÆΩ‰æùËµñÂàôÈúÄË¶ÅÊâÄÊúâÁöÑ parent RDD partition shuffle ÁªìÊûúÂæóÂà∞ÂêéÂÜçËøõË°å„ÄÇ‰∫å„ÄÅorg.apache.spark.Dependency.scala Ê∫êÁ†ÅËß£ÊûêDependencyÊòØ‰∏Ä‰∏™ÊäΩË±°Á±ªÔºö1234// Denpendency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125;ÂÆÉÊúâ‰∏§‰∏™Â≠êÁ±ªÔºöNarrowDependency Âíå ShuffleDenpendencyÔºåÂàÜÂà´ÂØπÂ∫îÁ™Ñ‰æùËµñÂíåÂÆΩ‰æùËµñ„ÄÇÔºà1ÔºâNarrowDependency‰πüÊòØ‰∏Ä‰∏™ÊäΩË±°Á±ªÂÆö‰πâ‰∫ÜÊäΩË±°ÊñπÊ≥ïgetParentsÔºåËæìÂÖ•partitionIdÔºåÁî®‰∫éËé∑Âæóchild RDD ÁöÑÊüê‰∏™partition‰æùËµñÁöÑparent RDDÁöÑÊâÄÊúâ partitions„ÄÇ1234567891011// Denpendency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125;Á™Ñ‰æùËµñÂèàÊúâ‰∏§‰∏™ÂÖ∑‰ΩìÁöÑÂÆûÁé∞ÔºöOneToOneDependencyÂíåRangeDependency„ÄÇÔºàaÔºâOneToOneDependencyÊåáchild RDDÁöÑpartitionÂè™‰æùËµñ‰∫éparent RDD ÁöÑ‰∏Ä‰∏™partitionÔºå‰∫ßÁîüOneToOneDependencyÁöÑÁÆóÂ≠êÊúâmapÔºåfilterÔºåflatMapÁ≠â„ÄÇÂèØ‰ª•ÁúãÂà∞getParentsÂÆûÁé∞ÂæàÁÆÄÂçïÔºåÂ∞±ÊòØ‰º†ËøõÂéª‰∏Ä‰∏™partitionIdÔºåÂÜçÊääpartitionIdÊîæÂú®ListÈáåÈù¢‰º†Âá∫Âéª„ÄÇ1234567891011121314151617// Denpendency.scalaclass OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; ÔºàbÔºâRangeDependencyÊåáchild RDD partitionÂú®‰∏ÄÂÆöÁöÑËåÉÂõ¥ÂÜÖ‰∏ÄÂØπ‰∏ÄÁöÑ‰æùËµñ‰∫éparent RDD partitionÔºå‰∏ªË¶ÅÁî®‰∫éunion„ÄÇ// Denpendency.scalaclass RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123;//inStartË°®Á§∫parent RDDÁöÑÂºÄÂßãÁ¥¢ÂºïÔºåoutStartË°®Á§∫child RDD ÁöÑÂºÄÂßãÁ¥¢Âºï override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart)//Ë°®Á§∫‰∫éÂΩìÂâçÁ¥¢ÂºïÁöÑÁõ∏ÂØπ‰ΩçÁΩÆ &#125; else &#123; Nil &#125; &#125;&#125;Ôºà2ÔºâShuffleDependencyÊåáÂÆΩ‰æùËµñË°®Á§∫‰∏Ä‰∏™parent RDDÁöÑpartition‰ºöË¢´child RDDÁöÑpartition‰ΩøÁî®Â§öÊ¨°„ÄÇÈúÄË¶ÅÁªèËøáshuffleÊâçËÉΩÂΩ¢Êàê„ÄÇ123456789101112131415161718192021// Denpendency.scalaclass ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123; //shuffleÈÉΩÊòØÂü∫‰∫éPairRDDËøõË°åÁöÑÔºåÊâÄ‰ª•‰º†ÂÖ•ÁöÑRDDË¶ÅÊòØkey-valueÁ±ªÂûãÁöÑ override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) //Ëé∑ÂèñshuffleId val shuffleId: Int = _rdd.context.newShuffleId() //ÂêëshuffleManagerÊ≥®ÂÜåshuffle‰ø°ÊÅØ val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;Áî±‰∫éshuffleÊ∂âÂèäÂà∞ÁΩëÁªú‰º†ËæìÔºåÊâÄ‰ª•Ë¶ÅÊúâÂ∫èÂàóÂåñserializerÔºå‰∏∫‰∫ÜÂáèÂ∞ëÁΩëÁªú‰º†ËæìÔºåÂèØ‰ª•mapÁ´ØËÅöÂêàÔºåÈÄöËøámapSideCombineÂíåaggregatorÊéßÂà∂ÔºåËøòÊúâkeyÊéíÂ∫èÁõ∏ÂÖ≥ÁöÑkeyOrderingÔºå‰ª•ÂèäÈáçËæìÂá∫ÁöÑÊï∞ÊçÆÂ¶Ç‰ΩïÂàÜÂå∫ÁöÑpartitionerÔºåËøòÊúâ‰∏Ä‰∫õclass‰ø°ÊÅØ„ÄÇPartition‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂú®shuffleÂ§ÑÊàõÁÑ∂ËÄåÊ≠¢ÔºåÂõ†Ê≠§shuffleÊòØÂàíÂàÜstageÁöÑ‰æùÊçÆ„ÄÇ‰∏â„ÄÅ‰∏§Áßç‰æùËµñÁöÑÂå∫ÂàÜÈ¶ñÂÖàÔºåÁ™Ñ‰æùËµñÂÖÅËÆ∏Âú®‰∏Ä‰∏™ÈõÜÁæ§ËäÇÁÇπ‰∏ä‰ª•ÊµÅÊ∞¥Á∫øÁöÑÊñπÂºèÔºàpipelineÔºâËÆ°ÁÆóÊâÄÊúâÁà∂ÂàÜÂå∫„ÄÇ‰æãÂ¶ÇÔºåÈÄê‰∏™ÂÖÉÁ¥†Âú∞ÊâßË°åmap„ÄÅÁÑ∂ÂêéfilterÊìç‰ΩúÔºõËÄåÂÆΩ‰æùËµñÂàôÈúÄË¶ÅÈ¶ñÂÖàËÆ°ÁÆóÂ•ΩÊâÄÊúâÁà∂ÂàÜÂå∫Êï∞ÊçÆÔºåÁÑ∂ÂêéÂú®ËäÇÁÇπ‰πãÈó¥ËøõË°åShuffleÔºåËøô‰∏éMapReduceÁ±ª‰ºº„ÄÇÁ¨¨‰∫åÔºåÁ™Ñ‰æùËµñËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËøõË°åÂ§±ÊïàËäÇÁÇπÁöÑÊÅ¢Â§çÔºåÂç≥Âè™ÈúÄÈáçÊñ∞ËÆ°ÁÆó‰∏¢Â§±RDDÂàÜÂå∫ÁöÑÁà∂ÂàÜÂå∫ÔºåËÄå‰∏î‰∏çÂêåËäÇÁÇπ‰πãÈó¥ÂèØ‰ª•Âπ∂Ë°åËÆ°ÁÆóÔºõËÄåÂØπ‰∫é‰∏Ä‰∏™ÂÆΩ‰æùËµñÂÖ≥Á≥ªÁöÑLineageÂõæÔºåÂçï‰∏™ËäÇÁÇπÂ§±ÊïàÂèØËÉΩÂØºËá¥Ëøô‰∏™RDDÁöÑÊâÄÊúâÁ•ñÂÖà‰∏¢Â§±ÈÉ®ÂàÜÂàÜÂå∫ÔºåÂõ†ËÄåÈúÄË¶ÅÊï¥‰ΩìÈáçÊñ∞ËÆ°ÁÆó„ÄÇ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark ÊäÄÊúØÂõ¢ÈòüÂºÄÊ∫êÊú∫Âô®Â≠¶‰π†Âπ≥Âè∞ MLflow]]></title>
    <url>%2F2018%2F06%2F12%2FApache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow%2F</url>
    <content type="text"><![CDATA[ËøëÊó•ÔºåÊù•Ëá™ Databricks ÁöÑ Matei Zaharia ÂÆ£Â∏ÉÊé®Âá∫ÂºÄÊ∫êÊú∫Âô®Â≠¶‰π†Âπ≥Âè∞ MLflow „ÄÇMatei Zaharia ÊòØ Apache Spark Âíå Apache Mesos ÁöÑÊ†∏ÂøÉ‰ΩúËÄÖÔºå‰πüÊòØ Databrick ÁöÑÈ¶ñÂ∏≠ÊäÄÊúØ‰∏ìÂÆ∂„ÄÇDatabrick ÊòØÁî± Apache Spark ÊäÄÊúØÂõ¢ÈòüÊâÄÂàõÁ´ãÁöÑÂïÜ‰∏öÂåñÂÖ¨Âè∏„ÄÇMLflow ÁõÆÂâçÂ∑≤Â§Ñ‰∫éÊó©ÊúüÊµãËØïÈò∂ÊÆµÔºåÂºÄÂèëËÄÖÂèØ‰∏ãËΩΩÊ∫êÁ†Å‰ΩìÈ™å„ÄÇMatei Zaharia Ë°®Á§∫ÂΩìÂâçÂú®‰ΩøÁî®Êú∫Âô®Â≠¶‰π†ÁöÑÂÖ¨Âè∏ÊôÆÈÅçÂ≠òÂú®Â∑•ÂÖ∑ËøáÂ§ö„ÄÅÈöæ‰ª•Ë∑üË∏™ÂÆûÈ™å„ÄÅÈöæ‰ª•ÈáçÁé∞ÁªìÊûú„ÄÅÈöæ‰ª•ÈÉ®ÁΩ≤Á≠âÈóÆÈ¢ò„ÄÇ‰∏∫ËÆ©Êú∫Âô®Â≠¶‰π†ÂºÄÂèëÂèòÂæó‰∏é‰º†ÁªüËΩØ‰ª∂ÂºÄÂèë‰∏ÄÊ†∑Âº∫Â§ß„ÄÅÂèØÈ¢ÑÊµãÂíåÊôÆÂèäÔºåËÆ∏Â§ö‰ºÅ‰∏öÂ∑≤ÂºÄÂßãÊûÑÂª∫ÂÜÖÈÉ®Êú∫Âô®Â≠¶‰π†Âπ≥Âè∞Êù•ÁÆ°ÁêÜ MLÁîüÂëΩÂë®Êúü„ÄÇÂÉèÊòØ Facebook„ÄÅGoogle Âíå Uber Â∞±Â∑≤ÂàÜÂà´ÊûÑÂª∫‰∫Ü FBLearner Flow„ÄÅTFX Âíå Michelangelo Êù•ÁÆ°ÁêÜÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂüπËÆ≠ÂíåÈÉ®ÁΩ≤„ÄÇ‰∏çËøáÁî±‰∫éËøô‰∫õÂÜÖÈÉ®Âπ≥Âè∞Â≠òÂú®Â±ÄÈôêÊÄßÂíåÁªëÂÆöÊÄßÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞‰∏éÁ§æÂå∫ÂÖ±‰∫´ÊàêÊûúÔºåÂÖ∂‰ªñÁî®Êà∑‰πüÊó†Ê≥ïËΩªÊòì‰ΩøÁî®„ÄÇMLflow Ê≠£ÊòØÂèóÁé∞ÊúâÁöÑ ML Âπ≥Âè∞ÂêØÂèëÔºå‰∏ªÊâìÂºÄÊîæÊÄßÔºöÂºÄÊîæÊé•Âè£ÔºöÂèØ‰∏é‰ªªÊÑè ML Â∫ì„ÄÅÁÆóÊ≥ï„ÄÅÈÉ®ÁΩ≤Â∑•ÂÖ∑ÊàñÁºñÁ®ãËØ≠Ë®Ä‰∏ÄËµ∑‰ΩøÁî®„ÄÇÂºÄÊ∫êÔºöÂºÄÂèëËÄÖÂèØËΩªÊùæÂú∞ÂØπÂÖ∂ËøõË°åÊâ©Â±ïÔºåÂπ∂Ë∑®ÁªÑÁªáÂÖ±‰∫´Â∑•‰ΩúÊµÅÊ≠•È™§ÂíåÊ®°Âûã„ÄÇMLflow ÁõÆÂâçÁöÑ alpha ÁâàÊú¨ÂåÖÂê´‰∏â‰∏™ÁªÑ‰ª∂ÔºöÂÖ∂‰∏≠ÔºåMLflow TrackingÔºàË∑üË∏™ÁªÑ‰ª∂ÔºâÊèê‰æõ‰∫Ü‰∏ÄÁªÑ API ÂíåÁî®Êà∑ÁïåÈù¢ÔºåÁî®‰∫éÂú®ËøêË°åÊú∫Âô®Â≠¶‰π†‰ª£Á†ÅÊó∂ËÆ∞ÂΩïÂíåÊü•ËØ¢ÂèÇÊï∞„ÄÅ‰ª£Á†ÅÁâàÊú¨„ÄÅÊåáÊ†áÂíåËæìÂá∫Êñá‰ª∂Ôºå‰ª•‰æø‰ª•ÂêéÂèØËßÜÂåñÂÆÉ‰ª¨„ÄÇ1234567891011121314import mlflow# Log parameters (key-value pairs)mlflow.log_param(&quot;num_dimensions&quot;, 8)mlflow.log_param(&quot;regularization&quot;, 0.1)# Log a metric; metrics can be updated throughout the runmlflow.log_metric(&quot;accuracy&quot;, 0.1)...mlflow.log_metric(&quot;accuracy&quot;, 0.45)# Log artifacts (output files)mlflow.log_artifact(&quot;roc.png&quot;)mlflow.log_artifact(&quot;model.pkl&quot;)MLflow ProjectsÔºàÈ°πÁõÆÁªÑ‰ª∂ÔºâÊèê‰æõ‰∫ÜÊâìÂåÖÂèØÈáçÁî®Êï∞ÊçÆÁßëÂ≠¶‰ª£Á†ÅÁöÑÊ†áÂáÜÊ†ºÂºè„ÄÇÊØè‰∏™È°πÁõÆÈÉΩÂè™ÊòØ‰∏Ä‰∏™ÂåÖÂê´‰ª£Á†ÅÊàñ Git Â≠òÂÇ®Â∫ìÁöÑÁõÆÂΩïÔºåÂπ∂‰ΩøÁî®‰∏Ä‰∏™ÊèèËø∞Á¨¶Êñá‰ª∂Êù•ÊåáÂÆöÂÆÉÁöÑ‰æùËµñÂÖ≥Á≥ª‰ª•ÂèäÂ¶Ç‰ΩïËøêË°å‰ª£Á†Å„ÄÇÊØè‰∏™ MLflow È°πÁõÆÈÉΩÊòØÁî±‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂêç‰∏∫ MLproject ÁöÑ YAML Êñá‰ª∂ËøõË°åËá™ÂÆö‰πâ„ÄÇ123456789101112name: My Projectconda_env: conda.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot; validate: parameters: data_file: path command: &quot;python validate.py &#123;data_file&#125;&quot;MLflow ModelsÔºàÊ®°ÂûãÁªÑ‰ª∂ÔºâÊèê‰æõ‰∫Ü‰∏ÄÁßçÁî®Â§öÁßçÊ†ºÂºèÊâìÂåÖÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑËßÑËåÉÔºåËøô‰∫õÊ†ºÂºèË¢´Áß∞‰∏∫ ‚Äúflavor‚Äù „ÄÇMLflow Êèê‰æõ‰∫ÜÂ§öÁßçÂ∑•ÂÖ∑Êù•ÈÉ®ÁΩ≤‰∏çÂêå flavor ÁöÑÊ®°Âûã„ÄÇÊØè‰∏™ MLflow Ê®°ÂûãË¢´‰øùÂ≠òÊàê‰∏Ä‰∏™ÁõÆÂΩïÔºåÁõÆÂΩï‰∏≠ÂåÖÂê´‰∫Ü‰ªªÊÑèÊ®°ÂûãÊñá‰ª∂Âíå‰∏Ä‰∏™ MLmodel ÊèèËø∞Á¨¶Êñá‰ª∂ÔºåÊñá‰ª∂‰∏≠ÂàóÂá∫‰∫ÜÁõ∏Â∫îÁöÑ flavor „ÄÇ12345678time_created: 2018-02-21T13:21:34.12flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn pickled_model: model.pkl]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming Áä∂ÊÄÅÁÆ°ÁêÜÂáΩÊï∞ÁöÑÈÄâÊã©ÊØîËæÉ]]></title>
    <url>%2F2018%2F06%2F06%2FSparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅupdateStateByKeyÂÆòÁΩëÂéüËØùÔºö1In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.‰πüÂç≥ÊòØËØ¥ÂÆÉ‰ºöÁªüËÆ°ÂÖ®Â±ÄÁöÑkeyÁöÑÁä∂ÊÄÅÔºåÂ∞±ÁÆóÊ≤°ÊúâÊï∞ÊçÆËæìÂÖ•ÔºåÂÆÉ‰πü‰ºöÂú®ÊØè‰∏Ä‰∏™ÊâπÊ¨°ÁöÑÊó∂ÂÄôËøîÂõû‰πãÂâçÁöÑkeyÁöÑÁä∂ÊÄÅ„ÄÇÁº∫ÁÇπÔºöËã•Êï∞ÊçÆÈáèÂ§™Â§ßÁöÑËØùÔºåÈúÄË¶ÅcheckpointÁöÑÊï∞ÊçÆ‰ºöÂç†Áî®ËæÉÂ§ßÁöÑÂ≠òÂÇ®ÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇÁ®ãÂ∫èÁ§∫‰æãÂ¶Ç‰∏ãÔºö12345678910111213141516171819202122232425262728object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //Ê≥®ÊÑèÔºöË¶Å‰ΩøÁî®updateStateByKeyÂøÖÈ°ªËÆæÁΩÆcheckpointÁõÆÂΩï ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; /*Áä∂ÊÄÅÊõ¥Êñ∞ÂáΩÊï∞ * @param currentValues keyÁõ∏ÂêåvalueÂΩ¢ÊàêÁöÑÂàóË°® * @param preValues keyÂØπÂ∫îÁöÑvalueÔºåÂâç‰∏ÄÁä∂ÊÄÅ * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seqÂàóË°®‰∏≠ÊâÄÊúâvalueÊ±ÇÂíå val pre = preValues.getOrElse(0) //Ëé∑Âèñ‰∏ä‰∏ÄÁä∂ÊÄÅÂÄº Some(curr + pre) &#125; &#125;‰∫å„ÄÅmapWithStatemapWithStateÔºö‰πüÊòØÁî®‰∫éÂÖ®Â±ÄÁªüËÆ°keyÁöÑÁä∂ÊÄÅÔºå‰ΩÜÊòØÂÆÉÂ¶ÇÊûúÊ≤°ÊúâÊï∞ÊçÆËæìÂÖ•Ôºå‰æø‰∏ç‰ºöËøîÂõû‰πãÂâçÁöÑkeyÁöÑÁä∂ÊÄÅÔºåÊúâ‰∏ÄÁÇπÂ¢ûÈáèÁöÑÊÑüËßâ„ÄÇÊïàÁéáÊõ¥È´òÔºåÁîü‰∫ß‰∏≠Âª∫ËÆÆ‰ΩøÁî®ÂÆòÊñπ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö1234567891011121314151617181920212223242526272829303132object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulNetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) val initialRDD = ssc.sparkContext .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1))) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;‰∏â„ÄÅÊ∫êÁ†ÅÂàÜÊûêupateStateByKeyÔºömapËøîÂõûÁöÑÊòØMappedDStreamÔºåËÄåMappedDStreamÂπ∂Ê≤°ÊúâupdateStateByKeyÊñπÊ≥ïÔºåÂπ∂‰∏îÂÆÉÁöÑÁà∂Á±ªDStream‰∏≠‰πüÊ≤°ÊúâËØ•ÊñπÊ≥ï„ÄÇ‰ΩÜÊòØDStreamÁöÑ‰º¥ÁîüÂØπË±°‰∏≠Êúâ‰∏Ä‰∏™ÈöêÂºèËΩ¨Êç¢ÂáΩÊï∞Ôºö123456object DStream &#123; implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairDStreamFunctions[K, V] = &#123; new PairDStreamFunctions[K, V](stream) &#125;Ë∑üËøõÂéª PairDStreamFunctions ÔºåÂèëÁé∞ÊúÄÁªàË∞ÉÁî®ÁöÑÊòØËá™Â∑±ÁöÑupdateStateByKey„ÄÇÂÖ∂‰∏≠updateFuncÂ∞±Ë¶Å‰º†ÂÖ•ÁöÑÂèÇÊï∞Ôºå‰ªñÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÔºåSeq[V]Ë°®Á§∫ÂΩìÂâçkeyÂØπÂ∫îÁöÑÊâÄÊúâÂÄºÔºå123456Option[S] ÊòØÂΩìÂâçkeyÁöÑÂéÜÂè≤Áä∂ÊÄÅÔºåËøîÂõûÁöÑÊòØÊñ∞ÁöÑÁä∂ÊÄÅ„ÄÇdef updateStateByKey[S: ClassTag]( updateFunc: (Seq[V], Option[S]) =&gt; Option[S] ): DStream[(K, S)] = ssc.withScope &#123; updateStateByKey(updateFunc, defaultPartitioner())&#125;ÊúÄÁªàË∞ÉÁî®Ôºö12345678910def updateStateByKey[S: ClassTag]( updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)], partitioner: Partitioner, rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123; val cleanedFunc = ssc.sc.clean(updateFunc) val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123; cleanedFunc(it) &#125; new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)&#125;ÂÜçË∑üËøõÂéª new StateDStream:Âú®ËøôÈáåÈù¢newÂá∫‰∫Ü‰∏Ä‰∏™StateDStreamÂØπË±°„ÄÇÂú®ÂÖ∂computeÊñπÊ≥ï‰∏≠Ôºå‰ºöÂÖàËé∑Âèñ‰∏ä‰∏Ä‰∏™batchËÆ°ÁÆóÂá∫ÁöÑRDDÔºàÂåÖÂê´‰∫ÜËá≥Á®ãÂ∫èÂºÄÂßãÂà∞‰∏ä‰∏Ä‰∏™batchÂçïËØçÁöÑÁ¥ØËÆ°ËÆ°Êï∞ÔºâÔºåÁÑ∂ÂêéÂú®Ëé∑ÂèñÊú¨Ê¨°batch‰∏≠StateDStreamÁöÑÁà∂Á±ªËÆ°ÁÆóÂá∫ÁöÑRDDÔºàÊú¨Ê¨°batchÁöÑÂçïËØçËÆ°Êï∞ÔºâÂàÜÂà´ÊòØprevStateRDDÂíåparentRDDÔºåÁÑ∂ÂêéÂú®Ë∞ÉÁî® computeUsingPreviousRDD ÊñπÊ≥ïÔºö1234567891011121314151617181920private [this] def computeUsingPreviousRDD( batchTime: Time, parentRDD: RDD[(K, V)], prevStateRDD: RDD[(K, S)]) = &#123; // Define the function for the mapPartition operation on cogrouped RDD; // first map the cogrouped tuple to tuples of required type, // and then apply the update function val updateFuncLocal = updateFunc val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123; val i = iterator.map &#123; t =&gt; val itr = t._2._2.iterator val headOption = if (itr.hasNext) Some(itr.next()) else None (t._1, t._2._1.toSeq, headOption) &#125; updateFuncLocal(batchTime, i) &#125; val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner) val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning) Some(stateRDD)&#125;Âú®ËøôÈáå‰∏§‰∏™RDDËøõË°åcogroupÁÑ∂ÂêéÂ∫îÁî®updateStateByKey‰º†ÂÖ•ÁöÑÂáΩÊï∞„ÄÇÊàë‰ª¨Áü•ÈÅìcogroupÁöÑÊÄßËÉΩÊòØÊØîËæÉ‰Ωé‰∏ãÔºåÂèÇËÄÉhttp://lxw1234.com/archives/2015/07/384.htm„ÄÇmapWithState:123456789@Experimentaldef mapWithState[StateType: ClassTag, MappedType: ClassTag]( spec: StateSpec[K, V, StateType, MappedType] ): MapWithStateDStream[K, V, StateType, MappedType] = &#123; new MapWithStateDStreamImpl[K, V, StateType, MappedType]( self, spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]] )&#125;ËØ¥ÊòéÔºöStateSpec Â∞ÅË£Ö‰∫ÜÁä∂ÊÄÅÁÆ°ÁêÜÂáΩÊï∞ÔºåÂπ∂Âú®ËØ•ÊñπÊ≥ï‰∏≠ÂàõÂª∫‰∫ÜMapWithStateDStreamImplÂØπË±°„ÄÇMapWithStateDStreamImpl ‰∏≠ÂàõÂª∫‰∫Ü‰∏Ä‰∏™InternalMapWithStateDStreamÁ±ªÂûãÂØπË±°internalStreamÔºåÂú®MapWithStateDStreamImplÁöÑcomputeÊñπÊ≥ï‰∏≠Ë∞ÉÁî®‰∫ÜinternalStreamÁöÑgetOrComputeÊñπÊ≥ï„ÄÇ12345678910111213141516private[streaming] class MapWithStateDStreamImpl[ KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag]( dataStream: DStream[(KeyType, ValueType)], spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType]) extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123; private val internalStream = new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec) override def slideDuration: Duration = internalStream.slideDuration override def dependencies: List[DStream[_]] = List(internalStream) override def compute(validTime: Time): Option[RDD[MappedType]] = &#123; internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125; &#125;InternalMapWithStateDStream‰∏≠Ê≤°ÊúâgetOrComputeÊñπÊ≥ïÔºåËøôÈáåË∞ÉÁî®ÁöÑÊòØÂÖ∂Áà∂Á±ª DStream ÁöÑgetOrCpmputeÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ï‰∏≠ÊúÄÁªà‰ºöË∞ÉÁî®InternalMapWithStateDStreamÁöÑComputeÊñπÊ≥ïÔºö12345678910111213141516171819202122232425262728293031323334/** Method that generates an RDD for the given time */override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123; // Get the previous state or create a new empty state RDD val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123; case Some(rdd) =&gt; if (rdd.partitioner != Some(partitioner)) &#123; // If the RDD is not partitioned the right way, let us repartition it using the // partition index as the key. This is to ensure that state RDD is always partitioned // before creating another state RDD using it MapWithStateRDD.createFromRDD[K, V, S, E]( rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime) &#125; else &#123; rdd &#125; case None =&gt; MapWithStateRDD.createFromPairRDD[K, V, S, E]( spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)), partitioner, validTime ) &#125; // Compute the new state RDD with previous state RDD and partitioned data RDD // Even if there is no data RDD, use an empty one to create a new state RDD val dataRDD = parent.getOrCompute(validTime).getOrElse &#123; context.sparkContext.emptyRDD[(K, V)] &#125; val partitionedDataRDD = dataRDD.partitionBy(partitioner) val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt; (validTime - interval).milliseconds &#125; Some(new MapWithStateRDD( prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))&#125;Ê†πÊçÆÁªôÂÆöÁöÑÊó∂Èó¥ÁîüÊàê‰∏Ä‰∏™MapWithStateRDDÔºåÈ¶ñÂÖàËé∑Âèñ‰∫ÜÂÖàÂâçÁä∂ÊÄÅÁöÑRDDÔºöpreStateRDDÂíåÂΩìÂâçÊó∂Èó¥ÁöÑRDD:dataRDDÔºåÁÑ∂ÂêéÂØπdataRDDÂü∫‰∫éÂÖàÂâçÁä∂ÊÄÅRDDÁöÑÂàÜÂå∫Âô®ËøõË°åÈáçÊñ∞ÂàÜÂå∫Ëé∑ÂèñpartitionedDataRDD„ÄÇÊúÄÂêéÂ∞ÜpreStateRDDÔºåpartitionedDataRDDÂíåÁî®Êà∑ÂÆö‰πâÁöÑÂáΩÊï∞mappingFunction‰º†ÁªôÊñ∞ÁîüÊàêÁöÑMapWithStateRDDÂØπË±°ËøîÂõû„ÄÇÂêéÁª≠Ëã•ÊúâÂÖ¥Ë∂£ÂèØ‰ª•ÁªßÁª≠Ë∑üËøõMapWithStateRDDÁöÑcomputeÊñπÊ≥ïÔºåÈôê‰∫éÁØáÂπÖ‰∏çÂÜçÂ±ïÁ§∫„ÄÇ]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL ‰πãÂ§ñÈÉ®Êï∞ÊçÆÊ∫êÂ¶Ç‰ΩïÊàê‰∏∫Âú®‰ºÅ‰∏öÂºÄÂèë‰∏≠ÁöÑ‰∏ÄÊääÂà©Âô®]]></title>
    <url>%2F2018%2F06%2F06%2FSpark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 Ê¶ÇËø∞1.Spark1.2‰∏≠ÔºåSpark SQLÂºÄÂßãÊ≠£ÂºèÊîØÊåÅÂ§ñÈÉ®Êï∞ÊçÆÊ∫ê„ÄÇSpark SQLÂºÄÊîæ‰∫Ü‰∏ÄÁ≥ªÂàóÊé•ÂÖ•Â§ñÈÉ®Êï∞ÊçÆÊ∫êÁöÑÊé•Âè£ÔºåÊù•ËÆ©ÂºÄÂèëËÄÖÂèØ‰ª•ÂÆûÁé∞„ÄÇ‰ΩøÂæóSpark SQLÂèØ‰ª•Âä†ËΩΩ‰ªª‰ΩïÂú∞ÊñπÁöÑÊï∞ÊçÆÔºå‰æãÂ¶ÇmysqlÔºåhiveÔºåhdfsÔºåhbaseÁ≠âÔºåËÄå‰∏îÊîØÊåÅÂæàÂ§öÁßçÊ†ºÂºèÂ¶Çjson, parquet, avro, csvÊ†ºÂºè„ÄÇÊàë‰ª¨ÂèØ‰ª•ÂºÄÂèëÂá∫‰ªªÊÑèÁöÑÂ§ñÈÉ®Êï∞ÊçÆÊ∫êÊù•ËøûÊé•Âà∞Spark SQLÔºåÁÑ∂ÂêéÊàë‰ª¨Â∞±ÂèØ‰ª•ÈÄöËøáÂ§ñÈÉ®Êï∞ÊçÆÊ∫êAPIÊù•ËøõË°åÊìç‰Ωú„ÄÇ2.Êàë‰ª¨ÈÄöËøáÂ§ñÈÉ®Êï∞ÊçÆÊ∫êAPIËØªÂèñÂêÑÁßçÊ†ºÂºèÁöÑÊï∞ÊçÆÔºå‰ºöÂæóÂà∞‰∏Ä‰∏™DataFrameÔºåËøôÊòØÊàë‰ª¨ÁÜüÊÇâÁöÑÊñπÂºèÂïäÔºåÂ∞±ÂèØ‰ª•‰ΩøÁî®DataFrameÁöÑAPIÊàñËÄÖSQLÁöÑAPIËøõË°åÊìç‰ΩúÂìà„ÄÇ3.Â§ñÈÉ®Êï∞ÊçÆÊ∫êÁöÑAPIÂèØ‰ª•Ëá™Âä®ÂÅö‰∏Ä‰∫õÂàóÁöÑË£ÅÂâ™Ôºå‰ªÄ‰πàÂè´ÂàóÁöÑË£ÅÂâ™ÔºåÂÅáÂ¶Ç‰∏Ä‰∏™userË°®Êúâid,name,age,gender4‰∏™ÂàóÔºåÂú®ÂÅöselectÁöÑÊó∂ÂÄô‰Ω†Âè™ÈúÄË¶Åid,nameËøô‰∏§ÂàóÔºåÈÇ£‰πàÂÖ∂‰ªñÂàó‰ºöÈÄöËøáÂ∫ïÂ±ÇÁöÑ‰ºòÂåñÂéªÁªôÊàë‰ª¨Ë£ÅÂâ™Êéâ„ÄÇ4.‰øùÂ≠òÊìç‰ΩúÂèØ‰ª•ÈÄâÊã©‰ΩøÁî®SaveModeÔºåÊåáÂÆöÂ¶Ç‰Ωï‰øùÂ≠òÁé∞ÊúâÊï∞ÊçÆÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ„ÄÇ2.ËØªÂèñjsonÊñá‰ª∂ÂêØÂä®shellËøõË°åÊµãËØï1234567891011121314151617181920//Ê†áÂáÜÂÜôÊ≥ïval df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)//Âè¶Â§ñ‰∏ÄÁßçÂÜôÊ≥ïspark.read.json(&quot;path&quot;)ÁúãÁúãÊ∫êÁ†ÅËøô‰∏§ËÄÖ‰πãÈó¥Âà∞Â∫ïÊúâÂï•‰∏çÂêåÂë¢Ôºü/** * Loads a JSON file and returns the results as a `DataFrame`. * * See the documentation on the overloaded `json()` method with varargs for more details. * * @since 1.4.0 */ def json(path: String): DataFrame = &#123; // This method ensures that calls that explicit need single argument works, see SPARK-16009 json(Seq(path): _*) &#125;Êàë‰ª¨Ë∞ÉÁî®josn() ÊñπÊ≥ïÂÖ∂ÂÆûËøõË°å‰∫Ü overloaded ÔºåÊàë‰ª¨ÁªßÁª≠Êü•Áúã def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*) ËøôÂè•ËØùÊòØ‰∏çÊòØÂæàÁÜüÊÇâÔºåÂÖ∂ÂÆûÂ∞±ÊòØÊàë‰ª¨ÁöÑÊ†áÂáÜÂÜôÊ≥ï1234567891011121314151617 scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+3 ËØªÂèñparquetÊï∞ÊçÆ12345678910val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+4 ËØªÂèñhive‰∏≠ÁöÑÊï∞ÊçÆ1234567891011121314151617181920212223242526272829303132spark.sql(&quot;show tables&quot;).show+--------+----------+-----------+|database| tableName|isTemporary|+--------+----------+-----------+| default|states_raw| false|| default|states_seq| false|| default| t1| false|+--------+----------+-----------+spark.table(&quot;states_raw&quot;).show+-----+------+| code| name|+-----+------+|hello| java||hello|hadoop||hello| hive||hello| sqoop||hello| hdfs||hello| spark|+-----+------+scala&gt; spark.sql(&quot;select name from states_raw &quot;).show+------+| name|+------+| java||hadoop|| hive|| sqoop|| hdfs|| spark|+------+5 ‰øùÂ≠òÊï∞ÊçÆÊ≥®ÊÑèÔºö‰øùÂ≠òÁöÑÊñá‰ª∂Â§π‰∏çËÉΩÂ≠òÂú®ÔºåÂê¶ÂàôÊä•Èîô(ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÂèØ‰ª•ÈÄâÊã©‰∏çÂêåÁöÑÊ®°Âºè)Ôºöorg.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;‰øùÂ≠òÊàêÊñáÊú¨Ê†ºÂºèÔºåÂè™ËÉΩ‰øùÂ≠ò‰∏ÄÂàóÔºåÂê¶ÂàôÊä•ÈîôÔºöorg.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;123456789101112131415161718192021222324252627282930val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)//‰øùÂ≠òdf.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)ÁªìÊûúÔºö[hadoop@hadoop out]$ pwd/home/hadoop/data/out[hadoop@hadoop out]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:34 _SUCCESS[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt MichaelAndyJustin//‰øùÂ≠ò‰∏∫jsonÊ†ºÂºèdf.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)ÁªìÊûú[hadoop@hadoop data]$ cd out1[hadoop@hadoop out1]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:35 _SUCCESS[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;‰∏äÈù¢ËØ¥‰∫ÜÂú®‰øùÂ≠òÊï∞ÊçÆÊó∂Â¶ÇÊûúÁõÆÂΩïÂ∑≤ÁªèÂ≠òÂú®ÔºåÂú®ÈªòËÆ§Ê®°Âºè‰∏ã‰ºöÊä•ÈîôÔºåÈÇ£Êàë‰ª¨‰∏ãÈù¢ËÆ≤Ëß£‰øùÂ≠òÁöÑÂá†ÁßçÊ®°ÂºèÔºö6 ËØªÂèñmysql‰∏≠ÁöÑÊï∞ÊçÆ1234567891011121314151617181920212223val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;).option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123456&quot;).load()scala&gt; jdbcDF.printSchemaroot |-- TBL_ID: long (nullable = false) |-- CREATE_TIME: integer (nullable = false) |-- DB_ID: long (nullable = true) |-- LAST_ACCESS_TIME: integer (nullable = false) |-- OWNER: string (nullable = true) |-- RETENTION: integer (nullable = false) |-- SD_ID: long (nullable = true) |-- TBL_NAME: string (nullable = true) |-- TBL_TYPE: string (nullable = true) |-- VIEW_EXPANDED_TEXT: string (nullable = true) |-- VIEW_ORIGINAL_TEXT: string (nullable = true)jdbcDF.show7 spark SQLÊìç‰ΩúmysqlË°®Êï∞ÊçÆ123456789101112131415161718192021222324252627282930313233CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url &quot;jdbc:mysql://localhost:3306&quot;, dbtable &quot;basic01.tbls&quot;, user &apos;root&apos;, password &apos;123456&apos;, driver &quot;com.mysql.jdbc.Driver&quot;);Êü•ÁúãÔºöshow tables;default states_raw falsedefault states_seq falsedefault t1 falsejdbctable trueselect * from jdbctable;1 1519944170 6 0 hadoop 0 1 page_views MANAGED_TABLE NULL NULL2 1519944313 6 0 hadoop 0 2 page_views_bzip2 MANAGED_TABLE NULL NULL3 1519944819 6 0 hadoop 0 3 page_views_snappy MANAGED_TABLE NULL NULL21 1520067771 6 0 hadoop 0 21 tt MANAGED_TABLE NULL NULL22 1520069148 6 0 hadoop 0 22 page_views_seq MANAGED_TABLE NULL NULL23 1520071381 6 0 hadoop 0 23 page_views_rcfile MANAGED_TABLE NULL NULL24 1520074675 6 0 hadoop 0 24 page_views_orc_zlib MANAGED_TABLE NULL NULL27 1520078184 6 0 hadoop 0 27 page_views_lzo_index MANAGED_TABLE NULL NULL30 1520083461 6 0 hadoop 0 30 page_views_lzo_index1 MANAGED_TABLE NULL NULL31 1524370014 1 0 hadoop 0 31 t1 EXTERNAL_TABLE NULL NULL37 1524468636 1 0 hadoop 0 37 states_raw MANAGED_TABLE NULL NULL38 1524468678 1 0 hadoop 0 38 states_seq MANAGED_TABLE NULL NULLmysql‰∏≠ÁöÑtblsÁöÑÊï∞ÊçÆÂ∑≤ÁªèÂ≠òÂú®jdbctableË°®‰∏≠‰∫Ü„ÄÇjdbcDF.show8 ÂàÜÂå∫Êé®ÊµãÔºàPartition DiscoveryÔºâË°®ÂàÜÂå∫ÊòØÂú®ÂÉèHiveËøôÊ†∑ÁöÑÁ≥ªÁªü‰∏≠‰ΩøÁî®ÁöÑÂ∏∏ËßÅ‰ºòÂåñÊñπÊ≥ï„ÄÇ Âú®ÂàÜÂå∫Ë°®‰∏≠ÔºåÊï∞ÊçÆÈÄöÂ∏∏Â≠òÂÇ®Âú®‰∏çÂêåÁöÑÁõÆÂΩï‰∏≠ÔºåÂàÜÂå∫ÂàóÂÄºÂú®ÊØè‰∏™ÂàÜÂå∫ÁõÆÂΩïÁöÑË∑ØÂæÑ‰∏≠ÁºñÁ†Å„ÄÇ ÊâÄÊúâÂÜÖÁΩÆÁöÑÊñá‰ª∂Ê∫êÔºàÂåÖÊã¨Text / CSV / JSON / ORC / ParquetÔºâÈÉΩËÉΩÂ§üËá™Âä®ÂèëÁé∞ÂíåÊé®Êñ≠ÂàÜÂå∫‰ø°ÊÅØ„ÄÇ ‰æãÂ¶ÇÔºåÊàë‰ª¨ÂàõÂª∫Â¶Ç‰∏ãÁöÑÁõÆÂΩïÁªìÊûÑ;123456789hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CNÊ∑ªÂä†jsonÊñá‰ª∂Ôºöpeople.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CNÊàë‰ª¨‰ΩøÁî®spark sqlËØªÂèñÂ§ñÈÉ®Êï∞ÊçÆÊ∫êÔºö1234567891011121314151617val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+Êàë‰ª¨ÊîπÂèòËØªÂèñÁöÑÁõÆÂΩï12345678910111213141516val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) |-- country: string (nullable = true)scala&gt; df.show+----+-------+-------+| age| name|country|+----+-------+-------+|null|Michael| CN|| 30| Andy| CN|| 19| Justin| CN|+----+-------+-------+Â§ßÂÆ∂ÊúâÊ≤°ÊúâÂèëÁé∞‰ªÄ‰πàÂë¢ÔºüSpark SQLÂ∞ÜËá™Âä®‰ªéË∑ØÂæÑ‰∏≠ÊèêÂèñÂàÜÂå∫‰ø°ÊÅØ„ÄÇÊ≥®ÊÑèÔºåÂàÜÂå∫ÂàóÁöÑÊï∞ÊçÆÁ±ªÂûãÊòØËá™Âä®Êé®Êñ≠ÁöÑ„ÄÇÁõÆÂâçÊîØÊåÅÊï∞Â≠óÊï∞ÊçÆÁ±ªÂûãÔºåÊó•ÊúüÔºåÊó∂Èó¥Êà≥ÂíåÂ≠óÁ¨¶‰∏≤Á±ªÂûã„ÄÇÊúâÊó∂Áî®Êà∑ÂèØËÉΩ‰∏çÊÉ≥Ëá™Âä®Êé®Êñ≠ÂàÜÂå∫ÂàóÁöÑÊï∞ÊçÆÁ±ªÂûã„ÄÇÂØπ‰∫éËøô‰∫õÁî®‰æãÔºåËá™Âä®Á±ªÂûãÊé®Êñ≠ÂèØ‰ª•ÈÄöËøáspark.sql.sources.partitionColumnTypeInference.enabledËøõË°åÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫true„ÄÇÂΩìÁ¶ÅÁî®Á±ªÂûãÊé®Êñ≠Êó∂ÔºåÂ≠óÁ¨¶‰∏≤Á±ªÂûãÂ∞ÜÁî®‰∫éÂàÜÂå∫Âàó„ÄÇ‰ªéSpark 1.6.0ÂºÄÂßãÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÂàÜÂå∫ÂèëÁé∞‰ªÖÂú®ÁªôÂÆöË∑ØÂæÑ‰∏ãÊâæÂà∞ÂàÜÂå∫„ÄÇÂØπ‰∫é‰∏äÈù¢ÁöÑÁ§∫‰æãÔºåÂ¶ÇÊûúÁî®Êà∑Â∞ÜË∑ØÂæÑ/table/gender=male‰º†ÈÄíÁªôSparkSession.read.parquetÊàñSparkSession.read.loadÔºåÂàô‰∏ç‰ºöÂ∞ÜÊÄßÂà´ËßÜ‰∏∫ÂàÜÂå∫Âàó„ÄÇÂ¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÊåáÂÆöÂêØÂä®ÂàÜÂå∫ÂèëÁé∞ÁöÑÂü∫Êú¨Ë∑ØÂæÑÔºåÂàôÂèØ‰ª•basePathÂú®Êï∞ÊçÆÊ∫êÈÄâÈ°π‰∏≠ËøõË°åËÆæÁΩÆ„ÄÇ‰æãÂ¶ÇÔºåÂΩìpath/to/table/gender=maleÊòØÊï∞ÊçÆË∑ØÂæÑÂπ∂‰∏îÁî®Êà∑Â∞ÜbasePathËÆæÁΩÆ‰∏∫path/to/table/Êó∂ÔºåÊÄßÂà´Â∞ÜÊòØÂàÜÂå∫Âàó„ÄÇ]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinuxÁ≥ªÁªüÈáçË¶ÅÂèÇÊï∞Ë∞É‰ºòÔºå‰Ω†Áü•ÈÅìÂêó]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%2F</url>
    <content type="text"><![CDATA[ÂΩìÂâç‰ºöËØùÁîüÊïàulimit -u -&gt; Êü•ÁúãÂΩìÂâçÊúÄÂ§ßËøõÁ®ãÊï∞ulimit -n -&gt;Êü•ÁúãÂΩìÂâçÊúÄÂ§ßÊñá‰ª∂Êï∞ulimit -u xxx -&gt; ‰øÆÊîπÂΩìÂâçÊúÄÂ§ßËøõÁ®ãÊï∞‰∏∫xxxulimit -n xxx -&gt; ‰øÆÊîπÂΩìÂâçÊúÄÂ§ßÊñá‰ª∂Êï∞‰∏∫xxxÊ∞∏‰πÖÁîüÊïà1.vi /etc/security/limits.confÔºåÊ∑ªÂä†Â¶Ç‰∏ãÁöÑË°åsoft noproc 11000hard noproc 11000soft nofile 4100hard nofile 4100 ËØ¥ÊòéÔºö‰ª£Ë°®ÈíàÂØπÊâÄÊúâÁî®Êà∑noproc ÊòØ‰ª£Ë°®ÊúÄÂ§ßËøõÁ®ãÊï∞nofile ÊòØ‰ª£Ë°®ÊúÄÂ§ßÊñá‰ª∂ÊâìÂºÄÊï∞2.ËÆ© SSH Êé•Âèó Login Á®ãÂºèÁöÑÁôªÂÖ•ÔºåÊñπ‰æøÂú® ssh ÂÆ¢Êà∑Á´ØÊü•Áúã ulimit -a ËµÑÊ∫êÈôêÂà∂Ôºö1)„ÄÅvi /etc/ssh/sshd_configÊää UserLogin ÁöÑÂÄºÊîπ‰∏∫ yesÔºåÂπ∂Êää # Ê≥®ÈáäÂéªÊéâ2)„ÄÅÈáçÂêØ sshd ÊúçÂä°/etc/init.d/sshd restart3)„ÄÅ‰øÆÊîπÊâÄÊúâ linux Áî®Êà∑ÁöÑÁéØÂ¢ÉÂèòÈáèÊñá‰ª∂Ôºövi /etc/profileulimit -u 10000ulimit -n 4096ulimit -d unlimitedulimit -m unlimitedulimit -s unlimitedulimit -t unlimitedulimit -v unlimited4)„ÄÅÁîüÊïàsource /etc/profile]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkÂä®ÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜÊ∫êÁ†ÅËß£ÊûêÔºÅ]]></title>
    <url>%2F2018%2F06%2F03%2FSpark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅSparkÂÜÖÂ≠òÁÆ°ÁêÜÊ®°ÂºèSparkÊúâ‰∏§ÁßçÂÜÖÂ≠òÁÆ°ÁêÜÊ®°ÂºèÔºåÈùôÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜ(Static MemoryManager)ÂíåÂä®ÊÄÅÔºàÁªü‰∏ÄÔºâÂÜÖÂ≠òÁÆ°ÁêÜÔºàUnified MemoryManagerÔºâ„ÄÇÂä®ÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜ‰ªéSpark1.6ÂºÄÂßãÂºïÂÖ•ÔºåÂú®SparkEnv.scala‰∏≠ÁöÑÊ∫êÁ†ÅÂèØ‰ª•ÁúãÂà∞ÔºåSparkÁõÆÂâçÈªòËÆ§ÈááÁî®Âä®ÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜÊ®°ÂºèÔºåËã•Â∞Üspark.memory.useLegacyModeËÆæÁΩÆ‰∏∫trueÔºåÂàô‰ºöÊîπ‰∏∫ÈááÁî®ÈùôÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜ„ÄÇ12345678// SparkEnv.scala val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false) val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) &#125;‰∫å„ÄÅSparkÂä®ÊÄÅÂÜÖÂ≠òÁÆ°ÁêÜÁ©∫Èó¥ÂàÜÈÖçÁõ∏ÊØî‰∫éStatic MemoryManagerÊ®°ÂºèÔºåUnified MemoryManagerÊ®°ÂûãÊâìÁ†¥‰∫ÜÂ≠òÂÇ®ÂÜÖÂ≠òÂíåËøêË°åÂÜÖÂ≠òÁöÑÁïåÈôêÔºå‰ΩøÊØè‰∏Ä‰∏™ÂÜÖÂ≠òÂå∫ËÉΩÂ§üÂä®ÊÄÅ‰º∏Áº©ÔºåÈôç‰ΩéOOMÁöÑÊ¶ÇÁéá„ÄÇÁî±‰∏äÂõæÂèØÁü•Ôºåexecutor JVMÂÜÖÂ≠ò‰∏ªË¶ÅÁî±‰ª•‰∏ãÂá†‰∏™Âå∫ÂüüÁªÑÊàêÔºöÔºà1ÔºâReserved MemoryÔºàÈ¢ÑÁïôÂÜÖÂ≠òÔºâÔºöËøôÈÉ®ÂàÜÂÜÖÂ≠òÈ¢ÑÁïôÁªôÁ≥ªÁªü‰ΩøÁî®ÔºåÈªòËÆ§‰∏∫300MBÔºåÂèØÈÄöËøáspark.testing.reservedMemoryËøõË°åËÆæÁΩÆ„ÄÇ12// UnifiedMemoryManager.scalaprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024Âè¶Â§ñÔºåJVMÂÜÖÂ≠òÁöÑÊúÄÂ∞èÂÄº‰πü‰∏éreserved MemoryÊúâÂÖ≥ÔºåÂç≥minSystemMemory = reserved Memory1.5ÔºåÂç≥ÈªòËÆ§ÊÉÖÂÜµ‰∏ãJVMÂÜÖÂ≠òÊúÄÂ∞èÂÄº‰∏∫300MB1.5=450MB„ÄÇ12// UnifiedMemoryManager.scala val minSystemMemory = (reservedMemory * 1.5).ceil.toLongÔºà2ÔºâSpark Memeoy:ÂàÜ‰∏∫execution MemoryÂíåstorage Memory„ÄÇÂéªÈô§Êéâreserved MemoryÔºåÂâ©‰∏ãusableMemoryÁöÑ‰∏ÄÈÉ®ÂàÜÁî®‰∫éexecutionÂíåstorageËøô‰∏§Á±ªÂ†ÜÂÜÖÂ≠òÔºåÈªòËÆ§ÊòØ0.6ÔºåÂèØÈÄöËøáspark.memory.fractionËøõË°åËÆæÁΩÆ„ÄÇ‰æãÂ¶ÇÔºöJVMÂÜÖÂ≠òÊòØ1GÔºåÈÇ£‰πàÁî®‰∫éexecutionÂíåstorageÁöÑÈªòËÆ§ÂÜÖÂ≠ò‰∏∫Ôºà1024-300Ôºâ*0.6=434MB„ÄÇ1234// UnifiedMemoryManager.scala val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6) (usableMemory * memoryFraction).toLong‰ªñ‰ª¨ÁöÑËæπÁïåÁî±spark.memory.storageFractionËÆæÂÆöÔºåÈªòËÆ§‰∏∫0.5„ÄÇÂç≥ÈªòËÆ§Áä∂ÊÄÅ‰∏ãstorage MemoryÂíåexecution Memory‰∏∫1Ôºö1.1234// UnifiedMemoryManager.scala onHeapStorageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)Ôºà3Ôºâuser Memory:Ââ©‰ΩôÂÜÖÂ≠òÔºåÁî®Êà∑Ê†πÊçÆÈúÄË¶Å‰ΩøÁî®ÔºåÈªòËÆ§Âç†usableMemoryÁöÑÔºà1-0.6Ôºâ=0.4.‰∏â„ÄÅÂÜÖÂ≠òÊéßÂà∂ËØ¶Ëß£È¶ñÂÖàÊàë‰ª¨ÂÖàÊù•‰∫ÜËß£‰∏Ä‰∏ãSparkÂÜÖÂ≠òÁÆ°ÁêÜÂÆûÁé∞Á±ª‰πãÂâçÁöÑÂÖ≥Á≥ª„ÄÇ1.MemoryManager‰∏ªË¶ÅÂäüËÉΩÊòØÔºöÔºà1ÔºâËÆ∞ÂΩïÁî®‰∫ÜÂ§öÂ∞ëStorageMemoryÂíåExecutionMemoryÔºõÔºà2ÔºâÁî≥ËØ∑Storage„ÄÅExecutionÂíåUnroll MemoryÔºõÔºà3ÔºâÈáäÊîæStroageÂíåExecution Memory„ÄÇExecutionÂÜÖÂ≠òÁî®Êù•ÊâßË°åshuffle„ÄÅjoins„ÄÅsortsÂíåaggegationsÊìç‰ΩúÔºåStorageÂÜÖÂ≠òÁî®‰∫éÁºìÂ≠òÂíåÂπøÊí≠Êï∞ÊçÆÔºåÊØè‰∏Ä‰∏™JVM‰∏≠ÈÉΩÂ≠òÂú®ÁùÄ‰∏Ä‰∏™MemoryManager„ÄÇÊûÑÈÄ†MemoryManagerÈúÄË¶ÅÊåáÂÆöonHeapStorageMemoryÂíåonHeapExecutionMemoryÂèÇÊï∞„ÄÇ123456 // MemoryManager.scalaprivate[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging &#123;ÂàõÂª∫StorageMemoryPoolÂíåExecutionMemoryPoolÂØπË±°ÔºåÁî®Êù•ÂàõÂª∫Â†ÜÂÜÖÊàñÂ†ÜÂ§ñÁöÑStorageÂíåExecutionÂÜÖÂ≠òÊ±†ÔºåÁÆ°ÁêÜStorageÂíåExecutionÁöÑÂÜÖÂ≠òÂàÜÈÖç„ÄÇ123456789// MemoryManager.scala @GuardedBy(&quot;this&quot;) protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(&quot;this&quot;) protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºå‰∏ç‰ΩøÁî®Â†ÜÂ§ñÂÜÖÂ≠òÔºåÂèØÈÄöËøásaprk.memory.offHeap.enabledËÆæÁΩÆÔºåÈªòËÆ§Â†ÜÂ§ñÂÜÖÂ≠ò‰∏∫0ÔºåÂèØ‰ΩøÁî®spark.memory.offHeap.sizeÂèÇÊï∞ËÆæÁΩÆ„ÄÇ123456789101112// All the code you will ever need final val tungstenMemoryMode: MemoryMode = &#123; if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123; require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0, &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;) require(Platform.unaligned(), &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;) MemoryMode.OFF_HEAP &#125; else &#123; MemoryMode.ON_HEAP &#125; &#125;12// MemoryManager.scala protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)ÈáäÊîænumBytesÂ≠óËäÇÁöÑExecutionÂÜÖÂ≠òÊñπÊ≥ï12345678910// MemoryManager.scaladef releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) &#125; &#125;ÈáäÊîæÊåáÂÆötaskÁöÑÊâÄÊúâExecutionÂÜÖÂ≠òÂπ∂Â∞ÜËØ•taskÊ†áËÆ∞‰∏∫inactive„ÄÇ12345// MemoryManager.scala private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123; onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) &#125;ÈáäÊîænumBytesÂ≠óËäÇÁöÑStoargeÂÜÖÂ≠òÊñπÊ≥ï1234567// MemoryManager.scaladef releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes) &#125; &#125;ÈáäÊîæÊâÄÊúâStorageÂÜÖÂ≠òÊñπÊ≥ï12345// MemoryManager.scalafinal def releaseAllStorageMemory(): Unit = synchronized &#123; onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() &#125;2.Êé•‰∏ãÊù•Êàë‰ª¨‰∫ÜËß£‰∏Ä‰∏ãÔºåUnifiedMemoryManagerÊòØÂ¶Ç‰ΩïÂØπÂÜÖÂ≠òËøõË°åÊéßÂà∂ÁöÑÔºüÂä®ÊÄÅÂÜÖÂ≠òÊòØÂ¶Ç‰ΩïÂÆûÁé∞ÁöÑÂë¢ÔºüUnifiedMemoryManageÁªßÊâø‰∫ÜMemoryManager1234567891011// UnifiedMemoryManage.scalaprivate[spark] class UnifiedMemoryManager private[memory] ( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) &#123;ÈáçÂÜô‰∫ÜmaxOnHeapStorageMemoryÊñπÊ≥ïÔºåÊúÄÂ§ßStorageÂÜÖÂ≠ò=ÊúÄÂ§ßÂÜÖÂ≠ò-ÊúÄÂ§ßExecutionÂÜÖÂ≠ò„ÄÇ1234// UnifiedMemoryManage.scala override def maxOnHeapStorageMemory: Long = synchronized &#123; maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed &#125;Ê†∏ÂøÉÊñπÊ≥ïacquireStorageMemoryÔºöÁî≥ËØ∑StorageÂÜÖÂ≠ò„ÄÇ12345678910111213141516171819202122232425262728293031// UnifiedMemoryManage.scalaoverride def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized &#123; assertInvariants() assert(numBytes &gt;= 0) val (executionPool, storagePool, maxMemory) = memoryMode match &#123; //Ê†πÊçÆ‰∏çÂêåÁöÑÂÜÖÂ≠òÊ®°ÂºèÂéªÂàõÂª∫StorageMemoryPoolÂíåExecutionMemoryPool case MemoryMode.ON_HEAP =&gt; ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP =&gt; ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapMemory) &#125; if (numBytes &gt; maxMemory) &#123; // Ëã•Áî≥ËØ∑ÂÜÖÂ≠òÂ§ß‰∫éÊúÄÂ§ßÂÜÖÂ≠òÔºåÂàôÁî≥ËØ∑Â§±Ë¥• logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; + s&quot;memory limit ($maxMemory bytes)&quot;) return false &#125; if (numBytes &gt; storagePool.memoryFree) &#123; // Â¶ÇÊûúStorageÂÜÖÂ≠òÊ±†Ê≤°ÊúâË∂≥Â§üÁöÑÂÜÖÂ≠òÔºåÂàôÂêëExecutionÂÜÖÂ≠òÊ±†ÂÄüÁî® val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//ÂΩìExecutionÂÜÖÂ≠òÊúâÁ©∫Èó≤Êó∂ÔºåStorageÊâçËÉΩÂÄüÂà∞ÂÜÖÂ≠ò executionPool.decrementPoolSize(memoryBorrowedFromExecution)//Áº©Â∞èExecutionÂÜÖÂ≠ò storagePool.incrementPoolSize(memoryBorrowedFromExecution)//Â¢ûÂä†StorageÂÜÖÂ≠ò &#125; storagePool.acquireMemory(blockId, numBytes)Ê†∏ÂøÉÊñπÊ≥ïacquireExecutionMemoryÔºöÁî≥ËØ∑ExecutionÂÜÖÂ≠ò„ÄÇ123456789// UnifiedMemoryManage.scalaoverride private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized &#123;//‰ΩøÁî®‰∫ÜsynchronizedÂÖ≥ÈîÆÂ≠óÔºåË∞ÉÁî®acquireExecutionMemoryÊñπÊ≥ïÂèØËÉΩ‰ºöÈòªÂ°ûÔºåÁõ¥Âà∞ExecutionÂÜÖÂ≠òÊ±†ÊúâË∂≥Â§üÁöÑÂÜÖÂ≠ò„ÄÇ ... executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize) &#125;ÊñπÊ≥ïÊúÄÂêéË∞ÉÁî®‰∫ÜExecutionMemoryPoolÁöÑacquireMemoryÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÁöÑÂèÇÊï∞ÈúÄË¶Å‰∏§‰∏™ÂáΩÊï∞ÔºömaybeGrowExecutionPool()ÂíåcomputeMaxExecutionPoolSize()„ÄÇÊØè‰∏™TaskËÉΩÂ§ü‰ΩøÁî®ÁöÑÂÜÖÂ≠òË¢´ÈôêÂà∂Âú®pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks„ÄÇÂÖ∂‰∏≠maxPoolSize‰ª£Ë°®‰∫Üexecution poolÁöÑÊúÄÂ§ßÂÜÖÂ≠òÔºåpoolSizeË°®Á§∫ÂΩìÂâçËøô‰∏™poolÁöÑÂ§ßÂ∞è„ÄÇ1234// ExecutionMemoryPool.scala val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks)maybeGrowExecutionPool()ÊñπÊ≥ïÂÆûÁé∞‰∫ÜÂ¶Ç‰ΩïÂä®ÊÄÅÂ¢ûÂä†ExecutionÂÜÖÂ≠òÂå∫ÁöÑÂ§ßÂ∞è„ÄÇÂú®ÊØèÊ¨°Áî≥ËØ∑executionÂÜÖÂ≠òÁöÑÂêåÊó∂ÔºåexecutionÂÜÖÂ≠òÊ±†‰ºöËøõË°åÂ§öÊ¨°Â∞ùËØïÔºåÊØèÊ¨°Â∞ùËØïÈÉΩÂèØËÉΩ‰ºöÂõûÊî∂‰∏Ä‰∫õÂ≠òÂÇ®ÂÜÖÂ≠ò„ÄÇ123456789101112131415// UnifiedMemoryManage.scala def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123; if (extraMemoryNeeded &gt; 0) &#123;//Â¶ÇÊûúÁî≥ËØ∑ÁöÑÂÜÖÂ≠òÂ§ß‰∫é0 //ËÆ°ÁÆóexecutionÂèØÂÄüÂà∞ÁöÑstorageÂÜÖÂ≠òÔºåÊòØstorageÂâ©‰ΩôÂÜÖÂ≠òÂíåÂèØÂÄüÂá∫ÂÜÖÂ≠òÁöÑÊúÄÂ§ßÂÄº val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage &gt; 0) &#123;//Â¶ÇÊûúÂèØ‰ª•Áî≥ËØ∑Âà∞ÂÜÖÂ≠ò val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//ÂÆûÈôÖÈúÄË¶ÅÁöÑÂÜÖÂ≠òÔºåÂèñÂÆûÈôÖÈúÄË¶ÅÁöÑÂÜÖÂ≠òÂíåstorageÂÜÖÂ≠òÂå∫ÂüüÂÖ®ÈÉ®ÂèØÁî®ÂÜÖÂ≠òÂ§ßÂ∞èÁöÑÊúÄÂ∞èÂÄº storagePool.decrementPoolSize(spaceToReclaim)//storageÂÜÖÂ≠òÂå∫ÂüüÂáèÂ∞ë executionPool.incrementPoolSize(spaceToReclaim)//executionÂÜÖÂ≠òÂå∫ÂüüÂ¢ûÂä† &#125; &#125; &#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Ê∫êÁ†ÅÈòÖËØª</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆ-Èõ∂Âü∫Á°ÄÂ≠¶ÂëòÊ∑±Âú≥ÊüêÂè∏È´òËñ™Èù¢ËØïÈ¢ò]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Âï•‰πü‰∏çËØ¥ÔºÅÁõ¥Êé•‰∏äÈ¢òÈù¢ËØïÊó∂Èó¥Ôºö20180531ÁÆÄÂçïËØ¥‰∏ãhdfsËØªÊñá‰ª∂ÂíåÂÜôÊñá‰ª∂ÁöÑÊµÅÁ®ãÊØèÂ§©Êï∞ÊçÆÈáèÊúâÂ§öÂ§ßÔºüÁîü‰∫ßÈõÜÁæ§ËßÑÊ®°ÊúâÂ§öÂ§ßÔºüËØ¥Âá†‰∏™sparkÂºÄÂèë‰∏≠ÈÅáÂà∞ÁöÑÈóÆÈ¢òÔºåÂíåËß£ÂÜ≥ÁöÑÊñπÊ°àÈòêËø∞‰∏Ä‰∏ãÊúÄËøëÂºÄÂèëÁöÑÈ°πÁõÆÔºå‰ª•ÂèäÊãÖ‰ªªÁöÑËßíËâ≤‰ΩçÁΩÆkafkaÊúâÂÅöËøáÂì™‰∫õË∞É‰ºòÊàë‰ª¨È°πÁõÆ‰∏≠Êï∞ÊçÆÂÄæÊñúÁöÑÂú∫ÊôØÂíåËß£ÂÜ≥ÊñπÊ°àÈõ∂Âü∫Á°Ä‚ûïÂõõ‰∏™ÊúàÁ¥ßË∑üËã•Ê≥ΩÂ§ßÊï∞ÊçÆÂ≠¶‰π†‰πãÂêéÊòØËøôÊ†∑]]></content>
      <categories>
        <category>Èù¢ËØïÈ¢ò</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ªéHive‰∏≠ÁöÑstored as file_foramtÁúãhiveË∞É‰ºò]]></title>
    <url>%2F2018%2F05%2F30%2F%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅË°åÂºèÊï∞ÊçÆÂ∫ìÂíåÂàóÂºèÊï∞ÊçÆÂ∫ìÁöÑÂØπÊØî1„ÄÅÂ≠òÂÇ®ÊØîËæÉË°åÂºèÊï∞ÊçÆÂ∫ìÂ≠òÂÇ®Âú®hdfs‰∏äÂºèÊåâË°åËøõË°åÂ≠òÂÇ®ÁöÑÔºå‰∏Ä‰∏™blockÂ≠òÂÇ®‰∏ÄÊàñÂ§öË°åÊï∞ÊçÆ„ÄÇËÄåÂàóÂºèÊï∞ÊçÆÂ∫ìÂú®hdfs‰∏äÂàôÊòØÊåâÁÖßÂàóËøõË°åÂ≠òÂÇ®Ôºå‰∏Ä‰∏™blockÂèØËÉΩÊúâ‰∏ÄÂàóÊàñÂ§öÂàóÊï∞ÊçÆ„ÄÇ2„ÄÅÂéãÁº©ÊØîËæÉÂØπ‰∫éË°åÂºèÊï∞ÊçÆÂ∫ìÔºåÂøÖÁÑ∂ÊåâË°åÂéãÁº©ÔºåÂΩì‰∏ÄË°å‰∏≠ÊúâÂ§ö‰∏™Â≠óÊÆµÔºåÂêÑ‰∏™Â≠óÊÆµÂØπÂ∫îÁöÑÊï∞ÊçÆÁ±ªÂûãÂèØËÉΩ‰∏ç‰∏ÄËá¥ÔºåÂéãÁº©ÊÄßËÉΩÂéãÁº©ÊØîÂ∞±ÊØîËæÉÂ∑Æ„ÄÇÂØπ‰∫éÂàóÂºèÊï∞ÊçÆÂ∫ìÔºåÂøÖÁÑ∂ÊåâÂàóÂéãÁº©ÔºåÊØè‰∏ÄÂàóÂØπÂ∫îÁöÑÊòØÁõ∏ÂêåÊï∞ÊçÆÁ±ªÂûãÁöÑÊï∞ÊçÆÔºåÊïÖÂàóÂºèÊï∞ÊçÆÂ∫ìÁöÑÂéãÁº©ÊÄßËÉΩË¶ÅÂº∫‰∫éË°åÂºèÊï∞ÊçÆÂ∫ì„ÄÇ3„ÄÅÊü•ËØ¢ÊØîËæÉÂÅáËÆæÊâßË°åÁöÑÊü•ËØ¢Êìç‰ΩúÊòØÔºöselect id,name from table_emp;ÂØπ‰∫éË°åÂºèÊï∞ÊçÆÂ∫ìÔºåÂÆÉË¶ÅÈÅçÂéÜ‰∏ÄÊï¥Âº†Ë°®Â∞ÜÊØè‰∏ÄË°å‰∏≠ÁöÑid,nameÂ≠óÊÆµÊãºÊé•ÂÜçÂ±ïÁé∞Âá∫Êù•ÔºåËøôÊ†∑ÈúÄË¶ÅÊü•ËØ¢ÁöÑÊï∞ÊçÆÈáèÂ∞±ÊØîËæÉÂ§ßÔºåÊïàÁéá‰Ωé„ÄÇÂØπ‰∫éÂàóÂºèÊï∞ÊçÆÂ∫ìÔºåÂÆÉÂè™ÈúÄÊâæÂà∞ÂØπÂ∫îÁöÑid,nameÂ≠óÊÆµÁöÑÂàóÂ±ïÁé∞Âá∫Êù•Âç≥ÂèØÔºåÈúÄË¶ÅÊü•ËØ¢ÁöÑÊï∞ÊçÆÈáèÂ∞èÔºåÊïàÁéáÈ´ò„ÄÇÂÅáËÆæÊâßË°åÁöÑÊü•ËØ¢Êìç‰ΩúÊòØÔºöselect * from table_emp;ÂØπ‰∫éËøôÁßçÊü•ËØ¢Êï¥‰∏™Ë°®ÂÖ®ÈÉ®‰ø°ÊÅØÁöÑÊìç‰ΩúÔºåÁî±‰∫éÂàóÂºèÊï∞ÊçÆÂ∫ìÈúÄË¶ÅÂ∞ÜÂàÜÊï£ÁöÑË°åËøõË°åÈáçÊñ∞ÁªÑÂêàÔºåË°åÂºèÊï∞ÊçÆÂ∫ìÊïàÁéáÂ∞±È´ò‰∫éÂàóÂºèÊï∞ÊçÆÂ∫ì„ÄÇ‰ΩÜÊòØÔºåÂú®Â§ßÊï∞ÊçÆÈ¢ÜÂüüÔºåËøõË°åÂÖ®Ë°®Êü•ËØ¢ÁöÑÂú∫ÊôØÂ∞ë‰πãÂèàÂ∞ëÔºåËøõËÄåÊàë‰ª¨‰ΩøÁî®ËæÉÂ§öÁöÑËøòÊòØÂàóÂºèÊï∞ÊçÆÂ∫ìÂèäÂàóÂºèÂÇ®Â≠ò„ÄÇ‰∫å„ÄÅstored as file_format ËØ¶Ëß£1„ÄÅÂª∫‰∏ÄÂº†Ë°®Êó∂ÔºåÂèØ‰ª•‰ΩøÁî®‚Äústored as file_format‚ÄùÊù•ÊåáÂÆöËØ•Ë°®Êï∞ÊçÆÁöÑÂ≠òÂÇ®Ê†ºÂºèÔºåhive‰∏≠ÔºåË°®ÁöÑÈªòËÆ§Â≠òÂÇ®Ê†ºÂºè‰∏∫TextFile„ÄÇ123456789101112131415161718192021CREATE TABLE tt (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;CREATE TABLE tt2 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;CREATE TABLE tt3 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;#‰ª•‰∏ä‰∏âÁßçÊñπÂºèÂ≠òÂÇ®ÁöÑÊ†ºÂºèÈÉΩÊòØTEXTFILE„ÄÇ2„ÄÅTEXTFILE„ÄÅSEQUENCEFILE„ÄÅRCFILE„ÄÅORCÁ≠âÂõõÁßçÂÇ®Â≠òÊ†ºÂºèÂèäÂÆÉ‰ª¨ÂØπ‰∫éhiveÂú®Â≠òÂÇ®Êï∞ÊçÆÂíåÊü•ËØ¢Êï∞ÊçÆÊó∂ÊÄßËÉΩÁöÑ‰ºòÂä£ÊØîËæÉ12345678file_format: | SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameTEXTFILE: Âè™ÊòØhive‰∏≠Ë°®Êï∞ÊçÆÈªòËÆ§ÁöÑÂ≠òÂÇ®Ê†ºÂºèÔºåÂÆÉÂ∞ÜÊâÄÊúâÁ±ªÂûãÁöÑÊï∞ÊçÆÈÉΩÂ≠òÂÇ®‰∏∫StringÁ±ªÂûãÔºå‰∏ç‰æø‰∫éÊï∞ÊçÆÁöÑËß£ÊûêÔºå‰ΩÜÂÆÉÂç¥ÊØîËæÉÈÄöÁî®„ÄÇ‰∏çÂÖ∑Â§áÈöèÊú∫ËØªÂÜôÁöÑËÉΩÂäõ„ÄÇÊîØÊåÅÂéãÁº©„ÄÇSEQUENCEFILE: ËøôÁßçÂÇ®Â≠òÊ†ºÂºèÊØîTEXTFILEÊ†ºÂºèÂ§ö‰∫ÜÂ§¥ÈÉ®„ÄÅÊ†áËØÜ„ÄÅ‰ø°ÊÅØÈïøÂ∫¶Á≠â‰ø°ÊÅØÔºåËøô‰∫õ‰ø°ÊÅØ‰ΩøÂæóÂÖ∂ÂÖ∑Â§áÈöèÊú∫ËØªÂÜôÁöÑËÉΩÂäõ„ÄÇÊîØÊåÅÂéãÁº©Ôºå‰ΩÜÂéãÁº©ÁöÑÊòØvalue„ÄÇÔºàÂ≠òÂÇ®Áõ∏ÂêåÁöÑÊï∞ÊçÆÔºåSEQUENCEFILEÊØîTEXTFILEÁï•Â§ßÔºâRCFILEÔºàRecord Columnar FileÔºâ: Áé∞Âú®Ê∞¥Âπ≥‰∏äÂàíÂàÜ‰∏∫ÂæàÂ§ö‰∏™Row Group,ÊØè‰∏™Row GroupÈªòËÆ§Â§ßÂ∞è4MBÔºåRow GroupÂÜÖÈÉ®ÂÜçÊåâÂàóÂ≠òÂÇ®‰ø°ÊÅØ„ÄÇÁî±facebookÂºÄÊ∫êÔºåÊØîÊ†áÂáÜË°åÂºèÂ≠òÂÇ®ËäÇÁ∫¶10%ÁöÑÁ©∫Èó¥„ÄÇORC: ‰ºòÂåñËøáÂêéÁöÑRCFile,Áé∞Âú®Ê∞¥Âπ≥‰∏äÂàíÂàÜ‰∏∫Â§ö‰∏™Stripes,ÂÜçÂú®Stripe‰∏≠ÊåâÂàóÂ≠òÂÇ®„ÄÇÊØè‰∏™StripeÁî±‰∏Ä‰∏™Index Data„ÄÅ‰∏Ä‰∏™Row Data„ÄÅ‰∏Ä‰∏™Stripe FooterÁªÑÊàê„ÄÇÊØè‰∏™StripesÁöÑÂ§ßÂ∞è‰∏∫250MBÔºåÊØè‰∏™Index DataËÆ∞ÂΩïÁöÑÊòØÊï¥ÂûãÊï∞ÊçÆÊúÄÂ§ßÂÄºÊúÄÂ∞èÂÄº„ÄÅÂ≠óÁ¨¶‰∏≤Êï∞ÊçÆÂâçÂêéÁºÄ‰ø°ÊÅØÔºåÊØè‰∏™ÂàóÁöÑ‰ΩçÁΩÆÁ≠âÁ≠âËØ∏Â¶ÇÊ≠§Á±ªÁöÑ‰ø°ÊÅØ„ÄÇËøôÂ∞±‰ΩøÂæóÊü•ËØ¢ÂçÅÂàÜÂæóÈ´òÊïàÔºåÈªòËÆ§ÊØè‰∏Ä‰∏áË°åÊï∞ÊçÆÂª∫Á´ã‰∏Ä‰∏™Index Data„ÄÇORCÂ≠òÂÇ®Â§ßÂ∞è‰∏∫TEXTFILEÁöÑ40%Â∑¶Âè≥Ôºå‰ΩøÁî®ÂéãÁº©ÂàôÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â∞ÜËøô‰∏™Êï∞Â≠óÈôçÂà∞10%~20%„ÄÇORCËøôÁßçÊñá‰ª∂Ê†ºÂºèÂèØ‰ª•‰ΩúÁî®‰∫éË°®ÊàñËÄÖË°®ÁöÑÂàÜÂå∫ÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂá†ÁßçÊñπÂºèËøõË°åÊåáÂÆöÔºö123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=OrcThe parameters are all placed in the TBLPROPERTIES (see Create Table). They are:Key|Default|Notes|-|-|-|orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)|orc.compress.size|262,144|number of bytes in each compression chunk|orc.stripe.size|67,108,864|number of bytes in each stripe|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)|orc.create.index|true|whether to create row indexes|orc.bloom.filter.columns |‚Äù‚Äù| comma separated list of column names for which bloom filter should be created|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)Á§∫‰æãÔºöÂàõÂª∫Â∏¶ÂéãÁº©ÁöÑORCÂ≠òÂÇ®Ë°®1234567create table Addresses ( name string, street string, city string, state string, zip int) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);PARQUET: Â≠òÂÇ®Â§ßÂ∞è‰∏∫TEXTFILEÁöÑ60%~70%ÔºåÂéãÁº©ÂêéÂú®20%~30%‰πãÈó¥„ÄÇÊ≥®ÊÑèÔºö‰∏çÂêåÁöÑÂ≠òÂÇ®Ê†ºÂºè‰∏ç‰ªÖË°®Áé∞Âú®Â≠òÂÇ®Á©∫Èó¥‰∏äÁöÑ‰∏çÂêåÔºåÂØπ‰∫éÊï∞ÊçÆÁöÑÊü•ËØ¢ÔºåÊïàÁéá‰πü‰∏ç‰∏ÄÊ†∑„ÄÇÂõ†‰∏∫ÂØπ‰∫é‰∏çÂêåÁöÑÂ≠òÂÇ®Ê†ºÂºèÔºåÊâßË°åÁõ∏ÂêåÁöÑÊü•ËØ¢Êìç‰ΩúÔºå‰ªñ‰ª¨ËÆøÈóÆÁöÑÊï∞ÊçÆÈáèÂ§ßÂ∞èÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ„ÄÇÂ¶ÇÊûúË¶Å‰ΩøÁî®TEXTFILE‰Ωú‰∏∫hiveË°®Êï∞ÊçÆÁöÑÂ≠òÂÇ®Ê†ºÂºèÔºåÂàôÂøÖÈ°ªÂÖàÂ≠òÂú®‰∏ÄÂº†Áõ∏ÂêåÊï∞ÊçÆÁöÑÂ≠òÂÇ®Ê†ºÂºè‰∏∫TEXTFILEÁöÑË°®table_t0,ÁÑ∂ÂêéÂú®Âª∫Ë°®Êó∂‰ΩøÁî®‚Äúinsert into table table_stored_file_ORC select from table_t0;‚ÄùÂàõÂª∫„ÄÇÊàñËÄÖ‰ΩøÁî®‚Äùcreate table as select from table_t0;‚ÄùÂàõÂª∫„ÄÇ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark‰πãÂ∫èÂàóÂåñÂú®Áîü‰∫ß‰∏≠ÁöÑÂ∫îÁî®]]></title>
    <url>%2F2018%2F05%2F29%2FSpark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Â∫èÂàóÂåñÂú®ÂàÜÂ∏ÉÂºèÂ∫îÁî®ÁöÑÊÄßËÉΩ‰∏≠ÊâÆÊºîÁùÄÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÊ†ºÂºèÂåñÂØπË±°ÁºìÊÖ¢ÔºåÊàñËÄÖÊ∂àËÄóÂ§ßÈáèÁöÑÂ≠óËäÇÊ†ºÂºèÂåñÔºå‰ºöÂ§ßÂ§ßÈôç‰ΩéËÆ°ÁÆóÊÄßËÉΩ„ÄÇÂú®Áîü‰∫ß‰∏≠ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰ºöÂàõÂª∫Â§ßÈáèÁöÑËá™ÂÆö‰πâÂÆû‰ΩìÂØπË±°ÔºåËøô‰∫õÂØπË±°Âú®ÁΩëÁªú‰º†ËæìÊó∂ÈúÄË¶ÅÂ∫èÂàóÂåñÔºåËÄå‰∏ÄÁßçÂ•ΩÁöÑÂ∫èÂàóÂåñÊñπÂºèÂèØ‰ª•ËÆ©Êï∞ÊçÆÊúâÊõ¥Â•ΩÁöÑÂéãÁº©ÊØîÔºå‰ªéËÄåÊèêÂçáÁΩëÁªú‰º†ËæìÈÄüÁéáÔºåÊèêÈ´òspark‰Ωú‰∏öÁöÑËøêË°åÈÄüÂ∫¶„ÄÇÈÄöÂ∏∏ËøôÊòØÂú®sparkÂ∫îÁî®‰∏≠Á¨¨‰∏Ä‰ª∂ÈúÄË¶Å‰ºòÂåñÁöÑ‰∫ãÊÉÖ„ÄÇSparkÁöÑÁõÆÊ†áÊòØÂú®‰æøÂà©‰∏éÊÄßËÉΩ‰∏≠ÂèñÂæóÂπ≥Ë°°ÔºåÊâÄ‰ª•Êèê‰æõ2ÁßçÂ∫èÂàóÂåñÁöÑÈÄâÊã©„ÄÇJava serializationÂú®ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåSpark‰ºö‰ΩøÁî®JavaÁöÑObjectOutputStreamÊ°ÜÊû∂ÂØπÂØπË±°ËøõË°åÂ∫èÂàóÂåñÔºåÂπ∂‰∏îÂèØ‰ª•‰∏é‰ªª‰ΩïÂÆûÁé∞java.io.SerializableÁöÑÁ±ª‰∏ÄËµ∑Â∑•‰Ωú„ÄÇÊÇ®ËøòÂèØ‰ª•ÈÄöËøáÊâ©Â±ïjava.io.ExternalizableÊù•Êõ¥Á¥ßÂØÜÂú∞ÊéßÂà∂Â∫èÂàóÂåñÁöÑÊÄßËÉΩ„ÄÇJavaÂ∫èÂàóÂåñÊòØÁÅµÊ¥ªÁöÑÔºå‰ΩÜÈÄöÂ∏∏Áõ∏ÂΩìÊÖ¢ÔºåÂπ∂‰∏î‰ºöÂØºËá¥ËÆ∏Â§öÁ±ªÁöÑÂ§ßÂûãÂ∫èÂàóÂåñÊ†ºÂºè„ÄÇÊµãËØï‰ª£Á†ÅÔºöÊµãËØïÁªìÊûúÔºöKryo serializationSparkËøòÂèØ‰ª•‰ΩøÁî®KryoÂ∫ìÔºàÁâàÊú¨2ÔºâÊù•Êõ¥Âø´Âú∞Â∫èÂàóÂåñÂØπË±°„ÄÇKryoÊØîJava‰∏≤Ë°åÂåñÔºàÈÄöÂ∏∏Â§öËææ10ÂÄçÔºâË¶ÅÂø´ÂæóÂ§öÔºå‰πüÊõ¥Á¥ßÂáëÔºå‰ΩÜÊòØ‰∏çÊîØÊåÅÊâÄÊúâÂèØ‰∏≤Ë°åÂåñÁ±ªÂûãÔºåÂπ∂‰∏îË¶ÅÊ±ÇÊÇ®ÊèêÂâçÊ≥®ÂÜåÊÇ®Â∞ÜÂú®Á®ãÂ∫è‰∏≠‰ΩøÁî®ÁöÑÁ±ªÔºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÊµãËØï‰ª£Á†ÅÔºöÊµãËØïÁªìÊûúÔºöÊµãËØïÁªìÊûú‰∏≠ÂèëÁé∞Ôºå‰ΩøÁî® Kryo serialization ÁöÑÂ∫èÂàóÂåñÂØπË±° ÊØî‰ΩøÁî® Java serializationÁöÑÂ∫èÂàóÂåñÂØπË±°Ë¶ÅÂ§ßÔºå‰∏éÊèèËø∞ÁöÑ‰∏ç‰∏ÄÊ†∑ÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢ÔºüÊü•ÊâæÂÆòÁΩëÔºåÂèëÁé∞Ëøô‰πà‰∏ÄÂè•ËØù Finally, if you don‚Äôt register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.„ÄÇ‰øÆÊîπ‰ª£Á†ÅÂêéÂú®ÊµãËØï‰∏ÄÊ¨°„ÄÇÊµãËØïÁªìÊûúÔºöÊÄªÁªìÔºöKryo serialization ÊÄßËÉΩÂíåÂ∫èÂàóÂåñÂ§ßÂ∞èÈÉΩÊØîÈªòËÆ§Êèê‰æõÁöÑ Java serialization Ë¶ÅÂ•ΩÔºå‰ΩÜÊòØ‰ΩøÁî®KryoÈúÄË¶ÅÂ∞ÜËá™ÂÆö‰πâÁöÑÁ±ªÂÖàÊ≥®ÂÜåËøõÂéªÔºå‰ΩøÁî®Ëµ∑Êù•ÊØîJava serializationÈ∫ªÁÉ¶„ÄÇËá™‰ªéSpark 2.0.0‰ª•Êù•ÔºåÊàë‰ª¨Âú®‰ΩøÁî®ÁÆÄÂçïÁ±ªÂûã„ÄÅÁÆÄÂçïÁ±ªÂûãÊï∞ÁªÑÊàñÂ≠óÁ¨¶‰∏≤Á±ªÂûãÁöÑÁÆÄÂçïÁ±ªÂûãÊù•Ë∞ÉÊï¥RDDsÊó∂ÔºåÂú®ÂÜÖÈÉ®‰ΩøÁî®KryoÂ∫èÂàóÂåñÂô®„ÄÇÈÄöËøáÊü•ÊâæsparkcontextÂàùÂßãÂåñÁöÑÊ∫êÁ†ÅÔºåÂèØ‰ª•ÂèëÁé∞Êüê‰∫õÁ±ªÂûãÂ∑≤ÁªèÂú®sparkcontextÂàùÂßãÂåñÁöÑÊó∂ÂÄôË¢´Ê≥®ÂÜåËøõÂéª„ÄÇ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ëã•Ê≥ΩÊï∞ÊçÆÂ∏¶‰Ω†ÈöèÊó∂‰∫ÜËß£‰∏öÁïåÈù¢ËØïÈ¢òÔºåÈöèÊó∂Ë∑≥È´òËñ™]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA%2F</url>
    <content type="text"><![CDATA[ÈìæÂÆ∂(‰∏ÄÈù¢Ôºå‰∫åÈù¢)0.Ëá™Êàë‰ªãÁªç1.Â∞ÅË£ÖÁªßÊâøÂ§öÊÄÅÊ¶ÇÂøµ2.mvcËÆæËÆ°ÊÄùÊÉ≥3.Á∫øÁ®ãÊ±†,ÁúãËøáÊ∫êÁ†ÅÂêó4.sshÊ°ÜÊû∂‰∏≠ÂàÜÂà´ÂØπÂ∫îmvc‰∏≠ÈÇ£‰∏ÄÂ±Ç5.shellÂëΩ‰ª§ÔºàÊü•ËØ¢‰∏Ä‰∏™Êñá‰ª∂ÊúâÂ§öÂ∞ëË°å„ÄÇ chown ‰øÆÊîπÊñá‰ª∂ÊùÉÈôêÔºå Âè™ËÆ∞ÂæóÈÇ£‰πàÂ§ö‰∫Ü Ôºâ6.spring ioc aop ÂéüÁêÜ7.ÂçïÂà©Ê®°Âºè8.SQLÈ¢òÔºåÊÉ≥‰∏çËµ∑Êù•‰∫Ü„ÄÇ„ÄÇ9.jvm ËøêË°åÊó∂Êï∞ÊçÆÂå∫Âüü10.spring mvcÁü•ÈÅìÂêó„ÄÇ„ÄÇ11.Â∑•ÂéÇÊ®°Âºè12.mr ËÆ°ÁÆóÊµÅÁ®ã13.hiveÊü•ËØ¢ËØ≠Âè•ÔºàË°®1ÔºöÊó∂Èó¥ È£üÂ†ÇÊ∂àË¥π Ë°®‰∫åÔºöÂêÑ‰∏™Êó∂Èó¥ÊÆµ Áî®Êà∑ ÊØè‰∏™È£üÂ†ÇÊ∂àË¥π Êü•ËØ¢Áî®Êà∑Âú®ÊØè‰∏™Êó∂Èó¥Âá∫Áé∞Âú®ÈÇ£‰∏™È£üÂ†ÇÁªüËÆ°Ê∂àË¥πËÆ∞ÂΩï ÔºåÂ§ßÊ¶ÇÊòØËøôÊ†∑ÁöÑ„ÄÇ„ÄÇÔºâ14.gitÁöÑ‰ΩøÁî®15.hadoopÁöÑÁêÜËß£16.hiveÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°®ÁöÑÂå∫Âà´17.hiveÂ≠òÂÇ®Ê†ºÂºèÂíåÂéãÁº©Ê†ºÂºè18.ÂØπspark‰∫ÜËß£ÂêóÔºü ÂΩìÊó∂È´òÁ∫ßÁè≠ËøòÊ≤°Â≠¶„ÄÇ„ÄÇ19.hive‰∫éÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÁöÑÂå∫Âà´20.ÂêÑÁßçÊéíÂ∫è ÊâãÂÜôÂ†ÜÊéíÂ∫è,ËØ¥ËØ¥ÂéüÁêÜ21.ÈìæË°®ÈóÆÈ¢òÔºåÊµèËßàÂô®ËÆøÈóÆËÆ∞ÂΩïÔºåÂâçËøõÂêéÈÄÄÂΩ¢ÊàêÈìæË°®ÔºåÊñ∞Âä†‰∏Ä‰∏™ËÆ∞ÂΩïÔºåÂ§öÂá∫‰∏Ä‰∏™ÂàÜÊîØÔºåÂà†Èô§‰ª•ÂâçÁöÑÂàÜÊîØ„ÄÇËÆæËÆ°ÁªìÊûÑÔºåÂ¶ÇÊûúËøô‰∏™ÁªìÊûÑÂÜôÂú®ÂáΩÊï∞‰∏≠ÊÄé‰πàÁª¥Êä§„ÄÇ22‰∏≠Èó¥‰πüÁ©øÊèí‰∫ÜÈ°πÁõÆ„ÄÇÊó†ËÆ∫ÊòØÂ∑≤ÁªèÊâæÂà∞Â∑•‰ΩúÁöÑËøòÊòØÊ≠£Âú®Â∑•‰ΩúÁöÑÔºåÊàëÁöÑËßâÁöÑÈù¢ËØïÈ¢òÈÉΩÂèØ‰ª•ÁªôÊÇ®‰ª¨Â∏¶Êù•‰∏Ä‰∫õÂêØÂèë„ÄÇÂèØ‰ª•‰∫ÜËß£Â§ßÊï∞ÊçÆË°å‰∏öÈúÄË¶Å‰ªÄ‰πàÊ†∑ÁöÑ‰∫∫ÊâçÔºå‰ªÄ‰πàÊäÄËÉΩÔºåÂØπÂ∫îÂéªË°•ÂÖÖËá™Â∑±ÁöÑ‰∏çË∂≥‰πãÂ§ÑÔºå‰∏∫‰∏ã‰∏Ä‰∏™È´òËñ™Â∑•‰ΩúÂÅöÂáÜÂ§á„ÄÇËã•Ê≥ΩÂ§ßÊï∞ÊçÆÂêéÈù¢‰ºöÈöèÊó∂Êõ¥Êñ∞Â≠¶ÂëòÈù¢ËØïÈ¢òÔºåËÆ©Â§ßÂÆ∂‰∫ÜËß£Â§ßÊï∞ÊçÆË°å‰∏öÁöÑÂèëÂ±ïË∂ãÂäøÔºåÊó®Âú®Â∏ÆÂä©Ê≠£Âú®Ëâ∞ËæõÊâìÊãºÁöÑÊÇ®ÊåáÂá∫‰∏ÄÊù°Âå∫Áõ¥ÁöÑÊú™Êù•‰πãË∑ØÔºÅÔºàÂ∞ëËµ∞ÂºØË∑ØÂô¢Âô¢„ÄÇ„ÄÇÔºâ]]></content>
      <categories>
        <category>Èù¢ËØïÈ¢ò</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰∏ÄÊ¨°Ë∑≥ÊßΩÁªèÂéÜÔºàÈòøÈáå/ÁæéÂõ¢/Â§¥Êù°/ÁΩëÊòì/ÊúâËµû...)]]></title>
    <url>%2F2018%2F05%2F24%2F%E6%9C%89%E8%B5%9E...)%2F</url>
    <content type="text"><![CDATA[‰∏∫Âï•Ë∑≥ÊßΩÊØèÊ¨°ËØ¥Âõ†‰∏∫ÁîüÊ¥ªÊàêÊú¨ÁöÑÊó∂ÂÄôÈù¢ËØïÂÆòÈÉΩ‰ºöÂæàÊÉäÂ•áÔºåÈöæÈÅìÊúâÊàë‰ª¨ËøôÈáåË¥µÔºüÂ•ΩÊÉ≥Áõ¥Êé•ÁªôÂá∫‰∏ãÈù¢ËøôÂº†ÂõæÔºåÂé¶Èó®ÁöÑÊàø‰ª∑ÁúüÁöÑÂ•ΩË¥µÂ•ΩË¥µÂ•ΩË¥µ„ÄÇ„ÄÇ„ÄÇÈù¢ËØïËøáÁ®ãÔºàÂÖàÊâì‰∏™ÂπøÂëäÔºåÊúâÂÖ¥Ë∂£Âä†ÂÖ•ÈòøÈáåÁöÑÊ¨¢ËøéÂèëÁÆÄÂéÜËá≥ zhangzb2007@gmail.comÔºåÊàñÁÆÄ‰π¶‰∏äÁªôÊàëÂèë‰ø°ÊÅØÔºâÈù¢ÁöÑÊòØJavaÂ≤óÔºåÊÄªÂÖ±Èù¢‰∫Ü7ÂÆ∂ÂÖ¨Âè∏ÔºåÈÄöËøá‰∫Ü6ÂÆ∂„ÄÇÊåâËá™Â∑±ÁöÑ‰ø°ÂøÉÊèêÂçáÂ∫¶ÊàëÊääÈù¢ËØïËøáÁ®ãÂàÜ‰∏∫‰∏äÂçäÂú∫Âíå‰∏ãÂçäÂú∫„ÄÇ‰∏äÂçäÂú∫ÊõπÊìç‰∏ìËΩ¶ËøôÊòØÂêâÂà©ÈõÜÂõ¢‰∏ãÂ±ûÂ≠êÂÖ¨Âè∏ÔºåÂ∑≤ÁªèÊòØ‰∏ÄÂÆ∂Áã¨ËßíÂÖΩ„ÄÇ‰∏ÄÈù¢‰∏≠ËßÑ‰∏≠Áü©ÔºåÊ≤°Âï•ÁâπÂà´ÁöÑ„ÄÇ‰∫åÈù¢Â•ΩÂÉèÊòØ‰∏™‰∏ªÁÆ°ÔºåÈöî‰∫ÜÂ•ΩÂá†Â§©ÔºåÂü∫Êú¨Ê≤°ÈóÆÊäÄÊúØÈóÆÈ¢òÔºåÂèçËÄåÊòØÈóÆËÅå‰∏öËßÑÂàíÔºåÂØπÂä†Áè≠ÊúâÂï•ÁúãÊ≥ïÔºåÊúâÁÇπÊé™Êâã‰∏çÂèäÔºåÊÑüËßâÂõûÁ≠îÁöÑ‰∏çÂ•Ω„ÄÇ‰ΩÜÊòØËøáÂá†Â§©ËøòÊòØÊî∂Âà∞HRÁöÑÁé∞Âú∫Èù¢ËØïÈÄöÁü•„ÄÇÁé∞Âú∫ÊòØÊäÄÊúØÈù¢Âä†HRÈù¢ÔºåÊäÄÊúØÈù¢Ë¢´ÈóÆ‰∫ÜÂá†‰∏™ÈóÆÈ¢òÊúâÁÇπÊáµÈÄºÔºöa. zookeeperÁöÑwatcher‰πêËßÇÈîÅÊÄé‰πàÂÆûÁé∞ b. ‰∏Ä‰∏™È°πÁõÆÁöÑÊï¥‰∏™ÊµÅÁ®ã c. ËØ¥Âá∫‰∏Ä‰∏™Á©∫Èó¥Êç¢Êó∂Èó¥ÁöÑÂú∫ÊôØ d. centos7ÁöÑÂÜÖÂ≠òÂàÜÈÖçÊñπÂºèÂíå6ÊúâÂï•‰∏çÂêå f. ‰Ω†ÂØπÂÖ¨Âè∏Êúâ‰ªÄ‰πà‰ª∑ÂÄº„ÄÇHRË∑üÊàëËØ¥ËäÇÂêéÔºàÈÇ£‰ºöÂÜçËøá‰∏§Â§©Â∞±ÊòØÊ∏ÖÊòéÔºâ‰ºöÁªôÊàëÊ∂àÊÅØÔºåÁªìÊûúËøá‰∫ÜÂçä‰∏™ÊúàÁ™ÅÁÑ∂Êé•Âà∞‰ªñ‰ª¨ÁöÑÁîµËØùÔºåËØ¥ÊàëÈÄöËøá‰∫ÜÔºåÁªôÊàëËÆ≤‰∫Ü‰ªñ‰ª¨ÁöÑËñ™ËµÑÊñπÊ°àÔºåÊ≤°Â§™Â§ßÂê∏ÂºïÂäõÔºåÂÜçÂä†‰∏äËøôÁßçËé´ÂêçÂÖ∂Â¶ôÁöÑÊó∂Èó¥Á≠âÂæÖÔºåÁõ¥Êé•Êãí‰∫Ü„ÄÇÁæé‰∫öÊüèÁßë‰º∞ËÆ°ÂæàÂ§ö‰∫∫Ê≤°Âê¨ËØ¥ËøáËøôÂÆ∂ÂÖ¨Âè∏ÔºåËøôÊòØ‰∏ÄÂÆ∂Âé¶Èó®Êú¨ÂúüÂÖ¨Âè∏ÔºåÂÅöÊîøÂ∫úÂÆâÈò≤È°πÁõÆÁöÑÔºåÂú®Âé¶Èó®‰πüËøòÊòØÂ∞èÊúâÂêçÊ∞î„ÄÇ‰ΩÜÊòØÈù¢ËØïÂÆåÁõ¥Êé•È¢†Ë¶Ü‰∫ÜÊàëÂØπËøôÂÆ∂ÂÖ¨Âè∏ÁöÑËÆ§Áü•„ÄÇËøõÈó®ÊúÄÊòæÁúºÁöÑÂú∞ÊñπÊòØÂÖöÊ¥ªÂä®ÂÆ§ÔºåÂú®Á≠âÈù¢ËØïÂÆòÁöÑ‰∏ÄÂ∞èÊÆµÊó∂Èó¥ÈáåÊúâÂ•ΩÂá†Êã®‰∫∫Âà∞ÈáåÈù¢ÂèÇËßÇ„ÄÇÈù¢ËØïÂâçÂÅö‰∫Ü‰∏Ä‰ªΩÁ¨îËØïÈ¢òÔºåÂü∫Êú¨ÈÉΩÊòØweb/Êï∞ÊçÆÂ∫ìÊñπÈù¢ÁöÑ„ÄÇÁ¨¨‰∏ÄÈù¢ÁÆÄÂçïÈóÆ‰∫ÜÂá†‰∏™redisÁöÑÈóÆÈ¢ò‰πãÂêéÈù¢ËØïÂÆò‰ªãÁªç‰∫Ü‰ªñ‰ª¨ÁöÑÈ°πÁõÆÔºå‰ªñ‰ª¨ÈÉΩÊòØÂÅöCÂíåC++ÁöÑÔºåÊÉ≥Êâæ‰∏Ä‰∏™‰∫∫Êê≠‰∏ÄÂ•óÂ§ßÊï∞ÊçÆÈõÜÁæ§ÔºåÂ§ÑÁêÜ‰ªñ‰ª¨ÊØèÂ§©Âá†ÁôæGÁöÑÊï∞ÊçÆÔºåÁÑ∂ÂêéÊúçÂä°Âô®ÂÖ®ÈÉ®ÊòØwindowsÔºÅ‰∫åÈù¢ÊòØÂè¶‰∏Ä‰∏™ÈÉ®Èó®ÁöÑÔºåÂç∞Ë±°‰∏≠Â∞±ÈóÆ‰∫Ükafka‰∏∫‰ªÄ‰πàÊÄßËÉΩËøô‰πàÂ•ΩÔºåÁÑ∂ÂêéÂ∞±ÂºÄÂßãÈóÆ‰π∞Êàø‰∫ÜÊ≤°ÊúâÔºåÁªìÂ©ö‰∫ÜÊ≤°ÊúâÔºå‰ªñÂØπÊàëÁé∞Âú®ÁöÑÂÖ¨Âè∏ÊØîËæÉ‰∫ÜËß£ÔºåÂèàÊâØ‰∫ÜÊå∫‰πÖ„ÄÇ‰∏âÈù¢Â∫îËØ•ÊòØ‰∏™ÈÉ®Èó®ËÄÅÂ§ß‰∫ÜÔºåÊ≤°ÊúâÈóÆÊäÄÊúØÈóÆÈ¢òÔºå‰πüÊòØÈóÆ‰π∞Êàø‰∫ÜÊ≤°ÔºåÁªìÂ©öÊ≤°ÔºåÈóÆÂêÑÁßçÁîüÊ¥ªÈóÆÈ¢òÔºåÊúâÁÇπÂÉè‰∫∫Âè£ÊôÆÊü•„ÄÇÊàëÊúâÁÇπÂ•ΩÂ•áÔºåÈóÆ‰ªñ‰ª¨‰∏∫Âï•Ëøô‰πàÂÖ≥ÂøÉËøô‰∫õÈóÆÈ¢òÔºå‰ªñÁõ¥Êé•ËØ¥‰ªñ‰ª¨Êõ¥Âº∫Ë∞ÉÂëòÂ∑•ÁöÑÁ®≥ÂÆöÊÄßÔºåÈ°πÁõÆÊØîËæÉÁÆÄÂçïÔºåËÉΩÂäõ‰∏çÁî®Ë¶ÅÊ±ÇÂ§™È´òÔºå‰∏çË¶ÅÂ§™Â∑ÆÂ∞±Ë°å„ÄÇÊ±óÔºåÁõ¥Êé•Êãí‰∫Ü„ÄÇÊúâËµûÁªùÂØπÊé®ËçêÁöÑ‰∏ÄÂÆ∂ÂÖ¨Âè∏ÔºåÊïàÁéáË∂ÖÈ´ò„ÄÇ‰∏≠ÂçàÊâæ‰∫Ü‰∏Ä‰∏™ÁΩëÂèãÂ∏ÆÂøôÂÜÖÊé®ÔºåÊôö‰∏äÂ∞±ÂºÄÂßã‰∏ÄÈù¢ÔºåÁ¨¨‰∫åÂ§©Êó©‰∏ä‰∫åÈù¢ÔºåÁ¨¨‰∏âÂ§©HRÂ∞±Á∫¶Áé∞Âú∫Èù¢ËØïÊó∂Èó¥ÔºåÂø´ÁöÑË∂Ö‰πéÊÉ≥Ë±°„ÄÇÁé∞Âú∫Èù¢‰πüÊòØÂÖà‰∏Ä‰∏™ÊäÄÊúØÈù¢ÔºåÊúÄÂêéÊâçHRÈù¢„ÄÇÈù¢ËØïÁöÑÊï¥‰ΩìÈöæÂ∫¶‰∏≠Á≠â„ÄÇÁé∞Âú®Â∞±ËÆ∞ÂæóÂá†‰∏™ÈóÆÈ¢òÔºöG1ÂíåCMSÁöÑÂå∫Âà´ÔºåG1ÊúâÂï•Âä£ÂäøÔºõKafkaÁöÑÊï¥‰ΩìÊû∂ÊûÑÔºõNettyÁöÑ‰∏ÄÊ¨°ËØ∑Ê±ÇËøáÁ®ãÔºõËá™ÊóãÈîÅ/ÂÅèÂêëÈîÅ/ËΩªÈáèÁ∫ßÈîÅÔºàËøô‰∏™ÈóÆÈ¢òÂú®Â§¥Êù°ÁöÑÈù¢ËØïÈáå‰πüÂá∫Áé∞‰∫Ü‰∏ÄÊ¨°Ôºâ„ÄÅhbaseÁ∫ø‰∏äÈóÆÈ¢òÊéíÊü•ÔºàÂàöÂ•ΩÈÅáÂà∞ËøáNUMAÊû∂ÊûÑ‰∏ãÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÔºåÂÄüÊ≠§ÊäähbaseÁöÑÂÜÖÊ†∏‰ªãÁªç‰∫Ü‰∏ãÔºâ„ÄÇËøôÈáå‰∏çÂæó‰∏çËØ¥‰∏ãÊúâËµûÁöÑ‰∫∫ÔºåÁúüÁöÑÂæàËµû„ÄÇÁªàÈù¢ÁöÑÈù¢ËØïÂÆòÊòØ‰∏Ä‰∏™Á†îÂèëÂõ¢ÈòüÁöÑË¥üË¥£‰∫∫ÔºåÂÖ®Á®ã‰∏ÄÁõ¥ÂæÆÁ¨ëÔºå‰∏≠Èó¥ÁîµËØùÂìç‰∫Ü‰∏ÄÊ¨°Ôºå‰∏ÄÁõ¥Ë∑üÊàëÈÅìÊ≠â„ÄÇÈù¢ÂÆå‰πãÂêéËøòÊèê‰æõ‰∫ÜÂõ¢ÈòüÁöÑ‰∏â‰∏™Á†îÂèëÊñπÂêëËÆ©ÊàëËá™Â∑±ÈÄâÊã©„ÄÇÂêéÈù¢Áúã‰ªñÁöÑÊúãÂèãÂúàÁä∂ÊÄÅÔºå‰ªñÈÇ£Â§©È´òÁÉßÔºåÈù¢ÂÆåÊàëÂ∞±ÂéªÊâìÁÇπÊª¥‰∫ÜÔºå‰ΩÜÊòØÊï¥‰∏™ËøáÁ®ãÂÆåÂÖ®Áúã‰∏çÂá∫Êù•„ÄÇÂ∏ÆÊàëÂÜÖÊé®ÁöÑÁΩëÂèãÊòØÂú®ÂæÆ‰ø°Áæ§ÈáåÊâæÂà∞ÁöÑÔºåÁü•ÈÅìÊàëËøá‰∫Ü‰πãÂêé‰∏ªÂä®ÊâæÊàëÔºåËÆ©ÊàëËøáÂéªÊù≠Â∑ûÊúâÂï•ÈóÆÈ¢òÈöèÊó∂Êâæ‰ªñ„ÄÇËôΩÁÑ∂ÊúÄÁªàÊ≤°ÊúâÂéªÔºå‰ΩÜËøòÊòØÂèØ‰ª•ÊòéÊòæÊÑüÂèóÂà∞‰ªñ‰ª¨ÁöÑÁÉ≠ÊÉÖ„ÄÇÂ≠óËäÇË∑≥Âä®(‰ªäÊó•Â§¥Êù°)HRÁæéÁúâÊâìÁîµËØùËøáÊù•ËØ¥ÊòØÂ≠óËäÇË∑≥Âä®ÂÖ¨Âè∏ÔºåÊÉ≥Á∫¶‰∏ãËßÜÈ¢ëÈù¢ËØïÊó∂Èó¥„ÄÇÈÇ£‰ºöÊòØÊúâÁÇπÊáµÁöÑÔºåÊàëÂè™Áü•ÈÅì‰ªäÊó•Â§¥Êù°ÂíåÊäñÈü≥„ÄÇÂêéÈù¢ÊÉ≥Âà∞Âåó‰∫¨ÁöÑÂè∑Á†ÅÊâçÊÉ≥Ëµ∑Êù•„ÄÇÂ§¥Êù°ÂèØ‰ª•ËØ¥ÊòØËøôÊ¨°ÊâÄÊúâÈù¢ËØïÈáåÊµÅÁ®ãÊúÄËßÑËåÉÁöÑÔºåÊî∂Âà∞ÁÆÄÂéÜÂêéÊúâÈÇÆ‰ª∂ÈÄöÁü•ÔºåÈ¢ÑÁ∫¶Èù¢ËØïÊó∂Èó¥ÂêéÈÇÆ‰ª∂Áü≠‰ø°ÈÄöÁü•ÔºåÈù¢ËØïÂÆåÂêé‰∏çË∂ÖËøá‰∏ÄÂ§©ÈÄöÁü•Èù¢ËØïÁªìÊûúÔºåÊØèÊ¨°Èù¢ËØïÊúâÈù¢ËØïÂèçÈ¶à„ÄÇËøòÊúâ‰∏Ä‰∏™ÊØîËæÉÁâπÂà´ÁöÑÔºåÂ§ßÈÉ®ÂàÜÂÖ¨Âè∏ÁöÑÁîµËØùÊàñËÄÖËßÜÈ¢ëÈù¢ËØïÂü∫Êú¨ÊòØ‰∏ãÁè≠ÂêéÔºåÂ§¥Êù°ÈÉΩÊòØ‰∏äÁè≠Êó∂Èó¥ÔºåËøò‰∏çÁªôÁ∫¶‰∏ãÁè≠Êó∂Èó¥ÔºàÈöæÈÅì‰ªñ‰ª¨‰∏çÂä†Áè≠ÔºüÔºâ„ÄÇ‰∏ÄÈù¢Èù¢ËØïÂÆòÂàö‰∏äÊù•Â∞±ËØ¥‰ªñ‰ª¨ÊòØÂÅögoÁöÑÔºåÈóÆÊàëÊúâÊ≤°ÊúâÂÖ¥Ë∂£Ôºå‰ªñËá™Â∑±‰πüÊòØJavaËΩ¨ÁöÑ„ÄÇÊàëËØ¥Ê≤°ÈóÆÈ¢òÔºå‰ªñÂÖàÈóÆ‰∫Ü‰∏Ä‰∫õJavaÂü∫Á°ÄÈóÆÈ¢òÔºåÁÑ∂ÂêéÊúâ‰∏ÄÈÅìÁºñÁ®ãÈ¢òÔºåÊ±Ç‰∏ÄÊ£µÊ†ë‰∏§‰∏™ËäÇÁÇπÁöÑÊúÄËøëÁöÑÂÖ¨ÂÖ±Áà∂ËäÇÁÇπ„ÄÇÊÄùË∑ØÂü∫Êú¨ÊòØÂØπÁöÑÔºå‰ΩÜÊòØÊúâ‰∫õÁªÜËäÇÊúâÈóÆÈ¢òÔºåÈù¢ËØïÂÆò‰∫∫ÂæàÂ•ΩÔºåËæπÁúãËæπË∑üÊàëËÆ®ËÆ∫ÔºåÊàëËæπÊîπËøõÔºåÂâçÂâçÂêéÂêé‰º∞ËÆ°Áî®Êù•Âø´ÂçäÂ∞èÊó∂„ÄÇÁÑ∂ÂêéÂèàÁªßÁª≠ÈóÆÈóÆÈ¢òÔºåHTTP 301 302ÊúâÂï•Âå∫Âà´ÔºüËÆæËÆ°‰∏Ä‰∏™Áü≠ÈìæÊé•ÁÆóÊ≥ïÔºõmd5ÈïøÂ∫¶ÊòØÂ§öÂ∞ëÔºüÊï¥‰∏™Èù¢ËØïËøáÁ®ã‰∏Ä‰∏™Â§öÂ∞èÊó∂ÔºåËá™ÊàëÊÑüËßâ‰∏çÊòØÂæàÂ•ΩÔºåÊàë‰ª•‰∏∫ËøôÊ¨°Â∫îËØ•ÊåÇ‰∫ÜÔºåÁªìÊûúÊôö‰∏äÊî∂Âà∞Èù¢ËØïÈÄöËøáÁöÑÈÄöÁü•„ÄÇ‰∫åÈù¢ÊòØÂú®‰∏Ä‰∏™‰∏äÂçàËøõË°åÁöÑÔºåÊàë‰ª•‰∏∫zoomËßÜÈ¢ëÁ≥ªÁªü‰ºöËá™Âä®Ëøû‰∏äÔºà‰∏ÄÈù¢Â∞±ÊòØËá™Âä®Ëøû‰∏äÔºâÔºåÂ∞±Âú®ÈÇ£ËæπÁ≠âÔºåËøá‰∫Ü5ÂàÜÈíüËøòÊòØ‰∏çË°åÔºåÊàëÂ∞±ËÅîÁ≥ªHRÔºåÂéüÊù•Ë¶ÅÊîπidÔºåÁªà‰∫éËøû‰∏äÂêéÈù¢ËØïÂÆòÁöÑË°®ÊÉÖ‰∏çÊòØÂæàÂ•ΩÁúãÔºåÊúâÁÇπ‰∏çËÄêÁÉ¶ÁöÑÊ†∑Â≠êÔºå‰∏çÊáÇÊòØ‰∏çÊòØÂõ†‰∏∫ÊàëËÄΩËØØ‰∫ÜÂá†ÂàÜÈíüÔºåËøôÁßçË°®ÊÉÖÂª∂Áª≠‰∫ÜÊï¥‰∏™Èù¢ËØïËøáÁ®ãÔºåÂÖ®Á®ãÊúâÁÇπÂéãÊäë„ÄÇÈóÆÁöÑÈóÆÈ¢òÂ§ßÈÉ®ÂàÜÂøò‰∫ÜÔºåÂè™ËÆ∞ÂæóÈóÆ‰∫Ü‰∏Ä‰∏™Á∫øÁ®ãÂÆâÂÖ®ÁöÑÈóÆÈ¢òÔºåThreadLocalÂ¶ÇÊûúÂºïÁî®‰∏Ä‰∏™staticÂèòÈáèÊòØ‰∏çÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÔºüÈóÆÁùÄÈóÆÁùÄÁ™ÅÁÑ∂ËØ¥‰ªäÂ§©Èù¢ËØïÂà∞Ê≠§‰∏∫Ê≠¢Ôºå‰∏ÄÁúãÊó∂Èó¥ÊâçËøáÂéª‰∫åÂçÅÂá†ÂàÜÈíü„ÄÇÁ¨¨‰∫åÂ§©Â∞±Êî∂Âà∞Èù¢ËØïÊ≤°ËøáÁöÑÈÄöÁü•ÔºåÊÑüËßâËá™Â∑±‰∫åÈù¢Á≠îÁöÑÊØî‰∏ÄÈù¢Â•ΩÂ§ö‰∫ÜÔºåÂÆûÂú®ÊÉ≥‰∏çÈÄö„ÄÇ‰∏ãÂçäÂú∫‰∏ÄÁõ¥ÊÑüËßâËá™Â∑±Â§™Ê∞¥‰∫ÜÔºå‰ª£Á†ÅÈáè‰∏çÂ§ßÔºå‰∏âÂπ¥ÂçäÁöÑITÁªèÈ™åËøòÊúâ‰∏ÄÂπ¥ÂéªÂÅö‰∫Ü‰∫ßÂìÅÔºåÈÉΩ‰∏çÊï¢ÊäïÂ§ßÂéÇ„ÄÇ‰∏äÂçäÂú∫ÁöÑÊäÄÊúØÈù¢Âü∫Êú¨Ëøá‰∫Ü‰πãÂêéËá™‰ø°ÂøÉÂ§ßÂ§ßÊèêÂçáÔºåÂºÄÂßãÊåëÊàòÊõ¥È´òÈöæÂ∫¶ÁöÑ„ÄÇÁæéÂõ¢Ëøô‰∏™ÊòØÂé¶Èó®ÁæéÂõ¢Ôºå‰ªñ‰ª¨Âú®ËøôËæπÂÅö‰∫Ü‰∏Ä‰∏™Âè´Ê¶õÊûúÊ∞ëÂÆøÁöÑAPPÔºåÂäûÂÖ¨Âú∞ÁÇπÂú®JFCÈ´òÊ°£ÂÜôÂ≠óÊ•ºÔºå‰ºëÊÅØÂå∫ÂèØ‰ª•Èù¢ÊúùÂ§ßÊµ∑ÔºåÁéØÂ¢ÉÊòØÂæà‰∏çÈîôÔºåÈù¢ËØïÂ∞±ÊúâÁÇπËôêÂøÉ‰∫Ü„ÄÇ‰∏§ÁÇπÂçäËøõÂéª„ÄÇ‰∏ÄÈù¢„ÄÇÊàëÁöÑÁÆÄÂéÜÂ§ßÈÉ®ÂàÜÊòØÂ§ßÊï∞ÊçÆÁõ∏ÂÖ≥ÁöÑÔºå‰ªñ‰∏çÊòØÂæà‰∫ÜËß£ÔºåÈóÆ‰∫Ü‰∏Ä‰∫õÂü∫Á°ÄÈóÆÈ¢òÂíånettyÁöÑÂÜôÊµÅÁ®ãÔºåËøòÈóÆ‰∫Ü‰∏Ä‰∏™redisÊï∞ÊçÆÁªìÊûÑÁöÑÂÆûÁé∞ÔºåÁªìÊûÑ‰ªñÈóÆ‰∫ÜÈáåÈù¢Â≠óÁ¨¶‰∏≤ÊòØÊÄé‰πàÂÆûÁé∞ÁöÑÔºåÊúâ‰ªÄ‰πà‰ºòÂäø„ÄÇ‰∏ÄÁõ¥ÊÑüËßâËøô‰∏™Â§™ÁÆÄÂçïÔºåÊ≤°Â•ΩÂ•ΩÁúãÔºåÂè™ËÆ∞ÂæóÊúâÊ†áËÆ∞ÈïøÂ∫¶ÔºåÂèØ‰ª•Áõ¥Êé•Âèñ„ÄÇÁÑ∂ÂêéÂ∞±Êù•‰∏§ÈÅìÁºñÁ®ãÈ¢ò„ÄÇÁ¨¨‰∏ÄÈ¢òÊòØÊ±Ç‰∏ÄÊ£µÊ†ëÊâÄÊúâÂ∑¶Âè∂Â≠êËäÇÁÇπÁöÑÂíåÔºåÊØîËæÉÁÆÄÂçïÔºå‰∏Ä‰∏™Ê∑±Â∫¶‰ºòÂÖàÂ∞±ÂèØ‰ª•ÊêûÂÆö„ÄÇÁ¨¨‰∫åÈ¢òÊòØÁªôÂÆö‰∏Ä‰∏™ÂÄºKÔºå‰∏Ä‰∏™Êï∞ÂàóÔºåÊ±ÇÊï∞Âàó‰∏≠‰∏§‰∏™ÂÄºaÂíåbÔºå‰ΩøÂæóa+b=k„ÄÇÊàëÊÉ≥Âà∞‰∫Ü‰∏Ä‰∏™‰ΩøÁî®Êï∞ÁªÑ‰∏ãÊ†áÁöÑÊñπÊ≥ïÔºàÊÑüËßâÊòØÂú®Âì™ÈáåÊúâËßÅËøáÔºå‰∏çÁÑ∂‰º∞ËÆ°ÊòØÊÉ≥‰∏çÂá∫Êù•ÔºâÔºåËøôÁßçÂèØÊòØËææÂà∞O(n)ÁöÑÂ§çÊùÇÂ∫¶Ôºõ‰ªñÂèàÂä†‰∫Ü‰∏™ÈôêÂà∂Êù°‰ª∂Ôºå‰∏çËÉΩ‰ΩøÁî®Êõ¥Â§öÂÜÖÂ≠òÔºåÊàëÊÉ≥Âà∞‰∫ÜÂø´Êéí+ÈÅçÂéÜÔºå‰ªñÈóÆÊúâÊ≤°ÊúâÊõ¥‰ºòÁöÑÔºåÂÆûÂú®ÊÉ≥‰∏çÂá∫Êù•Ôºå‰ªñÊèê‰∫Ü‰∏Ä‰∏™ÂèØ‰ª•‰∏§Á´ØÈÄºËøëÔºåÊÑüËßâÂæàÂ∑ßÂ¶ô„ÄÇ‰∫åÈù¢„ÄÇÈù¢ËØïÂÆòÈ´òÈ´òÁò¶Áò¶ÁöÑÔºåÊàëÂØπËøôÁßç‰∫∫ÁöÑÂç∞Ë±°ÈÉΩÊòØËÇØÂÆöÂæàÁâõÈÄºÔºåÂèØËÉΩÊòØÊ∫ê‰∫éÂ§ßÂ≠¶Êó∂‰ª£ÈÇ£‰∫õÂ§ßÁâõÈÉΩÈïøËøôÊ†∑„ÄÇÂÖàËÆ©ÊàëËÆ≤‰∏ãkafkaÁöÑÁªìÊûÑÔºåÁÑ∂ÂêéÊÄé‰πàÈò≤Ê≠¢ËÆ¢ÂçïÈáçÂ§çÊèê‰∫§ÔºåÁÑ∂ÂêéÂºÄÂßãÂõ¥ÁªïÁºìÂ≠òÂêåÊ≠•ÈóÆÈ¢òÂ±ïÂºÄ‰∫ÜÈïøËææÂçäÂ∞èÊó∂ÁöÑËÆ®ËÆ∫ÔºöÂÖàÂÜôÊï∞ÊçÆÂ∫ìÔºåÂÜçÂÜôÁºìÂ≠òÊúâ‰ªÄ‰πàÈóÆÈ¢òÔºüÂÖàÂÜôÁºìÂ≠òÂÜçÂÜôÊï∞ÊçÆÂ∫ìÊúâ‰ªÄ‰πàÈóÆÈ¢òÔºüÂÜôÂ∫ìÊàêÂäüÁºìÂ≠òÊõ¥Êñ∞Â§±Ë¥•ÊÄé‰πàÂäûÔºüÁºìÂ≠òÊõ¥Êñ∞ÊàêÂäüÂÜôÂ∫ìÂ§±Ë¥•ÊÄé‰πàÂäûÔºü‰ªñÂíåÊàë‰∏ÄËµ∑Âú®‰∏ÄÂº†Á∫∏‰∏äÂêÑÁßçÁîªÔºåÊÑüËßâ‰∏çÊòØÈù¢ËØïÔºåËÄåÊòØÂú®ËÆæËÆ°ÊñπÊ°à„ÄÇ‰∏âÈù¢„ÄÇËøôÊòØÂêéÁ´ØÂõ¢ÈòüË¥üË¥£‰∫∫‰∫ÜÔºåÂæàÂíåËîºÔºå‰∏ÄÁõ¥Á¨ëÂëµÂëµ„ÄÇÈóÆ‰∫ÜÊàë‰∏Ä‰∫õÂæÆÊúçÂä°ÁöÑÈóÆÈ¢òÔºåÊàëÊèêÂà∞‰∫ÜistioÔºå‰ªãÁªç‰∫ÜËÆæËÆ°ÁêÜÂøµÔºåÊÑüËßâ‰ªñÊúâÁÇπÊÑèÂ§ñ„ÄÇÁÑ∂Âêé‰ªñÈóÆjava8ÁöÑÊñ∞ÁâπÊÄßÔºåÈóÆÊàëÁü•‰∏çÁü•ÈÅìlambdaË°®ËææÂºèÊÄé‰πàÊù•ÁöÑÔºåÊàë‰ªélambdaÊºîÁÆóËØ¥Âà∞lispËØ¥Âà∞scalaÔºåÊÑüËßâ‰ªñÊõ¥ÊÑèÂ§ñ„ÄÇÊ≠§Â§ÑÊúâÁÇπÂêπÁâõ‰∫Ü„ÄÇÊàëÈóÆ‰∫Ü‰∏Ä‰∫õÂõ¢ÈòüÁöÑÈóÆÈ¢òÔºåÈ°πÁõÆÊú™Êù•ËßÑÂàíÁ≠âÔºåÊÑüËßâÊ¶õÊûúËøòÊòØÊå∫‰∏çÈîôÁöÑ„ÄÇÂõõÈù¢„ÄÇËøô‰∏™Â∫îËØ•ÊòØÊ¶õÊûúÂé¶Èó®ÁöÑË¥üË¥£‰∫∫‰∫ÜÔºåÊäÄÊúØÈóÆÈ¢òÈóÆÁöÑ‰∏çÂ§öÔºåÊõ¥Â§öÊòØ‰∏Ä‰∫õËÅå‰∏öËßÑÂàíÔºåÂØπ‰∏öÂä°ÁöÑÁúãÊ≥ïÁ≠â„ÄÇÈù¢ËØïÁªìÊùüÁöÑÊó∂ÂÄô‰ªñÂÖàÂá∫ÂéªÔºåÊàëÊî∂Êãæ‰∏ã‰∏úË•øÔºåÂá∫ÂéªÁöÑÊó∂ÂÄôÂèëÁé∞‰ªñÂú®ÁîµÊ¢ØÊóÅÂ∏ÆÊàëÂºÄÁîµÊ¢ØÔºåÂØπÂæÖÈù¢ËØïËÄÖÁöÑËøôÁßçÊÄÅÂ∫¶ÂÆûÂú®ËÆ©‰∫∫ÂæàÊúâÂ•ΩÊÑü„ÄÇÂá∫Êù•ÁöÑÊó∂ÂÄôÂ∑≤ÁªèÊòØÂÖ≠ÁÇπÂçä„ÄÇÁΩëÊòìÈù¢ÁöÑÊòØÁΩëÊòì‰∫ëÈü≥‰πêÔºåÂπ≥Êó∂ÁªèÂ∏∏Áî®ÔºåÊÑüËßâÂ¶ÇÊûúÂèØ‰ª•ÂèÇ‰∏éÁ†îÂèëÂ∫îËØ•ÊòØÁßçÊå∫ÁæéÂ¶ôÁöÑÊÑüËßâ„ÄÇ‰∏ÄÈù¢„ÄÇ‰∏ãÂçàÊâìËøáÊù•ÁöÑÔºåÈóÆÊàëÊúâÊ≤°ÊúâÁ©∫ÔºåÊàëËØ¥ÊúâÔºå‰ªñËØ¥‰Ω†‰∏çÁî®‰∏äÁè≠ÂêóÔºüÊúâÊÄÅÂ∫¶ÁúüÁöÑÂèØ‰ª•‰∏∫ÊâÄÊ¨≤‰∏∫ÔºàËã¶Á¨ëÔºâ„ÄÇÁÑ∂ÂêéÈóÆ‰∫Ü‰∏∫‰ªÄ‰πàÁ¶ªËÅåÔºåËÅä‰∫Ü‰ºöÊàø‰ª∑ÔºåÈóÆ‰∫ÜÂá†‰∏™nettyÁöÑÈóÆÈ¢òÔºågcÁöÑÈóÆÈ¢òÔºåÊúÄÂêéÈóÆ‰∏ãÂØπ‰∏öÂä°ÁöÑÁúãÊ≥ï„ÄÇÁÑ∂ÂêéÁ∫¶‰∫Ü‰∏™‰∫åÈù¢ÁöÑÊó∂Èó¥ÔºåÁªìÊûúÊó∂Èó¥Âà∞‰∫ÜÊ≤°‰∫∫ËÅîÁ≥ªÊàëÔºåÁ¨¨‰∫åÂ§©ÊâìÁîµËØùË∑üÊàëÈÅìÊ≠âÈáçÊñ∞Á∫¶‰∫ÜÊó∂Èó¥Ôºå‰∏çÂæó‰∏çËØ¥ÊÄÅÂ∫¶ËøòÊòØÂæàÂ•ΩÁöÑ„ÄÇ‰∫åÈù¢ÈóÆÁöÑÂèçËÄåÂæàÂü∫Á°ÄÔºåÊ≤°Â§™Â§öÁâπÂà´ÁöÑ„ÄÇËÆ©ÊàëÊèêÈóÆÁöÑÊó∂ÂÄôÊàëÊääÁæéÂõ¢‰∫åÈù¢ÈáåÁöÑÁºìÂ≠òÈóÆÈ¢òÊãøÂá∫Êù•ÈóÆ‰ªñÔºåÂæàËÄêÂøÉÁöÑÁªôÊàëËß£Á≠î‰∫ÜÂ•ΩÂá†ÂàÜÈíüÔºå‰∫∫ÂæàÂ•Ω„ÄÇÈòøÈáåËøô‰∏™ÂÖ∂ÂÆû‰∏çÊòØÊúÄÂêéÈù¢ËØïÁöÑÔºå‰ΩÜÊòØÊòØÊúÄÂêéÁªìÊùüÁöÑÔºå‰∏çÂæó‰∏çËØ¥ÈòøÈáå‰∫∫ÁúüÁöÑÂ•ΩÂøôÔºåÂë®‰∏âË∑üÊàëÈ¢ÑÁ∫¶Êó∂Èó¥ÔºåÁÑ∂ÂêéÂ∑≤ÁªèÊéíÂà∞‰∏ã‰∏ÄÂë®ÁöÑÂë®‰∏Ä„ÄÇÊÄª‰Ωì‰∏äÊÑüËßâÈòøÈáåÁöÑÈù¢ËØïÈ£éÊ†ºÊòØÂñúÊ¨¢Âú®Êüê‰∏™ÁÇπ‰∏ä‰∏çÊñ≠Ê∑±ÂÖ•ÔºåÁõ¥Âà∞‰Ω†ËØ¥‰∏çÁü•ÈÅì„ÄÇ‰∏ÄÈù¢„ÄÇËá™Êàë‰ªãÁªçÔºåÁÑ∂Âêé‰ªãÁªçÁé∞Âú®ÁöÑÈ°πÁõÆÊû∂ÊûÑÔºåÁ¨¨‰∏ÄÈÉ®ÂàÜÂ∞±ÊòØÊó•Âøó‰∏ä‰º†ÂíåÊé•Êî∂ÔºåÁÑ∂ÂêéÂ∞±Â¶Ç‰Ωï‰øùËØÅÊó•Âøó‰∏ä‰º†ÁöÑÂπÇÁ≠âÊÄßÂºÄÂßã‰∏çÊñ≠Ê∑±ÂÖ•ÔºåÂÖàËÆ©ÊàëËÆæËÆ°‰∏Ä‰∏™ÊñπÊ°àÔºåÁÑ∂ÂêéÈóÆÊúâÊ≤°Êúâ‰ªÄ‰πàÊîπËøõÁöÑÔºåÁÑ∂ÂêéÂ¶Ç‰ΩïÂú®‰øùËØÅÂπÇÁ≠âÁöÑÂâçÊèê‰∏ãÊèêÈ´òÊÄßËÉΩÔºå‰∏≠Èó¥Á©øÊèíÂàÜÂ∏ÉÂºèÈîÅ„ÄÅredis„ÄÅmq„ÄÅÊï∞ÊçÆÂ∫ìÈîÅÁ≠âÂêÑÁßçÈóÆÈ¢ò„ÄÇËøô‰∏™ÈóÆÈ¢òËÆ®ËÆ∫‰∫ÜÂ∑Æ‰∏çÂ§öÂçäÂ∞èÊó∂„ÄÇÁÑ∂ÂêéÂ∞±ÈóÆÊàëÊúâÊ≤°Êúâ‰ªÄ‰πàË¶Å‰∫ÜËß£ÁöÑÔºåËä±‰∫ÜÂçÅÂá†ÂàÜÈíü‰ªãÁªç‰ªñ‰ª¨Áé∞Âú®ÂÅöÁöÑ‰∫ãÊÉÖ„ÄÅÊäÄÊúØÊ†à„ÄÅÊú™Êù•ÁöÑ‰∏Ä‰∫õËÆ°ÂàíÔºåÈùûÂ∏∏ËÄêÂøÉ„ÄÇ‰∫åÈù¢„ÄÇ‰πüÊòØ‰ªé‰ªãÁªçÈ°πÁõÆÂºÄÂßãÔºåÁÑ∂ÂêéÊäì‰Ωè‰∏Ä‰∏™ÁÇπÔºåÁªìÂêàÁßíÊùÄÁöÑÂú∫ÊôØÊ∑±ÂÖ•ÔºåÂ¶Ç‰ΩïÂÆûÁé∞ÂàÜÂ∏ÉÂºèÈîÅ„ÄÅÂ¶Ç‰Ωï‰øùËØÅÂπÇÁ≠âÊÄß„ÄÅÂàÜÂ∏ÉÂºè‰∫ãÂä°ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÈóÆÊàëÂàÜÂ∏ÉÂºèÈîÅÁöÑÁº∫ÁÇπÔºåÊàëËØ¥ÊÄßËÉΩ‰ºöÂá∫Áé∞Áì∂È¢àÔºå‰ªñÈóÆÊÄé‰πàËß£ÂÜ≥ÔºåÊàëÊÉ≥‰∫ÜÊØîËæÉ‰πÖÔºå‰ªñÊèêÁ§∫ËØ¥ÂèëÊï£‰∏ãÊÄùÁª¥ÔºåÊàëÊúÄÂêéÊÉ≥‰∫Ü‰∏™ÁÆÄÂçïÁöÑÊñπÊ°àÔºåÁõ¥Êé•‰∏ç‰ΩøÁî®ÂàÜÂ∏ÉÂºèÈîÅÔºå‰ªñÂ•ΩÂÉèÊå∫Êª°ÊÑè„ÄÇÊÑüËßâ‰ªñ‰ª¨Êõ¥ÁúãÈáçÊÄùËÄÉÁöÑËøáÁ®ãÔºåËÄå‰∏çÊòØÂÖ∑‰ΩìÊñπÊ°à„ÄÇËøòÈóÆ‰∫Ü‰∏ÄËá¥ÊÄßhashÂ¶Ç‰Ωï‰øùËØÅË¥üËΩΩÂùáË°°ÔºåkafkaÂíårocketmqÂêÑËá™ÁöÑ‰ºòÁº∫ÁÇπÔºådubboÁöÑ‰∏Ä‰∏™ËØ∑Ê±ÇËøáÁ®ã„ÄÅÂ∫èÂàóÂåñÊñπÂºèÔºåÂ∫èÂàóÂåñÊ°ÜÊû∂„ÄÅPBÁöÑÁº∫ÁÇπ„ÄÅÂ¶Ç‰Ωï‰ªéÊï∞ÊçÆÂ∫ìÂ§ßÊâπÈáèÂØºÂÖ•Êï∞ÊçÆÂà∞hbase„ÄÇ‰∏âÈù¢„ÄÇÊòØHRÂíå‰∏ªÁÆ°ÁöÑËÅîÂêàËßÜÈ¢ëÈù¢ËØï„ÄÇËøôÁßçÈù¢ËØïËøòÁ¨¨‰∏ÄÊ¨°ÈÅáÂà∞ÔºåÊúâÁÇπÁ¥ßÂº†„ÄÇ‰∏ªÁÆ°ÂÖàÈù¢Ôºå‰πüÊòØËÆ©ÊàëÂÖà‰ªãÁªçÈ°πÁõÆÔºåÈóÆÊàëÊúâÊ≤°ÊúâÁî®ËøámqÔºåÂ¶Ç‰Ωï‰øùËØÅÊ∂àÊÅØÂπÇÁ≠âÊÄß„ÄÇÊàëÂ∞±Êääkafka0.11ÁâàÊú¨ÁöÑÂπÇÁ≠âÊÄßÊñπÊ°àËØ¥‰∫Ü‰∏ãÔºåÂ∞±Ê≤°ÂÜçÈóÆÊäÄÊúØÈóÆÈ¢ò‰∫Ü„ÄÇÂêéÈù¢ÂèàÈóÆ‰∫Ü‰∏∫Âï•Á¶ªËÅåÔºåÂØπ‰∏öÂä°ÁöÑÁúãÊ≥ï‰πãÁ±ªÁöÑ„ÄÇÁÑ∂ÂêéÂ∞±‰∫§ÁªôHRÔºåÂè™ÈóÆ‰∫ÜÂá†‰∏™ÈóÆÈ¢òÔºåÁÑ∂ÂêéÂ∞±ÁªìÊùü‰∫ÜÔºåÂÖ®Á®ã‰∏çÂà∞ÂçäÂ∞èÊó∂„ÄÇ‰∏çÊáÇÊòØ‰∏çÊòØË∑üÈù¢ËØïÁöÑÈÉ®Èó®ÊúâÂÖ≥ÔºåÈòøÈáåÂØπÂπÇÁ≠âÊÄßËøô‰∏™ÈóÆÈ¢òÂæàÊâßÁùÄÔºå‰∏âÊ¨°ÈÉΩÈóÆÂà∞ÔºåËÄå‰∏îËøòÊòØ‰ªé‰∏çÂêåËßíÂ∫¶„ÄÇÊÄªÁªì‰ªéÈù¢ËØïÁöÑÈöæÊòìÁ®ãÂ∫¶ÁúãÈòøÈáå &gt; ÁæéÂõ¢ &gt; Â§¥Êù° &gt; ÊúâËµû &gt; ÁΩëÊòì &gt; ÊõπÊìç‰∏ìËΩ¶ &gt; Áæé‰∫öÊüèÁßë„ÄÇÊï¥‰∏™ËøáÁ®ãÁöÑ‰Ωì‰ºöÊòØÂü∫Á°ÄÁúüÁöÑÂæàÈáçË¶ÅÔºåÂü∫Á°ÄÂ•Ω‰∫ÜÂæàÂ§öÈóÆÈ¢òÂç≥‰ΩøÊ≤°ÈÅáÂà∞Ëøá‰πüÂèØ‰ª•‰∏æ‰∏ÄÂèç‰∏â„ÄÇ Âè¶Â§ñÂØπ‰∏ÄÊ†∑ÊäÄÊúØ‰∏ÄÂÆöË¶ÅÊáÇÂéüÁêÜÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊÄé‰πà‰ΩøÁî®ÔºåÂ∞§ÂÖ∂ÊòØÁº∫ÁÇπÔºåÂØπÈÄâÂûãÂæàÂÖ≥ÈîÆÔºåÂèØ‰ª•ÂæàÂ•ΩÁöÑÁî®Êù•ÂõûÁ≠î‰∏∫‰ªÄ‰πà‰∏çÈÄâxxx„ÄÇÂè¶Â§ñÂØπ‰∏Ä‰∫õÊØîËæÉÊñ∞ÁöÑÊäÄÊúØÊúâÊâÄ‰∫ÜËß£‰πüÊòØ‰∏Ä‰∏™Âä†ÂàÜÈ°π„ÄÇ]]></content>
      <categories>
        <category>Èù¢ËØïÈ¢ò</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive‰∏≠Ëá™ÂÆö‰πâUDAFÂáΩÊï∞Áîü‰∫ßÂ∞èÊ°à‰æã]]></title>
    <url>%2F2018%2F05%2F23%2FHive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[‰∏Ä„ÄÅUDAF ÂõûÈ°æ1.ÂÆö‰πâÔºöUDAF(User Defined Aggregation Funcation ) Áî®Êà∑Ëá™ÂÆö‰πâËÅöÁ±ªÊñπÊ≥ïÔºåÂíågroup byËÅîÂêà‰ΩøÁî®ÔºåÊé•ÂèóÂ§ö‰∏™ËæìÂÖ•Êï∞ÊçÆË°åÔºåÂπ∂‰∫ßÁîü‰∏Ä‰∏™ËæìÂá∫Êï∞ÊçÆË°å„ÄÇ2.HiveÊúâ‰∏§ÁßçUDAFÔºöÁÆÄÂçïÂíåÈÄöÁî®ÁÆÄÂçïÔºöÂà©Áî®ÊäΩË±°Á±ªUDAFÂíåUDAFEvaluatorÔºå‰ΩøÁî®JavaÂèçÂ∞ÑÂØºËá¥ÊÄßËÉΩÊçüÂ§±Ôºå‰∏îÊúâ‰∫õÁâπÊÄß‰∏çËÉΩ‰ΩøÁî®ÔºåÂ¶ÇÂèØÂèòÈïøÂ∫¶ÂèÇÊï∞ÂàóË°® „ÄÇÈÄöÁî®ÔºöÂà©Áî®Êé•Âè£GenericUDAFResolver2ÔºàÊàñÊäΩË±°Á±ªAbstractGenericUDAFResolverÔºâÂíåÊäΩË±°Á±ªGenericUDAFEvaluatorÔºåÂèØ‰ª•‰ΩøÁî®ÊâÄÊúâÂäüËÉΩÔºå‰ΩÜÊØîËæÉÂ§çÊùÇÔºå‰∏çÁõ¥ËßÇ„ÄÇ3.‰∏Ä‰∏™ËÆ°ÁÆóÂáΩÊï∞ÂøÖÈ°ªÂÆûÁé∞ÁöÑ5‰∏™ÊñπÊ≥ïÁöÑÂÖ∑‰ΩìÂê´‰πâÂ¶Ç‰∏ãÔºöinit()Ôºö‰∏ªË¶ÅÊòØË¥üË¥£ÂàùÂßãÂåñËÆ°ÁÆóÂáΩÊï∞Âπ∂‰∏îÈáçËÆæÂÖ∂ÂÜÖÈÉ®Áä∂ÊÄÅÔºå‰∏ÄËà¨Â∞±ÊòØÈáçËÆæÂÖ∂ÂÜÖÈÉ®Â≠óÊÆµ„ÄÇ‰∏ÄËà¨Âú®ÈùôÊÄÅÁ±ª‰∏≠ÂÆö‰πâ‰∏Ä‰∏™ÂÜÖÈÉ®Â≠óÊÆµÊù•Â≠òÊîæÊúÄÁªàÁöÑÁªìÊûú„ÄÇiterate()ÔºöÊØè‰∏ÄÊ¨°ÂØπ‰∏Ä‰∏™Êñ∞ÂÄºËøõË°åËÅöÈõÜËÆ°ÁÆóÊó∂ÂÄôÈÉΩ‰ºöË∞ÉÁî®ËØ•ÊñπÊ≥ïÔºåËÆ°ÁÆóÂáΩÊï∞‰ºöÊ†πÊçÆËÅöÈõÜËÆ°ÁÆóÁªìÊûúÊõ¥Êñ∞ÂÜÖÈÉ®Áä∂ÊÄÅ„ÄÇÂΩìËæì ÂÖ•ÂÄºÂêàÊ≥ïÊàñËÄÖÊ≠£Á°ÆËÆ°ÁÆó‰∫ÜÔºåÂàôÂ∞±ËøîÂõûtrue„ÄÇterminatePartial()ÔºöHiveÈúÄË¶ÅÈÉ®ÂàÜËÅöÈõÜÁªìÊûúÁöÑÊó∂ÂÄô‰ºöË∞ÉÁî®ËØ•ÊñπÊ≥ïÔºåÂøÖÈ°ªË¶ÅËøîÂõû‰∏Ä‰∏™Â∞ÅË£Ö‰∫ÜËÅöÈõÜËÆ°ÁÆóÂΩìÂâçÁä∂ÊÄÅÁöÑÂØπË±°„ÄÇmerge()ÔºöHiveËøõË°åÂêàÂπ∂‰∏Ä‰∏™ÈÉ®ÂàÜËÅöÈõÜÂíåÂè¶‰∏Ä‰∏™ÈÉ®ÂàÜËÅöÈõÜÁöÑÊó∂ÂÄô‰ºöË∞ÉÁî®ËØ•ÊñπÊ≥ï„ÄÇterminate()ÔºöHiveÊúÄÁªàËÅöÈõÜÁªìÊûúÁöÑÊó∂ÂÄôÂ∞±‰ºöË∞ÉÁî®ËØ•ÊñπÊ≥ï„ÄÇËÆ°ÁÆóÂáΩÊï∞ÈúÄË¶ÅÊääÁä∂ÊÄÅ‰Ωú‰∏∫‰∏Ä‰∏™ÂÄºËøîÂõûÁªôÁî®Êà∑„ÄÇ‰∫å„ÄÅÈúÄÊ±Ç‰ΩøÁî®UDAFÁÆÄÂçïÊñπÂºèÂÆûÁé∞ÁªüËÆ°Âå∫Âüü‰∫ßÂìÅÁî®Êà∑ËÆøÈóÆÊéíÂêç‰∏â„ÄÅËá™ÂÆö‰πâUDAFÂáΩÊï∞‰ª£Á†ÅÂÆûÁé∞12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hive.org.ruozedata;import java.util.*;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import org.apache.log4j.Logger;public class UserClickUDAF extends UDAF &#123; // Êó•ÂøóÂØπË±°ÂàùÂßãÂåñ public static Logger logger = Logger.getLogger(UserClickUDAF.class); // ÈùôÊÄÅÁ±ªÂÆûÁé∞UDAFEvaluator public static class Evaluator implements UDAFEvaluator &#123; // ËÆæÁΩÆÊàêÂëòÂèòÈáèÔºåÂ≠òÂÇ®ÊØè‰∏™ÁªüËÆ°ËåÉÂõ¥ÂÜÖÁöÑÊÄªËÆ∞ÂΩïÊï∞ private static Map&lt;String, String&gt; courseScoreMap; private static Map&lt;String, String&gt; city_info; private static Map&lt;String, String&gt; product_info; private static Map&lt;String, String&gt; user_click; //ÂàùÂßãÂåñÂáΩÊï∞,mapÂíåreduceÂùá‰ºöÊâßË°åËØ•ÂáΩÊï∞,Ëµ∑Âà∞ÂàùÂßãÂåñÊâÄÈúÄË¶ÅÁöÑÂèòÈáèÁöÑ‰ΩúÁî® public Evaluator() &#123; init(); &#125; // ÂàùÂßãÂåñÂáΩÊï∞Èó¥‰º†ÈÄíÁöÑ‰∏≠Èó¥ÂèòÈáè public void init() &#123; courseScoreMap = new HashMap&lt;String, String&gt;(); city_info = new HashMap&lt;String, String&gt;(); product_info = new HashMap&lt;String, String&gt;(); &#125; //mapÈò∂ÊÆµÔºåËøîÂõûÂÄº‰∏∫booleanÁ±ªÂûãÔºåÂΩì‰∏∫trueÂàôÁ®ãÂ∫èÁªßÁª≠ÊâßË°åÔºåÂΩì‰∏∫falseÂàôÁ®ãÂ∫èÈÄÄÂá∫ public boolean iterate(String pcid, String pcname, String pccount) &#123; if (pcid == null || pcname == null || pccount == null) &#123; return true; &#125; if (pccount.equals(&quot;-1&quot;)) &#123; // ÂüéÂ∏ÇË°® city_info.put(pcid, pcname); &#125; else if (pccount.equals(&quot;-2&quot;)) &#123; // ‰∫ßÂìÅË°® product_info.put(pcid, pcname); &#125; else &#123; // Â§ÑÁêÜÁî®Êà∑ÁÇπÂáªÂÖ≥ËÅî unionCity_Prod_UserClic1(pcid, pcname, pccount); &#125; return true; &#125; // Â§ÑÁêÜÁî®Êà∑ÁÇπÂáªÂÖ≥ËÅî private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123; if (product_info.containsKey(pcid)) &#123; if (city_info.containsKey(pcname)) &#123; String city_name = city_info.get(pcname); String prod_name = product_info.get(pcid); String cp_name = city_name + prod_name; // Â¶ÇÊûú‰πãÂâçÂ∑≤ÁªèPutËøáKeyÂÄº‰∏∫Âå∫Âüü‰ø°ÊÅØÔºåÂàôÊääËÆ∞ÂΩïÁõ∏Âä†Â§ÑÁêÜ if (courseScoreMap.containsKey(cp_name)) &#123; int pcrn = 0; String strTemp = courseScoreMap.get(cp_name); String courseScoreMap_pn = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim(); pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn); courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn)); &#125; else &#123; courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount); &#125; &#125; &#125; &#125; /** * Á±ª‰ºº‰∫écombiner,Âú®mapËåÉÂõ¥ÂÜÖÂÅöÈÉ®ÂàÜËÅöÂêàÔºåÂ∞ÜÁªìÊûú‰º†ÁªômergeÂáΩÊï∞‰∏≠ÁöÑÂΩ¢ÂèÇmapOutput * Â¶ÇÊûúÈúÄË¶ÅËÅöÂêàÔºåÂàôÂØπiteratorËøîÂõûÁöÑÁªìÊûúÂ§ÑÁêÜÔºåÂê¶ÂàôÁõ¥Êé•ËøîÂõûiteratorÁöÑÁªìÊûúÂç≥ÂèØ */ public Map&lt;String, String&gt; terminatePartial() &#123; return courseScoreMap; &#125; // reduce Èò∂ÊÆµÔºåÁî®‰∫éÈÄê‰∏™Ëø≠‰ª£Â§ÑÁêÜmapÂΩì‰∏≠ÊØè‰∏™‰∏çÂêåkeyÂØπÂ∫îÁöÑ terminatePartialÁöÑÁªìÊûú public boolean merge(Map&lt;String, String&gt; mapOutput) &#123; this.courseScoreMap.putAll(mapOutput); return true; &#125; // Â§ÑÁêÜmergeËÆ°ÁÆóÂÆåÊàêÂêéÁöÑÁªìÊûúÔºåÂç≥ÂØπmergeÂÆåÊàêÂêéÁöÑÁªìÊûúÂÅöÊúÄÂêéÁöÑ‰∏öÂä°Â§ÑÁêÜ public String terminate() &#123; return courseScoreMap.toString(); &#125; &#125;&#125;Âõõ„ÄÅÂàõÂª∫hive‰∏≠ÁöÑ‰∏¥Êó∂ÂáΩÊï∞123DROP TEMPORARY FUNCTION user_click;add jar /data/hive_udf-1.0.jar;CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;‰∫î„ÄÅË∞ÉÁî®Ëá™ÂÆö‰πâUDAFÂáΩÊï∞Â§ÑÁêÜÊï∞ÊçÆ1234567891011121314151617insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from ( select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from ( select * from ( select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info union all select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info union all select count(1) as type, product_id as pcid, city_id as pcname from user_click where action_time=&apos;2016-05-05&apos; group by product_id,city_id ) a order by type) b) c ;ÂÖ≠„ÄÅÂàõÂª∫Hive‰∏¥Êó∂Â§ñÈÉ®Ë°®1234567create external table tmp1(city_name string,product_name string,rn string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;location &apos;/works/tmp1&apos;;‰∏É„ÄÅÁªüËÆ°ÊúÄÁªàÂå∫ÂüüÂâç3‰∫ßÂìÅÊéíÂêç123456789select * from (select city_name, product_name, floor(sum(rn)) visit_num, row_number()over(partition by city_name order by sum(rn) desc) rn, &apos;2016-05-05&apos; action_time from tmp1 group by city_name,product_name) a where rn &lt;=3 ;ÂÖ´„ÄÅÊúÄÁªàÁªìÊûú]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark History Server Web UIÈÖçÁΩÆ]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.ËøõÂÖ•sparkÁõÆÂΩïÂíåÈÖçÁΩÆÊñá‰ª∂12[root@hadoop000 ~]# cd /opt/app/spark/conf[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf2.ÂàõÂª∫spark-historyÁöÑÂ≠òÂÇ®Êó•ÂøóË∑ØÂæÑ‰∏∫hdfs‰∏ä(ÂΩìÁÑ∂‰πüÂèØ‰ª•Âú®linuxÊñá‰ª∂Á≥ªÁªü‰∏ä)12345678[root@hadoop000 conf]# hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - root root 0 2017-02-14 12:43 /sparkdrwxrwx--- - root root 0 2017-02-14 12:58 /tmpdrwxr-xr-x - root root 0 2017-02-14 12:58 /userYou have new mail in /var/spool/mail/root[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 itemsdrwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylogÂú®HDFS‰∏≠ÂàõÂª∫‰∏Ä‰∏™ÁõÆÂΩïÔºåÁî®‰∫é‰øùÂ≠òSparkËøêË°åÊó•Âøó‰ø°ÊÅØ„ÄÇSpark History Server‰ªéÊ≠§ÁõÆÂΩï‰∏≠ËØªÂèñÊó•Âøó‰ø°ÊÅØ3.ÈÖçÁΩÆ12345[root@hadoop000 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080spark.eventLog.dir‰øùÂ≠òÊó•ÂøóÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑË∑ØÂæÑÔºåÂèØ‰ª•ÊòØhdfs://ÂºÄÂ§¥ÁöÑHDFSË∑ØÂæÑÔºå‰πüÂèØ‰ª•ÊòØfile://ÂºÄÂ§¥ÁöÑÊú¨Âú∞Ë∑ØÂæÑÔºåÈÉΩÈúÄË¶ÅÊèêÂâçÂàõÂª∫spark.yarn.historyServer.address : Spark history serverÁöÑÂú∞ÂùÄ(‰∏çÂä†http://).Ëøô‰∏™Âú∞ÂùÄ‰ºöÂú®SparkÂ∫îÁî®Á®ãÂ∫èÂÆåÊàêÂêéÊèê‰∫§ÁªôYARN RMÔºåÁÑ∂ÂêéÂèØ‰ª•Âú®RM UI‰∏äÁÇπÂáªÈìæÊé•Ë∑≥ËΩ¨Âà∞history server UI‰∏ä.4.Ê∑ªÂä†SPARK_HISTORY_OPTSÂèÇÊï∞12345[root@hadoop01 conf]# vi spark-env.sh#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.ÂêØÂä®ÊúçÂä°ÂíåÊü•Áúã123456[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01 ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer[root@hadoop01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@hadoop01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/java‰ª•‰∏äÈÖçÁΩÆÊòØÈíàÂØπ‰ΩøÁî®Ëá™Â∑±ÁºñËØëÁöÑSparkÈÉ®ÁΩ≤Âà∞ÈõÜÁæ§‰∏≠‰∏ÄÂà∞‰∏§Âè∞Êú∫Âô®‰∏ä‰Ωú‰∏∫Êèê‰∫§‰Ωú‰∏öÂÆ¢Êà∑Á´ØÁöÑÔºåÂ¶ÇÊûú‰Ω†ÊòØCDHÈõÜÁæ§‰∏≠ÈõÜÊàêÁöÑSparkÈÇ£‰πàÂèØ‰ª•Âú®ÁÆ°ÁêÜÁïåÈù¢Áõ¥Êé•Êü•ÁúãÔºÅ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Âü∫Êú¨Ê¶ÇÂøµ]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Âü∫‰∫é Spark ÊûÑÂª∫ÁöÑÁî®Êà∑Á®ãÂ∫èÔºåÂåÖÂê´‰∫Ü ‰∏Ä‰∏™driver Á®ãÂ∫èÂíåÈõÜÁæ§‰∏äÁöÑ executorsÔºõÔºàËµ∑‰∫Ü‰∏Ä‰∏™‰Ωú‰∏öÔºåÂ∞±ÊòØ‰∏Ä‰∏™ApplicationÔºâsparkÂêçËØçËß£ÈáäApplication jarÔºöÂ∫îÁî®Á®ãÂ∫èjarÂåÖÂåÖÂê´‰∫ÜÁî®Êà∑ÁöÑ Spark Á®ãÂ∫èÁöÑ‰∏Ä‰∏™ jar ÂåÖ. Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁî®Êà∑ÂèØËÉΩÊÉ≥Ë¶ÅÂàõÂª∫‰∏Ä‰∏™ÂõäÊã¨‰∫ÜÂ∫îÁî®ÂèäÂÖ∂‰æùËµñÁöÑ ‚ÄúËÉñ‚Äù jar ÂåÖ. ‰ΩÜÂÆûÈôÖ‰∏ä, Áî®Êà∑ÁöÑ jar ‰∏çÂ∫îËØ•ÂåÖÊã¨ Hadoop ÊàñÊòØ Spark ÁöÑÂ∫ì, Ëøô‰∫õÂ∫ì‰ºöÂú®ËøêË°åÊó∂Ë¢´ËøõË°åÂä†ËΩΩÔºõDriver ProgramÔºöËøô‰∏™ËøõÁ®ãËøêË°åÂ∫îÁî®Á®ãÂ∫èÁöÑ main ÊñπÊ≥ïÂπ∂‰∏îÊñ∞Âª∫ SparkContext ÔºõCluster ManagerÔºöÈõÜÁæ§ÁÆ°ÁêÜËÄÖÂú®ÈõÜÁæ§‰∏äËé∑ÂèñËµÑÊ∫êÁöÑÂ§ñÈÉ®ÊúçÂä° (‰æãÂ¶Ç:standalone,Mesos,Yarn)ÔºõÔºà‚ÄìmasterÔºâDeploy modeÔºöÈÉ®ÁΩ≤Ê®°ÂºèÂëäËØâ‰Ω†Âú®Âì™ÈáåÂêØÂä®driver program. Âú® ‚Äúcluster‚Äù Ê®°Âºè‰∏ã, Ê°ÜÊû∂Âú®ÈõÜÁæ§ÂÜÖÈÉ®ËøêË°å driver. Âú® ‚Äúclient‚Äù Ê®°Âºè‰∏ã, Êèê‰∫§ËÄÖÂú®ÈõÜÁæ§Â§ñÈÉ®ËøêË°å driver.ÔºõWorker NodeÔºöÂ∑•‰ΩúËäÇÁÇπÈõÜÁæ§‰∏≠‰ªª‰ΩïÂèØ‰ª•ËøêË°åÂ∫îÁî®‰ª£Á†ÅÁöÑËäÇÁÇπÔºõÔºàyarn‰∏äÂ∞±ÊòØnode managerÔºâExecutorÔºöÂú®‰∏Ä‰∏™Â∑•‰ΩúËäÇÁÇπ‰∏ä‰∏∫ÊüêÂ∫îÁî®ÂêØÂä®ÁöÑ‰∏Ä‰∏™ËøõÁ®ãÔºåËØ•ËøõÁ®ãË¥üË¥£ËøêË°å‰ªªÂä°ÔºåÂπ∂‰∏îË¥üË¥£Â∞ÜÊï∞ÊçÆÂ≠òÂú®ÂÜÖÂ≠òÊàñËÄÖÁ£ÅÁõò‰∏ä„ÄÇÊØè‰∏™Â∫îÁî®ÈÉΩÊúâÂêÑËá™Áã¨Á´ãÁöÑ executorsÔºõTaskÔºö‰ªªÂä°Ë¢´ÈÄÅÂà∞Êüê‰∏™ executor ‰∏äÊâßË°åÁöÑÂ∑•‰ΩúÂçïÂÖÉÔºõJobÔºöÂåÖÂê´ÂæàÂ§öÂπ∂Ë°åËÆ°ÁÆóÁöÑtask„ÄÇ‰∏Ä‰∏™ action Â∞±‰ºö‰∫ßÁîü‰∏Ä‰∏™jobÔºõStageÔºö‰∏Ä‰∏™ Job ‰ºöË¢´ÊãÜÂàÜÊàêÂ§ö‰∏™taskÁöÑÈõÜÂêàÔºåÊØè‰∏™taskÈõÜÂêàË¢´Áß∞‰∏∫ stageÔºåstage‰πãÈó¥ÊòØÁõ∏‰∫í‰æùËµñÁöÑ(Â∞±ÂÉè Mapreduce ÂàÜ mapÂíå reduce stages‰∏ÄÊ†∑)ÔºåÂèØ‰ª•Âú®Driver ÁöÑÊó•Âøó‰∏äÁúãÂà∞„ÄÇsparkÂ∑•‰ΩúÊµÅÁ®ã1‰∏™action‰ºöËß¶Âèë1‰∏™jobÔºå1‰∏™jobÂåÖÂê´n‰∏™stageÔºåÊØè‰∏™stageÂåÖÂê´n‰∏™taskÔºån‰∏™task‰ºöÈÄÅÂà∞n‰∏™executor‰∏äÊâßË°åÔºå‰∏Ä‰∏™ApplicationÊòØÁî±‰∏Ä‰∏™driver Á®ãÂ∫èÂíån‰∏™ executorÁªÑÊàê„ÄÇÊèê‰∫§ÁöÑÊó∂ÂÄôÔºåÈÄöËøáCluster ManagerÂíåDeploy modeÊéßÂà∂„ÄÇsparkÂ∫îÁî®Á®ãÂ∫èÂú®ÈõÜÁæ§‰∏äËøêË°å‰∏ÄÁªÑÁã¨Á´ãÁöÑËøõÁ®ãÔºåÈÄöËøáSparkContextÂçèË∞ÉÁöÑÂú®mainÊñπÊ≥ïÈáåÈù¢„ÄÇÂ¶ÇÊûúËøêË°åÂú®‰∏Ä‰∏™ÈõÜÁæ§‰πã‰∏äÔºåSparkContextËÉΩÂ§üËøûÊé•ÂêÑÁßçÁöÑÈõÜÁæ§ÁÆ°ÁêÜËÄÖÔºåÂéªËé∑ÂèñÂà∞‰Ωú‰∏öÊâÄÈúÄË¶ÅÁöÑËµÑÊ∫ê„ÄÇ‰∏ÄÊó¶ËøûÊé•ÊàêÂäüÔºåsparkÂú®ÈõÜÁæ§ËäÇÁÇπ‰πã‰∏äËøêË°åexecutorËøõÁ®ãÔºåÊù•Áªô‰Ω†ÁöÑÂ∫îÁî®Á®ãÂ∫èËøêË°åËÆ°ÁÆóÂíåÂ≠òÂÇ®Êï∞ÊçÆ„ÄÇÂÆÉ‰ºöÂèëÈÄÅ‰Ω†ÁöÑÂ∫îÁî®Á®ãÂ∫è‰ª£Á†ÅÂà∞executors‰∏ä„ÄÇÊúÄÂêéÔºåSparkContextÂèëÈÄÅtasksÂà∞executors‰∏äÂéªËøêË°å1„ÄÅÊØè‰∏™ApplicationÈÉΩÊúâËá™Â∑±Áã¨Á´ãÁöÑexecutorËøõÁ®ãÔºåËøô‰∫õËøõÁ®ãÂú®ËøêË°åÂë®ÊúüÂÜÖÈÉΩÊòØÂ∏∏È©ªÁöÑ‰ª•Â§öÁ∫øÁ®ãÁöÑÊñπÂºèËøêË°åtasks„ÄÇÂ•ΩÂ§ÑÊòØÊØè‰∏™ËøõÁ®ãÊó†ËÆ∫ÊòØÂú®Ë∞ÉÂ∫¶ËøòÊòØÊâßË°åÈÉΩÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑ„ÄÇÊâÄ‰ª•ÔºåËøôÂ∞±ÊÑèÂë≥ÁùÄÊï∞ÊçÆ‰∏çËÉΩË∑®Â∫îÁî®Á®ãÂ∫èËøõË°åÂÖ±‰∫´ÔºåÈô§ÈùûÂÜôÂà∞Â§ñÈÉ®Â≠òÂÇ®Á≥ªÁªüÔºàAlluxioÔºâ„ÄÇ2„ÄÅsparkÂπ∂‰∏çÂÖ≥ÂøÉÂ∫ïÂ±ÇÁöÑÈõÜÁæ§ÁÆ°ÁêÜ„ÄÇ3„ÄÅdriver Á®ãÂ∫è‰ºöÁõëÂê¨Âπ∂‰∏îÊé•Êî∂Â§ñÈù¢ÁöÑ‰∏Ä‰∫õexecutorËØ∑Ê±ÇÔºåÂú®Êï¥‰∏™ÁîüÂëΩÂë®ÊúüÈáåÈù¢„ÄÇÊâÄ‰ª•Ôºådriver Á®ãÂ∫èÂ∫îËØ•ËÉΩË¢´Worker NodeÈÄöËøáÁΩëÁªúËÆøÈóÆ„ÄÇ4„ÄÅÂõ†‰∏∫driver Âú®ÈõÜÁæ§‰∏äË∞ÉÂ∫¶TasksÔºådriver Â∞±Â∫îËØ•Èù†ËøëWorker Node„ÄÇ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÁæéÂë≥‰∏çÁî®Á≠âÂ§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò(201804Êúà)]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)%2F</url>
    <content type="text"><![CDATA[1.Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÁ∫ø‰∏ãÁè≠ÔºåÊüêÊüêÊüêÁöÑÂ∞è‰ºô‰º¥Áé∞Âú∫Èù¢ËØïÈ¢òÊà™Âõæ:2.ÂàÜ‰∫´Âè¶Â§ñ1ÂÆ∂ÁöÑÂøòËÆ∞ÂêçÂ≠óÂÖ¨Âè∏ÁöÑÂ§ßÊï∞ÊçÆÈù¢ËØïÈ¢òÔºö]]></content>
      <categories>
        <category>Èù¢ËØïÈ¢ò</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆÈù¢ËØïÈ¢ò</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark‰∏çÂæó‰∏çÁêÜËß£ÁöÑÈáçË¶ÅÊ¶ÇÂøµ‚Äî‚Äî‰ªéÊ∫êÁ†ÅËßíÂ∫¶ÁúãRDD]]></title>
    <url>%2F2018%2F05%2F20%2FSpark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD%2F</url>
    <content type="text"><![CDATA[1.RDDÊòØ‰ªÄ‰πàResilient Distributed DatasetÔºàÂºπÊÄßÂàÜÂ∏ÉÂºèÊï∞ÊçÆÈõÜÔºâÔºåÊòØ‰∏Ä‰∏™ËÉΩÂ§üÂπ∂Ë°åÊìç‰Ωú‰∏çÂèØÂèòÁöÑÂàÜÂå∫ÂÖÉÁ¥†ÁöÑÈõÜÂêà2.RDD‰∫îÂ§ßÁâπÊÄßA list of partitionsÊØè‰∏™rddÊúâÂ§ö‰∏™ÂàÜÂå∫protected def getPartitions: Array[Partition]A function for computing each splitËÆ°ÁÆó‰ΩúÁî®Âà∞ÊØè‰∏™ÂàÜÂå∫def compute(split: Partition, context: TaskContext): Iterator[T]A list of dependencies on other RDDsrdd‰πãÈó¥Â≠òÂú®‰æùËµñÔºàRDDÁöÑË°ÄÁºòÂÖ≥Á≥ªÔºâÂ¶ÇÔºöRDDA=&gt;RDDB=&gt;RDDC=&gt;RDDDprotected def getDependencies: Seq[Dependency[_]] = depsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)ÂèØÈÄâÔºåÈªòËÆ§ÂìàÂ∏åÁöÑÂàÜÂå∫@transient val partitioner: Option[Partitioner] = NoneOptionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)ËÆ°ÁÆóÊØè‰∏™ÂàÜÂå∫ÁöÑÊúÄ‰ºòÊâßË°å‰ΩçÁΩÆÔºåÂ∞ΩÈáèÂÆûÁé∞Êï∞ÊçÆÊú¨Âú∞ÂåñÔºåÂáèÂ∞ëIOÔºàËøôÂæÄÂæÄÊòØÁêÜÊÉ≥Áä∂ÊÄÅÔºâprotected def getPreferredLocations(split: Partition): Seq[String] = NilÊ∫êÁ†ÅÊù•Ëá™github„ÄÇ3.Â¶Ç‰ΩïÂàõÂª∫RDDÂàõÂª∫RDDÊúâ‰∏§ÁßçÊñπÂºè parallelize() Âíåtextfile()ÔºåÂÖ∂‰∏≠parallelizeÂèØÊé•Êî∂ÈõÜÂêàÁ±ªÔºå‰∏ªË¶Å‰Ωú‰∏∫ÊµãËØïÁî®„ÄÇtextfileÂèØËØªÂèñÊñá‰ª∂Á≥ªÁªüÔºåÊòØÂ∏∏Áî®ÁöÑ‰∏ÄÁßçÊñπÂºè1234567891011121314151617parallelize() def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) &#125;textfileÔºàÔºâ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;Ê∫êÁ†ÅÊÄªÁªìÔºö1Ôºâ.Âèñ_2ÊòØÂõ†‰∏∫Êï∞ÊçÆ‰∏∫ÔºàkeyÔºàÂÅèÁßªÈáèÔºâÔºåvalueÔºàÊï∞ÊçÆÔºâÔºâ4.Â∏∏ËßÅÁöÑtransformationÂíåactionÁî±‰∫éÊØîËæÉÁÆÄÂçïÔºåÂ§ßÊ¶ÇËØ¥‰∏Ä‰∏ãÂ∏∏Áî®ÁöÑÁî®Â§ÑÔºå‰∏çÂÅö‰ª£Á†ÅÊµãËØïtransformationMapÔºöÂØπÊï∞ÊçÆÈõÜÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ËøõË°åÊìç‰ΩúFlatMapÔºöÂÖàÂØπÊï∞ÊçÆÈõÜËøõË°åÊâÅÂπ≥ÂåñÂ§ÑÁêÜÔºåÁÑ∂ÂêéÂÜçMapFilterÔºöÂØπÊï∞ÊçÆËøõË°åËøáÊª§Ôºå‰∏∫trueÂàôÈÄöËøádestinctÔºöÂéªÈáçÊìç‰ΩúactionreduceÔºöÂØπÊï∞ÊçÆËøõË°åËÅöÈõÜreduceBykeyÔºöÂØπkeyÂÄºÁõ∏ÂêåÁöÑËøõË°åÊìç‰ΩúcollectÔºöÊ≤°ÊúâÊïàÊûúÁöÑactionÔºå‰ΩÜÊòØÂæàÊúâÁî®saveAstextFileÔºöÊï∞ÊçÆÂ≠òÂÖ•Êñá‰ª∂Á≥ªÁªüforeachÔºöÂØπÊØè‰∏™ÂÖÉÁ¥†ËøõË°åfuncÁöÑÊìç‰Ωú]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD„ÄÅDataFrameÂíåDataSetÁöÑÂå∫Âà´]]></title>
    <url>%2F2018%2F05%2F19%2FSpark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Âú®Spark‰∏≠ÔºåRDD„ÄÅDataFrame„ÄÅDatasetÊòØÊúÄÂ∏∏Áî®ÁöÑÊï∞ÊçÆÁ±ªÂûãÔºå‰ªäÂ§©Ë∞àË∞à‰ªñ‰ª¨ÁöÑÂå∫Âà´ÔºÅ‰∏Ä „ÄÅÂÖ±ÊÄß1„ÄÅRDD„ÄÅDataFrame„ÄÅDatasetÂÖ®ÈÉΩÊòØsparkÂπ≥Âè∞‰∏ãÁöÑÂàÜÂ∏ÉÂºèÂºπÊÄßÊï∞ÊçÆÈõÜÔºå‰∏∫Â§ÑÁêÜË∂ÖÂ§ßÂûãÊï∞ÊçÆÊèê‰æõ‰æøÂà©2„ÄÅ‰∏âËÄÖÈÉΩÊúâÊÉ∞ÊÄßÊú∫Âà∂ÔºåÂú®ËøõË°åÂàõÂª∫„ÄÅËΩ¨Êç¢ÔºåÂ¶ÇmapÊñπÊ≥ïÊó∂Ôºå‰∏ç‰ºöÁ´ãÂç≥ÊâßË°åÔºåÂè™ÊúâÂú®ÈÅáÂà∞ActionÂ¶ÇforeachÊó∂Ôºå‰∏âËÄÖÊâç‰ºöÂºÄÂßãÈÅçÂéÜËøêÁÆó„ÄÇ3„ÄÅ‰∏âËÄÖÈÉΩ‰ºöÊ†πÊçÆsparkÁöÑÂÜÖÂ≠òÊÉÖÂÜµËá™Âä®ÁºìÂ≠òËøêÁÆóÔºåËøôÊ†∑Âç≥‰ΩøÊï∞ÊçÆÈáèÂæàÂ§ßÔºå‰πü‰∏çÁî®ÊãÖÂøÉ‰ºöÂÜÖÂ≠òÊ∫¢Âá∫4„ÄÅ‰∏âËÄÖÈÉΩÊúâpartitionÁöÑÊ¶ÇÂøµ„ÄÇ‰∫å„ÄÅRDD‰ºòÁº∫ÁÇπ‰ºòÁÇπÔºö1„ÄÅÁõ∏ÊØî‰∫é‰º†ÁªüÁöÑMapReduceÊ°ÜÊû∂ÔºåSparkÂú®RDD‰∏≠ÂÜÖÁΩÆÂæàÂ§öÂáΩÊï∞Êìç‰ΩúÔºågroupÔºåmapÔºåfilterÁ≠âÔºåÊñπ‰æøÂ§ÑÁêÜÁªìÊûÑÂåñÊàñÈùûÁªìÊûÑÂåñÊï∞ÊçÆ„ÄÇ2„ÄÅÈù¢ÂêëÂØπË±°ÁöÑÁºñÁ®ãÈ£éÊ†º3„ÄÅÁºñËØëÊó∂Á±ªÂûãÂÆâÂÖ®ÔºåÁºñËØëÊó∂Â∞±ËÉΩÊ£ÄÊü•Âá∫Á±ªÂûãÈîôËØØÁº∫ÁÇπÔºö1„ÄÅÂ∫èÂàóÂåñÂíåÂèçÂ∫èÂàóÂåñÁöÑÊÄßËÉΩÂºÄÈîÄ2„ÄÅGCÁöÑÊÄßËÉΩÂºÄÈîÄÔºåÈ¢ëÁπÅÁöÑÂàõÂª∫ÂíåÈîÄÊØÅÂØπË±°, ÂäøÂøÖ‰ºöÂ¢ûÂä†GC‰∏â„ÄÅDataFrame1„ÄÅ‰∏éRDDÂíåDataset‰∏çÂêåÔºåDataFrameÊØè‰∏ÄË°åÁöÑÁ±ªÂûãÂõ∫ÂÆö‰∏∫RowÔºåÂè™ÊúâÈÄöËøáËß£ÊûêÊâçËÉΩËé∑ÂèñÂêÑ‰∏™Â≠óÊÆµÁöÑÂÄº„ÄÇÂ¶Ç12345df.foreach&#123; x =&gt; val v1=x.getAs[String](&quot;v1&quot;) val v2=x.getAs[String](&quot;v2&quot;)&#125;2„ÄÅDataFrameÂºïÂÖ•‰∫ÜschemaÂíåoff-heapschema : RDDÊØè‰∏ÄË°åÁöÑÊï∞ÊçÆ, ÁªìÊûÑÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ. Ëøô‰∏™ÁªìÊûÑÂ∞±Â≠òÂÇ®Âú®schema‰∏≠. SparkÈÄöËøáschameÂ∞±ËÉΩÂ§üËØªÊáÇÊï∞ÊçÆ, Âõ†Ê≠§Âú®ÈÄö‰ø°ÂíåIOÊó∂Â∞±Âè™ÈúÄË¶ÅÂ∫èÂàóÂåñÂíåÂèçÂ∫èÂàóÂåñÊï∞ÊçÆ, ËÄåÁªìÊûÑÁöÑÈÉ®ÂàÜÂ∞±ÂèØ‰ª•ÁúÅÁï•‰∫Ü.off-heap : ÊÑèÂë≥ÁùÄJVMÂ†Ü‰ª•Â§ñÁöÑÂÜÖÂ≠ò, Ëøô‰∫õÂÜÖÂ≠òÁõ¥Êé•ÂèóÊìç‰ΩúÁ≥ªÁªüÁÆ°ÁêÜÔºàËÄå‰∏çÊòØJVMÔºâ„ÄÇSparkËÉΩÂ§ü‰ª•‰∫åËøõÂà∂ÁöÑÂΩ¢ÂºèÂ∫èÂàóÂåñÊï∞ÊçÆ(‰∏çÂåÖÊã¨ÁªìÊûÑ)Âà∞off-heap‰∏≠, ÂΩìË¶ÅÊìç‰ΩúÊï∞ÊçÆÊó∂, Â∞±Áõ¥Êé•Êìç‰Ωúoff-heapÂÜÖÂ≠ò. Áî±‰∫éSparkÁêÜËß£schema, ÊâÄ‰ª•Áü•ÈÅìËØ•Â¶Ç‰ΩïÊìç‰Ωú.off-heapÂ∞±ÂÉèÂú∞Áõò, schemaÂ∞±ÂÉèÂú∞Âõæ, SparkÊúâÂú∞ÂõæÂèàÊúâËá™Â∑±Âú∞Áõò‰∫Ü, Â∞±ÂèØ‰ª•Ëá™Â∑±ËØ¥‰∫ÜÁÆó‰∫Ü, ‰∏çÂÜçÂèóJVMÁöÑÈôêÂà∂, ‰πüÂ∞±‰∏çÂÜçÊî∂GCÁöÑÂõ∞Êâ∞‰∫Ü.3„ÄÅÁªìÊûÑÂåñÊï∞ÊçÆÂ§ÑÁêÜÈùûÂ∏∏Êñπ‰æøÔºåÊîØÊåÅAvro, CSV, ElasticsearchÊï∞ÊçÆÁ≠âÔºå‰πüÊîØÊåÅHive, MySQLÁ≠â‰º†ÁªüÊï∞ÊçÆË°®4„ÄÅÂÖºÂÆπHiveÔºåÊîØÊåÅHql„ÄÅUDFÊúâschemaÂíåoff-heapÊ¶ÇÂøµÔºåDataFrameËß£ÂÜ≥‰∫ÜRDDÁöÑÁº∫ÁÇπ, ‰ΩÜÊòØÂç¥‰∏¢‰∫ÜRDDÁöÑ‰ºòÁÇπ. DataFrame‰∏çÊòØÁ±ªÂûãÂÆâÂÖ®ÁöÑÔºàÂè™ÊúâÁºñËØëÂêéÊâçËÉΩÁü•ÈÅìÁ±ªÂûãÈîôËØØÔºâ, API‰πü‰∏çÊòØÈù¢ÂêëÂØπË±°È£éÊ†ºÁöÑ.Âõõ„ÄÅDataSet1„ÄÅDataSetÊòØÂàÜÂ∏ÉÂºèÁöÑÊï∞ÊçÆÈõÜÂêà„ÄÇDataSetÊòØÂú®Spark1.6‰∏≠Ê∑ªÂä†ÁöÑÊñ∞ÁöÑÊé•Âè£„ÄÇÂÆÉÈõÜ‰∏≠‰∫ÜRDDÁöÑ‰ºòÁÇπÔºàÂº∫Á±ªÂûã ÂíåÂèØ‰ª•Áî®Âº∫Â§ßlambdaÂáΩÊï∞Ôºâ‰ª•ÂèäSpark SQL‰ºòÂåñÁöÑÊâßË°åÂºïÊìé„ÄÇDataSetÂèØ‰ª•ÈÄöËøáJVMÁöÑÂØπË±°ËøõË°åÊûÑÂª∫ÔºåÂèØ‰ª•Áî®ÂáΩÊï∞ÂºèÁöÑËΩ¨Êç¢Ôºàmap/flatmap/filterÔºâËøõË°åÂ§öÁßçÊìç‰Ωú„ÄÇ2„ÄÅDataSetÁªìÂêà‰∫ÜRDDÂíåDataFrameÁöÑ‰ºòÁÇπ, Âπ∂Â∏¶Êù•ÁöÑ‰∏Ä‰∏™Êñ∞ÁöÑÊ¶ÇÂøµEncoder„ÄÇDataSet ÈÄöËøáEncoderÂÆûÁé∞‰∫ÜËá™ÂÆö‰πâÁöÑÂ∫èÂàóÂåñÊ†ºÂºèÔºå‰ΩøÂæóÊüê‰∫õÊìç‰ΩúÂèØ‰ª•Âú®Êó†ÈúÄÂ∫èÂàóÂåñÊÉÖÂÜµ‰∏ãËøõË°å„ÄÇÂè¶Â§ñDatasetËøòËøõË°å‰∫ÜÂåÖÊã¨Tungsten‰ºòÂåñÂú®ÂÜÖÁöÑÂæàÂ§öÊÄßËÉΩÊñπÈù¢ÁöÑ‰ºòÂåñ„ÄÇ3„ÄÅDatasetÁ≠âÂêå‰∫éDataFrameÔºàSpark 2.XÔºâ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â§ßÊï∞ÊçÆ‰πãÂÆûÊó∂Êï∞ÊçÆÊ∫êÂêåÊ≠•‰∏≠Èó¥‰ª∂--Áîü‰∫ß‰∏äCanal‰∏éMaxwellÈ¢†Â≥∞ÂØπÂÜ≥]]></title>
    <url>%2F2018%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3%2F</url>
    <content type="text"><![CDATA[‰∏Ä.Êï∞ÊçÆÊ∫êÂêåÊ≠•‰∏≠Èó¥‰ª∂ÔºöCanalhttps://github.com/alibaba/canalhttps://github.com/Hackeruncle/syncClientMaxwellhttps://github.com/zendesk/maxwell‰∫å.Êû∂ÊûÑ‰ΩøÁî®MySQL ‚Äî- ‰∏≠Èó¥‰ª∂ mcp ‚Äî&gt;KAFKA‚Äî&gt;?‚Äî&gt;Â≠òÂÇ®HBASE/KUDU/Cassandra Â¢ûÈáèÁöÑa.ÂÖ®Èáè bootstrapb.Â¢ûÈáè1.ÂØπÊØîCanal(ÊúçÂä°Á´Ø)Maxwell(ÊúçÂä°Á´Ø+ÂÆ¢Êà∑Á´Ø)ËØ≠Ë®ÄJavaJavaÊ¥ªË∑ÉÂ∫¶Ê¥ªË∑ÉÊ¥ªË∑ÉHAÊîØÊåÅÂÆöÂà∂ ‰ΩÜÊòØÊîØÊåÅÊñ≠ÁÇπËøòÂéüÂäüËÉΩÊï∞ÊçÆËêΩÂú∞ÂÆöÂà∂ËêΩÂú∞Âà∞kafkaÂàÜÂå∫ÊîØÊåÅÊîØÊåÅbootstrap(ÂºïÂØº)‰∏çÊîØÊåÅÊîØÊåÅÊï∞ÊçÆÊ†ºÂºèÊ†ºÂºèËá™Áî±json(Ê†ºÂºèÂõ∫ÂÆö) spark json‚Äì&gt;DFÊñáÊ°£ËæÉËØ¶ÁªÜËæÉËØ¶ÁªÜÈöèÊú∫ËØªÊîØÊåÅÊîØÊåÅ‰∏™‰∫∫ÈÄâÊã©Maxwella.ÊúçÂä°Á´Ø+ÂÆ¢Êà∑Á´Ø‰∏Ä‰ΩìÔºåËΩªÈáèÁ∫ßÁöÑb.ÊîØÊåÅÊñ≠ÁÇπËøòÂéüÂäüËÉΩ+bootstrap+jsonCan do SELECT * from table (bootstrapping) initial loads of a table.supports automatic position recover on master promotionflexible partitioning schemes for Kakfa - by database, table, primary key, or columnMaxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).2.ÂÆòÁΩëËß£ËØªBÁ´ôËßÜÈ¢ë3.ÈÉ®ÁΩ≤3.1 MySQL Installhttps://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txthttps://ke.qq.com/course/262452?tuin=11cffd503.2 ‰øÆÊîπ12345678910111213141516171819202122$ vi /etc/my.cnf[mysqld]binlog_format=row$ service mysql start3.3 ÂàõÂª∫MaxwellÁöÑdbÂíåÁî®Êà∑mysql&gt; create database maxwell;Query OK, 1 row affected (0.03 sec)mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt;3.4Ëß£Âéã1[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz3.5ÊµãËØïSTDOUT:123bin/maxwell --user=&apos;maxwell&apos; \--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \--producer=stdoutÊµãËØï1Ôºöinsert sqlÔºö12mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);Query OK, 1 row affected (0.03 sec)maxwellËæìÂá∫Ôºö123456789101112131415161718&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;insert&quot;, &quot;ts&quot;: 1525959044, &quot;xid&quot;: 201, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 18, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;updateuser&quot;: null &#125;&#125;ÊµãËØï1Ôºöupdate sql:1mysql&gt; update ruozedata set age=29 where id=999;ÈóÆÈ¢ò: ROWÔºå‰Ω†ËßâÂæóbinlogÊõ¥Êñ∞Âá†‰∏™Â≠óÊÆµÔºümaxwellËæìÂá∫Ôºö12345678910111213141516171819202122&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;update&quot;, &quot;ts&quot;: 1525959208, &quot;xid&quot;: 255, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 29, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;, &quot;updateuser&quot;: null &#125;, &quot;old&quot;: &#123; &quot;age&quot;: 18, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot; &#125;&#125;4.ÂÖ∂‰ªñÊ≥®ÊÑèÁÇπÂíåÊñ∞ÁâπÊÄß4.1 kafka_version ÁâàÊú¨Using kafka version: 0.11.0.1 0.10jar:12345678[root@hadoop000 kafka-clients]# lltotal 4000-rw-r--r--. 1 ruoze games 746207 May 8 06:34 kafka-clients-0.10.0.1.jar-rw-r--r--. 1 ruoze games 951041 May 8 06:35 kafka-clients-0.10.2.1.jar-rw-r--r--. 1 ruoze games 1419544 May 8 06:35 kafka-clients-0.11.0.1.jar-rw-r--r--. 1 ruoze games 324016 May 8 06:34 kafka-clients-0.8.2.2.jar-rw-r--r--. 1 ruoze games 641408 May 8 06:34 kafka-clients-0.9.0.1.jar[root@hadoop000 kafka-clients]#]]></content>
      <categories>
        <category>ÂÖ∂‰ªñÁªÑ‰ª∂</category>
      </categories>
      <tags>
        <tag>È´òÁ∫ß</tag>
        <tag>maxwell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on YARN-ClusterÂíåYARN-ClientÁöÑÂå∫Âà´]]></title>
    <url>%2F2018%2F05%2F12%2FSpark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[‰∏Ä. YARN-ClusterÂíåYARN-ClientÁöÑÂå∫Âà´Ôºà1ÔºâSparkContextÂàùÂßãÂåñ‰∏çÂêåÔºåËøô‰πüÂØºËá¥‰∫ÜDriverÊâÄÂú®‰ΩçÁΩÆÁöÑ‰∏çÂêåÔºåYarnClusterÁöÑDriverÊòØÂú®ÈõÜÁæ§ÁöÑÊüê‰∏ÄÂè∞NM‰∏äÔºå‰ΩÜÊòØYarn-ClientÂ∞±ÊòØÂú®driverÊâÄÂú®ÁöÑÊú∫Âô®‰∏äÔºõÔºà2ÔºâËÄåDriver‰ºöÂíåExecutorsËøõË°åÈÄö‰ø°ÔºåËøô‰πüÂØºËá¥‰∫ÜYarn_clusterÂú®Êèê‰∫§App‰πãÂêéÂèØ‰ª•ÂÖ≥Èó≠ClientÔºåËÄåYarn-Client‰∏çÂèØ‰ª•ÔºõÔºà3ÔºâÊúÄÂêéÂÜçÊù•ËØ¥Â∫îÁî®Âú∫ÊôØÔºåYarn-ClusterÈÄÇÂêàÁîü‰∫ßÁéØÂ¢ÉÔºåYarn-ClientÈÄÇÂêà‰∫§‰∫íÂíåË∞ÉËØï„ÄÇ‰∫å. yarn client Ê®°Âºèyarn-client Ê®°ÂºèÁöÑËØù ÔºåÊää ÂÆ¢Êà∑Á´ØÂÖ≥ÊéâÁöÑËØù ÔºåÊòØ‰∏çËÉΩÊèê‰∫§‰ªªÂä°ÁöÑ „ÄÇ‰∏â.yarn cluster Ê®°Âºèyarn-cluster Ê®°ÂºèÁöÑËØùÔºå client ÂÖ≥Èó≠ÊòØÂèØ‰ª•Êèê‰∫§‰ªªÂä°ÁöÑ ÔºåÊÄªÁªì:1.spark-shell/spark-sql Âè™ÊîØÊåÅ yarn-clientÊ®°ÂºèÔºõ2.spark-submitÂØπ‰∫é‰∏§ÁßçÊ®°ÂºèÈÉΩÊîØÊåÅ„ÄÇ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Êû∂ÊûÑ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áîü‰∫ßÊîπÈÄ†Spark1.6Ê∫ê‰ª£Á†ÅÔºåcreate tableËØ≠Ê≥ïÊîØÊåÅOracleÂàóË°®ÂàÜÂå∫]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1.ÈúÄÊ±ÇÈÄöËøáSpark SQL JDBC ÊñπÊ≥ïÔºåÊäΩÂèñOracleË°®Êï∞ÊçÆ„ÄÇ2.ÈóÆÈ¢òÂ§ßÊï∞ÊçÆÂºÄÂèë‰∫∫ÂëòÂèçÊò†Ôºå‰ΩøÁî®ÊïàÊûú‰∏äÂàóË°®ÂàÜÂå∫‰ºò‰∫éÊï£ÂàóÂàÜÂå∫„ÄÇ‰ΩÜSpark SQL JDBCÊñπÊ≥ïÂè™ÊîØÊåÅÊï∞Â≠óÁ±ªÂûãÂàÜÂå∫ÔºåËÄå‰∏öÂä°Ë°®ÁöÑÂàóË°®ÂàÜÂå∫Â≠óÊÆµÊòØ‰∏™Â≠óÁ¨¶‰∏≤„ÄÇÁõÆÂâçOracleË°®‰ΩøÁî®ÂàóË°®ÂàÜÂå∫ÔºåÂØπÁúÅÁ∫ß‰ª£Á†ÅÂàÜ Âå∫„ÄÇÂèÇËÄÉ http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases3.OracleÁöÑÂàÜÂå∫3.1ÂàóË°®ÂàÜÂå∫:ËØ•ÂàÜÂå∫ÁöÑÁâπÁÇπÊòØÊüêÂàóÁöÑÂÄºÂè™ÊúâÂá†‰∏™ÔºåÂü∫‰∫éËøôÊ†∑ÁöÑÁâπÁÇπÊàë‰ª¨ÂèØ‰ª•ÈááÁî®ÂàóË°®ÂàÜÂå∫„ÄÇ‰æã‰∏Ä:1234567891011CREATE TABLE PROBLEM_TICKETS(PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,STATUS VARCHAR2(20))PARTITION BY LIST (STATUS)(PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02)3.2Êï£ÂàóÂàÜÂå∫:ËøôÁ±ªÂàÜÂå∫ÊòØÂú®ÂàóÂÄº‰∏ä‰ΩøÁî®Êï£ÂàóÁÆóÊ≥ïÔºå‰ª•Á°ÆÂÆöÂ∞ÜË°åÊîæÂÖ•Âì™‰∏™ÂàÜÂå∫‰∏≠„ÄÇÂΩìÂàóÁöÑÂÄºÊ≤°ÊúâÂêàÈÄÇÁöÑÊù°‰ª∂Êó∂ÔºåÂª∫ËÆÆ‰ΩøÁî®Êï£ÂàóÂàÜÂå∫„ÄÇ Êï£ÂàóÂàÜÂå∫‰∏∫ÈÄöËøáÊåáÂÆöÂàÜÂå∫ÁºñÂè∑Êù•ÂùáÂåÄÂàÜÂ∏ÉÊï∞ÊçÆÁöÑ‰∏ÄÁßçÂàÜÂå∫Á±ªÂûãÔºåÂõ†‰∏∫ÈÄöËøáÂú®I/OËÆæÂ§á‰∏äËøõË°åÊï£ÂàóÂàÜÂå∫Ôºå‰ΩøÂæóËøô‰∫õÂàÜÂå∫Â§ßÂ∞è‰∏ÄËá¥„ÄÇ‰æã‰∏Ä:1234567891011CREATE TABLE HASH_TABLE(COL NUMBER(8),INF VARCHAR2(100) )PARTITION BY HASH (COL)(PARTITION PART01 TABLESPACE HASH_TS01, PARTITION PART02 TABLESPACE HASH_TS02, PARTITION PART03 TABLESPACE HASH_TS03)4.ÊîπÈÄ†ËìùËâ≤‰ª£Á†ÅÊòØÊîπÈÄ†SparkÊ∫ê‰ª£Á†Å,Âä†ËØæÁ®ãÈ°æÈóÆÈ¢ÜÂèñPDF„ÄÇ1) Spark SQL JDBCÁöÑÂª∫Ë°®ËÑöÊú¨‰∏≠ÈúÄË¶ÅÂä†ÂÖ•ÂàóË°®ÂàÜÂå∫ÈÖçÁΩÆÈ°π„ÄÇ12345678910111213CREATE TEMPORARY TABLE TBLS_INUSING org.apache.spark.sql.jdbc OPTIONS (driver &quot;com.mysql.jdbc.Driver&quot;,url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,fetchSize &quot;1000&quot;,partitionColumn &quot;TBL_ID&quot;,numPartitions &quot;null&quot;,lowerBound &quot;null&quot;,upperBound &quot;null&quot;,user &quot;hive2user&quot;,password &quot;hive2user&quot;,partitionInRule &quot;1|15,16,18,19|20,21&quot;);2)Á®ãÂ∫èÂÖ•Âè£org.apache.spark.sql.execution.datasources.jdbc.DefaultSourceÔºåÊñπÊ≥ïcreateRelation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748override def createRelation(sqlContext: SQLContext,parameters: Map[String, String]): BaseRelation = &#123;val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)// add partition in ruleval partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)// validind all the partition in rule if (partitionColumn != null&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)&amp;&amp; partitionInRule == null )&#123; sys.error(&quot;Partitioning incompletely specified&quot;) &#125;val partitionInfo = if (partitionColumn == null) &#123; null&#125; else &#123; val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123; val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot; upperBound = &quot;0&quot; inGroups &#125; else&#123; Array[String]() &#125; JDBCPartitioningInfo( partitionColumn, lowerBound.toLong, upperBound.toLong, numPartitions.toInt, inPartitions)&#125;val parts = JDBCRelation.columnPartition(partitionInfo)val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))// parameters is immutableif(numPartitions != null)&#123;properties.put(&quot;numPartitions&quot; , numPartitions) &#125;JDBCRelation(url, table, parts, properties)(sqlContext) &#125; &#125;3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelationÔºåÊñπÊ≥ïcolumnPartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))val column = partitioning.columnvar i: Int = 0var ans = new ArrayBuffer[Partition]()// partition by long if(partitioning.inPartitions.length == 0)&#123;val numPartitions = partitioning.numPartitionsif (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.// Here we get a little roundoff, but that&apos;s (hopefully) OK.val stride: Long = (partitioning.upperBound / numPartitions- partitioning.lowerBound / numPartitions)var currentValue: Long = partitioning.lowerBoundwhile (i &lt; numPartitions) &#123;val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else nullcurrentValue += strideval upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =if (upperBound == null) &#123; lowerBound&#125; else if (lowerBound == null) &#123; upperBound&#125; else &#123; s&quot;$lowerBound AND $upperBound&quot; &#125; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125;&#125;// partition by in else&#123; while(i &lt; partitioning.inPartitions.length)&#123; val inContent = partitioning.inPartitions(i) val whereClause = s&quot;$column in ($inContent)&quot; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125; &#125; ans.toArray &#125;4)ÂØπÂ§ñÊñπÊ≥ïorg.apache.spark.sql.SQLContext , ÊñπÊ≥ïjdbc123456789101112def jdbc(url: String,table: String,columnName: String,lowerBound: Long,upperBound: Long,numPartitions: Int,inPartitions: Array[String] = Array[String]()): DataFrame = &#123;read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)&#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Ê∫êÁ†ÅÈòÖËØª</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áîü‰∫ß‰∏≠HiveÈùôÊÄÅÂíåÂä®ÊÄÅÂàÜÂå∫Ë°®ÔºåËØ•ÊÄéÊ†∑ÊäâÊã©Âë¢Ôºü]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[‰∏Ä.ÈúÄÊ±ÇÊåâÁÖß‰∏çÂêåÈÉ®Èó®‰Ωú‰∏∫ÂàÜÂå∫ÔºåÂØºÊï∞ÊçÆÂà∞ÁõÆÊ†áË°®‰∫å.‰ΩøÁî®ÈùôÊÄÅÂàÜÂå∫Ë°®Êù•ÂÆåÊàê71.ÂàõÂª∫ÈùôÊÄÅÂàÜÂå∫Ë°®Ôºö12345678910create table emp_static_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;2.ÊèíÂÖ•Êï∞ÊçÆÔºö12hive&gt;insert into table emp_static_partition partition(deptno=10) select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;3.Êü•ËØ¢Êï∞ÊçÆÔºö1hive&gt;select * from emp_static_partition;‰∏â.‰ΩøÁî®Âä®ÊÄÅÂàÜÂå∫Ë°®Êù•ÂÆåÊàê1.ÂàõÂª∫Âä®ÊÄÅÂàÜÂå∫Ë°®Ôºö123456789create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;„ÄêÊ≥®ÊÑè„ÄëÂä®ÊÄÅÂàÜÂå∫Ë°®‰∏éÈùôÊÄÅÂàÜÂå∫Ë°®ÁöÑÂàõÂª∫ÔºåÂú®ËØ≠Ê≥ï‰∏äÊòØÊ≤°Êúâ‰ªª‰ΩïÂå∫Âà´ÁöÑ2.ÊèíÂÖ•Êï∞ÊçÆÔºö12hive&gt;insert into table emp_dynamic_partition partition(deptno) select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;„ÄêÊ≥®ÊÑè„ÄëÂàÜÂå∫ÁöÑÂ≠óÊÆµÂêçÁß∞ÔºåÂÜôÂú®ÊúÄÂêéÔºåÊúâÂá†‰∏™Â∞±ÂÜôÂá†‰∏™ ‰∏éÈùôÊÄÅÂàÜÂå∫Áõ∏ÊØîÔºå‰∏çÈúÄË¶ÅwhereÈúÄË¶ÅËÆæÁΩÆÂ±ûÊÄßÁöÑÂÄºÔºö1hive&gt;set hive.exec.dynamic.partition.mode=nonstrictÔºõÂÅáÂ¶Ç‰∏çËÆæÁΩÆÔºåÊä•ÈîôÂ¶Ç‰∏ã:3.Êü•ËØ¢Êï∞ÊçÆÔºö1hive&gt;select * from emp_dynamic_partition;ÂàÜÂå∫Âàó‰∏∫deptnoÔºåÂÆûÁé∞‰∫ÜÂä®ÊÄÅÂàÜÂå∫Âõõ.ÊÄªÁªìÂú®Áîü‰∫ß‰∏äÊàë‰ª¨Êõ¥ÂÄæÂêëÊòØÈÄâÊã©Âä®ÊÄÅÂàÜÂå∫ÔºåÊó†ÈúÄÊâãÂ∑•ÊåáÂÆöÊï∞ÊçÆÂØºÂÖ•ÁöÑÂÖ∑‰ΩìÂàÜÂå∫ÔºåËÄåÊòØÁî±selectÁöÑÂ≠óÊÆµ(Â≠óÊÆµÂÜôÂú®ÊúÄÂêéÔºåÊúâÂá†‰∏™ÂÜôÂá†‰∏™)Ëá™Ë°åÂÜ≥ÂÆöÂØºÂá∫Âà∞Âì™‰∏Ä‰∏™ÂàÜÂå∫‰∏≠Ôºå Âπ∂Ëá™Âä®ÂàõÂª∫Áõ∏Â∫îÁöÑÂàÜÂå∫Ôºå‰ΩøÁî®‰∏äÊõ¥Âä†Êñπ‰æøÂø´Êç∑ ÔºåÂú®Áîü‰∫ßÂ∑•‰Ωú‰∏≠Áî®ÁöÑÈùûÂ∏∏Â§öÂ§ö„ÄÇ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5minÊéåÊè°ÔºåHiveÁöÑHiveServer2 ÂíåJDBCÂÆ¢Êà∑Á´Ø&‰ª£Á†ÅÁöÑÁîü‰∫ß‰ΩøÁî®]]></title>
    <url>%2F2018%2F05%2F04%2F5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF%26%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. ‰ªãÁªçÔºö‰∏§ËÄÖÈÉΩÂÖÅËÆ∏ËøúÁ®ãÂÆ¢Êà∑Á´Ø‰ΩøÁî®Â§öÁßçÁºñÁ®ãËØ≠Ë®ÄÔºåÈÄöËøáHiveServerÊàñËÄÖHiveServer2ÔºåÂÆ¢Êà∑Á´ØÂèØ‰ª•Âú®‰∏çÂêØÂä®CLIÁöÑÊÉÖÂÜµ‰∏ãÂØπHive‰∏≠ÁöÑÊï∞ÊçÆËøõË°åÊìç‰ΩúÔºå‰∏§ËÄÖÈÉΩÂÖÅËÆ∏ËøúÁ®ãÂÆ¢Êà∑Á´Ø‰ΩøÁî®Â§öÁßçÁºñÁ®ãËØ≠Ë®ÄÂ¶ÇjavaÔºåpythonÁ≠âÂêëhiveÊèê‰∫§ËØ∑Ê±ÇÔºåÂèñÂõûÁªìÊûúÔºà‰ªéhive0.15Ëµ∑Â∞±‰∏çÂÜçÊîØÊåÅhiveserver‰∫ÜÔºâÔºå‰ΩÜÊòØÂú®ËøôÈáåÊàë‰ª¨ËøòÊòØË¶ÅËØ¥‰∏Ä‰∏ãHiveServer„ÄÇHiveServerÊàñËÄÖHiveServer2ÈÉΩÊòØÂü∫‰∫éThriftÁöÑÔºå‰ΩÜHiveSeverÊúâÊó∂Ë¢´Áß∞‰∏∫Thrift serverÔºåËÄåHiveServer2Âç¥‰∏ç‰ºö„ÄÇÊó¢ÁÑ∂Â∑≤ÁªèÂ≠òÂú®HiveServerÔºå‰∏∫‰ªÄ‰πàËøòÈúÄË¶ÅHiveServer2Âë¢ÔºüËøôÊòØÂõ†‰∏∫HiveServer‰∏çËÉΩÂ§ÑÁêÜÂ§ö‰∫é‰∏Ä‰∏™ÂÆ¢Êà∑Á´ØÁöÑÂπ∂ÂèëËØ∑Ê±ÇÔºåËøôÊòØÁî±‰∫éHiveServer‰ΩøÁî®ÁöÑThriftÊé•Âè£ÊâÄÂØºËá¥ÁöÑÈôêÂà∂Ôºå‰∏çËÉΩÈÄöËøá‰øÆÊîπHiveServerÁöÑ‰ª£Á†Å‰øÆÊ≠£„ÄÇÂõ†Ê≠§Âú®Hive-0.11.0ÁâàÊú¨‰∏≠ÈáçÂÜô‰∫ÜHiveServer‰ª£Á†ÅÂæóÂà∞‰∫ÜHiveServer2ÔºåËøõËÄåËß£ÂÜ≥‰∫ÜËØ•ÈóÆÈ¢ò„ÄÇHiveServer2ÊîØÊåÅÂ§öÂÆ¢Êà∑Á´ØÁöÑÂπ∂ÂèëÂíåËÆ§ËØÅÔºå‰∏∫ÂºÄÊîæAPIÂÆ¢Êà∑Á´ØÂ¶ÇÈááÁî®jdbc„ÄÅodbc„ÄÅbeelineÁöÑÊñπÂºèËøõË°åËøûÊé•„ÄÇ2.ÈÖçÁΩÆÂèÇÊï∞Hiveserver2ÂÖÅËÆ∏Âú®ÈÖçÁΩÆÊñá‰ª∂hive-site.xml‰∏≠ËøõË°åÈÖçÁΩÆÁÆ°ÁêÜÔºåÂÖ∑‰ΩìÁöÑÂèÇÊï∞‰∏∫ÔºöÂèÇÊï∞ | Âê´‰πâ |-|-|hive.server2.thrift.min.worker.threads| ÊúÄÂ∞èÂ∑•‰ΩúÁ∫øÁ®ãÊï∞ÔºåÈªòËÆ§‰∏∫5„ÄÇhive.server2.thrift.max.worker.threads| ÊúÄÂ∞èÂ∑•‰ΩúÁ∫øÁ®ãÊï∞ÔºåÈªòËÆ§‰∏∫500„ÄÇhive.server2.thrift.port| TCP ÁöÑÁõëÂê¨Á´ØÂè£ÔºåÈªòËÆ§‰∏∫10000„ÄÇhive.server2.thrift.bind.host| TCPÁªëÂÆöÁöÑ‰∏ªÊú∫ÔºåÈªòËÆ§‰∏∫localhostÈÖçÁΩÆÁõëÂê¨Á´ØÂè£ÂíåË∑ØÂæÑ123456789vi hive-site.xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.48.130&lt;/value&gt;&lt;/property&gt;3. ÂêØÂä®hiveserver2‰ΩøÁî®hadoopÁî®Êà∑ÂêØÂä®123[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/[hadoop@hadoop001 bin]$ hiveserver2 which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)4. ÈáçÊñ∞ÂºÄ‰∏™Á™óÂè£Ôºå‰ΩøÁî®beelineÊñπÂºèËøûÊé•-n ÊåáÂÆöÊú∫Âô®ÁôªÈôÜÁöÑÂêçÂ≠óÔºåÂΩìÂâçÊú∫Âô®ÁöÑÁôªÈôÜÁî®Êà∑Âêç-u ÊåáÂÆö‰∏Ä‰∏™ËøûÊé•‰∏≤ÊØèÊàêÂäüËøêË°å‰∏Ä‰∏™ÂëΩ‰ª§Ôºåhiveserver2ÂêØÂä®ÁöÑÈÇ£‰∏™Á™óÂè£ÔºåÂè™Ë¶ÅÂú®ÂêØÂä®beelineÁöÑÁ™óÂè£‰∏≠ÊâßË°åÊàêÂäü‰∏ÄÊù°ÂëΩ‰ª§ÔºåÂè¶Â§ñ‰∏™Á™óÂè£ÈöèÂç≥ÊâìÂç∞‰∏Ä‰∏™OKÂ¶ÇÊûúÂëΩ‰ª§ÈîôËØØÔºåhiveserver2ÈÇ£‰∏™Á™óÂè£Â∞±‰ºöÊäõÂá∫ÂºÇÂ∏∏‰ΩøÁî®hadoopÁî®Êà∑ÂêØÂä®123456789[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoopwhich: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)scan complete in 4msConnecting to jdbc:hive2://localhost:10000/defaultConnected to: Apache Hive (version 1.1.0-cdh5.7.0)Driver: Hive JDBC (version 1.1.0-cdh5.7.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.7.0 by Apache Hive0: jdbc:hive2://localhost:10000/default&gt;‰ΩøÁî®SQL123456789101112131415160: jdbc:hive2://localhost:10000/default&gt; show databases;INFO : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)INFO : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 secondsINFO : Concurrency mode is disabled, not creating a lock managerINFO : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Starting task [Stage-0:DDL] in serial modeINFO : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 secondsINFO : OK+----------------+--+| database_name |+----------------+--+| default |+----------------+--+1 row selected5.‰ΩøÁî®ÁºñÂÜôjava‰ª£Á†ÅÊñπÂºèËøûÊé•5.1‰ΩøÁî®mavenÊûÑÂª∫È°πÁõÆÔºåpom.xmlÊñá‰ª∂Â¶Ç‰∏ãÔºö123456789101112131415161718192021222324252627282930313233343536373839&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;5.2JdbcApp.javaÊñá‰ª∂‰ª£Á†Å:123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class JdbcApp &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws Exception &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); //select table:ename String tableName = &quot;emp&quot;; String sql = &quot;select ename from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); while(res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ë∞àË∞àÊàëÂíåÂ§ßÊï∞ÊçÆÁöÑÊÉÖÁºòÂèäÂÖ•Èó®]]></title>
    <url>%2F2018%2F05%2F01%2F%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[&#8195;ÂΩìÂπ¥ÊàëÊòØÂÅöC#+JavaËΩØ‰ª∂ÂºÄÂèëÔºåÁÑ∂ÂêéËÄÉÂèñOCPÊù•‰∫Ü‰∏äÊµ∑ÔºåÁ´ãÂøóË¶ÅÂÅö‰∏ÄÂêçDBA„ÄÇÂè™ËÆ∞ÂæóÂΩìÂπ¥ËØïÁî®ÊúüÂàöËøáÊó∂ÔºåÈò¥Â∑ÆÈò≥ÈîôËΩÆÂà∞ÊàëË¥üË¥£ÂÖ¨Âè∏ÁöÑÂ§ßÊï∞ÊçÆÂπ≥Âè∞ËøôÂùóÔºåÂàöÂºÄÂßãÂæàÁóõËã¶Ôºå‰∏Ä‰∏™ÈôåÁîüÁöÑË°å‰∏öÔºå‰∏Ä‰∏™ËÆ®ËÆ∫ÁöÑÂ∞è‰ºô‰º¥ÈÉΩÊ≤°ÊúâÔºå‰∏Ä‰ªΩÁé∞ÊàêËµÑÊñôÈÉΩÊ≤°ÊúâÔºåÂøÉÊÉÖÁÑ¶Ëôë„ÄÇÂêéÊù•ÊàëË∞ÉÊï¥ÂøÉÊÄÅÔºå‰ªéDBËΩ¨ÁßªÂà∞ÂØπÂ§ßÊï∞ÊçÆÁöÑÁ†îÁ©∂ÔºåÂÜ≥ÂÆöÂïÉ‰∏ãËøôÂùóÁ°¨È™®Â§¥ÔºåÊääÂÆÉÂöºÁ¢éÔºåÊääÂÆÉÊ∂àÂåñÂê∏Êî∂„ÄÇ&#8195;Áî±‰∫éÂΩìÊó∂ÂÖ¨Âè∏ÈÉΩÊòØCDHÁéØÂ¢ÉÔºåÂàöÂºÄÂßãÂÆâË£ÖÂç°‰∫ÜÂæà‰πÖÈÉΩËøá‰∏çÂéªÔºåÂêéÈù¢ÈÄâÊã©Âú®Á∫øÂÆâË£ÖÔºåÂæàÊÖ¢ÔºåÊúâÊó∂ÈúÄË¶Å1Â§©„ÄÇÂêéÊù•ÂÆâË£ÖHDFS ,YARN,HIVEÁªÑ‰ª∂Ôºå‰∏çËøáÂØπÂÆÉ‰ª¨‰∏çÁêÜËß£Ôºå‰∏çÊòéÁôΩÔºåÊúâÊó∂ÂæàÂõ∞ÊÉë„ÄÇËøôÊ†∑ÁöÑËøáÁ®ãÂ§ßÊ¶ÇÊåÅÁª≠‰∏â‰∏™Êúà‰∫Ü„ÄÇ&#8195;ÂêéÊù•Áúã‰∫ÜÂæàÂ§öÂçöÊñáÔºåÈÉΩÊòØApache HadoopÁâàÊú¨Êê≠Âª∫Ôºå‰∫éÊòØÊàëÂÖàËØïËØïÁî®Apache HadoopÊê≠Âª∫ÈÉ®ÁΩ≤ÂçïËäÇÁÇπÂíåÈõÜÁæ§ÔºåÁÑ∂ÂêéÈÖçÁΩÆHAÔºåÊúÄÂêéÊàëÂèëÁé∞Ëá™Â∑±ÊØîËæÉÂñúÊ¨¢ËøôÁßçÊñπÂºèÔºåÂõ†‰∏∫ÊàëËÉΩ‰∫ÜËß£ÂÖ∂ÈÖçÁΩÆÂèÇÊï∞ÔºåÈÖçÁΩÆÊñá‰ª∂ÂíåÂ∏∏ËßÑÂëΩ‰ª§Á≠âÁ≠âÔºåÂÜçÂõûÂ§¥ÂéªÂØπÊØîCDHÂÆâË£ÖHDFSÊúçÂä°ÔºåÁúüÊòØÂ§™ÁàΩ‰∫ÜÔºåÂõ†‰∏∫Apache HadoopÁâàÊú¨ÊúâÁúüÊ≠£‰ΩìÈ™åÊÑüÔºåËøôÊó∂ÊàëÂ∞±ËøÖÈÄüË∞ÉÊï¥ÊñπÂêë : ÂÖàApacheÁâàÊú¨ÔºåÂÜçCDH„ÄÇ&#8195;Áî±‰∫éÂÖ¨Âè∏È°πÁõÆÁéØÂ¢ÉÔºåÊé®ËøõËá™Â∑±ÂÆûÂú®Â§™ÊÖ¢Ôºå‰∫éÊòØÊàëÂú®ÁΩë‰∏äÁúãÂêÑÁßçÁõ∏ÂÖ≥ËßÜÈ¢ëÊïôÁ®ãÔºõÂä†nÁßçÁæ§ÔºåÂú®Áæ§ÈáåÊΩúÊ∞¥ÔºåÁúãÊ∞¥Âèã‰ª¨ÊèêÁöÑÈóÆÈ¢òËá™Â∑±‰ºö‰∏ç‰ºöÔºå‰∏ç‰ºöÂ∞±ÂéªÊü•ËµÑÊñôÔºå‰ºöÂ∞±Â∏ÆÂä©‰ªñ‰ª¨‰∏ÄËµ∑Á†îÁ©∂Â≠¶‰π†ËøõÊ≠•„ÄÇ&#8195;ÂêéÊù•ËøôÊ†∑ÁöÑËøõÂ∫¶Â§™ÊÖ¢‰∫ÜÔºåÂõ†‰∏∫ÂæàÂ§öÁæ§ÈÉΩÊòØÊâìÂπøÂëäÔºåÊΩúÊ∞¥ÔºåÊ≤°ÊúâÁúüÊ≠£ÁöÑÊäÄÊúØËÆ®ËÆ∫Ê∞õÂõ¥Ôºå‰∫éÊòØÊàëËøÖÈÄüË∞ÉÊï¥ÊñπÂêëÔºåËá™Â∑±Âª∫‰∏™QQÁæ§ÔºåÊÖ¢ÊÖ¢ÊãõÂÖµ‰π∞È©¨ÔºåÂíåÁÆ°ÁêÜÂëò‰ª¨‰∏ÄËµ∑ÂéªÁÆ°ÁêÜÔºåÂú®ËøáÂéªÁöÑ‰∏§Âπ¥ÈáåÊàë‰πüÂ≠¶Âà∞‰∫ÜÂæàÂ§öÁü•ËØÜÂíåËÆ§ËØÜÂíåÊàë‰∏ÄÊ†∑ÂâçËøõÁöÑÂ∞è‰ºô‰º¥‰ª¨ÔºåÁé∞Âú®‰πüÊúâÂæàÂ§öÂ∑≤Êàê‰∏∫friends„ÄÇ&#8195;ÊØèÂΩìÂ§úÊôöÔºåÊàëÂ∞±‰ºöÊ∑±Ê∑±ÊÄùËÄÉ‰ªÖÂá≠ÂÖ¨Âè∏È°πÁõÆ,ÁΩë‰∏äÂÖçË¥πËØæÁ®ãËßÜÈ¢ëÔºåQQÁæ§Á≠âÔºåËøòÊòØ‰∏çÂ§üÁöÑÔºå‰∫éÊòØÊàëÂºÄÂßãÂí®ËØ¢ÂüπËÆ≠Êú∫ÊûÑÁöÑËØæÁ®ãÔºåÂú®ËøôÈáåÊèêÈÜíÂêÑ‰ΩçÂ∞è‰ºô‰º¥‰ª¨ÔºåÊä•Áè≠‰∏ÄÂÆöË¶ÅÊì¶‰∫ÆÁúºÁùõÔºåÈÄâÊã©ËÄÅÂ∏àÂæàÈáçË¶ÅÔºåÁúüÂøÉÂæàÈáçË¶ÅÔºåËÆ∏Â§öÂüπËÆ≠Êú∫ÊûÑÁöÑËÄÅÂ∏àÈÉΩÊòØJavaËΩ¨ÁöÑÔºåËÆ≤ÁöÑÊòØÂÖ®ÊòØÂü∫Á°ÄÔºåÊ†πÊú¨Ê≤°Êúâ‰ºÅ‰∏öÈ°πÁõÆÂÆûÊàòÁªèÈ™åÔºõËøòÊúâ‰∏çË¶ÅË∑üÈ£éÔºå‰∏ÄÂÆöÁúã‰ªîÁªÜÁúãÊ∏ÖÊ•öËØæÁ®ãÊòØÂê¶Á¨¶ÂêàÂΩìÂâçÁöÑ‰Ω†„ÄÇ&#8195;ËøôÊó∂ËøòÊòØËøúËøú‰∏çÂ§üÁöÑÔºå‰∫éÊòØÊàëÂºÄÂßãÊØèÂ§©‰∏ä‰∏ãÁè≠Âú∞ÈìÅ‰∏äÁúãÊäÄÊúØÂçöÂÆ¢ÔºåÁßØÊûÅÂàÜ‰∫´„ÄÇÁÑ∂ÂêéÂÜçÁî≥ËØ∑ÂçöÂÆ¢ÔºåÂÜôÂçöÊñáÔºåÂÜôÊÄªÁªìÔºåÂùöÊåÅÊØèÊ¨°ÂÅöÂÆå‰∏ÄÊ¨°ÂÆûÈ™åÂ∞±Â∞ÜÂçöÊñáÔºåÊ¢≥ÁêÜÂ•ΩÔºåÂÜôÂ•ΩÔºåËøôÊ†∑‰πÖËÄå‰πÖ‰πãÔºåÁü•ËØÜÁÇπÂ∞±ÊÖ¢ÊÖ¢Â§ØÂÆûÁßØÁ¥Ø‰∫Ü„ÄÇ&#8195;ÂÜçÁùÄÂêéÈù¢Â∞±ÂºÄÂßãÂèóÈÇÄÂá†Â§ßÂüπËÆ≠Êú∫ÊûÑÂÅöÂÖ¨ÂºÄËØæÔºåÂÜç‰∏ÄÊ¨°Â∞ÜÁü•ËØÜÁÇπÊ¢≥ÁêÜ‰∫ÜÔºå‰πüËÆ§ËØÜ‰∫ÜÊñ∞ÁöÑÂ∞è‰ºô‰º¥‰ª¨ÔºåÊàë‰ª¨ÊúâÁùÄÁõ∏ÂêåÁöÑÊñπÂêëÂíåÁõÆÊ†áÔºåÊàë‰ª¨Â∞ΩÊÉÖÁöÑËÆ®ËÆ∫ÁùÄÂ§ßÊï∞ÊçÆÁöÑÁü•ËØÜÁÇπÔºåÊÖ¢ÊÖ¢ÊúùÁùÄÊàë‰ª¨ÂøÉÁõÆ‰∏≠ÁöÑÁõÆÊ†áËÄåÂä™ÂäõÁùÄÔºÅ‰ª•‰∏äÂü∫Êú¨Â∞±ÊòØÊàëÂíåÂ§ßÊï∞ÊçÆÁöÑÊÉÖÁºòÔºå‰∏ãÈù¢ÊàëÊù•Ë∞àË∞àÊàëÂØπÂ§ßÊï∞ÊçÆÂÖ•Èó®ÁöÑÊÑüÊÇü„ÄÇ1. ÂøÉÊÄÅË¶ÅÁ´ØÊ≠£„ÄÇÊó¢ÁÑ∂ÊÉ≥Ë¶Å‰ªé‰∫ãËøôË°åÔºåÈÇ£‰πà‰∏ÄÂÆöË¶Å‰∏ãÂÆöÂÜ≥ÂøÉÔºåÂΩìÁÑ∂‰ªòÂá∫ÊòØËÇØÂÆöÂ§ßÂ§ßÁöÑÔºå‰∏çÂÖâÂÖâÊòØÊØõÁà∑Áà∑ÔºåËÄåÊõ¥Â§öÁöÑ‰ªòÂá∫ÊòØËá™Â∑±ÁöÑÈÇ£‰∏Ä‰ªΩÂùöÊåÅÔºåÂá°‰∫ãË¥µÂú®ÂùöÊåÅÔºåÁúüÁúü‰ΩìÁé∞Âú®ËøôÈáå„ÄÇÂêéÊù•ÊàëÂ∞ÜÊàëËÄÅÂ©Ü‰ªéÂåñÂ∑•ÂÆûÈ™åÂÆ§ÂàÜÊûêÂëòËΩ¨Ë°åÔºåÂÅöPythonÁà¨Ëô´ÂíåÊï∞ÊçÆÂàÜÊûêÔºåÂΩìÁÑ∂Ëøô‰∏™‰∏ªË¶ÅËøòÊòØÈù†Â•πÁöÑÈÇ£‰ªΩÂùöÊåÅ„ÄÇ2. ÂøÉÁõÆ‰∏≠Ë¶ÅÊúâËÆ°Âàí„ÄÇÂÖàÂ≠¶‰π†LinuxÂíåShellÔºåÂÜçÂ≠¶‰π†Êï∞ÊçÆÂ∫ìÂíåSQLÔºåÂÜçÂ≠¶‰π†JavaÂíåScalaÔºåÁÑ∂ÂêéÂ≠¶‰π†Apache Haoop„ÄÅHive„ÄÅKafka„ÄÅSparkÔºåÊúùÂ§ßÊï∞ÊçÆÁ†îÂèëÊàñÂºÄÂèëËÄåÂä™ÂäõÁùÄ„ÄÇ3. ÂêÑÁßçÊñπÂºèÂ≠¶‰π†„ÄÇQQÁæ§ÔºåÂçöÂÆ¢Ôºå‰∏ä‰∏ãÁè≠ÁúãÊäÄÊúØÊñáÁ´†ÔºåÈÄâÊã©Â•ΩÁöÑËÄÅÂ∏àÂíåËØæÁ®ãÂüπËÆ≠Ôºå(Êì¶‰∫ÆÁúºÁùõÔºåÂæàÂ§öËßÜÈ¢ëÔºåÂæàÂ§öÂ§ßÊï∞ÊçÆËÄÅÂ∏àÈÉΩÊòØÁûéÊâØÁöÑÔºåÊúÄÁªàÊÄªÁªì‰∏ÄÂè•ËØùÔºå‰∏çÂú®‰ºÅ‰∏ö‰∏äÁè≠ÁöÑÊïôÂ§ßÊï∞ÊçÆÈÉΩÊòØËÄçÊµÅÊ∞ìÁöÑ„ÄÇ)ÂèØ‰ª•Âä†ÈÄüËá™Â∑±ÂâçËøõÁöÑÈ©¨ÊãâÊùæÈáåÁ®ãÔºåÂÖ∂ÂÆû‰∏ÄËà¨ÈÉΩË¶ÅÁúãÂ§ßÂÆ∂ÊÄé‰πàË°°ÈáèÂüπËÆ≠Ëøô‰∏™‰∫ãÁöÑÔºåtimeÂíåmoneyÁöÑÊäâÊã©Ôºå‰ª•ÂèäÂø´ÈÄüjumpÂêéÁöÑÈ´òËñ™„ÄÇ4. È°πÁõÆÁªèÈ™å„ÄÇÂæàÂ§öÂ∞èÁôΩÈÉΩÊ≤°ÊúâÈ°πÁõÆÁªèÈ™å‰πüÊ≤°ÊúâÈù¢ËØïÁªèÈ™åÂíåÊäÄÂ∑ßÔºåÂ±°Â±°Èù¢ËØï‰ª•Â§±Ë¥•ÂëäÁªàÔºåËøôÊó∂Â§ßÂÆ∂ÂèØ‰ª•Êâæ‰Ω†‰ª¨ÁÜüÊÇâÁöÑÂ∞è‰ºô‰º¥‰ª¨ÁöÑÔºåËÆ©‰ªñÁªô‰Ω†ÂüπËÆ≠‰ªñÁöÑÈ°πÁõÆÔºåËøôÊ†∑Â∞±Êúâ‰∫ÜÔºåÂΩìÁÑ∂ÂèØ‰ª•Áõ¥Êé•‰∫íËÅîÁΩëÊêúÁ¥¢‰∏Ä‰∏™Â∞±Ë°åÔºå‰∏çËøá‰∏ÄËà¨ÂæàÈöæÊúâÂÆåÊï¥ÁöÑ„ÄÇËÄåÈù¢ËØïÔºåÂ∞±ÁúãÁúãÂÖ∂‰ªñ‰∫∫Èù¢ËØïÂàÜ‰∫´ÔºåÂ≠¶‰π†‰ªñ‰∫∫„ÄÇÊúÄÂêéÔºåÊÄªÁªì‰∏ÄÂè•ËØùÔºåÂùöÊåÅÊâçÊòØÊúÄÈáçË¶ÅÁöÑ„ÄÇÊúÄÂêéÔºåÊÄªÁªì‰∏ÄÂè•ËØùÔºåÂùöÊåÅÊâçÊòØÊúÄÈáçË¶ÅÁöÑ„ÄÇÊúÄÂêéÔºåÊÄªÁªì‰∏ÄÂè•ËØùÔºåÂùöÊåÅÊâçÊòØÊúÄÈáçË¶ÅÁöÑ„ÄÇ]]></content>
      <categories>
        <category>ÊÑüÊÉ≥</category>
      </categories>
      <tags>
        <tag>‰∫∫ÁîüÊÑüÊÇü</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2minÂø´ÈÄü‰∫ÜËß£ÔºåHiveÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°®]]></title>
    <url>%2F2018%2F05%2F01%2F2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Âú®‰∫ÜËß£ÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°®Âå∫Âà´ÂâçÔºåÊàë‰ª¨ÈúÄË¶ÅÂÖà‰∫ÜËß£‰∏Ä‰∏ãHiveÊû∂ÊûÑ ÔºöÂ§ßÂÆ∂ÂèØ‰ª•ÁÆÄÂçïÁúã‰∏Ä‰∏ãËøô‰∏™Êû∂ÊûÑÂõæÔºåÊàë‰ªãÁªçÂÖ∂‰∏≠Ë¶ÅÁÇπÔºöHiveÁöÑÊï∞ÊçÆÂàÜ‰∏∫‰∏§ÁßçÔºå‰∏ÄÁßç‰∏∫ÊôÆÈÄöÊï∞ÊçÆÔºå‰∏ÄÁßç‰∏∫ÂÖÉÊï∞ÊçÆ„ÄÇÂÖÉÊï∞ÊçÆÂ≠òÂÇ®ÁùÄË°®ÁöÑÂü∫Êú¨‰ø°ÊÅØÔºåÂ¢ûÂà†ÊîπÊü•ËÆ∞ÂΩïÔºåÁ±ª‰ºº‰∫éHadoopÊû∂ÊûÑ‰∏≠ÁöÑnamespace„ÄÇÊôÆÈÄöÊï∞ÊçÆÂ∞±ÊòØË°®‰∏≠ÁöÑËØ¶ÁªÜÊï∞ÊçÆ„ÄÇHiveÁöÑÂÖÉÊï∞ÊçÆÈªòËÆ§Â≠òÂÇ®Âú®derby‰∏≠Ôºå‰ΩÜÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÂ≠òÂÇ®Âú®MySQL‰∏≠„ÄÇÊôÆÈÄöÊï∞ÊçÆÂ¶ÇÊû∂ÊûÑÂõæÊâÄÁ§∫Â≠òÂÇ®Âú®hdfs‰∏≠„ÄÇ‰∏ãÈù¢Êàë‰ª¨Êù•‰ªãÁªçË°®ÁöÑ‰∏§ÁßçÁ±ªÂûãÔºöÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°®ÂÜÖÈÉ®Ë°®ÔºàMANAGEDÔºâÔºöhiveÂú®hdfs‰∏≠Â≠òÂú®ÈªòËÆ§ÁöÑÂ≠òÂÇ®Ë∑ØÂæÑÔºåÂç≥defaultÊï∞ÊçÆÂ∫ì„ÄÇ‰πãÂêéÂàõÂª∫ÁöÑÊï∞ÊçÆÂ∫ìÂèäË°®ÔºåÂ¶ÇÊûúÊ≤°ÊúâÊåáÂÆöË∑ØÂæÑÂ∫îÈÉΩÂú®/user/hive/warehouse‰∏ãÔºåÊâÄ‰ª•Âú®ËØ•Ë∑ØÂæÑ‰∏ãÁöÑË°®‰∏∫ÂÜÖÈÉ®Ë°®„ÄÇÂ§ñÈÉ®Ë°®ÔºàEXTERNALÔºâÔºöÊåáÂÆö‰∫Ü/user/hive/warehouse‰ª•Â§ñË∑ØÂæÑÊâÄÂàõÂª∫ÁöÑË°®ËÄåÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°®ÁöÑ‰∏ªË¶ÅÂå∫Âà´Â∞±ÊòØÂÜÖÈÉ®Ë°®ÔºöÂΩìÂà†Èô§ÂÜÖÈÉ®Ë°®Êó∂ÔºåMySQLÁöÑÂÖÉÊï∞ÊçÆÂíåHDFS‰∏äÁöÑÊôÆÈÄöÊï∞ÊçÆÈÉΩ‰ºöÂà†Èô§ ÔºõÂ§ñÈÉ®Ë°®ÔºöÂΩìÂà†Èô§Â§ñÈÉ®Ë°®Êó∂ÔºåMySQLÁöÑÂÖÉÊï∞ÊçÆ‰ºöË¢´Âà†Èô§ÔºåHDFS‰∏äÁöÑÊï∞ÊçÆ‰∏ç‰ºöË¢´Âà†Èô§Ôºõ1.ÂáÜÂ§áÊï∞ÊçÆ: ÊåâtabÈîÆÂà∂Ë°®Á¨¶‰Ωú‰∏∫Â≠óÊÆµÂàÜÂâ≤Á¨¶cat /tmp/ruozedata.txt 1 jepson 32 110 2 ruoze 22 112 3 www.ruozedata.com 18 120 2.ÂÜÖÈÉ®Ë°®ÊµãËØïÔºöÂú®HiveÈáåÈù¢ÂàõÂª∫‰∏Ä‰∏™Ë°®Ôºö123456789hive&gt; create table ruozedata(id int, &gt; name string, &gt; age int, &gt; tele string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\t&apos; &gt; STORED AS TEXTFILE;OKTime taken: 0.759 secondsËøôÊ†∑Êàë‰ª¨Â∞±Âú®HiveÈáåÈù¢ÂàõÂª∫‰∫Ü‰∏ÄÂº†ÊôÆÈÄöÁöÑË°®ÔºåÁé∞Âú®ÁªôËøô‰∏™Ë°®ÂØºÂÖ•Êï∞ÊçÆÔºö1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;ÂÜÖÈÉ®Ë°®Âà†Èô§1hive&gt; drop table ruozedata;3.Â§ñÈÉ®Ë°®ÊµãËØï:ÂàõÂª∫Â§ñÈÉ®Ë°®Â§ö‰∫ÜexternalÂÖ≥ÈîÆÂ≠óËØ¥Êòé‰ª•Âèähdfs‰∏älocation ‚Äò/hive/external‚Äô12345678hive&gt; create external table exter_ruozedata( &gt; id int, &gt; name string, &gt; age int, &gt; tel string) &gt; location &apos;/hive/external&apos;;OKTime taken: 0.098 secondsÂàõÂª∫Â§ñÈÉ®Ë°®ÔºåÈúÄË¶ÅÂú®ÂàõÂª∫Ë°®ÁöÑÊó∂ÂÄôÂä†‰∏äexternalÂÖ≥ÈîÆÂ≠óÔºåÂêåÊó∂ÊåáÂÆöÂ§ñÈÉ®Ë°®Â≠òÊîæÊï∞ÊçÆÁöÑË∑ØÂæÑÔºàÂΩìÁÑ∂Ôºå‰Ω†‰πüÂèØ‰ª•‰∏çÊåáÂÆöÂ§ñÈÉ®Ë°®ÁöÑÂ≠òÊîæË∑ØÂæÑÔºåËøôÊ†∑HiveÂ∞Ü Âú®HDFS‰∏äÁöÑ/user/hive/warehouse/Êñá‰ª∂Â§π‰∏ã‰ª•Â§ñÈÉ®Ë°®ÁöÑË°®ÂêçÂàõÂª∫‰∏Ä‰∏™Êñá‰ª∂Â§πÔºåÂπ∂Â∞ÜÂ±û‰∫éËøô‰∏™Ë°®ÁöÑÊï∞ÊçÆÂ≠òÊîæÂú®ËøôÈáåÔºâÂ§ñÈÉ®Ë°®ÂØºÂÖ•Êï∞ÊçÆÂíåÂÜÖÈÉ®Ë°®‰∏ÄÊ†∑Ôºö1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;Âà†Èô§Â§ñÈÉ®Ë°®1hive&gt; drop table exter_ruozedata;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveËá™ÂÆö‰πâÂáΩÊï∞(UDF)ÁöÑÈÉ®ÁΩ≤‰ΩøÁî®Ôºå‰Ω†‰ºöÂêóÔºü]]></title>
    <url>%2F2018%2F04%2F27%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[HiveËá™ÂÆö‰πâÂáΩÊï∞(UDF)ÁöÑÈÉ®ÁΩ≤‰ΩøÁî®Ôºå‰Ω†‰ºöÂêóÔºå‰∏âÁßçÊñπÂºèÔºÅ‰∏Ä.‰∏¥Êó∂ÂáΩÊï∞ideaÁºñÂÜôudfÊâìÂåÖMaven Projects ‚Äî-&gt;Lifecycle ‚Äî-&gt;package ‚Äî-&gt; Âè≥Âáª Run Maven Buildrz‰∏ä‰º†Ëá≥ÊúçÂä°Âô®Ê∑ªÂä†jarÂåÖhive&gt;add xxx.jar jar_filepath;Êü•ÁúãjarÂåÖhive&gt;list jars;ÂàõÂª∫‰∏¥Êó∂ÂáΩÊï∞hive&gt;create temporary function my_lower as ‚Äòcom.example.hive.udf.Lower‚Äô;‰∫å.ÊåÅ‰πÖÂáΩÊï∞ideaÁºñÂÜôudfÊâìÂåÖMaven Projects ‚Äî-&gt;Lifecycle ‚Äî-&gt;package ‚Äî-&gt; Âè≥Âáª Run Maven Buildrz‰∏ä‰º†Ëá≥ÊúçÂä°Âô®‰∏ä‰º†Âà∞HDFS$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jarÂàõÂª∫ÊåÅ‰πÖÂáΩÊï∞hive&gt;CREATE FUNCTION myfunc AS ‚Äòmyclass‚Äô USING JAR ‚Äòhdfs:///path/to/xxx.jar‚Äô;Ê≥®ÊÑèÁÇπÔºöÊ≠§ÊñπÊ≥ïÂú®show functionsÊó∂ÊòØÁúã‰∏çÂà∞ÁöÑÔºå‰ΩÜÊòØÂèØ‰ª•‰ΩøÁî®ÈúÄË¶Å‰∏ä‰º†Ëá≥hdfs‰∏â.ÊåÅ‰πÖÂáΩÊï∞ÔºåÂπ∂Ê≥®ÂÜåÁéØÂ¢É‰ªãÁªçÔºöCentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9‰∏ãËΩΩÊ∫êÁ†Åhive-1.1.0-cdh5.7.0-src.tar.gzhttp://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gzËß£ÂéãÊ∫êÁ†Åtar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/cd /home/hadoop/hive-1.1.0-cdh5.7.0Â∞ÜHelloUDF.javaÊñá‰ª∂Â¢ûÂä†Âà∞HIVEÊ∫êÁ†Å‰∏≠cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/‰øÆÊîπFunctionRegistry.java Êñá‰ª∂1234cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/vi FunctionRegistry.javaÂú®import‰∏≠Â¢ûÂä†Ôºöimport org.apache.hadoop.hive.ql.udf.HelloUDF;Âú®Êñá‰ª∂Â§¥ÈÉ® static Âùó‰∏≠Ê∑ªÂä†Ôºösystem.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);ÈáçÊñ∞ÁºñËØëcd /home/hadoop/hive-1.1.0-cdh5.7.0mvn clean package -DskipTests -Phadoop-2 -PdistÁºñËØëÁªìÊûúÂÖ®ÈÉ®‰∏∫ÔºöBUILD SUCCESSÊñá‰ª∂ÊâÄÂú®ÁõÆÂΩïÔºö/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/targetÈÖçÁΩÆhiveÁéØÂ¢ÉÈÖçÁΩÆhiveÁéØÂ¢ÉÊó∂ÔºåÂèØ‰ª•ÂÖ®Êñ∞ÈÖçÁΩÆÊàñÂ∞ÜÁºñËØëÂêéÂ∏¶UDFÂáΩÊï∞ÁöÑÂåÖÂ§çÂà∂Âà∞ÊóßhiveÁéØÂ¢É‰∏≠Ôºö7.1. ÂÖ®ÈÉ®ÈÖçÁΩÆÔºöÂèÇÁÖß‰πãÂâçÊñáÊ°£ HiveÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑÁºñËØëÂèäÈÉ®ÁΩ≤7.2. Â∞ÜÁºñËØëÂêéÂ∏¶UDFÂáΩÊï∞ÁöÑÂåÖÂ§çÂà∂Âà∞ÊóßhiveÁéØÂ¢ÉÂà∞/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib‰∏ãÔºåÊâæÂà∞hive-exec-1.1.0-cdh5.7.0.jarÂåÖÔºåÂπ∂Â∞ÜÊóßÁéØÂ¢É‰∏≠ÂØπÁÖßÁöÑÂåÖÊõøÊç¢ÊéâÂëΩ‰ª§Ôºö1234cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/libmv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bakcd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libcp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/libÊúÄÁªàÂêØÂä®hiveÊµãËØïÔºö123hivehive (default)&gt; show functions ; -- ËÉΩÊü•ÁúãÂà∞Êúâ helloudfhive(default)&gt;select deptno,dname,helloudf(dname) from dept; -- helloudfÂáΩÊï∞ÁîüÊïà]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveËá™ÂÆö‰πâÂáΩÊï∞(UDF)ÁöÑÁºñÁ®ãÂºÄÂèëÔºå‰Ω†‰ºöÂêóÔºü]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Êú¨Âú∞ÂºÄÂèëÁéØÂ¢ÉÔºöIntelliJ IDEA+Maven3.3.91. ÂàõÂª∫Â∑•Á®ãÊâìÂºÄIntelliJ IDEAFile‚Äì&gt;New‚Äì&gt;Project‚Ä¶‚Äì&gt;MavenÈÄâÊã©Create from archetye‚Äì&gt;org.apache.maven.archety:maven-archetype-quitkstart2. ÈÖçÁΩÆÂú®Â∑•Á®ã‰∏≠ÊâæÂà∞pom.xmlÊñá‰ª∂ÔºåÊ∑ªÂä†hadoop„ÄÅhive‰æùËµñ3. ÂàõÂª∫Á±ª„ÄÅÂπ∂ÁºñÂÜô‰∏Ä‰∏™HelloUDF.javaÔºå‰ª£Á†ÅÂ¶Ç‰∏ãÔºöÈ¶ñÂÖà‰∏Ä‰∏™UDFÂøÖÈ°ªÊª°Ë∂≥‰∏ãÈù¢‰∏§‰∏™Êù°‰ª∂:‰∏Ä‰∏™UDFÂøÖÈ°ªÊòØorg.apache.hadoop.hive.ql.exec.UDFÁöÑÂ≠êÁ±ªÔºàÊç¢Âè•ËØùËØ¥Â∞±ÊòØÊàë‰ª¨‰∏ÄËà¨ÈÉΩÊòØÂéªÁªßÊâøËøô‰∏™Á±ªÔºâ‰∏Ä‰∏™UDFÂøÖÈ°ªËá≥Â∞ëÂÆûÁé∞‰∫Üevaluate()ÊñπÊ≥ï4. ÊµãËØïÔºåÂè≥ÂáªËøêË°årun ‚ÄòHelloUDF.main()‚Äô5. ÊâìÂåÖÂú®IDEAËèúÂçï‰∏≠ÈÄâÊã©view‚Äì&gt;Tool Windows‚Äì&gt;Maven ProjectsÔºåÁÑ∂ÂêéÂú®Maven ProjectsÁ™óÂè£‰∏≠ÈÄâÊã©„ÄêÂ∑•Á®ãÂêç„Äë‚Äì&gt;Lifecycle‚Äì&gt;packageÔºåÂú®package‰∏≠Âè≥ÈîÆÈÄâÊã©Run Maven BuildÂºÄÂßãÊâìÂåÖÊâßË°åÊàêÂäüÂêéÂú®Êó•Âøó‰∏≠ÊâæÔºö[INFO] Building jar: (Ë∑ØÂæÑ)/hive-1.0.jar]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDLÔºå‰Ω†ÁúüÁöÑ‰∫ÜËß£ÂêóÔºü]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåÂ∏¶‰Ω†ÂÖ®Èù¢ÂâñÊûêHive DDLÔºÅÊ¶ÇÂøµDatabaseHive‰∏≠ÂåÖÂê´‰∫ÜÂ§ö‰∏™Êï∞ÊçÆÂ∫ìÔºåÈªòËÆ§ÁöÑÊï∞ÊçÆÂ∫ì‰∏∫defaultÔºåÂØπÂ∫î‰∫éHDFSÁõÆÂΩïÊòØ/user/hadoop/hive/warehouseÔºåÂèØ‰ª•ÈÄöËøáhive.metastore.warehouse.dirÂèÇÊï∞ËøõË°åÈÖçÁΩÆÔºàhive-site.xml‰∏≠ÈÖçÁΩÆÔºâTableHive‰∏≠ÁöÑË°®ÂèàÂàÜ‰∏∫ÂÜÖÈÉ®Ë°®ÂíåÂ§ñÈÉ®Ë°® ,Hive ‰∏≠ÁöÑÊØèÂº†Ë°®ÂØπÂ∫î‰∫éHDFS‰∏äÁöÑ‰∏Ä‰∏™ÁõÆÂΩïÔºåHDFSÁõÆÂΩï‰∏∫Ôºö/user/hadoop/hive/warehouse/[databasename.db]/tablePartitionÂàÜÂå∫ÔºåÊØèÂº†Ë°®‰∏≠ÂèØ‰ª•Âä†ÂÖ•‰∏Ä‰∏™ÂàÜÂå∫ÊàñËÄÖÂ§ö‰∏™ÔºåÊñπ‰æøÊü•ËØ¢ÔºåÊèêÈ´òÊïàÁéáÔºõÂπ∂‰∏îHDFS‰∏ä‰ºöÊúâÂØπÂ∫îÁöÑÂàÜÂå∫ÁõÆÂΩïÔºö/user/hadoop/hive/warehouse/[databasename.db]/tableDDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];IF NOT EXISTSÔºöÂä†‰∏äËøôÂè•ËØù‰ª£Ë°®Âà§Êñ≠Êï∞ÊçÆÂ∫ìÊòØÂê¶Â≠òÂú®Ôºå‰∏çÂ≠òÂú®Â∞±‰ºöÂàõÂª∫ÔºåÂ≠òÂú®Â∞±‰∏ç‰ºöÂàõÂª∫„ÄÇCOMMENTÔºöÊï∞ÊçÆÂ∫ìÁöÑÊèèËø∞LOCATIONÔºöÂàõÂª∫Êï∞ÊçÆÂ∫ìÁöÑÂú∞ÂùÄÔºå‰∏çÂä†ÈªòËÆ§Âú®/user/hive/warehouse/Ë∑ØÂæÑ‰∏ãWITH DBPROPERTIESÔºöÊï∞ÊçÆÂ∫ìÁöÑÂ±ûÊÄßDrop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICTÔºöÈªòËÆ§ÊòØrestrictÔºåÂ¶ÇÊûúËØ•Êï∞ÊçÆÂ∫ìËøòÊúâË°®Â≠òÂú®ÂàôÊä•ÈîôÔºõCASCADEÔºöÁ∫ßËÅîÂà†Èô§Êï∞ÊçÆÂ∫ì(ÂΩìÊï∞ÊçÆÂ∫ìËøòÊúâË°®Êó∂ÔºåÁ∫ßËÅîÂà†Èô§Ë°®ÂêéÂú®Âà†Èô§Êï∞ÊçÆÂ∫ì)„ÄÇAlter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)Use Database12USE database_name;USE DEFAULT;Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;‚Äú | ‚ÄùÔºöÂèØ‰ª•ÈÄâÊã©ÂÖ∂‰∏≠‰∏ÄÁßç‚Äú[ ]‚ÄùÔºöÂèØÈÄâÈ°πLIKE ‚Äòidentifier_with_wildcards‚ÄôÔºöÊ®°Á≥äÊü•ËØ¢Êï∞ÊçÆÂ∫ìDescribe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_nameÔºöÊü•ÁúãÊï∞ÊçÆÂ∫ìÁöÑÊèèËø∞‰ø°ÊÅØÂíåÊñá‰ª∂ÁõÆÂΩï‰ΩçÁΩÆË∑ØÂæÑ‰ø°ÊÅØÔºõEXTENDEDÔºöÂä†‰∏äÊï∞ÊçÆÂ∫ìÈîÆÂÄºÂØπÁöÑÂ±ûÊÄß‰ø°ÊÅØ„ÄÇhive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s)Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type1: ARRAY &lt; data_type &gt;map_type1: MAP &lt; primitive_type, data_type &gt;struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameconstraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARYÔºà‰∏¥Êó∂Ë°®ÔºâHive‰ªé0.14.0ÂºÄÂßãÊèê‰æõÂàõÂª∫‰∏¥Êó∂Ë°®ÁöÑÂäüËÉΩÔºåË°®Âè™ÂØπÂΩìÂâçsessionÊúâÊïàÔºåsessionÈÄÄÂá∫ÂêéÔºåË°®Ëá™Âä®Âà†Èô§„ÄÇËØ≠Ê≥ïÔºöCREATE TEMPORARY TABLE ‚Ä¶Ê≥®ÊÑèÔºöÂ¶ÇÊûúÂàõÂª∫ÁöÑ‰∏¥Êó∂Ë°®Ë°®ÂêçÂ∑≤Â≠òÂú®ÔºåÈÇ£‰πàÂΩìÂâçsessionÂºïÁî®Âà∞ËØ•Ë°®ÂêçÊó∂ÂÆûÈôÖÁî®ÁöÑÊòØ‰∏¥Êó∂Ë°®ÔºåÂè™ÊúâdropÊàñrename‰∏¥Êó∂Ë°®ÂêçÊâçËÉΩ‰ΩøÁî®ÂéüÂßãË°®‰∏¥Êó∂Ë°®ÈôêÂà∂Ôºö‰∏çÊîØÊåÅÂàÜÂå∫Â≠óÊÆµÂíåÂàõÂª∫Á¥¢ÂºïEXTERNALÔºàÂ§ñÈÉ®Ë°®ÔºâHive‰∏äÊúâ‰∏§ÁßçÁ±ªÂûãÁöÑË°®Ôºå‰∏ÄÁßçÊòØManaged Table(ÈªòËÆ§ÁöÑ)ÔºåÂè¶‰∏ÄÁßçÊòØExternal TableÔºàÂä†‰∏äEXTERNALÂÖ≥ÈîÆÂ≠óÔºâ„ÄÇÂÆÉ‰ø©ÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÔºöÂΩìÊàë‰ª¨dropË°®Êó∂ÔºåManaged Table‰ºöÂêåÊó∂Âà†ÂéªdataÔºàÂ≠òÂÇ®Âú®HDFS‰∏äÔºâÂíåmeta dataÔºàÂ≠òÂÇ®Âú®MySQLÔºâÔºåËÄåExternal TableÂè™‰ºöÂà†meta data„ÄÇ1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; );PARTITIONED BYÔºàÂàÜÂå∫Ë°®Ôºâ‰∫ßÁîüËÉåÊôØÔºöÂ¶ÇÊûú‰∏Ä‰∏™Ë°®‰∏≠Êï∞ÊçÆÂæàÂ§öÔºåÊàë‰ª¨Êü•ËØ¢Êó∂Â∞±ÂæàÊÖ¢ÔºåËÄóË¥πÂ§ßÈáèÊó∂Èó¥ÔºåÂ¶ÇÊûúË¶ÅÊü•ËØ¢ÂÖ∂‰∏≠ÈÉ®ÂàÜÊï∞ÊçÆËØ•ÊÄé‰πàÂäûÂë¢ÔºåËøôÊòØÊàë‰ª¨ÂºïÂÖ•ÂàÜÂå∫ÁöÑÊ¶ÇÂøµ„ÄÇÂèØ‰ª•Ê†πÊçÆPARTITIONED BYÂàõÂª∫ÂàÜÂå∫Ë°®Ôºå‰∏Ä‰∏™Ë°®ÂèØ‰ª•Êã•Êúâ‰∏Ä‰∏™ÊàñËÄÖÂ§ö‰∏™ÂàÜÂå∫ÔºåÊØè‰∏™ÂàÜÂå∫‰ª•Êñá‰ª∂Â§πÁöÑÂΩ¢ÂºèÂçïÁã¨Â≠òÂú®Ë°®Êñá‰ª∂Â§πÁöÑÁõÆÂΩï‰∏ãÔºõÂàÜÂå∫ÊòØ‰ª•Â≠óÊÆµÁöÑÂΩ¢ÂºèÂú®Ë°®ÁªìÊûÑ‰∏≠Â≠òÂú®ÔºåÈÄöËøádescribe tableÂëΩ‰ª§ÂèØ‰ª•Êü•ÁúãÂà∞Â≠óÊÆµÂ≠òÂú®Ôºå‰ΩÜÊòØËØ•Â≠óÊÆµ‰∏çÂ≠òÊîæÂÆûÈôÖÁöÑÊï∞ÊçÆÂÜÖÂÆπÔºå‰ªÖ‰ªÖÊòØÂàÜÂå∫ÁöÑË°®Á§∫„ÄÇÂàÜÂå∫Âª∫Ë°®ÂàÜ‰∏∫2ÁßçÔºå‰∏ÄÁßçÊòØÂçïÂàÜÂå∫Ôºå‰πüÂ∞±ÊòØËØ¥Âú®Ë°®Êñá‰ª∂Â§πÁõÆÂΩï‰∏ãÂè™Êúâ‰∏ÄÁ∫ßÊñá‰ª∂Â§πÁõÆÂΩï„ÄÇÂè¶Â§ñ‰∏ÄÁßçÊòØÂ§öÂàÜÂå∫ÔºåË°®Êñá‰ª∂Â§π‰∏ãÂá∫Áé∞Â§öÊñá‰ª∂Â§πÂµåÂ•óÊ®°Âºè„ÄÇÂçïÂàÜÂå∫Ôºö123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OKÂ§öÂàÜÂå∫Ôºö123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMATÂÆòÁΩëËß£ÈáäÔºö1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]DELIMITEDÔºöÂàÜÈöîÁ¨¶ÔºàÂèØ‰ª•Ëá™ÂÆö‰πâÂàÜÈöîÁ¨¶ÔºâÔºõFIELDS TERMINATED BY char:ÊØè‰∏™Â≠óÊÆµ‰πãÈó¥‰ΩøÁî®ÁöÑÂàÜÂâ≤Ôºõ‰æãÔºö-FIELDS TERMINATED BY ‚Äò\n‚Äô Â≠óÊÆµ‰πãÈó¥ÁöÑÂàÜÈöîÁ¨¶‰∏∫\n;COLLECTION ITEMS TERMINATED BY char:ÈõÜÂêà‰∏≠ÂÖÉÁ¥†‰∏éÂÖÉÁ¥†ÔºàarrayÔºâ‰πãÈó¥‰ΩøÁî®ÁöÑÂàÜÈöîÁ¨¶ÔºàcollectionÂçï‰æãÈõÜÂêàÁöÑË∑üÊé•Âè£ÔºâÔºõMAP KEYS TERMINATED BY charÔºöÂ≠óÊÆµÊòØK-VÂΩ¢ÂºèÊåáÂÆöÁöÑÂàÜÈöîÁ¨¶ÔºõLINES TERMINATED BY charÔºöÊØèÊù°Êï∞ÊçÆ‰πãÈó¥Áî±Êç¢Ë°åÁ¨¶ÂàÜÂâ≤ÔºàÈªòËÆ§[ \n ]Ôºâ‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãLINES TERMINATED BY charÊàë‰ª¨Â∞±‰ΩøÁî®ÈªòËÆ§ÁöÑÊç¢Ë°åÁ¨¶\nÔºåÂè™ÈúÄË¶ÅÊåáÂÆöFIELDS TERMINATED BY char„ÄÇÂàõÂª∫demo1Ë°®ÔºåÂ≠óÊÆµ‰∏éÂ≠óÊÆµ‰πãÈó¥‰ΩøÁî®\tÂàÜÂºÄÔºåÊç¢Ë°åÁ¨¶‰ΩøÁî®ÈªòËÆ§\nÔºö123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OKÂàõÂª∫demo2Ë°®ÔºåÂπ∂ÊåáÂÆöÂÖ∂‰ªñÂ≠óÊÆµÔºö123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED ASÔºàÂ≠òÂÇ®Ê†ºÂºèÔºâCreate Table As SelectÂàõÂª∫Ë°®ÔºàÊã∑Ë¥ùË°®ÁªìÊûÑÂèäÊï∞ÊçÆÔºåÂπ∂‰∏î‰ºöËøêË°åMapReduce‰Ωú‰∏öÔºâ12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;Âä†ËΩΩÊï∞ÊçÆ1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;Â§çÂà∂Êï¥Âº†Ë°®12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt;Â§çÂà∂Ë°®‰∏≠ÁöÑ‰∏Ä‰∫õÂ≠óÊÆµ1create table emp3 as select empno,ename from emp;LIKE‰ΩøÁî®likeÂàõÂª∫Ë°®Êó∂ÔºåÂè™‰ºöÂ§çÂà∂Ë°®ÁöÑÁªìÊûÑÔºå‰∏ç‰ºöÂ§çÂà∂Ë°®ÁöÑÊï∞ÊçÆ1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt;Âπ∂Ê≤°ÊúâÊü•ËØ¢Âà∞Êï∞ÊçÆdesc formatted table_nameÊü•ËØ¢Ë°®ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ12hive&gt; desc formatted emp;OKcol_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno intDetailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt;ÈÄöËøáÊü•ËØ¢ÂèØ‰ª•ÂàóÂá∫ÂàõÂª∫Ë°®Êó∂ÁöÑÊâÄÊúâ‰ø°ÊÅØÔºåÂπ∂‰∏îÊàë‰ª¨ÂèØ‰ª•Âú®mysql‰∏≠Êü•ËØ¢Âá∫Ëøô‰∫õ‰ø°ÊÅØÔºàÂÖÉÊï∞ÊçÆÔºâselect * from table_params;Êü•ËØ¢Êï∞ÊçÆÂ∫ì‰∏ãÁöÑÊâÄÊúâË°®1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt;Êü•ËØ¢ÂàõÂª∫Ë°®ÁöÑËØ≠Ê≥ï123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later)ÊåáÂÆöPURGEÂêéÔºåÊï∞ÊçÆ‰∏ç‰ºöÊîæÂà∞ÂõûÊî∂ÁÆ±Ôºå‰ºöÁõ¥Êé•Âà†Èô§DROP TABLEÂà†Èô§Ê≠§Ë°®ÁöÑÂÖÉÊï∞ÊçÆÂíåÊï∞ÊçÆ„ÄÇÂ¶ÇÊûúÈÖçÁΩÆ‰∫ÜÂûÉÂúæÁÆ±ÔºàÂπ∂‰∏îÊú™ÊåáÂÆöPURGEÔºâÔºåÂàôÂÆûÈôÖÂ∞ÜÊï∞ÊçÆÁßªËá≥.Trash / CurrentÁõÆÂΩï„ÄÇÂÖÉÊï∞ÊçÆÂÆåÂÖ®‰∏¢Â§±Âà†Èô§EXTERNALË°®Êó∂ÔºåË°®‰∏≠ÁöÑÊï∞ÊçÆ‰∏ç‰ºö‰ªéÊñá‰ª∂Á≥ªÁªü‰∏≠Âà†Èô§Alter TableÈáçÂëΩÂêç1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)Áî®Êà∑ÂèØ‰ª•Áî® ALTER TABLE ADD PARTITION Êù•Âêë‰∏Ä‰∏™Ë°®‰∏≠Â¢ûÂä†ÂàÜÂå∫„ÄÇÂàÜÂå∫ÂêçÊòØÂ≠óÁ¨¶‰∏≤Êó∂Âä†ÂºïÂè∑„ÄÇÊ≥®ÔºöÊ∑ªÂä†ÂàÜÂå∫Êó∂ÂèØËÉΩÂá∫Áé∞FAILED: SemanticException table is not partitioned but partition spec existsÈîôËØØ„ÄÇÂéüÂõ†ÊòØÔºå‰Ω†Âú®ÂàõÂª∫Ë°®Êó∂Âπ∂Ê≤°ÊúâÊ∑ªÂä†ÂàÜÂå∫ÔºåÈúÄË¶ÅÂú®ÂàõÂª∫Ë°®Êó∂ÂàõÂª∫ÂàÜÂå∫ÔºåÂÜçÊ∑ªÂä†ÂàÜÂå∫„ÄÇhive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 secondsÊü•ËØ¢ÁªìÊûú123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);Êü•ÁúãÂàÜÂå∫ËØ≠Âè•12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s)ÊåâÂàÜÂå∫Êü•ËØ¢1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂèàÂèàÂèàÊòØÊ∫êÁ†ÅÔºÅRDD ‰Ωú‰∏öÁöÑDAGÊòØÂ¶Ç‰ΩïÂàáÂàÜÁöÑÔºü]]></title>
    <url>%2F2018%2F04%2F23%2F%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Êàë‰ª¨ÈÉΩÁü•ÈÅìÔºåRDDÂ≠òÂú®ÁùÄ‰æùËµñÂÖ≥Á≥ªÔºåËøô‰∫õ‰æùËµñÂÖ≥Á≥ªÂΩ¢Êàê‰∫ÜÊúâÂêëÊó†ÁéØÂõæDAGÔºåDAGÈÄöËøáDAGSchedulerËøõË°åStageÁöÑÂàíÂàÜÔºåÂπ∂Âü∫‰∫éÊØè‰∏™StageÁîüÊàê‰∫ÜTaskSetÔºåÊèê‰∫§ÁªôTaskScheduler„ÄÇÈÇ£‰πàËøôÊï¥‰∏™ËøáÁ®ãÂú®Ê∫êÁ†Å‰∏≠ÊòØÂ¶Ç‰Ωï‰ΩìÁé∞ÁöÑÂë¢Ôºü1.‰Ωú‰∏öÁöÑÊèê‰∫§123// SparkContext.scala dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll())123// DAGScheduler.scala def runJob[T, U]( val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)ÂèØ‰ª•ÁúãÂà∞ÔºåSparkContextÁöÑrunjobÊñπÊ≥ïË∞ÉÁî®‰∫ÜDAGSchedulerÁöÑrunjobÊñπÊ≥ïÊ≠£ÂºèÂêëÈõÜÁæ§Êèê‰∫§‰ªªÂä°ÔºåÊúÄÁªàË∞ÉÁî®‰∫ÜsubmitJobÊñπÊ≥ï„ÄÇ12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2 def submitJob[T, U]( 3 rdd: RDD[T], 4 func: (TaskContext, Iterator[T]) =&gt; U, 5 partitions: Seq[Int], 6 callSite: CallSite, 7 resultHandler: (Int, U) =&gt; Unit, 8 properties: Properties): JobWaiter[U] = &#123; 9 // Check to make sure we are not launching a task on a partition that does not exist.10 val maxPartitions = rdd.partitions.length11 partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;12 throw new IllegalArgumentException(13 &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +14 &quot;Total number of partitions: &quot; + maxPartitions)15 &#125;1617 val jobId = nextJobId.getAndIncrement()18 if (partitions.size == 0) &#123;19 // Return immediately if the job is running 0 tasks20 return new JobWaiter[U](this, jobId, 0, resultHandler)21 &#125;2223 assert(partitions.size &gt; 0)24 val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]25 val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)26 //ÁªôeventProcessLoopÂèëÈÄÅJobSubmittedÊ∂àÊÅØ27 eventProcessLoop.post(JobSubmitted(28 jobId, rdd, func2, partitions.toArray, callSite, waiter,29 SerializationUtils.clone(properties)))30 waiter31 &#125;ËøôÈáåÂêëeventProcessLoopÂØπË±°ÂèëÈÄÅ‰∫ÜJobSubmittedÊ∂àÊÅØ„ÄÇ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481// DAGScheduler.scala2 private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) eventProcessLoopÊòØDAGSchedulerEventProcessLoopÁ±ªÁöÑ‰∏Ä‰∏™ÂØπË±°„ÄÇ 1// DAGScheduler.scala 2 private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; 3 case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; 4 dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) 5 6 case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; 7 dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) 8 9 case StageCancelled(stageId) =&gt;10 dagScheduler.handleStageCancellation(stageId)1112 case JobCancelled(jobId) =&gt;13 dagScheduler.handleJobCancellation(jobId)1415 case JobGroupCancelled(groupId) =&gt;16 dagScheduler.handleJobGroupCancelled(groupId)1718 case AllJobsCancelled =&gt;19 dagScheduler.doCancelAllJobs()2021 case ExecutorAdded(execId, host) =&gt;22 dagScheduler.handleExecutorAdded(execId, host)2324 case ExecutorLost(execId, reason) =&gt;25 val filesLost = reason match &#123;26 case SlaveLost(_, true) =&gt; true27 case _ =&gt; false28 &#125;29 dagScheduler.handleExecutorLost(execId, filesLost)3031 case BeginEvent(task, taskInfo) =&gt;32 dagScheduler.handleBeginEvent(task, taskInfo)3334 case GettingResultEvent(taskInfo) =&gt;35 dagScheduler.handleGetTaskResult(taskInfo)3637 case completion: CompletionEvent =&gt;38 dagScheduler.handleTaskCompletion(completion)3940 case TaskSetFailed(taskSet, reason, exception) =&gt;41 dagScheduler.handleTaskSetFailed(taskSet, reason, exception)4243 case ResubmitFailedStages =&gt;44 dagScheduler.resubmitFailedStages()45 &#125;DAGSchedulerEventProcessLoopÂØπÊé•Êî∂Âà∞ÁöÑÊ∂àÊÅØËøõË°åÂ§ÑÁêÜÔºåÂú®doOnReceiveÊñπÊ≥ï‰∏≠ÂΩ¢Êàê‰∏Ä‰∏™event loop„ÄÇÊé•‰∏ãÊù•Â∞ÜË∞ÉÁî®submitStage()ÊñπÊ≥ïËøõË°åstageÁöÑÂàíÂàÜ„ÄÇ2.stageÁöÑÂàíÂàÜ12345678910111213141516171819202122 1// DAGScheduler.scala 2 private def submitStage(stage: Stage) &#123; 3 val jobId = activeJobForStage(stage)//Êü•ÊâæËØ•StageÁöÑÊâÄÊúâÊøÄÊ¥ªÁöÑjob 4 if (jobId.isDefined) &#123; 5 logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;) 6 if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; 7 val missing = getMissingParentStages(stage).sortBy(_.id)//ÂæóÂà∞StageÁöÑÁà∂StageÔºåÂπ∂ÊéíÂ∫è 8 logDebug(&quot;missing: &quot; + missing) 9 if (missing.isEmpty) &#123;10 logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)11 submitMissingTasks(stage, jobId.get)//Â¶ÇÊûúStageÊ≤°ÊúâÁà∂StageÔºåÂàôÊèê‰∫§‰ªªÂä°ÈõÜ12 &#125; else &#123;13 for (parent &lt;- missing) &#123;//Â¶ÇÊûúÊúâÁà∂StageÔºåÈÄíÂΩíË∞ÉÁî®submiStage14 submitStage(parent)15 &#125;16 waitingStages += stage//Â∞ÜÂÖ∂Ê†áËÆ∞‰∏∫Á≠âÂæÖÁä∂ÊÄÅÔºåÁ≠âÂæÖ‰∏ãÊ¨°Êèê‰∫§17 &#125;18 &#125;19 &#125; else &#123;20 abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//Â¶ÇÊûúËØ•StageÊ≤°ÊúâÊøÄÊ¥ªÁöÑjobÔºåÂàô‰∏¢ÂºÉËØ•Stage21 &#125;22 &#125;Âú®submitStageÊñπÊ≥ï‰∏≠Âà§Êñ≠StageÁöÑÁà∂StageÊúâÊ≤°ÊúâË¢´Êèê‰∫§ÔºåÁõ¥Âà∞ÊâÄÊúâÁà∂StageÈÉΩË¢´Êèê‰∫§ÔºåÂè™ÊúâÁ≠âÁà∂StageÂÆåÊàêÂêéÊâçËÉΩË∞ÉÂ∫¶Â≠êStage„ÄÇ12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123; 3 val missing = new HashSet[Stage] //Áî®‰∫éÂ≠òÊîæÁà∂Stage 4 val visited = new HashSet[RDD[_]] //Áî®‰∫éÂ≠òÊîæÂ∑≤ËÆøÈóÆËøáÁöÑRDD 5 6 val waitingForVisit = new Stack[RDD[_]] 7 def visit(rdd: RDD[_]) &#123; 8 if (!visited(rdd)) &#123; //Â¶ÇÊûúRDDÊ≤°ÊúâË¢´ËÆøÈóÆËøáÔºåÂàôËøõË°åËÆøÈóÆ 9 visited += rdd //Ê∑ªÂä†Âà∞Â∑≤ËÆøÈóÆRDDÁöÑHashSet‰∏≠10 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)11 if (rddHasUncachedPartitions) &#123;12 for (dep &lt;- rdd.dependencies) &#123; //Ëé∑ÂèñËØ•RDDÁöÑ‰æùËµñ13 dep match &#123;14 case shufDep: ShuffleDependency[_, _, _] =&gt;//Ëã•‰∏∫ÂÆΩ‰æùËµñÔºåÂàôËØ•RDD‰æùËµñÁöÑRDDÊâÄÂú®ÁöÑstage‰∏∫Áà∂stage15 val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//ÁîüÊàêÁà∂Stage16 if (!mapStage.isAvailable) &#123;//Ëã•Áà∂Stage‰∏çÂ≠òÂú®ÔºåÂàôÊ∑ªÂä†Âà∞Áà∂StageÁöÑHashSET‰∏≠17 missing += mapStage18 &#125;19 case narrowDep: NarrowDependency[_] =&gt;//Ëã•‰∏∫Á™Ñ‰æùËµñÔºåÂàôÁªßÁª≠ËÆøÈóÆÁà∂RDD20 waitingForVisit.push(narrowDep.rdd)21 &#125;22 &#125;23 &#125;24 &#125;25 &#125;26 waitingForVisit.push(stage.rdd)27 while (waitingForVisit.nonEmpty) &#123;//Âæ™ÁéØÈÅçÂéÜÊâÄÊúâRDD28 visit(waitingForVisit.pop())29 &#125;30 missing.toList31 &#125;getmissingParentStages()ÊñπÊ≥ï‰∏∫Ê†∏ÂøÉÊñπÊ≥ï„ÄÇËøôÈáåÊàë‰ª¨Ë¶ÅÊáÇÂæóËøôÊ†∑‰∏Ä‰∏™ÈÄªËæëÔºöÊàë‰ª¨ÈÉΩÁü•ÈÅìÔºåStageÊòØÈÄöËøáshuffleÂàíÂàÜÁöÑÔºåÊâÄ‰ª•ÔºåÊØè‰∏ÄStageÈÉΩÊòØ‰ª•shuffleÂºÄÂßãÁöÑÔºåËã•‰∏Ä‰∏™RDDÊòØÂÆΩ‰æùËµñÔºåÂàôÂøÖÁÑ∂ËØ¥ÊòéËØ•RDDÁöÑÁà∂RDDÂú®Âè¶‰∏Ä‰∏™Stage‰∏≠ÔºåËã•‰∏Ä‰∏™RDDÊòØÁ™Ñ‰æùËµñÔºåÂàôËØ•RDDÊâÄ‰æùËµñÁöÑÁà∂RDDËøòÂú®Âêå‰∏Ä‰∏™Stage‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•Ê†πÊçÆËøô‰∏™ÈÄªËæëÔºåÊâæÂà∞ËØ•StageÁöÑÁà∂Stage„ÄÇ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Ê∫êÁ†ÅÈòÖËØª</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveÁîü‰∫ß‰∏äÔºåÂéãÁº©ÂíåÂ≠òÂÇ®ÁªìÂêà‰ΩøÁî®Ê°à‰æã]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[‰Ω†‰ª¨HiveÁîü‰∫ß‰∏äÔºåÂéãÁº©ÂíåÂ≠òÂÇ®ÔºåÁªìÂêà‰ΩøÁî®‰∫ÜÂêóÔºüÊ°à‰æãÔºöÂéüÊñá‰ª∂Â§ßÂ∞èÔºö19M1. ORC+ZlipÁªìÂêà12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views;Áî®ORC+Zlip‰πãÂêéÁöÑÊñá‰ª∂‰∏∫2.8MÁî®ORC+Zlip‰πãÂêéÁöÑÊñá‰ª∂‰∏∫2.8M###### 2. Parquet+gzipÁªìÂêà12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;Áî®Parquet+gzip‰πãÂêéÁöÑÊñá‰ª∂‰∏∫3.9M3. Parquet+LzoÁªìÂêà3.1 ÂÆâË£ÖLzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile3.2 ÂÆâË£ÖLzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile3.3 ËΩØËøûÊé•1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop3.4 ÊµãËØïlzoplzop xxx.logËã•ÁîüÊàêxxx.log.lzoÊñá‰ª∂ÔºåÂàôËØ¥ÊòéÊàêÂäü3.5 ÂÆâË£ÖHadoop-LZO12345 gitÊàñsvn ‰∏ãËΩΩhttps://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/3.6 ÈÖçÁΩÆÂú®core-site.xmlÈÖçÁΩÆ1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;Âú®mapred-site.xml‰∏≠ÈÖçÁΩÆ &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;Âú®hadoop-env.sh‰∏≠ÈÖçÁΩÆexport LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib3.7 ÊµãËØï12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views;Áî®Parquet+Lzo(Êú™Âª∫Á´ãÁ¥¢Âºï)‰πãÂêéÁöÑÊñá‰ª∂‰∏∫5.9M]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Ê°à‰æã</tag>
        <tag>ÂéãÁº©Ê†ºÂºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveÂ≠òÂÇ®Ê†ºÂºèÁöÑÁîü‰∫ßÂ∫îÁî®]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Áõ∏ÂêåÊï∞ÊçÆÔºåÂàÜÂà´‰ª•TextFile„ÄÅSequenceFile„ÄÅRcFile„ÄÅORCÂ≠òÂÇ®ÁöÑÊØîËæÉ„ÄÇÂéüÂßãÂ§ßÂ∞è: 19M1. TextFile(ÈªòËÆ§) Êñá‰ª∂Â§ßÂ∞è‰∏∫18.1M2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY ‚Äú\t‚Äù STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views;Áî®SequenceFileÂ≠òÂÇ®ÂêéÁöÑÊñá‰ª∂‰∏∫19.6M3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views;Áî®RcFileÂ≠òÂÇ®ÂêéÁöÑÊñá‰ª∂‰∏∫17.9M4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views;Áî®ORCFileÂ≠òÂÇ®ÂêéÁöÑÊñá‰ª∂‰∏∫7.7M5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; Áî®ORCFileÂ≠òÂÇ®ÂêéÁöÑÊñá‰ª∂‰∏∫13.1MÊÄªÁªìÔºöÁ£ÅÁõòÁ©∫Èó¥Âç†Áî®Â§ßÂ∞èÊØîËæÉORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>ÂéãÁº©Ê†ºÂºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â§ßÊï∞ÊçÆÂéãÁº©Ôºå‰Ω†‰ª¨ÁúüÁöÑ‰∫ÜËß£ÂêóÔºü]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåÂ∏¶‰Ω†‰ª¨ÂâñÊûêÂ§ßÊï∞ÊçÆ‰πãÂéãÁº©ÔºÅ1. ÂéãÁº©ÁöÑÂ•ΩÂ§ÑÂíåÂùèÂ§ÑÂ•ΩÂ§ÑÂáèÂ∞ëÂ≠òÂÇ®Á£ÅÁõòÁ©∫Èó¥Èôç‰ΩéIO(ÁΩëÁªúÁöÑIOÂíåÁ£ÅÁõòÁöÑIO)Âä†Âø´Êï∞ÊçÆÂú®Á£ÅÁõòÂíåÁΩëÁªú‰∏≠ÁöÑ‰º†ËæìÈÄüÂ∫¶Ôºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÁöÑÂ§ÑÁêÜÈÄüÂ∫¶ÂùèÂ§ÑÁî±‰∫é‰ΩøÁî®Êï∞ÊçÆÊó∂ÔºåÈúÄË¶ÅÂÖàÂ∞ÜÊï∞ÊçÆËß£ÂéãÔºåÂä†ÈáçCPUË¥üËç∑2. ÂéãÁº©Ê†ºÂºèÂéãÁº©ÊØîÂéãÁº©Êó∂Èó¥ÂèØ‰ª•ÁúãÂá∫ÔºåÂéãÁº©ÊØîË∂äÈ´òÔºåÂéãÁº©Êó∂Èó¥Ë∂äÈïøÔºåÂéãÁº©ÊØîÔºöSnappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2ÂéãÁº©Ê†ºÂºè‰ºòÁÇπÁº∫ÁÇπgzipÂéãÁº©ÊØîÂú®ÂõõÁßçÂéãÁº©ÊñπÂºè‰∏≠ËæÉÈ´òÔºõhadoopÊú¨Ë∫´ÊîØÊåÅÔºåÂú®Â∫îÁî®‰∏≠Â§ÑÁêÜgzipÊ†ºÂºèÁöÑÊñá‰ª∂Â∞±ÂíåÁõ¥Êé•Â§ÑÁêÜÊñáÊú¨‰∏ÄÊ†∑ÔºõÊúâhadoop nativeÂ∫ìÔºõÂ§ßÈÉ®ÂàÜlinuxÁ≥ªÁªüÈÉΩËá™Â∏¶gzipÂëΩ‰ª§Ôºå‰ΩøÁî®Êñπ‰æø‰∏çÊîØÊåÅsplitlzoÂéãÁº©/Ëß£ÂéãÈÄüÂ∫¶‰πüÊØîËæÉÂø´ÔºåÂêàÁêÜÁöÑÂéãÁº©ÁéáÔºõÊîØÊåÅsplitÔºåÊòØhadoop‰∏≠ÊúÄÊµÅË°åÁöÑÂéãÁº©Ê†ºÂºèÔºõÊîØÊåÅhadoop nativeÂ∫ìÔºõÈúÄË¶ÅÂú®linuxÁ≥ªÁªü‰∏ãËá™Ë°åÂÆâË£ÖlzopÂëΩ‰ª§Ôºå‰ΩøÁî®Êñπ‰æøÂéãÁº©ÁéáÊØîgzipË¶Å‰ΩéÔºõhadoopÊú¨Ë∫´‰∏çÊîØÊåÅÔºåÈúÄË¶ÅÂÆâË£ÖÔºõlzoËôΩÁÑ∂ÊîØÊåÅsplitÔºå‰ΩÜÈúÄË¶ÅÂØπlzoÊñá‰ª∂Âª∫Á¥¢ÂºïÔºåÂê¶Âàôhadoop‰πüÊòØ‰ºöÊäälzoÊñá‰ª∂ÁúãÊàê‰∏Ä‰∏™ÊôÆÈÄöÊñá‰ª∂Ôºà‰∏∫‰∫ÜÊîØÊåÅsplitÈúÄË¶ÅÂª∫Á¥¢ÂºïÔºåÈúÄË¶ÅÊåáÂÆöinputformat‰∏∫lzoÊ†ºÂºèÔºâsnappyÂéãÁº©ÈÄüÂ∫¶Âø´ÔºõÊîØÊåÅhadoop nativeÂ∫ì‰∏çÊîØÊåÅsplitÔºõÂéãÁº©ÊØî‰ΩéÔºõhadoopÊú¨Ë∫´‰∏çÊîØÊåÅÔºåÈúÄË¶ÅÂÆâË£ÖÔºõlinuxÁ≥ªÁªü‰∏ãÊ≤°ÊúâÂØπÂ∫îÁöÑÂëΩ‰ª§d. bzip2bzip2ÊîØÊåÅsplitÔºõÂÖ∑ÊúâÂæàÈ´òÁöÑÂéãÁº©ÁéáÔºåÊØîgzipÂéãÁº©ÁéáÈÉΩÈ´òÔºõhadoopÊú¨Ë∫´ÊîØÊåÅÔºå‰ΩÜ‰∏çÊîØÊåÅnativeÔºõÂú®linuxÁ≥ªÁªü‰∏ãËá™Â∏¶bzip2ÂëΩ‰ª§Ôºå‰ΩøÁî®Êñπ‰æøÂéãÁº©/Ëß£ÂéãÈÄüÂ∫¶ÊÖ¢Ôºõ‰∏çÊîØÊåÅnativeÊÄªÁªìÔºö‰∏çÂêåÁöÑÂú∫ÊôØÈÄâÊã©‰∏çÂêåÁöÑÂéãÁº©ÊñπÂºèÔºåËÇØÂÆöÊ≤°Êúâ‰∏Ä‰∏™‰∏ÄÂä≥Ê∞∏ÈÄ∏ÁöÑÊñπÊ≥ïÔºåÂ¶ÇÊûúÈÄâÊã©È´òÂéãÁº©ÊØîÔºåÈÇ£‰πàÂØπ‰∫écpuÁöÑÊÄßËÉΩË¶ÅÊ±ÇË¶ÅÈ´òÔºåÂêåÊó∂ÂéãÁº©„ÄÅËß£ÂéãÊó∂Èó¥ËÄóË¥π‰πüÂ§öÔºõÈÄâÊã©ÂéãÁº©ÊØî‰ΩéÁöÑÔºåÂØπ‰∫éÁ£ÅÁõòio„ÄÅÁΩëÁªúioÁöÑÊó∂Èó¥Ë¶ÅÂ§öÔºåÁ©∫Èó¥Âç†ÊçÆË¶ÅÂ§öÔºõÂØπ‰∫éÊîØÊåÅÂàÜÂâ≤ÁöÑÔºåÂèØ‰ª•ÂÆûÁé∞Âπ∂Ë°åÂ§ÑÁêÜ„ÄÇÂ∫îÁî®Âú∫ÊôØÔºö‰∏ÄËà¨Âú®HDFS „ÄÅHive„ÄÅHBase‰∏≠‰ºö‰ΩøÁî®ÔºõÂΩìÁÑ∂‰∏ÄËà¨ËæÉÂ§öÁöÑÊòØÁªìÂêàSpark Êù•‰∏ÄËµ∑‰ΩøÁî®„ÄÇ]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>ÂéãÁº©Ê†ºÂºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HadoopÂ∏∏Áî®ÂëΩ‰ª§Â§ßÂÖ®]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåHadoopÂ∏∏Áî®ÂëΩ‰ª§Â§ßÂÖ®1. ÂçïÁã¨ÂêØÂä®ÂíåÂÖ≥Èó≠hadoopÊúçÂä°ÂäüËÉΩÂëΩ‰ª§ÂêØÂä®ÂêçÁß∞ËäÇÁÇπhadoop-daemon.sh start namenodeÂêØÂä®Êï∞ÊçÆËäÇÁÇπhadoop-daemons.sh start datanode slaveÂêØÂä®secondarynamenodehadoop-daemon.sh start secondarynamenodeÂêØÂä®resourcemanageryarn-daemon.sh start resourcemanagerÂêØÂä®nodemanagerbin/yarn-daemons.sh start nodemanagerÂÅúÊ≠¢Êï∞ÊçÆËäÇÁÇπhadoop-daemons.sh stop datanode2. Â∏∏Áî®ÁöÑÂëΩ‰ª§ÂäüËÉΩÂëΩ‰ª§ÂàõÂª∫ÁõÆÂΩïhdfs dfs -mkdir /inputÊü•Áúãhdfs dfs -lsÈÄíÂΩíÊü•Áúãhdfs dfs ls -R‰∏ä‰º†hdfs dfs -put‰∏ãËΩΩhdfs dfs -getÂà†Èô§hdfs dfs -rm‰ªéÊú¨Âú∞Ââ™ÂàáÁ≤òË¥¥Âà∞hdfshdfs fs -moveFromLocal /input/xx.txt /input/xx.txt‰ªéhdfsÂâ™ÂàáÁ≤òË¥¥Âà∞Êú¨Âú∞hdfs fs -moveToLocal /input/xx.txt /input/xx.txtËøΩÂä†‰∏Ä‰∏™Êñá‰ª∂Âà∞Âè¶‰∏Ä‰∏™Êñá‰ª∂Âà∞Êú´Â∞æhdfs fs -appedToFile ./hello.txt /input/hello.txtÊü•ÁúãÊñá‰ª∂ÂÜÖÂÆπhdfs fs -cat /input/hello.txtÊòæÁ§∫‰∏Ä‰∏™Êñá‰ª∂Âà∞Êú´Â∞æhdfs fs -tail /input/hello.txt‰ª•Â≠óÁ¨¶‰∏≤ÁöÑÂΩ¢ÂºèÊâìÂç∞Êñá‰ª∂ÁöÑÂÜÖÂÆπhdfs fs -text /input/hello.txt‰øÆÊîπÊñá‰ª∂ÊùÉÈôêhdfs fs -chmod 666 /input/hello.txt‰øÆÊîπÊñá‰ª∂ÊâÄÂ±ûhdfs fs -chown ruoze.ruoze /input/hello.txt‰ªéÊú¨Âú∞Êñá‰ª∂Á≥ªÁªüÊã∑Ë¥ùÂà∞hdfsÈáåhdfs fs -copyFromLocal /input/hello.txt /input/‰ªéhdfsÊã∑Ë¥ùÂà∞Êú¨Âú∞hdfs fs -copyToLocal /input/hello.txt /input/‰ªéhdfsÂà∞‰∏Ä‰∏™Ë∑ØÂæÑÊã∑Ë¥ùÂà∞Âè¶‰∏Ä‰∏™Ë∑ØÂæÑhdfs fs -cp /input/xx.txt /output/xx.txt‰ªéhdfsÂà∞‰∏Ä‰∏™Ë∑ØÂæÑÁßªÂä®Âà∞Âè¶‰∏Ä‰∏™Ë∑ØÂæÑhdfs fs -mv /input/xx.txt /output/xx.txtÁªüËÆ°Êñá‰ª∂Á≥ªÁªüÁöÑÂèØÁî®Á©∫Èó¥‰ø°ÊÅØhdfs fs -df -h /ÁªüËÆ°Êñá‰ª∂Â§πÁöÑÂ§ßÂ∞è‰ø°ÊÅØhdfs fs -du -s -h /ÁªüËÆ°‰∏Ä‰∏™ÊåáÂÆöÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂ËäÇÁÇπÊï∞Èáèhadoop fs -count /aaaËÆæÁΩÆhdfsÁöÑÊñá‰ª∂ÂâØÊú¨Êï∞Èáèhadoop fs -setrep 3 /input/xx.txtÊÄªÁªìÔºö‰∏ÄÂÆöË¶ÅÂ≠¶‰ºöÊü•ÁúãÂëΩ‰ª§Â∏ÆÂä©1.hadoopÂëΩ‰ª§Áõ¥Êé•ÂõûËΩ¶Êü•ÁúãÂëΩ‰ª§Â∏ÆÂä©2.hdfsÂëΩ‰ª§„ÄÅhdfs dfsÂëΩ‰ª§Áõ¥Êé•ÂõûËΩ¶Êü•ÁúãÂëΩ‰ª§Â∏ÆÂä©3.hadoop fs Á≠â‰ª∑ hdfs dfsÂëΩ‰ª§ÔºåÂíåLinuxÁöÑÂëΩ‰ª§Â∑Æ‰∏çÂ§ö„ÄÇ]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 ÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑÊ∫êÁ†ÅÁºñËØë]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåSpark2.2.0 ÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑÊ∫êÁ†ÅÁºñËØëÁéØÂ¢ÉÂáÜÂ§áJDKÔºö Spark 2.2.0Âèä‰ª•‰∏äÁâàÊú¨Âè™ÊîØÊåÅJDK1.8MavenÔºö3.3.9ËÆæÁΩÆmavenÁéØÂ¢ÉÂèòÈáèÊó∂ÔºåÈúÄËÆæÁΩÆmavenÂÜÖÂ≠òÔºöexport MAVEN_OPTS=‚Äù-Xmx2g -XX:ReservedCodeCacheSize=512m‚ÄùScalaÔºö2.11.8GitÁºñËØë‰∏ãËΩΩsparkÁöÑtarÂåÖÔºåÂπ∂Ëß£Âéã12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgzÁºñËæëdev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.shÊ≥®Èáä‰ª•‰∏ãÂÜÖÂÆπÔºö#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n)Ê∑ªÂä†‰ª•‰∏ãÂÜÖÂÆπÔºö1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1ÁºñËæëpom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xmlÊ∑ªÂä†Âú®repositorysÂÜÖ&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;ÂÆâË£Ö1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -PyarnÁ®çÂæÆÁ≠âÂæÖÂá†Â∞èÊó∂ÔºåÁΩëÁªúËæÉÂ•ΩÁöÑËØùÔºåÈùûÂ∏∏Âø´„ÄÇ‰πüÂèØ‰ª•ÂèÇËÄÉJÂì•ÂçöÂÆ¢ÔºöÂü∫‰∫éCentOS6.4ÁéØÂ¢ÉÁºñËØëSpark-2.1.0Ê∫êÁ†Å http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Âü∫Á°Ä</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰∏∫‰ªÄ‰πàÊàë‰ª¨Áîü‰∫ß‰∏äË¶ÅÈÄâÊã©Spark On YarnÊ®°ÂºèÔºü]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºå‰∏∫‰ªÄ‰πàÊàë‰ª¨Áîü‰∫ß‰∏äË¶ÅÈÄâÊã©Spark On YarnÔºüÂºÄÂèë‰∏äÊàë‰ª¨ÈÄâÊã©local[2]Ê®°ÂºèÁîü‰∫ß‰∏äË∑ë‰ªªÂä°JobÔºåÊàë‰ª¨ÈÄâÊã©Spark On YarnÊ®°Âºè ÔºåÂ∞ÜSpark ApplicationÈÉ®ÁΩ≤Âà∞yarn‰∏≠ÔºåÊúâÂ¶Ç‰∏ã‰ºòÁÇπÔºö1.ÈÉ®ÁΩ≤ApplicationÂíåÊúçÂä°Êõ¥Âä†Êñπ‰æøÂè™ÈúÄË¶ÅyarnÊúçÂä°ÔºåÂåÖÊã¨SparkÔºåStormÂú®ÂÜÖÁöÑÂ§öÁßçÂ∫îÁî®Á®ãÂ∫è‰∏çË¶ÅË¶ÅËá™Â∏¶ÊúçÂä°ÔºåÂÆÉ‰ª¨ÁªèÁî±ÂÆ¢Êà∑Á´ØÊèê‰∫§ÂêéÔºåÁî±yarnÊèê‰æõÁöÑÂàÜÂ∏ÉÂºèÁºìÂ≠òÊú∫Âà∂ÂàÜÂèëÂà∞ÂêÑ‰∏™ËÆ°ÁÆóËäÇÁÇπ‰∏ä„ÄÇ2.ËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂yarnÂè™Ë¥üË¥£ËµÑÊ∫êÁöÑÁÆ°ÁêÜÂíåË∞ÉÂ∫¶ÔºåÂÆåÂÖ®Áî±Áî®Êà∑ÂíåËá™Â∑±ÂÜ≥ÂÆöÂú®yarnÈõÜÁæ§‰∏äËøêË°åÂì™ÁßçÊúçÂä°ÂíåApplicatioinÔºåÊâÄ‰ª•Âú®yarn‰∏äÊúâÂèØËÉΩÂêåÊó∂ËøêË°åÂ§ö‰∏™ÂêåÁ±ªÁöÑÊúçÂä°ÂíåApplication„ÄÇYarnÂà©Áî®CgroupsÂÆûÁé∞ËµÑÊ∫êÁöÑÈöîÁ¶ªÔºåÁî®Êà∑Âú®ÂºÄÂèëÊñ∞ÁöÑÊúçÂä°ÊàñËÄÖApplicationÊó∂Ôºå‰∏çÁî®ÊãÖÂøÉËµÑÊ∫êÈöîÁ¶ªÊñπÈù¢ÁöÑÈóÆÈ¢ò„ÄÇ3.ËµÑÊ∫êÂºπÊÄßÁÆ°ÁêÜYarnÂèØ‰ª•ÈÄöËøáÈòüÂàóÁöÑÊñπÂºèÔºåÁÆ°ÁêÜÂêåÊó∂ËøêË°åÂú®yarnÈõÜÁæ§ÁßçÁöÑÂ§ö‰∏™ÊúçÂä°ÔºåÂèØÊ†πÊçÆ‰∏çÂêåÁ±ªÂûãÁöÑÂ∫îÁî®Á®ãÂ∫èÂéãÂäõÊÉÖÂÜµÔºåË∞ÉÊï¥ÂØπÂ∫îÁöÑËµÑÊ∫ê‰ΩøÁî®ÈáèÔºåÂÆûÁé∞ËµÑÊ∫êÂºπÊÄßÁÆ°ÁêÜ„ÄÇSpark On YarnÊúâ‰∏§ÁßçÊ®°ÂºèÔºå‰∏ÄÁßçÊòØclusterÊ®°ÂºèÔºå‰∏ÄÁßçÊòØclientÊ®°Âºè„ÄÇËøêË°åclientÊ®°ÂºèÔºö‚Äú./spark-shell ‚Äìmaster yarn‚Äù‚Äú./spark-shell ‚Äìmaster yarn-client‚Äù‚Äú./spark-shell ‚Äìmaster yarn ‚Äìdeploy-mode client‚ÄùËøêË°åÁöÑÊòØclusterÊ®°Âºè‚Äú./spark-shell ‚Äìmaster yarn-cluster‚Äù‚Äú./spark-shell ‚Äìmaster yarn ‚Äìdeploy-mode cluster‚ÄùclientÂíåclusterÊ®°ÂºèÁöÑ‰∏ªË¶ÅÂå∫Âà´Ôºöa. clientÁöÑdriverÊòØËøêË°åÂú®ÂÆ¢Êà∑Á´ØËøõÁ®ã‰∏≠b. clusterÁöÑdriverÊòØËøêË°åÂú®Application Master‰πã‰∏≠]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>È´òÁ∫ß</tag>
        <tag>Êû∂ÊûÑ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑÁºñËØëÂèäÈÉ®ÁΩ≤]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåHiveÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑÁºñËØëÂèäÈÉ®ÁΩ≤‰∏Ä„ÄÅÈúÄË¶ÅÂÆâË£ÖÁöÑËΩØ‰ª∂Áõ∏ÂÖ≥ÁéØÂ¢ÉÔºöjdk-7u80hadoop-2.6.0-cdh5.7.1 ‰∏çÊîØÊåÅjdk1.8ÔºåÂõ†Ê≠§Ê≠§Â§Ñ‰πüÂª∂Áª≠jdk1.7apache-maven-3.3.9mysql5.1hadoop‰º™ÂàÜÂ∏ÉÈõÜÁæ§Â∑≤ÂêØÂä®‰∫å„ÄÅÂÆâË£Öjdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profile‰∏â„ÄÅÂÆâË£Ömaven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profileÂõõ„ÄÅÂÆâË£Ömysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges;‰∫î„ÄÅ‰∏ãËΩΩhiveÊ∫êÁ†ÅÂåÖÔºöËæìÂÖ•Ôºöhttp://archive.cloudera.com/cdh5/cdh/5/Ê†πÊçÆcdhÁâàÊú¨ÈÄâÊã©ÂØπÂ∫îhiveËΩØ‰ª∂ÂåÖÔºöhive-1.1.0-cdh5.7.1-src.tar.gzËß£ÂéãÂêé‰ΩøÁî®mavenÂëΩ‰ª§ÁºñËØëÊàêÂÆâË£ÖÂåÖÂÖ≠„ÄÅÁºñËØë:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# ÁºñËØëÁîüÊàêÁöÑÂåÖÂú®‰ª•‰∏ã‰ΩçÁΩÆÔºö# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz‰∏É„ÄÅÂÆâË£ÖÁºñËØëÁîüÊàêÁöÑHiveÂåÖÔºåÁÑ∂ÂêéÊµãËØï12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profileÂÖ´„ÄÅÊõ¥ÊîπÁéØÂ¢ÉÂèòÈáè12345su - hadoopcd /usr/local/hivecd conf1„ÄÅhive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop2„ÄÅhive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;‰πù„ÄÅÊã∑Ë¥ùmysqlÈ©±Âä®ÂåÖÂà∞$HIVE_HOME/lib‰∏äÊñπÁöÑhive-site.xml‰ΩøÁî®‰∫ÜjavaÁöÑmysqlÈ©±Âä®ÂåÖÈúÄË¶ÅÂ∞ÜËøô‰∏™ÂåÖ‰∏ä‰º†Âà∞hiveÁöÑlibÁõÆÂΩï‰πã‰∏ãËß£Âéã mysql-connector-java-5.1.45.zip ÂØπÂ∫îÁöÑÊñá‰ª∂Âà∞ÁõÆÂΩïÂç≥ÂèØ1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/Êú™Êã∑Ë¥ùÊúâÁõ∏ÂÖ≥Êä•ÈîôÔºöThe specified datastore driver (‚Äúcom.mysql.jdbc.Driver‚Äù) was not found in the CLASSPATH.Please check your CLASSPATH specification,and the name of the driver.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Âü∫Á°Ä</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HadoopÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑ‰º™ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[Ëã•Ê≥ΩÂ§ßÊï∞ÊçÆÔºåHadoopÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑ‰º™ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤(MapReduce+Yarn)‰øÆÊîπmapred-site.xml1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;‰øÆÊîπyarn-site.xml12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ÂêØÂä®123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.shÂÖ≥Èó≠1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Âü∫Á°Ä</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HadoopÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑ‰º™ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[HadoopÂÖ®ÁΩëÊúÄËØ¶ÁªÜÁöÑ‰º™ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤(HDFS)1.Ê∑ªÂä†hadoopÁî®Êà∑123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# ÊâæÂà∞root ALL=(ALL) ALLÔºåÊ∑ªÂä†hadoop ALL=(ALL) NOPASSWD:ALL2.‰∏ä‰º†Âπ∂Ëß£Âéã123[root@hadoop-01 software]# rz #‰∏ä‰º†hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz3.ËΩØËøûÊé•1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop4.ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile5.ËÆæÁΩÆÁî®Êà∑„ÄÅÁî®Êà∑ÁªÑ123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt6.ÂàáÊç¢hadoopÁî®Êà∑1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: ÂèØÊâßË°åÊñá‰ª∂# etc: ÈÖçÁΩÆÊñá‰ª∂# sbin: shellËÑöÊú¨ÔºåÂêØÂä®ÂÖ≥Èó≠hdfs,yarnÁ≠â7.ÈÖçÁΩÆÊñá‰ª∂12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # ÈÖçÁΩÆËá™Â∑±Êú∫Âô®ÁöÑIP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;8.ÈÖçÁΩÆhadoopÁî®Êà∑ÁöÑssh‰ø°‰ªªÂÖ≥Á≥ª8.1ÂÖ¨Èí•/ÂØÜÈí• ÈÖçÁΩÆÊó†ÂØÜÁ†ÅÁôªÂΩï12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys8.2 Êü•ÁúãÊó•ÊúüÔºåÁúãÊòØÂê¶ÈÖçÁΩÆÊàêÂäü1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # Á¨¨‰∏ÄÊ¨°ÂõûËΩ¶ËæìÂÖ•yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #‰∏çÈúÄË¶ÅÂõûËΩ¶ËæìÂÖ•yes,Âç≥OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # Á¨¨‰∏ÄÊ¨°ÂõûËΩ¶ËæìÂÖ•yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #‰∏çÈúÄË¶ÅÂõûËΩ¶ËæìÂÖ•yes,Âç≥OKSun Aug 20 14:22:29 CST 20179.Ê†ºÂºèÂåñÂíåÂêØÂä®123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found.9.1Ëß£ÂÜ≥ÊñπÊ≥ï:Ê∑ªÂä†ÁéØÂ¢ÉÂèòÈáè12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# Â∞Üexport JAVA_HOME=$&#123;JAVA_HOME&#125;Êîπ‰∏∫export JAVA_HOME=/usr/java/jdk1.8.0_4512345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied9.2Ëß£ÂÜ≥ÊñπÊ≥ï:Ê∑ªÂä†ÊùÉÈôê123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop9.3 ÁªßÁª≠ÂêØÂä®1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh9.4Ê£ÄÊü•ÊòØÂê¶ÊàêÂäü123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode9.5ËÆøÈóÆÔºö http://192.168.137.130:500709.6‰øÆÊîπdfsÂêØÂä®ÁöÑËøõÁ®ãÔºå‰ª•hadoop-01ÂêØÂä®ÂêØÂä®ÁöÑ‰∏â‰∏™ËøõÁ®ãÔºönamenode: hadoop-01 bin/hdfs getconf -namenodesdatanode: localhost datanodes (using default slaves file) etc/hadoop/slavessecondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt;9.7ÈáçÂêØ123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Âü∫Á°Ä</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxÂ∏∏Áî®ÂëΩ‰ª§Ôºà‰∫åÔºâ]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[LinuxÊúÄÂ∏∏Áî®ÂÆûÊàòÂëΩ‰ª§Ôºà‰∫åÔºâÂÆûÊó∂Êü•ÁúãÊñá‰ª∂ÂÜÖÂÆπ tail filenametail -f filename ÂΩìÊñá‰ª∂(Âêç)Ë¢´‰øÆÊîπÂêéÔºå‰∏çËÉΩÁõëËßÜÊñá‰ª∂ÂÜÖÂÆπtail -F filename ÂΩìÊñá‰ª∂(Âêç)Ë¢´‰øÆÊîπÂêéÔºå‰æùÁÑ∂ÂèØ‰ª•ÁõëËßÜÊñá‰ª∂ÂÜÖÂÆπÂ§çÂà∂„ÄÅÁßªÂä®Êñá‰ª∂cp oldfilename newfilename Â§çÂà∂mv oldfilename newfilename ÁßªÂä®/ÈáçÂëΩÂêçechoecho ‚Äúxxx‚Äù ËæìÂá∫echo ‚Äúxxx‚Äù &gt; filename Ë¶ÜÁõñecho ‚Äúxxx‚Äù &gt;&gt; filename ËøΩÂä†Âà†Èô§ rmrm -f Âº∫Âà∂Âà†Èô§rm -rf Âº∫Âà∂Âà†Èô§Êñá‰ª∂Â§πÔºår Ë°®Á§∫ÈÄíÂΩíÂèÇÊï∞ÔºåÊåáÈíàÂØπÊñá‰ª∂Â§πÂèäÊñá‰ª∂Â§πÈáåÈù¢Êñá‰ª∂Âà´Âêç aliasalias x=‚Äùxxxxxx‚Äù ‰∏¥Êó∂ÂºïÁî®Âà´Âêçalias x=‚Äùxxxxxx‚Äù ÈÖçÁΩÆÂà∞ÁéØÂ¢ÉÂèòÈáè‰∏≠Âç≥‰∏∫Ê∞∏‰πÖÁîüÊïàÊü•ÁúãÂéÜÂè≤ÂëΩ‰ª§ historyhistory ÊòæÁ§∫Âá∫ÊâÄÊúâÂéÜÂè≤ËÆ∞ÂΩïhistory n ÊòæÁ§∫Âá∫nÊù°ËÆ∞ÂΩï!n ÊâßË°åÁ¨¨nÊù°ËÆ∞ÂΩïÁÆ°ÈÅìÂëΩ‰ª§ Ôºà | ÔºâÁÆ°ÈÅìÁöÑ‰∏§ËæπÈÉΩÊòØÂëΩ‰ª§ÔºåÂ∑¶ËæπÁöÑÂëΩ‰ª§ÂÖàÊâßË°åÔºåÊâßË°åÁöÑÁªìÊûú‰Ωú‰∏∫Âè≥ËæπÂëΩ‰ª§ÁöÑËæìÂÖ•Êü•ÁúãËøõÁ®ã„ÄÅÊü•Áúãid„ÄÅÁ´ØÂè£ps -ef ÔΩúgrep ËøõÁ®ãÂêç Êü•ÁúãËøõÁ®ãÂü∫Êú¨‰ø°ÊÅØnetstat -nplÔΩúgrep ËøõÁ®ãÂêçÊàñËøõÁ®ãid Êü•ÁúãÊúçÂä°idÂíåÁ´ØÂè£ÊùÄÊ≠ªËøõÁ®ã killkill -9 ËøõÁ®ãÂêç/pid Âº∫Âà∂Âà†Èô§kill -9 $(pgrep ËøõÁ®ãÂêç)ÔºöÊùÄÊ≠ª‰∏éËØ•ËøõÁ®ãÁõ∏ÂÖ≥ÁöÑÊâÄÊúâËøõÁ®ãrpm ÊêúÁ¥¢„ÄÅÂç∏ËΩΩrpm -qa | grep xxx ÊêúÁ¥¢xxxrpm ‚Äìnodeps -e xxx Âà†Èô§xxx‚Äìnodeps ‰∏çÈ™åËØÅÂåÖÁöÑ‰æùËµñÊÄßÊü•ËØ¢find Ë∑ØÂæÑ -name xxx (Êé®Ëçê)which xxxlocal xxxÊü•ÁúãÁ£ÅÁõò„ÄÅÂÜÖÂ≠ò„ÄÅÁ≥ªÁªüÁöÑÊÉÖÂÜµdf -h Êü•ÁúãÁ£ÅÁõòÂ§ßÂ∞èÂèäÂÖ∂‰ΩøÁî®ÊÉÖÂÜµfree -m Êü•ÁúãÂÜÖÂ≠òÂ§ßÂ∞èÂèäÂÖ∂‰ΩøÁî®ÊÉÖÂÜµtop Êü•ÁúãÁ≥ªÁªüÊÉÖÂÜµËΩØËøûÊé•ln -s ÂéüÂßãÁõÆÂΩï ÁõÆÊ†áÁõÆÂΩïÂéãÁº©„ÄÅËß£Âéãtar -czf ÂéãÁº© tar -xzvf Ëß£Âéãzip ÂéãÁº© unzip Ëß£Âéã]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Âü∫Á°Ä</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinuxÂ∏∏Áî®ÂëΩ‰ª§Ôºà‰∏âÔºâ]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[LinuxÊúÄÂ∏∏Áî®ÂÆûÊàòÂëΩ‰ª§Ôºà‰∏âÔºâÁî®Êà∑„ÄÅÁî®Êà∑ÁªÑÁî®Êà∑useradd Áî®Êà∑Âêç Ê∑ªÂä†Áî®Êà∑userdel Áî®Êà∑Âêç Âà†Èô§Áî®Êà∑id Áî®Êà∑Âêç Êü•ÁúãÁî®Êà∑‰ø°ÊÅØpasswd Áî®Êà∑Âêç ‰øÆÊîπÁî®Êà∑ÂØÜÁ†Åsu - Áî®Êà∑Âêç ÂàáÊç¢Áî®Êà∑ll /home/ Êü•ÁúãÂ∑≤ÊúâÁöÑÁî®Êà∑Áî®Êà∑ÁªÑgroupadd Áî®Êà∑ÁªÑ Ê∑ªÂä†Áî®Êà∑ÁªÑcat /etc/group Áî®Êà∑ÁªÑÁöÑÊñá‰ª∂usermod -a -G Áî®Êà∑ÁªÑ Áî®Êà∑ Â∞ÜÁî®Êà∑Ê∑ªÂä†Âà∞Áî®Êà∑ÁªÑ‰∏≠Áªô‰∏Ä‰∏™ÊôÆÈÄöÁî®Êà∑Ê∑ªÂä†sudoÊùÉÈôê123vi /etc/sudoers #Âú®root ALL=(ALL) ALL ‰∏ãÈù¢Ê∑ªÂä†‰∏ÄË°å Áî®Êà∑ ALL=(ALL) NOPASSWD:ALL‰øÆÊîπÊñá‰ª∂ÊùÉÈôêchown ‰øÆÊîπÊñá‰ª∂ÊàñÊñá‰ª∂Â§πÁöÑÊâÄÂ±ûÁî®Êà∑ÂíåÁî®Êà∑ÁªÑchown -R Áî®Êà∑:Áî®Êà∑ÁªÑ Êñá‰ª∂Â§πÂêç -R ‰∏∫ÈÄíÂΩíÂèÇÊï∞ÔºåÊåáÈíàÂØπÊñá‰ª∂Â§πchown Áî®Êà∑:Áî®Êà∑ÁªÑ Êñá‰ª∂Âêçchmod: ‰øÆÊîπÊñá‰ª∂Â§πÊàñËÄÖÊñá‰ª∂ÁöÑÊùÉÈôêchmod -R 700 Êñá‰ª∂Â§πÂêçchmod 700 Êñá‰ª∂Â§πÂêçr =&gt; 4 w =&gt; 2 x =&gt; 1 ÂêéÂè∞ÊâßË°åÂëΩ‰ª§&amp;nohupscreenÂ§ö‰∫∫Âêà‰Ωú screenscreen -list Êü•Áúã‰ºöËØùscreen -S Âª∫Á´ã‰∏Ä‰∏™ÂêéÂè∞ÁöÑ‰ºöËØùscreen -r ËøõÂÖ•‰ºöËØùctrl+a+d ÈÄÄÂá∫‰ºöËØù]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Âü∫Á°Ä</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxÂ∏∏Áî®ÂëΩ‰ª§Ôºà‰∏ÄÔºâ]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[LinuxÊúÄÂ∏∏Áî®ÂÆûÊàòÂëΩ‰ª§Ôºà‰∏ÄÔºâÊü•ÁúãÂΩìÂâçÁõÆÂΩï pwdÊü•ÁúãIPifconfig Êü•ÁúãËôöÊãüÊú∫iphostname ‰∏ªÊú∫ÂêçÂ≠ói Êü•Áúã‰∏ªÊú∫ÂêçÊò†Â∞ÑÁöÑIPÂàáÊç¢ÁõÆÂΩï cdcd ~ ÂàáÊç¢ÂÆ∂ÁõÆÂΩïÔºàroot‰∏∫/rootÔºåÊôÆÈÄöÁî®Êà∑‰∏∫/home/Áî®Êà∑ÂêçÔºâcd /filename ‰ª•ÁªùÂØπË∑ØÂæÑÂàáÊç¢ÁõÆÂΩïcd - ËøîÂõû‰∏ä‰∏ÄÊ¨°Êìç‰ΩúË∑ØÂæÑÔºåÂπ∂ËæìÂá∫Ë∑ØÂæÑcd ../ ËøîÂõû‰∏ä‰∏ÄÂ±ÇÁõÆÂΩïÊ∏ÖÁêÜÊ°åÈù¢ clearÊòæÁ§∫ÂΩìÂâçÁõÆÂΩïÊñá‰ª∂ÂíåÊñá‰ª∂Â§π lsls -l(ll) ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØls -la ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØ+ÈöêËóèÊñá‰ª∂Ôºà‰ª• . ÂºÄÂ§¥Ôºå‰æãÔºö.sshÔºâls -lh ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØ+Êñá‰ª∂Â§ßÂ∞èls -lrt ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØ+ÊåâÊó∂Èó¥ÊéíÂ∫èÊü•ÁúãÊñá‰ª∂Â§πÂ§ßÂ∞è du -shÂëΩ‰ª§Â∏ÆÂä©man ÂëΩ‰ª§ÂëΩ‰ª§ ‚ÄìhelpÂàõÂª∫Êñá‰ª∂Â§π mkdirmkdir -p filename1/filename2 ÈÄíÂΩíÂàõÂª∫Êñá‰ª∂Â§πÂàõÂª∫Êñá‰ª∂ touch/vi/echo xx&gt;filenameÊü•ÁúãÊñá‰ª∂ÂÜÖÂÆπcat filename Áõ¥Êé•ÊâìÂç∞ÊâÄÊúâÂÜÖÂÆπmore filename Ê†πÊçÆÁ™óÂè£Â§ßÂ∞èËøõË°åÂàÜÈ°µÊòæÁ§∫Êñá‰ª∂ÁºñËæë viviÂàÜ‰∏∫ÂëΩ‰ª§Ë°åÊ®°ÂºèÔºåÊèíÂÖ•Ê®°ÂºèÔºåÂ∞æË°åÊ®°ÂºèÂëΩ‰ª§Ë°åÊ®°Âºè‚Äî&gt;ÊèíÂÖ•Ê®°ÂºèÔºöÊåâiÊàñaÈîÆÊèíÂÖ•Ê®°Âºè‚Äî&gt;ÂëΩ‰ª§Ë°åÊ®°ÂºèÔºöÊåâEscÈîÆÂëΩ‰ª§Ë°åÊ®°Âºè‚Äî&gt;Â∞æË°åÊ®°ÂºèÔºöÊåâShiftÂíå:ÈîÆÊèíÂÖ•Ê®°Âºèdd Âà†Èô§ÂÖâÊ†áÊâÄÂú®Ë°ån+dd Âà†Èô§ÂÖâÊ†á‰ª•‰∏ãÁöÑnË°ådG Âà†Èô§ÂÖâÊ†á‰ª•‰∏ãË°ågg Á¨¨‰∏ÄË°åÁ¨¨‰∏Ä‰∏™Â≠óÊØçG ÊúÄÂêé‰∏ÄË°åÁ¨¨‰∏Ä‰∏™Â≠óÊØçshift+$ ËØ•Ë°åÊúÄÂêé‰∏Ä‰∏™Â≠óÊØçÂ∞æË°åÊ®°Âºèq! Âº∫Âà∂ÈÄÄÂá∫qw ÂÜôÂÖ•Âπ∂ÈÄÄÂá∫qw! Âº∫Âà∂ÂÜôÂÖ•ÈÄÄÂá∫x ÈÄÄÂá∫ÔºåÂ¶ÇÊûúÂ≠òÂú®ÊîπÂä®ÔºåÂàô‰øùÂ≠òÂÜçÈÄÄÂá∫]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Âü∫Á°Ä</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFSÊû∂ÊûÑËÆæËÆ°ÂèäÂâØÊú¨ÊîæÁΩÆÁ≠ñÁï•]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFSÊû∂ÊûÑËÆæËÆ°ÂèäÂâØÊú¨ÊîæÁΩÆÁ≠ñÁï•HDFS‰∏ªË¶ÅÁî±3‰∏™ÁªÑ‰ª∂ÊûÑÊàêÔºåÂàÜÂà´ÊòØNameNode„ÄÅSecondaryNameNodeÂíåDataNodeÔºåHSFSÊòØ‰ª•master/slaveÊ®°ÂºèËøêË°åÁöÑÔºåÂÖ∂‰∏≠NameNode„ÄÅSecondaryNameNode ËøêË°åÂú®masterËäÇÁÇπÔºåDataNodeËøêË°åslaveËäÇÁÇπ„ÄÇNameNodeÂíåDataNodeÊû∂ÊûÑÂõæNameNode(ÂêçÁß∞ËäÇÁÇπ)Â≠òÂÇ®ÔºöÂÖÉ‰ø°ÊÅØÁöÑÁßçÁ±ªÔºåÂåÖÂê´:Êñá‰ª∂ÂêçÁß∞Êñá‰ª∂ÁõÆÂΩïÁªìÊûÑÊñá‰ª∂ÁöÑÂ±ûÊÄß[ÊùÉÈôê,ÂàõÂª∫Êó∂Èó¥,ÂâØÊú¨Êï∞]Êñá‰ª∂ÂØπÂ∫îÂì™‰∫õÊï∞ÊçÆÂùó‚Äì&gt;Êï∞ÊçÆÂùóÂØπÂ∫îÂì™‰∫õdatanodeËäÇÁÇπ‰ΩúÁî®ÔºöÁÆ°ÁêÜÁùÄÊñá‰ª∂Á≥ªÁªüÂëΩÂêçÁ©∫Èó¥Áª¥Êä§ËøôÊñá‰ª∂Á≥ªÁªüÊ†ëÂèäÊ†ë‰∏≠ÁöÑÊâÄÊúâÊñá‰ª∂ÂíåÁõÆÂΩïÁª¥Êä§ÊâÄÊúâËøô‰∫õÊñá‰ª∂ÊàñÁõÆÂΩïÁöÑÊâìÂºÄ„ÄÅÂÖ≥Èó≠„ÄÅÁßªÂä®„ÄÅÈáçÂëΩÂêçÁ≠âÊìç‰ΩúDataNode(Êï∞ÊçÆËäÇÁÇπ)Â≠òÂÇ®ÔºöÊï∞ÊçÆÂùó„ÄÅÊï∞ÊçÆÂùóÊ†°È™å„ÄÅ‰∏éNameNodeÈÄö‰ø°‰ΩúÁî®ÔºöËØªÂÜôÊñá‰ª∂ÁöÑÊï∞ÊçÆÂùóNameNodeÁöÑÊåáÁ§∫Êù•ËøõË°åÂàõÂª∫„ÄÅÂà†Èô§„ÄÅÂíåÂ§çÂà∂Á≠âÊìç‰ΩúÈÄöËøáÂøÉË∑≥ÂÆöÊúüÂêëNameNodeÂèëÈÄÅÊâÄÂ≠òÂÇ®Êñá‰ª∂ÂùóÂàóË°®‰ø°ÊÅØScondary NameNode(Á¨¨‰∫åÂêçÁß∞ËäÇÁÇπ)Â≠òÂÇ®: ÂëΩÂêçÁ©∫Èó¥ÈïúÂÉèÊñá‰ª∂fsimage+ÁºñËæëÊó•Âøóeditlog‰ΩúÁî®: ÂÆöÊúüÂêàÂπ∂fsimage+editlogÊñá‰ª∂‰∏∫Êñ∞ÁöÑfsimageÊé®ÈÄÅÁªôNamenNodeÂâØÊú¨ÊîæÁΩÆÁ≠ñÁï•Á¨¨‰∏ÄÂâØÊú¨ÔºöÊîæÁΩÆÂú®‰∏ä‰º†Êñá‰ª∂ÁöÑDataNode‰∏äÔºõÂ¶ÇÊûúÊòØÈõÜÁæ§Â§ñÊèê‰∫§ÔºåÂàôÈöèÊú∫ÊåëÈÄâ‰∏ÄÂè∞Á£ÅÁõò‰∏çÂ§™ÊÖ¢„ÄÅCPU‰∏çÂ§™ÂøôÁöÑËäÇÁÇπ‰∏äÁ¨¨‰∫åÂâØÊú¨ÔºöÊîæÁΩÆÂú®‰∏éÁ¨¨‰∏Ä‰∏™ÂâØÊú¨‰∏çÂêåÁöÑÊú∫Êû∂ÁöÑËäÇÁÇπ‰∏äÁ¨¨‰∏âÂâØÊú¨Ôºö‰∏éÁ¨¨‰∫å‰∏™ÂâØÊú¨Áõ∏ÂêåÊú∫Êû∂ÁöÑ‰∏çÂêåËäÇÁÇπ‰∏äÂ¶ÇÊûúËøòÊúâÊõ¥Â§öÁöÑÂâØÊú¨ÔºöÈöèÊú∫ÊîæÂú®ËäÇÁÇπ‰∏≠]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>Êû∂ÊûÑ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÈÖçÁΩÆÂ§öÂè∞ËôöÊãüÊú∫‰πãÈó¥ÁöÑSSH‰ø°‰ªª]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[Êú¨Êú∫ÁéØÂ¢É3Âè∞Êú∫Âô®ÊâßË°åÂëΩ‰ª§ssh-keygenÈÄâÂèñÁ¨¨‰∏ÄÂè∞,ÁîüÊàêauthorized_keysÊñá‰ª∂hadoop002 hadoop003‰º†Ëæìid_rsa.pubÊñá‰ª∂Âà∞hadoop001hadoop001Êú∫Âô® ÂêàÂπ∂id_rsa.pub2„ÄÅid_rsa.pub3Âà∞authorized_keysËÆæÁΩÆÊØèÂè∞Êú∫Âô®ÁöÑÊùÉÈôê12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keysÂ∞Üauthorized_keysÂàÜÂèëÂà∞hadoop002„ÄÅhadoop003Êú∫Âô®È™åËØÅ(ÊØèÂè∞Êú∫Âô®‰∏äÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§ÔºåÂè™ËæìÂÖ•yesÔºå‰∏çËæìÂÖ•ÂØÜÁ†ÅÔºåËØ¥ÊòéÈÖçÁΩÆÊàêÂäü)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ÁéØÂ¢ÉÊê≠Âª∫</tag>
        <tag>Âü∫Á°Ä</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
