<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Docker实践之常用命令及自定义Web首页</title>
      <link href="/2019/06/28/Docker%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89Web%E9%A6%96%E9%A1%B5/"/>
      <url>/2019/06/28/Docker%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89Web%E9%A6%96%E9%A1%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker --help</span><br><span class="line">//常用命令：</span><br><span class="line">--------------------------------------------</span><br><span class="line">  exec        Run a command in a running container</span><br><span class="line">  history     Show the history of an image</span><br><span class="line">  images      List images</span><br><span class="line">  kill        Kill one or more running containers</span><br><span class="line">  logs        Fetch the logs of a container</span><br><span class="line">  ps          List containers</span><br><span class="line">  pull        Pull an image or a repository from a registry</span><br><span class="line">  push        Push an image or a repository to a registry</span><br><span class="line">  rename      Rename a container</span><br><span class="line">  restart     Restart one or more containers</span><br><span class="line">  rm          Remove one or more containers</span><br><span class="line">  rmi         Remove one or more images</span><br><span class="line">  run         Run a command in a new container</span><br><span class="line">  search      Search the Docker Hub for images</span><br><span class="line">  start       Start one or more stopped containers</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics</span><br><span class="line">  stop        Stop one or more running containers</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span><br><span class="line">  top         Display the running processes of a container</span><br><span class="line">  version     Show the Docker version information</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker search nginx</span><br><span class="line">NAME                                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">nginx                                                  Official build of Nginx.                        10179               [OK]</span><br><span class="line">jwilder/nginx-proxy                                    Automated Nginx reverse proxy for docker con…   1454                                    [OK]</span><br><span class="line">richarvey/nginx-php-fpm                                Container running Nginx + PHP-FPM capable of…   645                                     [OK]</span><br><span class="line">jrcs/letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as p…   436                                     [OK]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker pull nginx   //拉取官方版本的nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">f17d81b4b692: Pull complete</span><br><span class="line">82dca86e04c3: Downloading  11.24MB/22.2MB</span><br><span class="line">82dca86e04c3: Pull complete</span><br><span class="line">046ccb106982: Pull complete</span><br><span class="line">Digest: sha256:d59a1aa7866258751a261bae525a1842c7ff0662d4f34a355d5f36826abc0341</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>docker相当于一个小型的linux系统，但是它又只是一个单一的进程，可以不对外暴露端口号，如果对外暴露端口号，那也只能有一个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \     //运行一个实例</span><br><span class="line">--name huluwa-niginx-v1 \            //自定义一个名字</span><br><span class="line">-d \                                 //后台运行</span><br><span class="line">-p 8080:80 \                         //对外暴露的端口号，对应linux的8080端口号</span><br><span class="line">nginx:latest                         //运行的镜像名及版本</span><br><span class="line">d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps     //查看当前活动的实例</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   2 minutes ago</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# ps -ef | grep docker</span><br><span class="line">root     23182     1  0 22:00 ?        00:00:07 /usr/bin/dockerd</span><br><span class="line">root     23189 23182  0 22:00 ?        00:00:03 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root     25014 23182  0 22:27 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8080 -container-ip 172.17.0.2 -container-port 80</span><br><span class="line">root     25021 23189  0 22:27 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root     25492 10525  0 22:35 pts/0    00:00:00 grep --color=auto docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker-proxy -proto tcp&lt;br&gt;</span><br><span class="line">-host-ip 0.0.0.0   //&lt;br&gt;</span><br><span class="line">-host-port 8080   //linux系统的端口号&lt;br&gt;</span><br><span class="line">-container-ip 172.17.0.2  //docker相当于一个小型的linux系统，这就是小型系统的IP地址&lt;br&gt;</span><br><span class="line">-container-port 80  //docker内部的一个端口号&lt;br&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# netstat -nlp |grep 8080</span><br><span class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      25014/docker-proxy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker images    //查看所有的镜像</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              62f816a209e6        7 days ago          109MB</span><br><span class="line">mysql               5.6                 a46c2a2722b9        2 weeks ago         256MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        2 months ago        1.84kB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps -a   //查看所有实例，不论什么状态</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                    NAMES</span><br><span class="line">9883abaaad85        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour               0.0.0.0:3308-&gt;3306/tcp   huluwa-mysql-v5</span><br><span class="line">34fb53521694        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Created                                                 huluwa-mysql-v4</span><br><span class="line">2a5c95f3c043        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v3</span><br><span class="line">84e65fd24271        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v2</span><br><span class="line">b3c12bcb28eb        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysqlv1</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   12 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   13 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 13 hours ago        Exited (0) 13 hours ago                                 elastic_ritchie</span><br></pre></td></tr></table></figure><p>正在运行的status就是Up，已经关闭的status就是Exited</p><h3 id="自定义首页"><a href="#自定义首页" class="headerlink" title="自定义首页"></a>自定义首页</h3><ol><li>登录初始的nginx Web页面</li></ol><p><img src="/assets/blogImg/2019-06-28-1.png" alt="enter description here"></p><ol start="2"><li><p>通过index.html配置一个自定义的首页</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 html]# pwd</span><br><span class="line">/root/docker/nginx/html</span><br><span class="line">[root@hadoop004 html]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 92 Nov 13 23:09 index.html</span><br></pre></td></tr></table></figure><p>在windows中打开index.html页面是这样的：<br><img src="/assets/blogImg/2019-06-28-2.png" alt="enter description here"></p></li><li><p>将本地的html文件挂载到container中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \</span><br><span class="line">--name huluwa-niginx-v2 \</span><br><span class="line">-v /root/docker/nginx/html:/usr/share/nginx/html:ro \ //本地的/root/docker/nginx/html和容器里的/usr/share/nginx/html建立一个映射，将本地的文件夹挂载到容器里</span><br><span class="line">-d \</span><br><span class="line">-p 8082:80 \</span><br><span class="line">nginx:latest</span><br><span class="line">c7937fd855963c7cca831d495436881a16e7e9befa61288cb28e2ab8b986decf</span><br><span class="line">[root@hadoop004 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                         PORTS                  NAMES</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About a minute ago   Up About a minute              0.0.0.0:8082-&gt;80/tcp   huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About an hour ago    Up About an hour               0.0.0.0:8080-&gt;80/tcp   huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 About an hour ago    Exited (0) About an hour ago                          elastic_ritchie</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>打开ip:8082页面查看</li></ol><p><img src="/assets/blogImg/2019-06-28-3.png" alt="enter description here"></p><p>发现首页已经被置换为本地文件中的index.html文件<br><br>-v 把本地文件或文件夹挂载到容器中<br><br>挂载的目的，就是把容器中的数据保存在本地，容器进程移除后之后，数据不会丢失，如果不挂载的话，容器进程挂掉之后，数据就全没有了<br><br>ro：可读<br><br>rw：可读写<br></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不得不会的Spark SQL常见4种数据源</title>
      <link href="/2019/06/20/%E4%B8%8D%E5%BE%97%E4%B8%8D%E4%BC%9A%E7%9A%84Spark%20SQL%E5%B8%B8%E8%A7%814%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
      <url>/2019/06/20/%E4%B8%8D%E5%BE%97%E4%B8%8D%E4%BC%9A%E7%9A%84Spark%20SQL%E5%B8%B8%E8%A7%814%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h3><h4 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h4><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。</p><p>修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          </span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><h4 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h3><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Encoders for most common types are automatically provided by importing spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// DataFrames can be saved as Parquet files, maintaining the schema information</span><br><span class="line">peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Read in the parquet file created above</span><br><span class="line">// Parquet files are self-describing so the schema is preserved</span><br><span class="line">// The result of loading a Parquet file is also a DataFrame</span><br><span class="line">val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet files can also be used to create a temporary view and then used in SQL statements</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">// +------------+</span><br><span class="line">// |       value|</span><br><span class="line">// +------------+</span><br><span class="line">// |Name: Justin|</span><br><span class="line">// +------------+</span><br></pre></td></tr></table></figure><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet</p><p>或SQLContext.read.load，Spark SQL将自动解析分区信息。</p><p>返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项：</p><p><code>spark.sql.parquet.mergeSchema</code>为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sqlContext from the previous example is used in this example.</span><br><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, stored into a partition directory</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths.</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">.config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><h4 id="内嵌Hive应用"><a href="#内嵌Hive应用" class="headerlink" title="内嵌Hive应用"></a>内嵌Hive应用</h4><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.warehouse.dir=</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive应用"><a href="#外部Hive应用" class="headerlink" title="外部Hive应用"></a>外部Hive应用</h4><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;rddtable2&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之五: ParNew收集器+CMS收集器的产品案例分析(响应时间优先)</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%94_ParNew%E6%94%B6%E9%9B%86%E5%99%A8+CMS%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BA%A7%E5%93%81%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90(%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88)/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%94_ParNew%E6%94%B6%E9%9B%86%E5%99%A8+CMS%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BA%A7%E5%93%81%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90(%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><font color="green" size="3"><b>双核,4个cores; 16G memory</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@alish2-cassandra-01 ~]# cat /proc/cpuinfo | grep &quot;cpu cores&quot;</span><br><span class="line">cpu cores       : 2</span><br><span class="line">cpu cores       : 2</span><br></pre></td></tr></table></figure><h3 id="公式简述"><a href="#公式简述" class="headerlink" title="公式简述"></a>公式简述</h3><p>响应时间优先的并发收集器，主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。</p><ol><li><font size="3" color="red">ParNew收集器</font><p>ParNew收集器是Serial收集器的多线程版本，许多运行在Server模式下的虚拟机中首选的新生代收集器，除Serial外，<font color="blue">只有它能与CMS收集器配合工作。</font></p></li><li><font size="3" color="red">CMS收集器</font><p>CMS， 全称Concurrent Low Pause Collector，是jdk1.4后期版本开始引入的新gc算法，在jdk5和jdk6中得到了进一步改进，它的主要适合场景是对响应时间的重要性需求 大于对吞吐量的要求，能够承受垃圾回收线程和应用线程共享处理器资源，并且应用中存在比较多的长生命周期的对象的应用。CMS是用于对tenured generation的回收，也就是年老代的回收，目标是尽量减少应用的暂停时间，减少FullGC发生的几率，利用和应用程序线程并发的垃圾回收线程来 标记清除年老代。<br>CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期是这样：<br><br><br><font size="3" color="blue">初始标记(CMS-initial-mark) -&gt; 并发标记(CMS-concurrent-mark) -&gt; 重新标记(CMS-remark) -&gt; 并发清除(CMS-concurrent-sweep) -&gt;并发重设状态等待下次CMS的触发(CMS-concurrent-reset)</font><br><br><br>其中的1，3两个步骤需要暂停所有的应用程序线程的。第一次暂停从root对象开始标记存活的对象，这个阶段称为初始标记；第二次暂停是在并发标记之后，暂停所有应用程序线程，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致）。第一次暂停会比较短，第二次暂停通常会比较长，并且remark这个阶段可以并行标记。<br><br><br>而并发标记、并发清除、并发重设阶段的所谓并发，是指一个或者多个垃圾回收线程和应用程序线程并发地运行，垃圾回收线程不会暂停应用程序的执行，如果你有多于一个处理器，那么并发收集线程将与应用线程在不同的处理器上运行，显然，这样的开销就是会降低应用的吞吐量。Remark阶段的并行，是指暂停了所有应用程序后，启动一定数目的垃圾回收进程进行并行标记，此时的应用线程是暂停的。</p></li></ol><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>($TOMCAT_HOME/bin/catalina.sh)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_OPTS=&quot;-server -Xmx10240m -Xms10240m -Xmn3840m -XX:PermSize=256m</span><br><span class="line"></span><br><span class="line">-XX:MaxPermSize=256m -Denv=denalicnprod</span><br><span class="line"></span><br><span class="line">-XX:SurvivorRatio=8  -XX:PretenureSizeThreshold=1048576</span><br><span class="line"></span><br><span class="line">-XX:+DisableExplicitGC  </span><br><span class="line"></span><br><span class="line">-XX:+UseParNewGC  -XX:ParallelGCThreads=10</span><br><span class="line"></span><br><span class="line">-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled</span><br><span class="line"></span><br><span class="line">-XX:+CMSScavengeBeforeRemark -XX:ParallelCMSThreads=10</span><br><span class="line"></span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</span><br><span class="line"></span><br><span class="line">-XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled</span><br><span class="line"></span><br><span class="line">-XX:+UseFastAccessorMethods</span><br><span class="line"></span><br><span class="line">-XX:LargePageSizeInBytes=128M</span><br><span class="line"></span><br><span class="line">-XX:SoftRefLRUPolicyMSPerMB=0</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDateStamps -Xloggc:gc.log -verbose:gc&quot;</span><br></pre></td></tr></table></figure><h3 id="公式解析"><a href="#公式解析" class="headerlink" title="公式解析"></a>公式解析</h3><table><thead><tr><th>参 数</th><th>含 义</th></tr></thead><tbody><tr><td>-server</td><td>一定要作为第一个参数，启用JDK的server版本，在多个CPU时性能佳</td></tr><tr><td>-Xms</td><td>java Heap初始大小。 默认是物理内存的1/64。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。</td></tr><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存。</td></tr><tr><td>-Xmn</td><td>设置年轻代大小，一般设置为Xmx的2/8~3/8,等同于-XX:NewSize 和 -XX:MaxNewSize 。</td></tr><tr><td>-XX:PermSize</td><td>设定内存的永久保存区初始大小，缺省值为64M</td></tr><tr><td>-XX:MaxPermSize</td><td>设定内存的永久保存区最大大小，缺省值为64M</td></tr><tr><td>-Denv</td><td>指定tomcat运行哪个project</td></tr><tr><td>-XX:SurvivorRatio</td><td>Eden区与Survivor区的大小比值, 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10</td></tr><tr><td>-XX:PretenureSizeThreshold</td><td>晋升年老代的对象大小。默认为0，比如设为1048576(1M)，则超过1M的对象将不在eden区分配，而直接进入年老代。</td></tr><tr><td>-XX:+DisableExplicitGC</td><td>关闭System.gc()</td></tr><tr><td><font color="#1E90FF">-XX:+UseParNewGC</font></td><td><font color="#1E90FF">设置年轻代为并发收集。可与CMS收集同时使用。</font></td></tr><tr><td>-XX:ParallelGCThreads</td><td></td></tr><tr><td><font color="#1E90FF">-XX:+UseConcMarkSweepGC</font></td><td><font color="#1E90FF">设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了。所以，此时年轻代大小最好用-Xmn设置。</font></td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>开启并行remark</td></tr><tr><td>-XX:+CMSScavengeBeforeRemark</td><td>这个参数还蛮重要的，它的意思是在执行CMS remark之前进行一次youngGC，这样能有效降低remark的时间</td></tr><tr><td>-XX:ParallelCMSThreads</td><td>CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4) ，如果你需要明确设定，可以通过-XX:ParallelCMSThreads=20来设定,其中ParallelGCThreads是年轻代的并行收集线程数</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction</td><td><font color="#3CB371">使用cms作为垃圾回收使用70％后开始CMS收集</font></td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td></tr><tr><td>-XX:+UseCMSCompactAtFullCollection</td><td>打开对年老代的压缩。可能会影响性能，但是可以消除内存碎片。</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此参数设置运行次FullGC以后对内存空间进行压缩、整理。</td></tr><tr><td>-XX:+CMSPermGenSweepingEnabled</td><td>为了避免Perm区满引起的full gc，<font color="#3CB371">建议开启CMS回收Perm区选项</font></td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr><tr><td>-XX:+UseFastAccessorMethods</td><td>原始类型的快速优化</td></tr><tr><td>-XX:LargePageSizeInBytes</td><td>内存页的大小，不可设置过大， 会影响Perm的大小</td></tr><tr><td>-XX:SoftRefLRUPolicyMSPerMB</td><td>“软引用”的对象在最后一次被访问后能存活0毫秒（默认为1秒）。</td></tr><tr><td>-XX:+PrintGCDetails</td><td>记录 GC 运行时的详细数据信息，包括新生成对象的占用内存大小以及耗费时间等</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td>打印垃圾收集的时间戳</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td></tr><tr><td>-XX:+PrintGCDateStamps</td><td>之前打印gc日志的时候使用是：-XX:+PrintGCTimeStamps，这个选项记录的是jvm启动时间为起点的相对时间，可读性较差，不利于定位问题，使用PrintGCDateStamps记录的是系统时间，更humanreadable</td></tr><tr><td>-Xloggc</td><td>与上面几个配合使用，把相关日志信息记录到文件以便分析</td></tr><tr><td>-verbose:gc</td><td>记录 GC 运行以及运行时间，一般用来查看 GC 是否是应用的瓶颈</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之一: 内存结构(堆内存和非堆内存)</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%80_%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84(%E5%A0%86%E5%86%85%E5%AD%98%E5%92%8C%E9%9D%9E%E5%A0%86%E5%86%85%E5%AD%98)/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%80_%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84(%E5%A0%86%E5%86%85%E5%AD%98%E5%92%8C%E9%9D%9E%E5%A0%86%E5%86%85%E5%AD%98)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p><strong>图为Java虚拟机运行时的数据区:</strong></p><p><img src="/assets/pic/2019-06-19-1-1.png" alt="数据区"></p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>也称”永久代” 、“非堆”， 它用于存储虚拟机加载的类信息、常量、静态变量、是<font color="red">各个线程共享的内存区域</font>。<font color="blue">默认最小值为16MB，最大值为64MB（未验证）</font>，可以通过-XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。<br><br><br>运行时常量池：是方法区的一部分，Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种符号引用，这部分内容将在类加载后放到方法区的运行时常量池中。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>描述的是java 方法执行的内存模型：每个方法被执行的时候 都会创建一个“栈帧”用于存储局部变量表(包括参数)、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。声明周期与线程相同，是<font color="red">线程私有</font>的。<br><br><br>局部变量表存放了编译器可知的各种基本数据类型(boolean、byte、char、short、int、float、long、 double)、对象引用(引用指针，并非对象本身)，其中64位长度的long和double类型的数据会占用2个局部变量的空间，其余数据类型只占1 个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量是完全确定的，在运行期间栈帧不会改变局部 变量表的大小空间。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>与虚拟机栈基本类似，区别在于虚拟机栈为虚拟机执行的java方法服务，而本地方法栈则是为Native方法服务。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>也叫做java 堆、GC堆，是java虚拟机所管理的内存中最大的一块内存区域，也是<font color="red">被各个线程共享的内存区域</font>，在JVM启动时创建。该内存区域存放了对象实例及数组(所有new的对象)。其大小通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，-Xmx为JVM可申请的最大内存。在JVM启动时，最大内存会被保留下来。为对象内存而保留的地址空间可以被分成年轻代和老年代。<br><br><br>默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过-XX:MinHeapFreeRation=来指定这个比列；当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过XX:MaxHeapFreeRation=来指定这个比列，对于运行系统，为避免在运行时频繁调整Heap的大小，通常-Xms与-Xmx的值设成一样。</p><table><thead><tr><th style="text-align:center">Parameter</th><th style="text-align:center">Default Value</th></tr></thead><tbody><tr><td style="text-align:center">MinHeapFreeRatio</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">MaxHeapFreeRatio</td><td style="text-align:center">70</td></tr><tr><td style="text-align:center">-Xms</td><td style="text-align:center">3670k</td></tr><tr><td style="text-align:center">-Xmx</td><td style="text-align:center">64m</td></tr></tbody></table><font color="red">注：如果是64位系统，这些值一般需要扩张30％，来容纳在64位系统下变大的对象。</font><p>从J2SE 1.2开始，JVM使用分代收集算法，在不同年代的区域里使用不同的算法。堆被划分为新生代和老年代。新生代主要存储新创建的对象和尚未进入老年代的对象。老年代存储经过多次新生代GC(MinorGC)任然存活的对象。</p><p><img src="/assets/pic/2019-06-19-1-2.png" alt="堆"></p><font color="red"><b><br>注1：图中的Perm不是堆内存，是永久代<br><br>注2：图中的Virtaul则是各区域还未被分配的内存，即最大内存-当前分配的内存<br></b></font><p><strong>新生代：</strong></p><p>新生代包括一块eden（伊甸园）和2块survivor(通常又称S0和S1或From和To)。大多数对象都是在eden中初始化。而对于2块survivor来说，总有一块是空的，它会在下一个复制收集过程中作为eden中的活跃对象和另一块survivor的目的地。在对象衰老之前（也就是被复制到tenured之前），它们会在两块survivor区域之间以这样的方式复制。可通过-Xmn参数来指定新生代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及Survivor Space的大小。</p><p><strong>老年代：</strong></p><p>用于存放经过多次新生代Minor GC依然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：</p><ol><li>大对象，可通过启动参数设置-XX:PretenureSizeThreshold=1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。</li><li>大的数组对象，即数组中无引用外部对象。</li></ol><p>老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。</p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>是最小的一块内存区域，它的作用是当前线程所执行的字节码的行号指示器，在虚拟机的模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
            <tag> 内存结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之七: Java程序性能分析工具Java VisualVM(Visual GC)</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%83_Java%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7Java%20VisualVM(Visual%20GC)/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%83_Java%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7Java%20VisualVM(Visual%20GC)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。</p><p>在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。</p><a id="more"></a><h3 id="Visual-GC-监控垃圾回收器"><a href="#Visual-GC-监控垃圾回收器" class="headerlink" title="Visual GC(监控垃圾回收器)"></a>Visual GC(监控垃圾回收器)</h3><p>Java VisualVM默认没有安装Visual GC插件，需要手动安装，JDK的安装目录的bin目露下双击jvisualvm.exe，即可打开Java VisualVM，点击菜单栏 工具-&gt;插件 安装Visual GC</p><p><img src="/assets/pic/2019-06-19-7-1.png" alt="Visual GC(监控垃圾回收器)1"></p><p>安装完成后重启Java VisualVM，Visual GC界面自动打开，即可看到JVM中堆内存的分代情况</p><p><img src="/assets/pic/2019-06-19-7-2.png" alt="Visual GC(监控垃圾回收器)2"></p><p>被监控的程序运行一段时间后Visual GC显示如下</p><p><img src="/assets/pic/2019-06-19-7-3.png" alt="Visual GC(监控垃圾回收器)3"></p><p>要看懂上面的图必须理解Java虚拟机的一些基本概念：</p><font color="blue" size="3"><b>堆(Heap)</b></font>：<font size="3">JVM管理的内存叫堆</font><p><strong>分代</strong>：根据对象的生命周期长短，把堆分为3个代：Young，Old和Permanent，根据不同代的特点采用不同的收集算法，扬长避短也。</p><ul><li><font color="blue">Young（年轻代）</font>年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor复制过来的对象。而且，Survivor区总有一个是空的。</li><li><font color="blue">Tenured（年老代）</font>年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。</li><li><font color="blue">Perm（持久代）</font>用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。</li></ul><font color="blue" size="3"><b>GC的基本概念</b></font><p>gc分为full gc 跟 minor gc，当每一块区满的时候都会引发gc。</p><ul><li><font color="blue">Scavenge GC</font><p>一般情况下，当新对象生成，并且在Eden申请空间失败时，就触发了Scavenge GC，堆Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。</p></li><li><font color="blue">Full GC</font><p>对整个堆进行整理，包括Young、Tenured和Perm。Full GC比Scavenge GC要慢，因此应该尽可能减少Full GC。有如下原因可能导致Full GC:</p><ul><li>上一次GC之后Heap的各域分配策略动态变化</li><li>System.gc()被显示调用</li><li>Perm域被写满</li><li>Tenured被写满</li></ul></li></ul><font color="blue" size="3"><b>内存溢出 out of memory</b></font><p>是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。</p><font color="blue" size="3"><b>内存泄露 memory leak</b></font><p>是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。<strong>其实说白了就是该内存空间使用完毕之后未回收。</strong></p><h3 id="Java-VisualVM的其他功能"><a href="#Java-VisualVM的其他功能" class="headerlink" title="Java VisualVM的其他功能"></a>Java VisualVM的其他功能</h3><ol><li><p>监视界面（cpu，类，堆，线程）</p><p><img src="/assets/pic/2019-06-19-7-4.png" alt="监视界面"></p></li><li><p>线程界面</p><p><img src="/assets/pic/2019-06-19-7-5.png" alt="线程界面"></p></li></ol><ol start="3"><li><p>Profile界面（性能剖析）</p><p>点击CPU按钮执行cpu分析查看方法</p><p><img src="/assets/pic/2019-06-19-7-6.png" alt="Profile界面"></p><p>点击内存按钮执行内存分析查看类</p><p><img src="/assets/pic/2019-06-19-7-7.png" alt="Profile界面"></p></li><li><p>堆dump和线程dump操作</p><p>Dump文件是进程的内存镜像，可以把程序的执行状态通过调试器保存到dump文件中，堆dump的dump文件内容如下图所示</p><p><img src="/assets/pic/2019-06-19-7-8.png" alt="Dump"></p></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之八: GC插件&amp;错误not_supported_for_this_jvm&amp;命令jstatd</title>
      <link href="/2019/06/19/JVM_8/"/>
      <url>/2019/06/19/JVM_8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a> <font size="4"><b>1.插件安装</b></font><p>tools-&gt;plugin-&gt;Available Plugin 会有值得安装的插件，如：VisualGC</p><p><img src="/assets/pic/2019-06-19-8-1.png" alt="插件安装"></p><p>插件列表: <a href="https://visualvm.dev.java.net/plugins.html" target="_blank" rel="noopener">https://visualvm.dev.java.net/plugins.html</a></p><p>注意：上面提供的端口配置有些麻烦，不如直接这样做:</p><font size="4"><b>2.要使用 VisualGC 必须在远程机上启动jstatd代理程序，否则会显示<font color="#FF4500">“not supported for this jvm” </font>错误</b></font> <font color="blue">而启动 jstatd 时会有一个权限问题，需要做如下修改：</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@xxx-01 ~]# java -version</span><br><span class="line">java version &quot;1.7.0_55&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_55-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)</span><br><span class="line">[root@xxx-01 ~]# jstatd </span><br><span class="line">Could not create remote object</span><br><span class="line">access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">java.security.AccessControlException: access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)</span><br><span class="line">        at java.security.AccessController.checkPermission(AccessController.java:559)</span><br><span class="line">        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)</span><br><span class="line">        at java.lang.System.setProperty(System.java:783)</span><br><span class="line">        at sun.tools.jstatd.Jstatd.main(Jstatd.java:139)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@xxx-01 ~]# cd  /usr/java/jdk1.7.0_55</span><br><span class="line">[root@xxx-01 ~]# vi /usr/java/jdk1.7.0_55/jstatd.all.policy</span><br><span class="line">    grant codebase &quot;file:$&#123;JAVA_HOME&#125;/lib/tools.jar&quot; &#123;  </span><br><span class="line">     permission java.security.AllPermission;  </span><br><span class="line">    &#125;;  </span><br><span class="line">[root@xxx-01 jdk1.7.0_55]# jstatd -J-Djava.security.policy=/usr/java/jdk1.7.0_55/jstatd.all.policy  &amp;</span><br></pre></td></tr></table></figure><font color="blue">然后后台模式启动 jstatd命令</font> <font color="blue">主机面GC:</font><p><img src="/assets/pic/2019-06-19-8-2.png" alt="主机面GC"></p><font color="blue">Threads:</font><p><img src="/assets/pic/2019-06-19-8-3.png" alt="Threads"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之四: 堆内存分配的CMS公式解析</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%9B%9B_%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%9B%9B_%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="JVM-堆内存组成"><a href="#JVM-堆内存组成" class="headerlink" title="JVM 堆内存组成"></a>JVM 堆内存组成</h3><p>Java堆由Perm区和Heap区组成，Heap区由Old区和New区（也叫Young区）组成，New区由Eden区、From区和To区（Survivor）组成。</p><p><img src="/assets/pic/2019-06-19-4-1.png" alt="JVM 堆内存组成"></p><p>Eden区用于存放新生成的对象。Eden中的对象生命不会超过一次Minor GC。Survivor Space 有两个，存放每次垃圾回收后存活的对象，即图的S0和S1。Old Generation Old区，也称老生代，主要存放应用程序中生命周期长的存活对象</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：</p><p>eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况。</p><p>即：</p><font color="blue">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2))</font><p>进而推断出：</p><font color="blue">CMSInitiatingOccupancyFraction &lt;= ((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100</font><table><thead><tr><th style="text-align:left">参数</th><th>含义</th></tr></thead><tbody><tr><td style="text-align:left">Xmx-Xmn</td><td>Old区大小</td></tr><tr><td style="text-align:left">CMSInitiatingOccupancyFraction/100</td><td>Old区百分之多少时,cms开始gc</td></tr><tr><td style="text-align:left">1-CMSInitiatingOccupancyFraction/100</td><td>Old区开始gc回收时剩余空间百分比</td></tr><tr><td style="text-align:left">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)</td><td>Old区开始gc回收时剩余空间大小</td></tr><tr><td style="text-align:left">(Xmn-Xmn/(SurvivorRatior+2))</td><td>eden+from survivor区的大小</td></tr></tbody></table><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存</td></tr><tr><td>-Xmn</td><td>java heap最小值，一般设置为Xmx的3、4分之一,等同于-XX:NewSize 和 -XX:MaxNewSize ,其实为<font color="blue">young区大小</font></td></tr><tr><td>-XX</td><td>CMSInitiatingOccupancyFraction=70 :使用cms作为垃圾回收使用70％后开始CMS收集</td></tr><tr><td>-XX</td><td>SurvivorRatio=2: 生还者池的大小，默认是2</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
            <tag> 堆内存分配的CMS公式解析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之六: JVM参数设置及分析</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%85%AD_JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E6%9E%90/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%85%AD_JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>不管是YGC还是Full GC,GC过程中都会对导致程序运行中中断,正确的选择不同的GC策略,调整JVM、GC的参数，可以极大的减少由于GC工作，而导致的程序运行中断方面的问题，进而适当的提高Java程序的工作效率。但是调整GC是以个极为复杂的过程，由于各个程序具备不同的特点，如：web和GUI程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要cup个数，内存不同），所以使用的GC种类也会不同(如何选择见GC种类及如何选择)。本文将注重介绍JVM、GC的一些重要参数的设置来提高系统的性能。</p><p>JVM内存组成及GC相关内容请见之前的文章:JVM内存组成 GC策略&amp;内存申请</p><font size="3"><strong>JVM参数的含义</strong></font>实例见实例分析<br><br>参数名称|含义|默认值| |<br>——|—–|—–|—<br>-Xms|初始堆大小|物理内存的1/64(&lt;1GB)|默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.<br>-Xmx|最大堆大小|物理内存的1/4(&lt;1GB)|默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制<br>-Xmn|年轻代大小(1.4or lator)||注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。<br>整个堆大小=年轻代大小 + 年老代大小 + 持久代大小.<br>增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8<br>-XX:NewSize|设置年轻代大小(for 1.3/1.4)|<br>-XX:MaxNewSize|年轻代最大值(for 1.3/1.4)|<br>-XX:PermSize|设置持久代(perm gen)初始值|物理内存的1/64<br>-XX:MaxPermSize|设置持久代最大值|物理内存的1/4<br>-Xss|每个线程的堆栈大小||JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右。<br>一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。<br>-XX:ThreadStackSize|Thread Stack Size||(0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.]<br>-XX:NewRatio|年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)||-XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5<br>Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。<br>-XX:SurvivorRatio|Eden区与Survivor区的大小比值||设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10<br>-XX:LargePageSizeInBytes|内存页的大小不可设置过大， 会影响Perm的大小||=128m<br>-XX:+UseFastAccessorMethods|原始类型的快速优化<br>-XX:+DisableExplicitGC|关闭System.gc()||这个参数需要严格的测试<br>-XX:MaxTenuringThreshold|垃圾最大年龄||如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率<br>该参数只有在串行GC时才有效.<br>-XX:+AggressiveOpts|加快编译<br>-XX:+UseBiasedLocking|锁机制的性能改善<br>-Xnoclassgc|禁用垃圾回收<br>-XX:SoftRefLRUPolicyMSPerMB|每兆堆空闲空间中SoftReference的存活时间|1s|softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap<br>-XX:PretenureSizeThreshold|对象超过多大是直接在旧生代分配|0|单位字节 新生代采用Parallel Scavenge GC时无效<br>另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.<br>-XX:TLABWasteTargetPercent|TLAB占eden区的百分比|1%<br>-XX:+CollectGen0First|FullGC时是否先YGC|false<br><br><font size="3"><strong>并行收集器相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseParallelGC</td><td>Full GC采用parallel MSC<br>(此项待验证)</td><td></td><td>选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证)</td></tr><tr><td>-XX:+UseParNewGC</td><td>设置年轻代为并行收集</td><td></td><td>可与CMS收集同时使用<br>JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值</td></tr><tr><td>-XX:ParallelGCThreads</td><td>并行收集器的线程数</td><td></td><td>此值最好配置与处理器数目相等 同样适用于CMS</td></tr><tr><td>-XX:+UseParallelOldGC</td><td>年老代垃圾收集方式为并行收集(Parallel Compacting)</td><td></td><td>这个是JAVA 6出现的参数选项</td></tr><tr><td>-XX:MaxGCPauseMillis</td><td>每次年轻代垃圾回收的最长时间(最大暂停时间)</td><td></td><td>如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.</td></tr></tbody></table><p>-XX:+UseAdaptiveSizePolicy 自动选择年轻代区大小和相应的Survivor区比例<br>设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开.<br>-XX:GCTimeRatio|设置垃圾回收时间占程序运行时间的百分比||公式为1/(1+n)<br>-XX:+ScavengeBeforeFullGC|Full GC前调用YGC|true|Do young generation GC prior to a full GC. (Introduced in 1.4.1.)</p><font size="3"><strong>CMS相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseConcMarkSweepGC</td><td>使用CMS内存收集</td><td></td><td>测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.???</td></tr><tr><td>-XX:+AggressiveHeap</td><td></td><td></td><td>试图是使用大量的物理内存，长时间大内存使用的优化，能检查计算资源（内存， 处理器数量），至少需要256MB内存，大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升）</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>多少次后进行内存压缩</td><td>由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生”碎片”,使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理.</td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>降低标记停顿</td></tr><tr><td>-XX+UseCMSCompactAtFullCollection</td><td>在FULL GC的时候， 对年老代的压缩</td><td></td><td>CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。可能会影响性能,但是可以消除碎片</td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td><td>禁止hostspot自行触发CMS GC</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction=70</td><td>使用cms作为垃圾回收，使用70％后开始CMS收集</td><td>92</td><td>为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式<strong>CMSInitiatingOccupancyFraction</strong>计算公式</td></tr><tr><td>-XX:CMSInitiatingPermOccupancyFraction</td><td>设置Perm Gen使用到达多少比率时触发</td><td>92</td></tr><tr><td>-XX:+CMSIncrementalMode</td><td>设置为增量模式</td><td></td><td>用于单CPU情况</td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr></tbody></table><font size="3"><strong>辅助信息</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+PrintGC</td><td></td><td></td><td>输出形式:<br>[GC 118250K-&gt;113543K(130112K), 0.0094143 secs]<br>[Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs]</td></tr><tr><td>-XX:+PrintGCDetails</td><td></td><td></td><td>输出形式:<br>[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs]<br>[GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs]</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td></td></tr><tr><td>-XX:+PrintGC:PrintGCTimeStamps</td><td></td><td></td><td>可与-XX:+PrintGC -XX:+PrintGCDetails混合使用<br>输出形式:11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs]</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td><td></td><td>输出形式:Total time for which application threads were stopped: 0.0468229 seconds</td></tr><tr><td>-XX:+PrintGCApplicationConcurrentTime</td><td>打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用</td><td></td><td>输出形式:Application time: 0.5291524 seconds</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td><td></td></tr><tr><td>-Xloggc:filename</td><td>把相关日志信息记录到文件以便分析.<br>与上面几个配合使用</td><td></td></tr><tr><td>-XX:+PrintClassHistogram</td><td>garbage collects before printing the histogram.</td><td></td></tr><tr><td>-XX:+PrintTLAB</td><td>查看TLAB空间的使用情况</td><td></td></tr><tr><td>XX:+PrintTenuringDistribution</td><td>查看每次minor GC后新的存活周期的阈值</td><td></td><td>Desired survivor size 1048576 bytes, new threshold 7 (max 15)</td></tr></tbody></table><p>new threshold 7即标识新的存活周期的阈值为7。</p><font size="3" color="#FF4500"><strong>GC性能方面的考虑</strong></font><p>对于GC的性能主要有2个方面的指标：吞吐量throughput（工作时间不算gc的时间占总的时间比）和暂停pause（gc发生时app对外显示的无法响应）</p><ol><li><p>Total Heap</p><p>默认情况下，vm会增加/减少heap大小以维持free space在整个vm中占的比例，这个比例由MinHeapFreeRatio和MaxHeapFreeRatio指定。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>对vm分配尽可能多的memory；</li><li>将Xms和Xmx设为一样的值。如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。</li><li>处理器核数增加，内存也跟着增大。</li></ul></li><li><p>The Young Generation</p><p>另外一个对于app流畅性运行影响的因素是young generation的大小。young generation越大，minor collection越少；但是在固定heap size情况下，更大的young generation就意味着小的tenured generation，就意味着更多的major collection(major collection会引发minor collection)。<br><br>NewRatio反映的是young和tenured generation的大小比例。NewSize和MaxNewSize反映的是young generation大小的下限和上限，将这两个值设为一样就固定了young generation的大小（同Xms和Xmx设为一样）。<br><br>如果希望，SurvivorRatio也可以优化survivor的大小，不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>首先决定能分配给vm的最大的heap size，然后设定最佳的young generation的大小；</li><li>如果heap size固定后，增加young generation的大小意味着减小tenured generation大小。让tenured generation在任何时候够大，能够容纳所有live的data（留10%-20%的空余）。</li></ul></li></ol><font size="3" color="#FF4500"><strong>经验&amp;&amp;规则</strong></font><ul><li><p>年轻代大小选择</p><ul><li>响应时间优先的应用:尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象.</li><li>吞吐量优先的应用:尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用.</li><li>避免设置过小.当新生代设置过小时会导致:1.YGC次数更加频繁 2.可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC.</li></ul></li><li><p>年老代大小选择</p><ul><li>响应时间优先的应用:年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎 片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得:<br>并发垃圾收集信息、持久代并发收集次数、传统GC信息、花在年轻代和年老代回收上的时间比例。</li><li>吞吐量优先的应用:一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.原因是,这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象.</li></ul></li><li><p>较小堆引起的碎片问题</p><p>因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置:<br><br>-XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩.<br><br>-XX:CMSFullGCsBeforeCompaction=0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩</p></li><li><p>用64位操作系统，Linux下64位的jdk比32位jdk要慢一些，但是吃得内存更多，吞吐量更大</p></li><li>XMX和XMS设置一样大，MaxPermSize和MinPermSize设置一样大，这样可以减轻伸缩堆大小带来的压力</li><li>使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 然后老生代利用CMS并行收集， 这样能保证系统低延迟的吞吐效率。 实际上cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间</li><li>系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。(相关工具的使用方法将在后面的blog中介绍)</li><li>仔细了解自己的应用，如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。</li><li>采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿</li><li>JVM参数的设置(特别是 –Xmx –Xms –Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。</li></ul><p><strong>promotion failed</strong></p><p>垃圾回收时promotion failed是个很头痛的问题，一般可能是两种原因产生，第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间；第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。</p><p><strong>解决方方案一</strong></p><p>第一个原因我的最终解决办法是去掉救助空间，设置-XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0即可，第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。</p><p><strong>解决方案一的改进方案</strong></p><p>又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。</p><p>-Xmx4000M -Xms4000M -Xmn600M -XX:PermSize=500M -XX:MaxPermSize=500M -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log</p><font size="3" color="#FF4500">CMSInitiatingOccupancyFraction值与Xmn的关系公式</font><p>上面介绍了promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即：<br>(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2)) 进而推断出：</p><pre><code>CMSInitiatingOccupancyFraction &lt;=((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100 </code></pre><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当xmx=128 xmn=36 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-36)-(36-36/(1+2)))/(128-36)*100 =73.913 </span><br><span class="line"></span><br><span class="line">当xmx=128 xmn=24 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-24)-(24-24/(1+2)))/(128-24)*100=84.615… </span><br><span class="line"></span><br><span class="line">当xmx=3000 xmn=600 SurvivorRatior=1时  CMSInitiatingOccupancyFraction&lt;=((3000.0-600)-(600-600/(1+2)))/(3000-600)*100=83.33</span><br></pre></td></tr></table></figure><p>CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。</p><p>令：</p><p>网上一童鞋推断出的公式是：:(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn 这个公式个人认为不是很严谨，在内存小的时候会影响xmn的计算。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之三: 内存分配策略</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%89_%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%89_%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p><img src="/assets/pic/2019-06-19-3-1.png" alt="内存分配策略"></p><p>了解GC其中很重要一点就是了解JVM的内存分配策略：<font color="red">即对象在哪里分配和对象什么时候回收。</font><br><br><br>Java技术体系中所提倡的自动内存管理可以归结于两个部分：给对象分配内存以及回收分配给对象的内存。<br><br><br>我们都知道，Java对象分配，都是在Java堆上进行分配的，虽然存在JIT编译后被拆分为标量类型并简介地在栈上进行分配。如果采用分代算法，那么新生的对象是分配在新生代的Eden区上的。如果启动了本地线程分配缓冲，将按线程优先在TLAB上进行分配。<br><br><br>事实上，Java的分配规则不是百分百固定的，其取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。<br><br><br>简单来说，对象内存分配主要是在堆中分配。但是分配的规则并不是固定的，取决于使用的收集器组合以及JVM内存相关参数的设定。<br><br><br>下面Serial和Serial Old收集器做一个内存分配和回收的策略总结。</p><h3 id="对象优先在新生代Eden分配"><a href="#对象优先在新生代Eden分配" class="headerlink" title="对象优先在新生代Eden分配"></a>对象优先在新生代Eden分配</h3><p>首先，让我们来看一下新生代的内存分配情况<br><br><br>内存分配情况：将JVM内存划分为一块较大的Eden空间（80%）和两块小的Servivor（各占10%）。当回收时，将Eden和Survivor中还存活的对象一次性采用复制算法直接复制到另外一块Servivor空间上，最后清理到院Eden空间和原先的Survivor空间中的数据。<br><br><br>大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，JVM将发起一次Minor GC。<br><br><br>在这里先说明两个概念：</p><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多是具有朝生夕灭的特性，所以Minor GC非常频繁，而且该速度也比较快。</li><li><strong>老年代GC（Major GC/Full GC）</strong>：指发生在老年代的GC，出现了Major GC，一般可能也会伴随着一次Minor GC，但是与Minor GC不同的是，Major GC的速度慢十倍以上。</li></ul><h3 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h3><p>我们先对所谓的大对象做一个定义：大对象，这里指的是需要大量连续内存空间的Java对象。最典型的大对象可以是很长的字符串和数组。<br><br><br>JVM对大对象的态度：大对象对于JVM的内存分配来说是十分麻烦的，如果我们将大对象分配在新生代中，那样子的话很容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。<br><br><br>为了避免上述情况的经常发生而导致不需要的GC活动所浪费的资源和时间，可采用的分配策略是将大对象直接分配到老年代中去，虚拟机中也提供了<strong>-XX:PretenureSizeThreshold</strong>参数，令大于这个设置值的对象直接在老年代里面分配内容。</p><p><code>-XX:PretenureSizeThreshold只对Serial和ParNew收集器有效。</code></p><h3 id="长期存活的对象将进入老年代"><a href="#长期存活的对象将进入老年代" class="headerlink" title="长期存活的对象将进入老年代"></a>长期存活的对象将进入老年代</h3><p>当JVM采用分代收集的思想来管理内存时，为了识别哪些对象应该放在新生代、哪些对象应该放在老年代，JVM给每个对象定义了一个对象年龄计数器。<br><br><br>对象年龄计数器：如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，便可以被移动到Survivor空间中，年龄计数器将设置该对象的年龄为1.对于对象在Survivor区每经过一次Minor GC，年龄便增加1岁，当它的年龄增加到一定程度（可通过参数-XX:MaxTenuringThreshold设置）默认15，该对象便会进入到老年代中。成为老年代的对象。</p><h3 id="动态对象年龄判定"><a href="#动态对象年龄判定" class="headerlink" title="动态对象年龄判定"></a>动态对象年龄判定</h3><p>事实上，有的虚拟机并不永远地要求对象的年龄必须达到MaxTeruringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Surivior空间的一半，年龄大于或等于该年龄的对象就可以直接进行老年代，无须等到MaxTeruringThreshold中所要求的年龄。</p><h3 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h3><p>在发生Minor GC之前，虚拟机会先检查老年代中最大的可用的连续空间是否大于新生代中所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的，如果不成立，则虚拟机会查看HandlePromotionFaiure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次GC是有风险的；如果小于，或者HandlePromotionFaiure设置不允许冒险，那么这时就要改为进行一次Full GC。</p><p>所谓冒险：也就是说当用来轮转的Survivor区无法承受新生代中所存活的对象内存时，需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代中，前提是老年代中。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
            <tag> 内存分配策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM快速调优手册之二: 常见的垃圾收集器</title>
      <link href="/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%8C_%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
      <url>/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%8C_%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<br><br><br>Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。</p><p><img src="/assets/pic/2019-06-19-2-1.png" alt="收集器"></p><font size="3"><b>HotSpot虚拟机的垃圾回收器</b></font><p>图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。</p><font size="3"><b>概念理解</b></font><ul><li><font size="3"><b>并发和并行</b></font><p>这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，它们可以解释如下</p><ul><li><strong>并行（Parallel）</strong>：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</li><li><strong>并发（Concurrent）</strong>：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。</li></ul></li><li><font size="3"><b>Minor GC 和 Full GC</b></font><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。</li><li><strong>老年代GC（Major GC / Full GC）</strong>：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。</li></ul></li><li><font size="3"><b>吞吐量</b></font><p>吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。<br><br><br>虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。</p></li></ul><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经（在JDK 1.3.1之前）是虚拟机新生代收集的唯一选择。</p><p><img src="/assets/pic/2019-06-19-2-2.png" alt="Serial收集器"></p><ul><li><p><strong>特性</strong></p><p>这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。Stop The World</p></li><li><p><strong>应用场景</strong></p><p>Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。</p></li><li><p><strong>优势</strong></p><p>简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。</p></li></ul><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p><img src="/assets/pic/2019-06-19-2-3.png" alt="ParNew收集器"></p><ul><li><p><strong>特性</strong></p><p>ParNew收集器其实就是Serial收集器的<strong>多线程版本</strong>，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。</p></li><li><p><strong>应用场景</strong></p><p>ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器。<br><br><br>很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器配合工作。<br><br><br>在JDK 1.5时期，HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器——CMS收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。<br><br><br>不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。</p></li><li><p><strong>Serial收集器 VS ParNew收集器</strong></p><p>ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。</p><p>然而，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。</p></li></ul><h3 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h3><ul><li><p><strong>特性</strong></p><p>Parallel Scavenge收集器是一个<strong>新生代收集器</strong>，它也是使用<strong>复制算法</strong>的收集器，又是并行的多线程收集器。</p></li><li><p><strong>应用场景</strong></p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。</p></li><li><p><strong>对比分析</strong></p><ul><li><p><strong>Parallel Scavenge收集器 VS CMS等收集器</strong></p><p>Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个<strong>可控制的吞吐量（Throughput）</strong>。</p><p>由于与吞吐量关系密切，Parallel Scavenge收集器也经常称为“吞吐量优先”收集器。</p></li><li><p><strong>Parallel Scavenge收集器 VS ParNew收集器</strong></p><p>Parallel Scavenge收集器与ParNew收集器的一个重要区别是它具有自适应调节策略。</p><p><strong>GC自适应的调节策略</strong></p><p>Parallel Scavenge收集器有一个参数-XX:+UseAdaptiveSizePolicy。当这个参数打开之后，就不需要手工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。</p></li></ul></li></ul><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-4.png" alt="Serial Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Serial Old是Serial收集器的<strong>老年代版本</strong>，它同样是一个<strong>单线程收集器</strong>，使用<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><ul><li><p><strong>Client模式</strong></p><p>Serial Old收集器的主要意义也是在于给Client模式下的虚拟机使用。</p></li><li><p><strong>Server模式</strong></p><p>如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。</p></li></ul></li></ul><h3 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-5.png" alt="Parallel Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Parallel Old是Parallel Scavenge收集器的<strong>老年代版本</strong>，使用<strong>多线程</strong>和<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><p>在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。</p><p>这个收集器是在JDK 1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择（Parallel Scavenge收集器无法与CMS收集器配合工作）。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合。</p></li></ul><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p><img src="/assets/pic/2019-06-19-2-6.png" alt="CMS收集器"></p><ul><li><p><strong>特性</strong></p><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。</p><p>CMS收集器是基于<strong>“标记—清除”算法</strong>实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤:</p><ol><li><p><strong>初始标记（CMS initial mark）</strong></p><p>初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。</p></li><li><p><strong>并发标记（CMS concurrent mark）</strong></p><p>并发标记阶段就是进行GC Roots Tracing的过程。</p></li><li><p><strong>重新标记（CMS remark）</strong></p><p>重新标记阶段是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，仍然需要“Stop The World”。</p></li><li><p><strong>并发清除（CMS concurrent sweep）</strong></p><p>并发清除阶段会清除对象。</p><p>由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。</p></li></ol></li><li><p><strong>优点</strong></p><p>CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：<strong>并发收集、低停顿</strong>。</p></li><li><p><strong>缺点</strong></p><ul><li><p><strong>CMS收集器对CPU资源非常敏感</strong></p><p>其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。</p><p>CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得很大。</p></li><li><p><strong>CMS收集器无法处理浮动垃圾</strong></p><p>CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。</p><p>由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。<br>也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。</p></li><li><p><strong>CMS收集器会产生大量空间碎片</strong></p><p>CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。</p><p>空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。</p></li></ul></li></ul><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p><img src="/assets/pic/2019-06-19-2-7.png" alt="G1收集器"></p><ul><li><p><strong>特性</strong></p><p>G1（Garbage-First）是一款<strong>面向服务端应用</strong>的垃圾收集器。HotSpot开发团队赋予它的使命是未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</p><p>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。</p><p>G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。</p><ul><li><p><strong>并行与并发</strong></p><p>G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</p></li><li><p><strong>分代收集</strong></p><p>与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。</p></li><li><p><strong>空间整合</strong></p><p>与CMS的“标记—清理”算法不同，G1从<strong>整体来看是基于“标记—整理”算法</strong>实现的收集器，从<strong>局部（两个Region之间）上来看是基于“复制”算法</strong>实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</p></li><li><p><strong>可预测的停顿</strong></p><p>这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。</p></li></ul></li><li><p><strong>执行过程</strong></p><p>G1收集器的运作大致可划分为以下几个步骤：</p><ul><li><p><strong>初始标记（Initial Marking）</strong></p><p>初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。</p></li><li><p><strong>并发标记（Concurrent Marking）</strong></p><p>并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。</p></li><li><p><strong>最终标记（Final Marking）</strong></p><p>最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。</p></li><li><p><strong>筛选回收（Live Data Counting and Evacuation）</strong></p><p>筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</p></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>虽然我们是在对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的收集器出现，更加没有万能的收集器，所以我们选择的只是对具体应用最合适的收集器。这点不需要多加解释就能证明：如果有一种放之四海皆准、任何场景下都适用的完美收集器存在，那HotSpot虚拟机就没必要实现那么多不同的收集器了。</p><p><img src="/assets/pic/2019-06-19-2-8.png" alt="总结"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 调优 </tag>
            
            <tag> 垃圾收集器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:连续5周若泽数据第20-21名学员喜捷offer(含面试题)</title>
      <link href="/2019/06/18/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD5%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC20-21%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/"/>
      <url>/2019/06/18/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD5%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC20-21%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第20个小伙伴，<font color="red">25K</font></b></font><p><img src="/assets/blogImg/2019-06-18-1.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.第21个小伙伴，<font color="red">2.4W*14</font></b></font><p><img src="/assets/blogImg/2019-06-18-2.png" alt="就业2"></p><font color="blue" size="4"><b>接下来是现场技术面试题:</b></font><ol><li>谈谈Spark RDD 的几大特性，并深入讲讲体现在哪</li><li>说说你参与过的项目，和一些业务场景</li><li>请说说Spark的宽窄依赖</li><li>Spark的stage划分，task跟分区的关系</li><li>详细讲讲Spark的内存管理，计算与存储是如何协调的</li><li>rdd df ds 之间的区别 ，什么时候使用ds</li><li>聊聊kafka消费如何保证不会重复消费</li><li>你项目里说到了数据延迟和数据重跑，请你说说当时是怎么解决的，如何保障幂等性！</li></ol><font color="#00CD00">（ps: 这面试题，若泽数据高级班&amp;线下班的小伙伴，so easy!）</font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Flume源码导入IDEA方式</title>
      <link href="/2019/06/17/%E7%94%9F%E4%BA%A7Flume%E6%BA%90%E7%A0%81%E5%AF%BC%E5%85%A5IDEA%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/06/17/%E7%94%9F%E4%BA%A7Flume%E6%BA%90%E7%A0%81%E5%AF%BC%E5%85%A5IDEA%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz"><a href="#下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz" class="headerlink" title="下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz"></a>下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz</h4><p>下载地址:<a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz</a></p><h4 id="win安装好maven-3-3-9"><a href="#win安装好maven-3-3-9" class="headerlink" title="win安装好maven-3.3.9"></a>win安装好maven-3.3.9</h4><h4 id="解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径"><a href="#解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径" class="headerlink" title="解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径"></a>解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径</h4><h4 id="编译：mvn-clean-compile"><a href="#编译：mvn-clean-compile" class="headerlink" title="编译：mvn clean compile"></a>编译：mvn clean compile</h4><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.0:enforce (clean) on project flume-parent: Some Enforcer rules have failed. Look above for specific messages explaining</span><br><span class="line">why the rule failed. -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p>换成以下编译命令，跳过enforcer</p><p><code>mvn clean compile validate -Denforcer.skip=true</code></p><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flume-ng-morphline-solr-sink: Could not resolve dependencies for project org.apache.flume.flume-ng-sinks:flume-ng-morphline-solr-sink:jar:1.6.0-cdh5.7.0: Fail</span><br><span class="line">ed to collect dependencies at org.kitesdk:kite-morphlines-all:pom:1.0.0-cdh5.7.0 -&gt; org.kitesdk:kite-morphlines-useragent:jar:1.0.0-cdh5.7.0 -&gt; ua_parser:ua-parser:jar:1.3.0: Failed to read artifact d</span><br><span class="line">escriptor for ua_parser:ua-parser:jar:1.3.0: Could not transfer artifact ua_parser:ua-parser:pom:1.3.0 from/to maven-twttr (http://maven.twttr.com): Connect to maven.twttr.com:80 [maven.twttr.com/31.1</span><br><span class="line">3.83.8] failed: Connection timed out: connect -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><code>flume-ng-morphline-solr-sink</code>我们用不到，可以直接注释掉，在<code>flume-ng-sinks</code>下的pom中找到并注释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;modules&gt;</span><br><span class="line">    &lt;module&gt;flume-hdfs-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-irc-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-hbase-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-elasticsearch-sink&lt;/module&gt;</span><br><span class="line">    &lt;!--&lt;module&gt;flume-ng-morphline-solr-sink&lt;/module&gt; --&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-kafka-sink&lt;/module&gt;</span><br><span class="line">&lt;/modules&gt;</span><br></pre></td></tr></table></figure><p>然后重新编译<code>mvn clean compile validate -Denforcer.skip=true</code>，成功</p><p><img src="/assets/pic/2019-06-17-1.png" alt="编译"></p><h4 id="导入IDEA"><a href="#导入IDEA" class="headerlink" title="导入IDEA"></a>导入IDEA</h4><p><img src="/assets/pic/2019-06-17-2.png" alt="1"></p><p><img src="/assets/pic/2019-06-17-3.png" alt="2"></p><p><img src="/assets/pic/2019-06-17-4.png" alt="3"></p><p><img src="/assets/pic/2019-06-17-5.png" alt="4"></p><p><img src="/assets/pic/2019-06-17-6.png" alt="5"></p><p><img src="/assets/pic/2019-06-17-7.png" alt="6"></p><p>然后等到导入完毕！</p><p><img src="/assets/pic/2019-06-17-8.png" alt="7"></p><p>导入后没有任何报错，这时我们就可以对源码进行修改了！</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
      <link href="/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
      <url>/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use test</span><br><span class="line">mysql&gt; create table hlw_offset(</span><br><span class="line">        topic varchar(32),</span><br><span class="line">        groupid varchar(50),</span><br><span class="line">        partitions int,</span><br><span class="line">        fromoffset bigint,</span><br><span class="line">        untiloffset bigint,</span><br><span class="line">        primary key(topic,groupid,partitions)</span><br><span class="line">        );</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;</span><br><span class="line">&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;</span><br><span class="line">--------------------------------------------------</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.27&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;config&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）StreamingContext</span><br><span class="line">2）从kafka中获取数据(从外部存储获取offset--&gt;根据offset获取kafka中的数据)</span><br><span class="line">3）根据业务进行逻辑处理</span><br><span class="line">4）将处理结果存到外部存储中--保存offset</span><br><span class="line">5）启动程序，等待程序结束</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import kafka.common.TopicAndPartition</span><br><span class="line">import kafka.message.MessageAndMetadata</span><br><span class="line">import kafka.serializer.StringDecoder</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">import scalikejdbc._</span><br><span class="line">import scalikejdbc.config._</span><br><span class="line">object JDBCOffsetApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建SparkStreaming入口</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)</span><br><span class="line">    val ssc = new StreamingContext(conf,Seconds(5))</span><br><span class="line">    //kafka消费主题</span><br><span class="line">    val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet</span><br><span class="line">    //kafka参数</span><br><span class="line">    //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改</span><br><span class="line">    val kafkaParams = Map[String,String](</span><br><span class="line">      &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),</span><br><span class="line">      &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),</span><br><span class="line">      &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)</span><br><span class="line">    )</span><br><span class="line">    //先使用scalikejdbc从MySQL数据库中读取offset信息</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来</span><br><span class="line">    //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到</span><br><span class="line">    DBs.setup()</span><br><span class="line">    val fromOffset = DB.readOnly( implicit session =&gt; &#123;</span><br><span class="line">      SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123;</span><br><span class="line">        (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))</span><br><span class="line">      &#125;).list().apply()</span><br><span class="line">    &#125;).toMap</span><br><span class="line">    //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费</span><br><span class="line">      val messages = if (fromOffset.isEmpty) &#123;</span><br><span class="line">        println(&quot;从头开始消费...&quot;)</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        println(&quot;从已存在记录开始消费...&quot;)</span><br><span class="line">        val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)</span><br><span class="line">      &#125;</span><br><span class="line">      messages.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">        if(!rdd.isEmpty())&#123;</span><br><span class="line">          //输出rdd的数据量</span><br><span class="line">          println(&quot;数据统计记录为：&quot;+rdd.count())</span><br><span class="line">          //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组</span><br><span class="line">//          trait HasOffsetRanges &#123;</span><br><span class="line">//            def offsetRanges: Array[OffsetRange]</span><br><span class="line">//          &#125;</span><br><span class="line">          val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">          offsetRanges.foreach(x =&gt; &#123;</span><br><span class="line">            //输出每次消费的主题，分区，开始偏移量和结束偏移量</span><br><span class="line">            println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;)</span><br><span class="line">           //将最新的偏移量信息保存到MySQL表中</span><br><span class="line">            DB.autoCommit( implicit session =&gt; &#123;</span><br><span class="line">              SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)</span><br><span class="line">            .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)</span><br><span class="line">              .update().apply()</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义的ValueUtils工具类如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import com.typesafe.config.ConfigFactory</span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">object ValueUtils &#123;</span><br><span class="line">val load = ConfigFactory.load()</span><br><span class="line">  def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;</span><br><span class="line">val value = load.getString(key)</span><br><span class="line">    if(StringUtils.isNotEmpty(value)) &#123;</span><br><span class="line">      value</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      defaultValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>application.conf内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">metadata.broker.list = &quot;192.168.137.251:9092&quot;</span><br><span class="line">auto.offset.reset = &quot;smallest&quot;</span><br><span class="line">group.id = &quot;hlw_offset_group&quot;</span><br><span class="line">kafka.topics = &quot;hlw_offset&quot;</span><br><span class="line">serializer.class = &quot;kafka.serializer.StringEncoder&quot;</span><br><span class="line">request.required.acks = &quot;1&quot;</span><br><span class="line"># JDBC settings</span><br><span class="line">db.default.driver = &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;</span><br><span class="line">db.default.user=&quot;root&quot;</span><br><span class="line">db.default.password=&quot;123456&quot;</span><br></pre></td></tr></table></figure></li><li><p>自定义kafka producer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.&#123;Date, Properties&#125;</span><br><span class="line">import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;</span><br><span class="line">object KafkaProducer &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val properties = new Properties()</span><br><span class="line">    properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))</span><br><span class="line">    properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))</span><br><span class="line">    properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))</span><br><span class="line">    val producerConfig = new ProducerConfig(properties)</span><br><span class="line">    val producer = new Producer[String,String](producerConfig)</span><br><span class="line">    val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)</span><br><span class="line">    //每次产生100条数据</span><br><span class="line">    var i = 0</span><br><span class="line">    for (i &lt;- 1 to 100) &#123;</span><br><span class="line">      val runtimes = new Date().toString</span><br><span class="line">     val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)</span><br><span class="line">      producer.send(messages)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;数据发送完毕...&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</span><br></pre></td></tr></table></figure></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台输出结果：</span><br><span class="line">从头开始消费...</span><br><span class="line">数据统计记录为：500</span><br><span class="line">---hlw_offset,0,0,500---</span><br></pre></td></tr></table></figure></li></ol><pre><code>查看MySQL表，offset记录成功<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |          0 |         500 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></code></pre><ol start="5"><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台结果输出：</span><br><span class="line">从已存在记录开始消费...</span><br><span class="line">数据统计记录为：300</span><br><span class="line">---hlw_offset,0,500,800---</span><br></pre></td></tr></table></figure></li><li><p>查看更新后的offset MySQL数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |        500 |         800 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> spark streaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中Cache与Persist的巅峰对决</title>
      <link href="/2019/06/14/Spark%E4%B8%ADCache%E4%B8%8EPersist%E7%9A%84%E5%B7%85%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
      <url>/2019/06/14/Spark%E4%B8%ADCache%E4%B8%8EPersist%E7%9A%84%E5%B7%85%E5%B3%B0%E5%AF%B9%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="Cache的产生背景"><a href="#Cache的产生背景" class="headerlink" title="Cache的产生背景"></a>Cache的产生背景</h2><p>我们先做一个简单的测试读取一个本地文件做一次collect操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br></pre></td></tr></table></figure><p>上面我们进行了两次相同的操作，观察日志我们发现这样一句话<code>Submitting ResultStage 0 (file:///home/hadoop/data/input.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25), which has no missing parents</code>，每次都要去本地读取input.txt文件，这里大家能想到存在什么问题吗? 如果我的文件很大，每次都对相同的RDD进行同一个action操作，那么每次都要到本地读取文件，得到相同的结果。不断进行这样的重复操作，耗费资源浪费时间啊。这时候我们可能想到能不能把RDD保存在内存中呢？答案是可以的，这就是我们所要学习的cache。</p><h2 id="Cache的作用"><a href="#Cache的作用" class="headerlink" title="Cache的作用"></a>Cache的作用</h2><p>通过上面的讲解我们知道, 有时候很多地方都会用到同一个RDD, 那么每个地方遇到Action操作的时候都会对同一个算子计算多次, 这样会造成效率低下的问题。通过cache操作可以把RDD持久化到内存或者磁盘。</p><p>现在我们利用上面说的例子，把rdd进行cache操作</p><p>rdd.cache这时候我们打开192.168.137.130:4040界面查看storage界面中是否有我们的刚才cache的文件，发现并没有。这时候我们进行一个action操作rdd.count。继续查看storage是不是有东西了哈</p><p><img src="/assets/pic/2019-06-14-1.png" alt="Cache"></p><p>并且给我们列出了很多信息，存储级别（后面详解），大小（会发现要比源文件大，这也是一个调优点）等等。</p><p>说到这里小伙伴能能想到什么呢？ cacha是一个Tranformation还是一个Action呢？相信大伙应该知道了。</p><p>cache这个方法也是个Tranformation,当第一次遇到Action算子的时才会进行持久化，所以说我们第一次进行了cache操作在ui中并没有看到结果，进行了count操作才有。</p><h2 id="源码详细解析"><a href="#源码详细解析" class="headerlink" title="源码详细解析"></a>源码详细解析</h2><p><strong>Spark版本：2.2.0</strong></p><p>源码分析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">  */</span><br><span class="line"> def cache(): this.type = persist()</span><br></pre></td></tr></table></figure><p>从源码中可以明显看出cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：（这里注释cache的storage level）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，是不是和上面对上了哈，这里我们能够得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><p>我相信小伙伴们肯定很好奇这个缓存级别到底有多少种呢？我们继续怼源码看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: DeveloperApi ::</span><br><span class="line"> * Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,</span><br><span class="line"> * or ExternalBlockStore, whether to drop the RDD to disk if it falls out of memory or</span><br><span class="line"> * ExternalBlockStore, whether to keep the data in memory in a serialized format, and whether</span><br><span class="line"> * to replicate the RDD partitions on multiple nodes.</span><br><span class="line"> *</span><br><span class="line"> * The [[org.apache.spark.storage.StorageLevel]] singleton object contains some static constants</span><br><span class="line"> * for commonly useful storage levels. To create your own storage level object, use the</span><br><span class="line"> * factory method of the singleton object (`StorageLevel(...)`).</span><br><span class="line"> */</span><br><span class="line">@DeveloperApi</span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure><p>我们先来看看存储类型，源码中我们可以看出有五个参数，分别代表：</p><p><code>useDisk</code>:使用硬盘（外存）;</p><p><code>useMemory</code>:使用内存;</p><p><code>useOffHeap</code>:使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。这部分内存也会被频繁的使用而且也可能导致OOM，它是通过存储在堆中的DirectByteBuffer对象进行引用，可以避免堆和堆外数据进行来回复制；</p><p><code>deserialized</code>:反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象;</p><p><code>replication</code>:备份数（在多个节点上备份，默认为1）。</p><p>我们接着看看缓存级别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Various [[org.apache.spark.storage.StorageLevel]] defined and utility functions for creating</span><br><span class="line"> * new storage levels.</span><br><span class="line"> */</span><br><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，<strong>但这些有什么区别呢？</strong>可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，和上面说的存储类型是相对应的，四个参数是因为有一个是有默认值的。</p><p>好吧这里我又想问小伙伴们一个问题了，这几种存储方式什么意思呢？该如何选择呢？</p><p>官网上进行了详细的解释。我这里介绍一个有兴趣的同学可以去官网看看哈。</p><p><strong>MEMORY_ONLY</strong></p><blockquote><p>使用反序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，某些分区将不会被缓存，并且将在需要时重新计算。这是默认级别。</p></blockquote><p><strong>MEMORY_AND_DISK</strong></p><blockquote><p>使用反序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</p></blockquote><p><strong>MEMORY_ONLY_SER（(Java and Scala)）</strong></p><blockquote><p>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，但是会加大cpu负担。</p></blockquote><p>一个简单的案例感官行的认识存储级别的差别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19M     page_views.dat</span><br><span class="line"></span><br><span class="line">val rdd1=sc.textFile(&quot;file:///home/hadoop/data/page_views.dat&quot;)</span><br><span class="line">rdd1.persist().count</span><br></pre></td></tr></table></figure><p>ui查看缓存大小：</p><p><img src="/assets/pic/2019-06-14-2.png" alt="ui查看缓存大小"></p><p>是不是明显变大了，我们先删除缓存<font color="red">rdd1.unpersist()</font></p><p>使用MEMORY_ONLY_SER级别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">rdd1.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">rdd1.count</span><br></pre></td></tr></table></figure><p><img src="/assets/pic/2019-06-14-3.png" alt="MEMORY_ONLY_SER"></p><p>这里我就用这两种方式进行对比，大家可以试试其他方式。</p><p>那如何选择呢？哈哈官网也说了。</p><p>你可以在内存使用和CPU效率之间来做出不同的选择不同的权衡。</p><p>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</p><p>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</p><p>不要泄漏到磁盘，除非你在内存中计算需要很大的花费，或者可以过滤大量数据，保存部分相对重要的在内存中。否则存储在磁盘中计算速度会很慢，性能急剧降低。</p><p>后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p><h2 id="删除缓存中的数据"><a href="#删除缓存中的数据" class="headerlink" title="删除缓存中的数据"></a>删除缓存中的数据</h2><p>spark自动监视每个节点上的缓存使用，并以最近最少使用的（LRU）方式丢弃旧数据分区。如果您想手动删除RDD，而不是等待它从缓存中掉出来，请使用 RDD.unpersist()方法。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
            <tag> Cache </tag>
            
            <tag> Persist </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019端午-线下项目第14期圆满结束</title>
      <link href="/2019/06/11/2019%E7%AB%AF%E5%8D%88-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC14%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/"/>
      <url>/2019/06/11/2019%E7%AB%AF%E5%8D%88-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC14%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><center><br>2019年端午节<br><br>4天3夜上海线下班<br><br>圆满结束<br><br>一句话，上海温度有点燥热<br><br>但，会议室空调很给力<br><br><br><br>小伙伴们来自<font color="blue">11个城市</font><br><br><b><br>北京、上海、深圳<br><br>广州、杭州、合肥、徐州<br><br>石家庄、大庆、天津、厦门<br></b><br><br><br><br>大家为了一个真实目标<br><br>学习真正企业级大数据生产项目<br><br><font color="blue">3个生产项目+2个Topic分享</font><br><br>一年我们只在节假日&amp;周末举办<br><br>错过了就是错过了<br><br>期待8月下旬线下项目班第15期<br><br></center><p><img src="/assets/blogImg/2019-06-11-1.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-06-11-2.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-06-11-3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 线下实战班 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线下实战班 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产HDFS Block损坏恢复最佳实践(含思考题)</title>
      <link href="/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/"/>
      <url>/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="文件ruozedata-md"><a href="#文件ruozedata-md" class="headerlink" title="文件ruozedata.md"></a>文件ruozedata.md</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure><h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure><p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p><p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p><p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p><p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul><li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li><li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li></ul><p>块扫描: <a href="https://blog.cloudera.com/blog/2016/12/hdfs-datanode-scanners-and-disk-checker-explained/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2016/12/hdfs-datanode-scanners-and-disk-checker-explained/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> HDFS </tag>
            
            <tag> Block损坏恢复 </tag>
            
            <tag> 实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你真的了解volatile关键字吗？</title>
      <link href="/2019/06/05/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3volatile%E5%85%B3%E9%94%AE%E5%AD%97/"/>
      <url>/2019/06/05/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3volatile%E5%85%B3%E9%94%AE%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="volatile的语义"><a href="#volatile的语义" class="headerlink" title="volatile的语义"></a>volatile的语义</h2><p>一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：</p><ul><li><p>保证了不同线程对这个变量进行操作时的可见性</p><p>即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的</p></li><li><p>禁止进行指令重排序</p><p>举例，线程1先执行，线程2后执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1//线程1</span><br><span class="line">2boolean stop = false;</span><br><span class="line">3while(!stop)&#123;</span><br><span class="line">4    doSomething();</span><br><span class="line">5&#125;</span><br><span class="line">6//线程2</span><br><span class="line">7stop = true;</span><br></pre></td></tr></table></figure></li></ul><p>这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法</p><p>但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？</p><p>不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）</p><p>无法中断，导致死循环的原因：</p><ul><li>每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中</li><li>那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去</li></ul><p>当我们使用volatile修饰了flag之后就不一样了，使用volatile关键字会强制将修改的值立即写入主存：</p><ul><li>使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存值无效</li><li>由于线程1的工作内存中缓存变量stop的缓存值无效，所以线程1再次读取变量stop的值时会去主存读取stop的值</li><li>那么在线程2修改stop值时（这里包括2个操作：修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存值无效，然后线程1读取时，发现自己的缓存值无效，它就会等待缓存值对应的主存地址被更新之后，然后去对应的主存读取最新的值</li><li>那么，线程1读取到的就是最新的正确的值</li></ul><h2 id="volatile与原子性（无法保证所有的操作都具有原子性）"><a href="#volatile与原子性（无法保证所有的操作都具有原子性）" class="headerlink" title="volatile与原子性（无法保证所有的操作都具有原子性）"></a>volatile与原子性（无法保证所有的操作都具有原子性）</h2><p>从上面知道volatile关键保证了操作的可见性，但是volatile能保证对变量操作的原子性吗？</p><h3 id="看如下的一个例子"><a href="#看如下的一个例子" class="headerlink" title="看如下的一个例子"></a>看如下的一个例子</h3><p>有个被volatile修饰的int类型的变量inc初始值为0，此时有10个线程对这个变量去进行增加的操作，每个变量增加到1000，那么最终结果按道理来说是1000*10=10000的，但是并不能，运行出来可能是一个小于10000的数字（调用了，Thread.yield()方法，暂停当前正在执行的线程对象,并执行其他线程）</p><h3 id="存在的误区"><a href="#存在的误区" class="headerlink" title="存在的误区"></a>存在的误区</h3><p>可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000</p><h3 id="事实"><a href="#事实" class="headerlink" title="事实"></a>事实</h3><p>在前面已经提到过，自增操作是不具备原子性的，它的步骤包括：</p><ul><li>读取变量的原始值</li><li>进行加1操作</li><li>写入工作内存，刷到主存中去</li></ul><p>那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：</p><ul><li>假如某个时刻变量inc的值为10，此时线程1对变量进行自增操作：线程1先读取了变量inc的原始值，然后线程1被阻塞了</li><li>然后线程2对变量进行自增操作：线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存值无效，同时线程2从主存中读取到的值也是没有任何修改的10</li><li>因此线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存</li><li>然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。</li><li>那么两个线程分别进行了一次自增操作后，inc却只增加1</li></ul><h3 id="存在的疑问"><a href="#存在的疑问" class="headerlink" title="存在的疑问"></a>存在的疑问</h3><p>解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存值无效吗？然后其他线程去读就会读到新的值，对，这个没错。</p><p>这个就是上面的happens-before规则中的volatile变量规则，但是要注意：</p><ul><li>线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改</li><li><p>然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值</p><p>根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的</p></li></ul><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>解决的方法也就是提供原子性的自增操作即可：</p><p>在Java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增，自减、以及加法操作，减法操作进行了封装，保证这些操作是原子性操作；atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作</p><p>针对本案例，可以使用AtomicInteger来替换int，它利用了CAS算法来保证了原子性</p><h2 id="volatile与有序性（防止指令重排）"><a href="#volatile与有序性（防止指令重排）" class="headerlink" title="volatile与有序性（防止指令重排）"></a>volatile与有序性（防止指令重排）</h2><p>在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性；volatile关键字禁止指令重排序有两层意思：</p><ul><li><p>当程序执行到volatile变量的读操作或者写操作时</p><p>在volatile这个操作前面的操作的更改肯定全部已经进行</p><p>且结果已经对后面的操作可见</p></li><li><p>在volatile这个操作后面的操作肯定还没有进行</p><p>在进行指令优化时</p><p>不能将在对volatile变量访问的语句放在其后面执行</p><p>也不能把volatile变量后面的语句放到其前面执行</p></li></ul><p><strong>例子</strong></p><p>可能上面说的比较绕，举个简单的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1//x、y为非volatile变量</span><br><span class="line">2//flag为volatile变量</span><br><span class="line">3x = 2;        //语句1</span><br><span class="line">4y = 0;        //语句2</span><br><span class="line">5flag = true;    //语句3</span><br><span class="line">6x = 4;         //语句4</span><br><span class="line">7y = -1;        //语句5</span><br></pre></td></tr></table></figure><p>由于flag变量为volatile变量，那么在进行指令重排序的过程的时候：</p><ul><li>不会将语句3放到语句1、语句2前面</li><li>也不会将语句3放到语句4、语句5后面</li><li>但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的</li></ul><p>并且volatile关键字能保证，执行到语句3时：</p><ul><li>语句1和语句2必定是执行完毕了的</li><li>且语句1和语句2的执行结果对语句3、语句4、语句5是可见的</li></ul><h2 id="指令重排的应用（双重懒加载的单例模式）"><a href="#指令重排的应用（双重懒加载的单例模式）" class="headerlink" title="指令重排的应用（双重懒加载的单例模式）"></a>指令重排的应用（双重懒加载的单例模式）</h2><p>一个最经典的使用场景就是双重懒加载的单例模式了：</p><p><img src="/assets/pic/2019-06-12.png" alt="单例模式"></p><p>这里的volatile关键字主要是为了防止指令重排</p><p>singleton = new Singleton()这段代码，其实是分三步走的：</p><ul><li>分配内存空间</li><li>初始化对象</li><li><p>将singleton对象指向分配的内存地址</p><p>加上 volatile 是为了让以上的三步操作顺序执行，反之有可能第三步在第二步之前被执行，那么就有可能某个线程拿到的单例对象是还没有初始化的，以致于报错</p></li></ul><h2 id="volatile的实现机制"><a href="#volatile的实现机制" class="headerlink" title="volatile的实现机制"></a>volatile的实现机制</h2><p>前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的；下面这段话摘自《深入理解Java虚拟机》 ：<br>观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令：</p><ul><li>lock前缀指令实际上相当于一个 内存屏障（也成内存栅栏），内存屏障会提供3个功能：</li><li><p>它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置</p><p>也不会把前面的指令排到内存屏障的后面</p><p>即在执行到内存屏障这句指令时，在它前面的操作已经全部完成</p></li><li><p>它会强制将对缓存的修改操作立即写入主存</p></li><li>如果是写操作（即修改操作），它会导致其他CPU中对应的缓存值无效</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:连续4周若泽数据第16-19名学员喜捷offer(含面试题)</title>
      <link href="/2019/06/03/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD4%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC16-19%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/"/>
      <url>/2019/06/03/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD4%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC16-19%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第16个小伙伴，<font color="red">20K*14</font></b></font><p><img src="/assets/blogImg/2019-06-03-1.png" alt="就业1"></p><font color="#00CD00" size="4"><b>2.第17个小伙伴，<font color="red">22K</font></b></font><p><img src="/assets/blogImg/2019-06-03-2.png" alt="就业2"></p><font color="#00CD00" size="4"><b>3.第18个小伙伴，<font color="red">15K</font>(学生)</b></font><p><img src="/assets/blogImg/2019-06-03-3.png" alt="就业3"></p><font color="#00CD00" size="4"><b>4.第19个小伙伴，<font color="red">25K</font></b></font><p><img src="/assets/blogImg/2019-06-03-4.png" alt="就业4"></p><font color="blue" size="4"><b>接下来是面试题:</b></font><p><img src="/assets/blogImg/2019-06-03-5.png" alt="面试1"></p><p><img src="/assets/blogImg/2019-06-03-6.png" alt="面试2"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之四</title>
      <link href="/2019/05/31/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B/"/>
      <url>/2019/05/31/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val acc = sc.accumulator(0, “Error Accumulator”)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">val newData = data.map(x =&gt; &#123;</span><br><span class="line">  if (x % 2 == 0) &#123;</span><br><span class="line"> accum += 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">newData.count</span><br><span class="line">acc.value</span><br><span class="line">newData.foreach(println)</span><br><span class="line">acc.value</span><br></pre></td></tr></table></figure><p>上述现象，会造成acc.value的最终值变为10</p><a id="more"></a><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>Spark中的一系列transform操作都会构造成一长串的任务链，此时就需要通过一个action操作来触发（lazy的特性），accumulator也是如此。</p><ul><li>因此在一个action操作之后，调用value方法查看，是没有任何变化</li><li>第一次action操作之后，调用value方法查看，变成了5</li><li>第二次action操作之后，调用value方法查看，变成了10</li></ul><p>原因就在于第二次action操作的时候，又执行了一次累加器的操作，同个累加器，在原有的基础上又加了5，从而变成了10</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过上述的现象描述，我们可以很快知道解决的方法：只进行一次action操作。基于此，我们只要切断任务之间的依赖关系就可以了，即使用cache、persist。这样操作之后，那么后续的累加器操作就不会受前面的transform操作影响了</p><h2 id="案例地址"><a href="#案例地址" class="headerlink" title="案例地址"></a>案例地址</h2><p>相关的工程案例地址在Github上：<a href="https://github.com/lemonahit/spark-train/tree/master/01-Accumulator" target="_blank" rel="noopener">https://github.com/lemonahit/spark-train/tree/master/01-Accumulator</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">/**</span><br><span class="line">  * 使用Spark Accumulators完成Job的数据量处理</span><br><span class="line">  * 统计emp表中NULL出现的次数以及正常数据的条数 &amp; 打印正常数据的信息</span><br><span class="line">  *</span><br><span class="line">  * 若泽数据学员-呼呼呼 on 2017/11/9.</span><br><span class="line">  */</span><br><span class="line">object AccumulatorsApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;AccumulatorsApp&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val lines = sc.textFile(&quot;E:/emp.txt&quot;)</span><br><span class="line">    // long类型的累加器值</span><br><span class="line">    val nullNum = sc.longAccumulator(&quot;NullNumber&quot;)</span><br><span class="line">    val normalData = lines.filter(line =&gt; &#123;</span><br><span class="line">      var flag = true</span><br><span class="line">      val splitLines = line.split(&quot;\t&quot;)</span><br><span class="line">      for (splitLine &lt;- splitLines)&#123;</span><br><span class="line">        if (&quot;&quot;.equals(splitLine))&#123;</span><br><span class="line">          flag = false</span><br><span class="line">          nullNum.add(1)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line">    // 使用cache方法，将RDD的第一次计算结果进行缓存；防止后面RDD进行重复计算，导致累加器的值不准确</span><br><span class="line">    normalData.cache()</span><br><span class="line">    // 打印每一条正常数据</span><br><span class="line">    normalData.foreach(println)</span><br><span class="line">    // 打印正常数据的条数</span><br><span class="line">    println(&quot;NORMAL DATA NUMBER: &quot; + normalData.count())</span><br><span class="line">    // 打印emp表中NULL出现的次数</span><br><span class="line">    println(&quot;NULL: &quot; + nullNum.value)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上海某公司的生产MySQL灾难性挽救</title>
      <link href="/2019/05/30/%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91/"/>
      <url>/2019/05/30/%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h3><p>本人(<a href="www.ruozedata.com">若泽数据</a>J哥)的媳妇，是个漂亮的妹子，同时也是一枚爬虫&amp;Spark开发工程师。</p><p>前天，她的公司MySQL(阿里云ECS服务器)，由于磁盘爆了加上人为的修复，导致各种问题，然后经过2天的折腾，终于公司的大神修复不了了。于是就丢给她了，顺理成章的就丢给我了。我想说，难道J哥这么出名吗？那为了在妹子面前不能丢我们真正大佬的神技，于是乎我就很爽快接了这个MySQL故障恢复，此次故障的是一个数据盘，1T。<br>这时的我，说真的并没有意识到，此事是如此的繁杂，特此写此博文记录一下，毕竟J哥我年纪也大了。</p><p>PS:<br>这里吐槽一下，并没有周日全备+周1~周6增量备份机制哟，不然恢复就爽歪歪了。<br><a id="more"></a></p><h3 id="2-故障现象"><a href="#2-故障现象" class="headerlink" title="2.故障现象"></a>2.故障现象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看表结构、查询表数据都如下抛错:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019530_1.png" alt="enter description here"></p><h3 id="3-尝试修复第一次，失败"><a href="#3-尝试修复第一次，失败" class="headerlink" title="3.尝试修复第一次，失败"></a>3.尝试修复第一次，失败</h3><p>3.1 使用repair命令修复表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; repair table wenshu.wenshu2018;  </span><br><span class="line">错误依旧:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p>3.2 谷歌一篇有指导意义的<br><a href="https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine" target="_blank" rel="noopener">https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine</a></p><ul><li>3.2.1 让其扩容数据磁盘为1.5T，试试，依旧这个错误；</li><li>3.2.2 临时目录修改为大的磁盘空间，试试，依旧这个错误；</li><li>3.2.3 取消磁盘限额，试试，依旧这个错误；</li><li>3.2.4 就是一开始的repair命令修复，试试，依旧这个错误；</li></ul><p>这时的我，也无语了，什么鬼！谷歌一页页搜索验证，没有用！</p><h3 id="4-先部署相同系统的相同版本的机器和MySQL"><a href="#4-先部署相同系统的相同版本的机器和MySQL" class="headerlink" title="4.先部署相同系统的相同版本的机器和MySQL"></a>4.先部署相同系统的相同版本的机器和MySQL</h3><p>于是J哥，快速在【若泽数据】的阿里云账号上买了1台Ubuntu 16.04.6的按量付费机器<br>迅速部署MySQL5.7.26。</p><ul><li>4.1 购买按量付费机器(假如不会购买，找J哥)</li><li>4.2 部署MySQL</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.更新apt-get</span><br><span class="line">$ apt-get update</span><br><span class="line"></span><br><span class="line">b.安装MySQL-Server</span><br><span class="line">$ apt-get install mysql-server</span><br><span class="line"></span><br><span class="line">之后会问你，是否要下载文件， 输入 y 就好了</span><br><span class="line">然后会出现让你设置 root 密码的界面</span><br><span class="line">输入密码: ruozedata123</span><br><span class="line">然后再重复一下，</span><br><span class="line">再次输入密码: ruozedata123</span><br><span class="line"></span><br><span class="line">c.安装MySQL-Client</span><br><span class="line">$ apt install mysql-client</span><br><span class="line"></span><br><span class="line">d.我们可以使用</span><br><span class="line">$ mysql -uroot -pruozedata123</span><br><span class="line">来连接服务器本地的 MySQL</span><br></pre></td></tr></table></figure><h3 id="5-尝试先通过frm文件恢复表结构，失败"><a href="#5-尝试先通过frm文件恢复表结构，失败" class="headerlink" title="5.尝试先通过frm文件恢复表结构，失败"></a>5.尝试先通过frm文件恢复表结构，失败</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a. 建立一个数据库，比如wenshu.</span><br><span class="line"></span><br><span class="line">b. 在ruozedata数据库下建立同名的数据表wenshu2018，表结构随意，这里只有一个id字段，操作过程片段如下：</span><br><span class="line"></span><br><span class="line">mysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------+</span><br><span class="line">| Tables_in_aa |</span><br><span class="line">+--------------+</span><br><span class="line">| wenshu2018   |</span><br><span class="line">+--------------+</span><br><span class="line">1 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; desc wenshu2018;</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| Field | Type       | Null | Key | Default | Extra |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| id    | bigint(20) | NO   |     | NULL    |       |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">c.停止mysql服务器，将wenshu2018.frm文件scp远程拷贝到新的正常数据库的数据目录wenshu下，覆盖掉下边同名的frm文件：</span><br><span class="line"></span><br><span class="line">d.重新启动MYSQL服务</span><br><span class="line"></span><br><span class="line">e.测试下是否恢复成功，进入wenshu数据库，用desc命令测试下，错误为:</span><br><span class="line">mysql Tablespace is missing for table `wenshu`.`wenshu2018`.</span><br></pre></td></tr></table></figure><h3 id="6-尝试有没有备份的表结构恢复数据，失败"><a href="#6-尝试有没有备份的表结构恢复数据，失败" class="headerlink" title="6.尝试有没有备份的表结构恢复数据，失败"></a>6.尝试有没有备份的表结构恢复数据，失败</h3><p>媳妇公司给出一个表结构,如下，经过测试无法恢复，原因就是无法和ibd文件匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS cpws_batch;</span><br><span class="line">CREATE TABLE cpws_batch  (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,</span><br><span class="line">  source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  crawl_time datetime NULL DEFAULT NULL,</span><br><span class="line">  status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id) USING BTREE,</span><br><span class="line">  INDEX ix_status(status) USING BTREE,</span><br><span class="line">  INDEX ix_doc_id(doc_id) USING BTREE</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;</span><br></pre></td></tr></table></figure><h3 id="7-如何获取正确的表结构，这是【成功的第一步】"><a href="#7-如何获取正确的表结构，这是【成功的第一步】" class="headerlink" title="7.如何获取正确的表结构，这是【成功的第一步】"></a>7.如何获取正确的表结构，这是【成功的第一步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s get.dbsake.net &gt; /tmp/dbsake</span><br><span class="line">$ chmod u+x /tmp/dbsake</span><br><span class="line">$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm </span><br><span class="line">--</span><br><span class="line">-- Table structure for table wenshu_0_1000</span><br><span class="line">-- Created with MySQL Version 5.7.25</span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br></pre></td></tr></table></figure><p>对比Step6的表结构，感觉就差分区设置而已，坑！<br>这时，J哥有种信心，恢复应该小菜了。</p><h3 id="8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"><a href="#8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！" class="headerlink" title="8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"></a>8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！</h3><h3 id="9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"><a href="#9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】" class="headerlink" title="9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"></a>9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">9.1首先需要一个跟要恢复的表结构完全一致的表，至关重要</span><br><span class="line">mysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;</span><br><span class="line">USE wenshu;</span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br><span class="line"></span><br><span class="line">9.2然后DISCARD TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.3把要恢复的ibd文件复制到mysql的data文件夹下，修改用户和用户组为mysql</span><br><span class="line">$ scp wenshu2018#P#p*.ibd  新建机器IP:/mnt/mysql_data/wenshu/</span><br><span class="line">$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd</span><br><span class="line"></span><br><span class="line">9.4然后执行IMPORT TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.5等待，有戏，耗时3h，这时我相信应该么问题的</span><br><span class="line"></span><br><span class="line">9.6查询数据，果然恢复有结果，心里暗暗自喜</span><br><span class="line">mysql&gt; select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"><a href="#10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择" class="headerlink" title="10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"></a>10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择</h3><ul><li>10.1 重新购买一台新的服务器，在初始化配置时，就加上1块1.5T的大磁盘。好处是无需挂盘操作，坏处是需要重新做第一个表，浪费3h；</li><li>10.2 购买1.5T的大磁盘，挂载这个机器上。好处是无需再做一次第一个表，坏处是需要修改mysql的数据目录指向为这个大磁盘。系统盘扩容最大也就500G，所以必须外加一个数据盘1.5T容量。</li></ul><p>所以J哥是职场老手了！贼笑！</p><h3 id="11-服务器加数据磁盘，1-5T，购买、挂载、格式化"><a href="#11-服务器加数据磁盘，1-5T，购买、挂载、格式化" class="headerlink" title="11.服务器加数据磁盘，1.5T，购买、挂载、格式化"></a>11.服务器加数据磁盘，1.5T，购买、挂载、格式化</h3><p>接下来的操作是我媳妇独立完成的，这里表扬一下:</p><ul><li>11.1 先买云盘 <a href="https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8</a></li><li>11.2 再挂载云盘 到对应机器 <a href="https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB</a></li><li>11.3 最后Linux格式化数据盘 <a href="https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr</a></li></ul><p>图2所示，df -h命令查看，大磁盘/dev/vdb1<br><img src="/assets/blogImg/2019530_2.png" alt="enter description here"></p><h3 id="12-MySQL修改数据目录为大磁盘，重新启动失败，解决"><a href="#12-MySQL修改数据目录为大磁盘，重新启动失败，解决" class="headerlink" title="12.MySQL修改数据目录为大磁盘，重新启动失败，解决"></a>12.MySQL修改数据目录为大磁盘，重新启动失败，解决</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">12.1 修改数据目录为大磁盘</span><br><span class="line">$ mkdir -p /mnt/mysql_data</span><br><span class="line">$ chown mysql:mysql /mnt/mysql_data</span><br><span class="line">$ vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br><span class="line">datadir         = /mnt/mysql_data</span><br><span class="line"></span><br><span class="line">12.2 无法启动mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">无法启动成功，查看日志</span><br><span class="line">2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().</span><br><span class="line">2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error</span><br><span class="line"></span><br><span class="line">12.3 百思不得其解，CentOS也没有这么麻烦，Ubuntu难道这么搞事吗？</span><br><span class="line">12.4 新增mysqld内容</span><br><span class="line">$ vi /etc/apparmor.d/local/usr.sbin.mysqld</span><br><span class="line"># Site-specific additions and overrides for usr.sbin.mysqld.</span><br><span class="line"># For more details, please see /etc/apparmor.d/local/README.</span><br><span class="line">/mnt/mysql_data/ r,</span><br><span class="line">/mnt/mysql_data/** rwk,</span><br><span class="line"></span><br><span class="line">12.5 reload apparmor的配置并重启</span><br><span class="line">$ service apparmor reload </span><br><span class="line">$ service apparmor restart </span><br><span class="line"> </span><br><span class="line">12.6 重启mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">如果启动不了，查看/var/log/mysql/error.log</span><br><span class="line">如果出现：InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable 仔细核对目录权限</span><br><span class="line"></span><br><span class="line">12.7 进mysql查询数据验证，成功</span><br><span class="line">select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"><a href="#13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。" class="headerlink" title="13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"></a>13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。</h3><h2 id="最后-若泽数据J哥总结一下"><a href="#最后-若泽数据J哥总结一下" class="headerlink" title="最后@若泽数据J哥总结一下:"></a>最后@若泽数据J哥总结一下:</h2><ul><li>表结构正确的获取；</li><li>机器磁盘规划提前思考；</li><li>ibd数据文件恢复；</li><li>最后加上一个聪明的媳妇！(PS:老板会给媳妇涨薪水不🙅‍♂️)</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 其他组件 </category>
          
          <category> 故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>入门Impala只需此篇</title>
      <link href="/2019/05/17/%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)/"/>
      <url>/2019/05/17/%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="学习路径"><a href="#学习路径" class="headerlink" title="学习路径"></a>学习路径</h2><ul><li>官网：<a href="http://impala.apache.org/" target="_blank" rel="noopener">http://impala.apache.org/</a></li><li>使用手册：<a href="http://impala.apache.org/docs/build/html/index.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/index.html</a></li><li>Sql：<a href="http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html</a></li><li>窗口函数：<a href="http://impala.apache.org/docs/build/html/topics/impala_functions.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_functions.html</a></li><li>基本操作：<a href="http://impala.apache.org/docs/build/html/topics/impala_tutorial.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_tutorial.html</a></li><li>impala-shell：<a href="http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html</a></li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ol><li>Apache Impala是Apache Hadoop的开源原生分析数据库;</li><li>Impala于2017年11月15日从Apache孵化成顶级项目。在以前称为“Cloudera Impala”的文档中，现在的官方名称是“Apache Impala”。</li><li>Impala为Hadoop上的BI /分析查询提供低延迟和高并发性（不是由Apache Hive等批处理框架提供）。即使在多租户环境中，Impala也可以线性扩展。</li><li>利用与Hadoop部署相同的文件和数据格式以及元数据，安全性和资源管理框架 - 无冗余基础架构或数据转换/复制。</li><li>对于Apache Hive用户，Impala使用相同的元数据和ODBC驱动程序。与Hive一样，Impala支持SQL</li><li>Impala与本机Hadoop安全性和Kerberos集成以进行身份验证，通过Sentry模块，您可以确保为正确的用户和应用程序授权使用正确的数据。</li><li>使用Impala，无论是使用SQL查询还是BI应用程序，更多用户都可以通过单个存储库和元数据存储进行交互</li></ol><h2 id="什么是Impala"><a href="#什么是Impala" class="headerlink" title="什么是Impala"></a>什么是Impala</h2><ol><li>Impala是一种面向实时或者面向批处理的框架;</li><li>Impala的数据可以存储在HDFS,HBase和Amazon Simple Storage Servive(S3)中;</li><li>Impala和Hive使用了相同的元数据存储;</li><li>可以通过SQL的语法,JDBC,ODBC和用户界面(Hue中的Impala进行查询);</li></ol><p>我们知道Hive底层是MapReduce,在这里就可以看出区别了,Impala并不是为了替换构建在MapReduce上的批处理框架,就像我们说的Hive,Hive适用于长时间运行的批处理作业,例如涉及到Extract,Transform和Load(ETL)类型的作业.而Impala是进行实时处理的.</p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li>通过sql进行大量数据处理;</li><li>可以进行分布式部署,进行分布式查询;</li><li>可以和不同组件之间进行数据共享,不需要复制或者导入,导出等步骤,例如:可以先使用hive对数据进行ETL操作然后使用Impala进行查询.因为Impala和hive公用同一个元数据,这样就可以方便的对hive生成的数据进行分析.</li></ol><h2 id="Impala如何与Apache-Hadoop一起使用"><a href="#Impala如何与Apache-Hadoop一起使用" class="headerlink" title="Impala如何与Apache Hadoop一起使用"></a>Impala如何与Apache Hadoop一起使用</h2><ul><li><p>Impala解决方案由以下组件组成：</p><ol><li>客户端 - 包括Hue，ODBC客户端，JDBC客户端和Impala Shell的实体都可以与Impala进行交互。这些接口通常用于发出查询或完成管理任务，例如连接到Impala。</li><li>Hive Metastore - 存储有关Impala可用数据的信息。例如，Metastore让Impala知道哪些数据库可用，以及这些数据库的结构是什么。在创建，删除和更改模式对象，将数据加载到表中等等时，通过Impala SQL语句，相关的元数据更改将通过Impala 1.2中引入的专用目录服务自动广播到所有Impala节点。</li><li>Impala - 此过程在DataNodes上运行，协调并执行查询。Impala的每个实例都可以接收，计划和协调来自Impala客户端的查询。</li><li><p>HBase和HDFS -数据的存储。</p><p>下面这幅图应该说的很清楚了:</p></li></ol></li></ul><p><img src="/assets/blogImg/impala.png" alt="enter description here"></p><h2 id="使用Impala执行的查询流程如下："><a href="#使用Impala执行的查询流程如下：" class="headerlink" title="使用Impala执行的查询流程如下："></a>使用Impala执行的查询流程如下：</h2><ul><li>用户应用程序通过ODBC或JDBC向Impala发送SQL查询，这些查询提供标准化的查询接口。用户应用程序可以连接到impalad群集中的任何应用程序。这impalad将成为查询的协调者。</li><li>Impala会解析查询并对其进行分析，以确定impalad整个群集中的实例需要执行哪些任务 。计划执行以实现最佳效率。</li><li>本地impalad实例访问HDFS和HBase等服务以提供数据。</li><li>每个都impalad将数据返回给协调impalad，协调将这些结果发送给客户端。</li></ul><h2 id="impala-shell"><a href="#impala-shell" class="headerlink" title="impala-shell"></a>impala-shell</h2><p>使用Impala shell工具（impala-shell）来设置数据库和表，插入数据和发出查询</p><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>-B or –delimited</td><td>导致使用分隔符分割的普通文本格式打印查询结果。当为其他 Hadoop 组件生成数据时有用。对于避免整齐打印所有输出的性能开销有用，特别是使用查询返回大量的结果集进行基准测试的时候。使用 –output_delimiter 选项指定分隔符。使用 -B 选项常用于保存所有查询结果到文件里而不是打印到屏幕上。在 Impala 1.0.1 中添加</td></tr><tr><td>–print_header</td><td>是否打印列名。整齐打印时是默认启用。同时使用 -B 选项时，在首行打印列名</td></tr><tr><td>-o filename or –output_file filename</td><td>保存所有查询结果到指定的文件。通常用于保存在命令行使用 -q 选项执行单个查询时的查询结果。对交互式会话同样生效；此时你只会看到获取了多少行数据，但看不到实际的数据集。当结合使用 -q 和 -o 选项时，会自动将错误信息输出到 /dev/null(To suppress these incidental messages when combining the -q and -o options, redirect stderr to /dev/null)。在 Impala 1.0.1 中添加</td></tr><tr><td>–output_delimiter=character</td><td>当使用 -B 选项以普通文件格式打印查询结果时，用于指定字段之间的分隔符(Specifies the character to use as a delimiter between fields when query results are printed in plain format by the -B option)。默认是制表符 tab (’\t’)。假如输出结果中包含了分隔符，该列会被引起且/或转义( If an output value contains the delimiter character, that field is quoted and/or escaped)。在 Impala 1.0.1 中添加</td></tr><tr><td>-p or –show_profiles</td><td>对 shell 中执行的每一个查询，显示其查询执行计划 (与 EXPLAIN 语句输出相同) 和发生低级故障(low-level breakdown)的执行步骤的更详细的信息</td></tr><tr><td>-h or –help</td><td>显示帮助信息</td></tr><tr><td>-i hostname or –impalad=hostname</td><td>指定连接运行 impalad 守护进程的主机。默认端口是 21000。你可以连接到集群中运行 impalad 的任意主机。假如你连接到 impalad 实例通过 –fe_port 标志使用了其他端口，则应当同时提供端口号，格式为 hostname:port</td></tr><tr><td>-q query or –query=query</td><td>从命令行中传递一个查询或其他 shell 命令。执行完这一语句后 shell 会立即退出。限制为单条语句，可以是 SELECT, CREATE TABLE, SHOW TABLES, 或其他 impala-shell 认可的语句。因为无法传递 USE 语句再加上其他查询，对于 default 数据库之外的表，应在表名前加上数据库标识符(或者使用 -f 选项传递一个包含 USE 语句和其他查询的文件)</td></tr><tr><td>-f query_file or –query_file=query_file</td><td>传递一个文件中的 SQL 查询。文件内容必须以分号分隔</td></tr><tr><td>-k or –kerberos</td><td>当连接到 impalad 时使用 Kerberos 认证。如果要连接的 impalad 实例不支持 Kerberos，将显示一个错误</td></tr><tr><td>-s kerberos_service_name or –kerberos_service_name=name</td><td>Instructs impala-shell to authenticate to a particular impalad service principal. 如何没有设置 kerberos_service_name ，默认使用 impala。如何启用了本选项，而试图建立不支持Kerberos 的连接时，返回一个错误(If this option is used in conjunction with a connection in which Kerberos is not supported, errors are returned)</td></tr><tr><td>-V or –verbose</td><td>启用详细输出</td></tr><tr><td>–quiet</td><td>关闭详细输出</td></tr><tr><td>-v or –version</td><td>显示版本信息</td></tr><tr><td>-c</td><td>查询执行失败时继续执行</td></tr><tr><td>-r or –refresh_after_connect</td><td>建立连接后刷新 Impala 元数据，与建立连接后执行 REFRESH 语句效果相同</td></tr><tr><td>-d default_db or –database=default_db</td><td>指定启动后使用的数据库，与建立连接后使用 USE 语句选择数据库作用相同，如果没有指定，那么使用 default 数据库</td></tr><tr><td>-l</td><td>启用 LDAP 认证</td></tr><tr><td>-u</td><td>当使用 -l 选项启用 LDAP 认证时，提供用户名(使用短用户名，而不是完整的 LDAP 专有名称(distinguished name)) ，shell 会提示输入密码</td></tr></tbody></table><h2 id="概念与架构"><a href="#概念与架构" class="headerlink" title="概念与架构"></a>概念与架构</h2><h3 id="Impala-Server的组件"><a href="#Impala-Server的组件" class="headerlink" title="Impala Server的组件"></a>Impala Server的组件</h3><p>Impala服务器是分布式，大规模并行处理（MPP）数据库引擎。它由在群集中的特定主机上运行的不同守护程序进程组成。</p><p><strong>The Impala Daemon</strong></p><p>Impala的核心组件是Impala daemon。Impala daemon执行的一些关键功能是：</p><ul><li>读取和写入数据文件。</li><li>接受从impala-shell命令，Hue，JDBC或ODBC传输的查询。</li><li>并行化查询并在群集中分配工作。</li><li>将中间查询结果发送回中央协调器。</li><li>可以通过以下方式之一部署Impala守护程序：<ol><li>HDFS和Impala位于同一位置，每个Impala守护程序与DataNode在同一主机上运行。</li><li>Impala单独部署在计算群集中，可从HDFS，S3，ADLS等远程读取。Impala守护进程与StateStore保持持续通信，以确认哪些守护进程是健康的并且可以接受新工作。</li></ol></li></ul><p><b>在Impala 2.9及更高版本中，您可以控制哪些主机充当查询协调器，哪些主机充当查询执行程序，以提高大型群集上高度并发工作负载的可伸缩性。</b></p><p><strong>Impala Statestore</strong></p><p>Impala Statestore进程检查集群中所有Impala daemon的运行状况，并把信息反馈给Impala daemon进程。您只需要在群集中的一台主机上执行此类过程。如果Impala守护程序由于硬件故障，网络错误，软件问题或其他原因而脱机，则StateStore会通知所有其他Impala daemon程序，以便将来的查询可以避免向无法访问的Impala守护程序发出请求。</p><p>因为StateStore的目的是在出现问题时提供帮助并向协调器广播元数据，因此对Impala集群的正常操作并不总是至关重要的。如果StateStore未运行或无法访问，则在处理Impala已知的数据时，Impala守护程序会像往常一样继续运行和分配工作。如果其他Impala守护程序失败，则群集变得不那么健壮，并且当StateStore脱机时，元数据变得不那么一致。当StateStore重新联机时，它会重新建立与Impala守护程序的通信并恢复其监视和广播功能。</p><p><strong>The Impala Catalog Service</strong></p><p>Impala Catalog Service进程可以把Impala SQL语句中的元数据更改信息反馈到集群中的所有Impala守护程序。只需要在群集中的一台主机上执行此类过程。因为请求是通过StateStore守护程序传递的，所以要在同一主机上运行statestored和catalogd服务。</p><p>当通过Impala发出的语句执行元数据更改时，Impala Catalog Service进程避免了REFRESH和INVALIDATE METADATA语句的使用,该进程可以为我们更新元数据信息。</p><p><strong>使用–load_catalog_in_background选项控制何时加载表的元数据。</strong></p><ul><li>如果设置为false，则在第一次引用表时会加载表的元数据。这意味着第一次运行可能比后续运行慢。在impala2.2开始，默认load_catalog_in_background是 false。</li><li>如果设置为true，即使没有查询需要该元数据，目录服务也会尝试加载表的元数据。因此，当运行需要它的第一个查询时，可能已经加载了元数据。但是，由于以下原因，我们建议不要将选项设置为true。</li></ul><p>后台加载可能会干扰查询特定的元数据加载。这可能在启动时或在使元数据无效之后发生，持续时间取决于元数据的数量，并且可能导致看似随机的长时间运行的查询难以诊断。</p><p>Impala可能会加载从未使用过的表的元数据，这会增加目录服务和Impala守护程序的目录大小，从而增加内存使用量。</p><p>负载均衡和高可用性的大多数注意事项适用于impalad守护程序。该statestored和catalogd守护进程不具备高可用性的特殊要求，因为这些守护进程的问题不会造成数据丢失。如果这些守护程序由于特定主机上的中断而变得不可用，则可以停止Impala服务，删除Impala StateStore和Impala目录服务器角色，在其他主机上添加角色，然后重新启动Impala服务。</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Impala支持一组数据类型，可用于表列，表达式值，函数参数和返回值。</p><p>注意： 目前，Impala仅支持标量类型，而不支持复合类型或嵌套类型。访问包含任何具有不受支持类型的列的表会导致错误。</p><p>有关Impala和Hive数据类型之间的差异，请参阅:<br><a href="http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_delta" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_delta</a></p><ul><li>ARRAY复杂类型（仅限Impala 2.3或更高版本）</li><li>BIGINT数据类型</li><li>BOOLEAN数据类型</li><li>CHAR数据类型（仅限Impala 2.0或更高版本）</li><li>DECIMAL数据类型（仅限Impala 3.0或更高版本）</li><li>双数据类型</li><li>FLOAT数据类型</li><li>INT数据类型</li><li>MAP复杂类型（仅限Impala 2.3或更高版本）</li><li>REAL数据类型</li><li>SMALLINT数据类型</li><li>STRING数据类型</li><li>STRUCT复杂类型（仅限Impala 2.3或更高版本）</li><li>TIMESTAMP数据类型</li><li>TINYINT数据类型</li><li>VARCHAR数据类型（仅限Impala 2.0或更高版本）</li><li>复杂类型（仅限Impala 2.3或更高版本）</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:连续3周若泽数据第13-15名学员喜捷offer(年薪总和108W)</title>
      <link href="/2019/05/14/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD3%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC13-15%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%B9%B4%E8%96%AA%E6%80%BB%E5%92%8C108W)/"/>
      <url>/2019/05/14/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD3%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC13-15%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%B9%B4%E8%96%AA%E6%80%BB%E5%92%8C108W)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第13个小伙伴，<font color="red">19.5K*12</font></b></font><p><img src="/assets/blogImg/2019-05-14-1.png" alt="就业1"></p><font color="#00CD00" size="4"><b>2.第14个小伙伴，<font color="red">25k*13</font></b></font><p><img src="/assets/blogImg/2019-05-14-2.png" alt="就业2"></p><font color="#00CD00" size="4"><b>3.第15个小伙伴，<font color="red">33K*16+20W</font>期权</b></font><p><img src="/assets/blogImg/2019-05-14-3.png" alt="就业3"></p><font color="#00CD00" size="4"><b>前面拿到offer的2个小伙伴，已经入职</b></font> <font color="blue" size="4"><b>两个都是Mac Pro标配，羡慕不？</b></font><p><img src="/assets/blogImg/2019-05-14-4.png" alt="1"></p><p><img src="/assets/blogImg/2019-05-14-5.png" alt="2"></p><font color="#00CD00" size="4"><b>去年在校生年薪30W的offer，<br><br>已经毕业开始去杭州了，<br><br>时间真心很快，当你自学时，<br><br>别人早已通过若泽数据，已经开始新的人生征途了。</b></font> <font color="blue" size="4"><b>明年毕业的我，拿了大数据30万的offer！</b></font><p><img src="/assets/blogImg/2019-05-14-6.png" alt="3"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</title>
      <link href="/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/"/>
      <url>/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="若泽数据"><a href="#若泽数据" class="headerlink" title="若泽数据"></a><a href="www.ruozedata.com">若泽数据</a></h2><h2 id="CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践"><a href="#CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践" class="headerlink" title="CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)"></a>CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</h2><p>视频:<a href="https://www.bilibili.com/video/av52167219" target="_blank" rel="noopener">https://www.bilibili.com/video/av52167219</a><br>PS:建议先看课程视频1-2篇，再根据视频或文档部署，<br>如有问题，及时与@若泽数据J哥联系。</p><a id="more"></a><hr><h2 id="一-准备工作"><a href="#一-准备工作" class="headerlink" title="一.准备工作"></a>一.准备工作</h2><h4 id="1-离线部署主要分为三块"><a href="#1-离线部署主要分为三块" class="headerlink" title="1.离线部署主要分为三块:"></a>1.离线部署主要分为三块:</h4><p>a.MySQL离线部署<br>b.CM离线部署<br>c.Parcel文件离线源部署</p><h4 id="2-规划"><a href="#2-规划" class="headerlink" title="2.规划:"></a>2.规划:</h4><table><thead><tr><th>节点</th><th>MySQL部署组件</th><th>Parcel文件离线源</th><th>CM服务进程</th><th>大数据组件</th></tr></thead><tbody><tr><td>hadoop001</td><td>MySQL</td><td>Parcel</td><td>Activity Monitor<br></td><td>NN RM DN NM</td></tr><tr><td>hadoop002</td><td></td><td></td><td>Alert Publisher<br>Event Server</td><td>DN NM</td></tr><tr><td>hadoop003</td><td></td><td></td><td>Host Monitor<br>Service Monitor</td><td>DN NM</td></tr></tbody></table><h3 id="3-下载源"><a href="#3-下载源" class="headerlink" title="3.下载源:"></a>3.下载源:</h3><ul><li>CM<br><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz" target="_blank" rel="noopener">cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</a></li><li>Parcel<br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json" target="_blank" rel="noopener">manifest.json</a></li><li><p>JDK<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a><br>下载jdk-8u202-linux-x64.tar.gz</p></li><li><p>MySQL<br><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a><br>下载mysql-5.7.26-el7-x86_64.tar.gz</p></li><li><p>MySQL jdbc jar<br><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.47.jar</a><br>下载完成后要重命名去掉版本号，<br>mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar</p></li></ul><hr><p>###准备好百度云,下载安装包:<br>链接:<a href="https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA" target="_blank" rel="noopener">https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA</a> 密码:viqp</p><h2 id="二-集群节点初始化"><a href="#二-集群节点初始化" class="headerlink" title="二.集群节点初始化"></a>二.集群节点初始化</h2><h3 id="1-阿里云上海区购买3台，按量付费虚拟机"><a href="#1-阿里云上海区购买3台，按量付费虚拟机" class="headerlink" title="1.阿里云上海区购买3台，按量付费虚拟机"></a>1.阿里云上海区购买3台，按量付费虚拟机</h3><p>CentOS7.2操作系统，2核8G最低配置</p><h3 id="2-当前笔记本或台式机配置hosts文件"><a href="#2-当前笔记本或台式机配置hosts文件" class="headerlink" title="2.当前笔记本或台式机配置hosts文件"></a>2.当前笔记本或台式机配置hosts文件</h3><ul><li>MAC: /etc/hosts</li><li>Window: C:\windows\system32\drivers\etc\hosts</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">公网地址: </span><br><span class="line">106.15.234.222 hadoop001  </span><br><span class="line">106.15.235.200 hadoop002  </span><br><span class="line">106.15.234.239 hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-设置所有节点的hosts文件"><a href="#3-设置所有节点的hosts文件" class="headerlink" title="3.设置所有节点的hosts文件"></a>3.设置所有节点的hosts文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">私有地铁、内网地址:</span><br><span class="line">echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="4-关闭所有节点的防火墙及清空规则"><a href="#4-关闭所有节点的防火墙及清空规则" class="headerlink" title="4.关闭所有节点的防火墙及清空规则"></a>4.关闭所有节点的防火墙及清空规则</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure><h3 id="5-关闭所有节点的selinux"><a href="#5-关闭所有节点的selinux" class="headerlink" title="5.关闭所有节点的selinux"></a>5.关闭所有节点的selinux</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">将SELINUX=enforcing改为SELINUX=disabled </span><br><span class="line">设置后需要重启才能生效</span><br></pre></td></tr></table></figure><h3 id="6-设置所有节点的时区一致及时钟同步"><a href="#6-设置所有节点的时区一致及时钟同步" class="headerlink" title="6.设置所有节点的时区一致及时钟同步"></a>6.设置所有节点的时区一致及时钟同步</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">6.1.时区</span><br><span class="line">[root@hadoop001 ~]# date</span><br><span class="line">Sat May 11 10:07:53 CST 2019</span><br><span class="line">[root@hadoop001 ~]# timedatectl</span><br><span class="line">      Local time: Sat 2019-05-11 10:10:31 CST</span><br><span class="line">  Universal time: Sat 2019-05-11 02:10:31 UTC</span><br><span class="line">        RTC time: Sat 2019-05-11 10:10:29</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br><span class="line">     NTP enabled: yes</span><br><span class="line">NTP synchronized: yes</span><br><span class="line"> RTC in local TZ: yes</span><br><span class="line">      DST active: n/a</span><br><span class="line"></span><br><span class="line">#查看命令帮助，学习至关重要，无需百度，太👎</span><br><span class="line">[root@hadoop001 ~]# timedatectl --help</span><br><span class="line">timedatectl [OPTIONS...] COMMAND ...</span><br><span class="line"></span><br><span class="line">Query or change system time and date settings.</span><br><span class="line"></span><br><span class="line">  -h --help                Show this help message</span><br><span class="line">     --version             Show package version</span><br><span class="line">     --no-pager            Do not pipe output into a pager</span><br><span class="line">     --no-ask-password     Do not prompt for password</span><br><span class="line">  -H --host=[USER@]HOST    Operate on remote host</span><br><span class="line">  -M --machine=CONTAINER   Operate on local container</span><br><span class="line">     --adjust-system-clock Adjust system clock when changing local RTC mode</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  status                   Show current time settings</span><br><span class="line">  set-time TIME            Set system time</span><br><span class="line">  set-timezone ZONE        Set system time zone</span><br><span class="line">  list-timezones           Show known time zones</span><br><span class="line">  set-local-rtc BOOL       Control whether RTC is in local time</span><br><span class="line">  set-ntp BOOL             Control whether NTP is enabled</span><br><span class="line"></span><br><span class="line">#查看哪些时区</span><br><span class="line">[root@hadoop001 ~]# timedatectl list-timezones</span><br><span class="line">Africa/Abidjan</span><br><span class="line">Africa/Accra</span><br><span class="line">Africa/Addis_Ababa</span><br><span class="line">Africa/Algiers</span><br><span class="line">Africa/Asmara</span><br><span class="line">Africa/Bamako</span><br><span class="line"></span><br><span class="line">#所有节点设置亚洲上海时区 </span><br><span class="line">[root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">6.2.时间</span><br><span class="line">#所有节点安装ntp</span><br><span class="line">[root@hadoop001 ~]# yum install -y ntp</span><br><span class="line"></span><br><span class="line">#选取hadoop001为ntp的主节点</span><br><span class="line">[root@hadoop001 ~]# vi /etc/ntp.conf </span><br><span class="line"></span><br><span class="line">#time</span><br><span class="line">server 0.asia.pool.ntp.org</span><br><span class="line">server 1.asia.pool.ntp.org</span><br><span class="line">server 2.asia.pool.ntp.org</span><br><span class="line">server 3.asia.pool.ntp.org</span><br><span class="line">#当外部时间不可用时，可使用本地硬件时间</span><br><span class="line">server 127.127.1.0 iburst local clock </span><br><span class="line">#允许哪些网段的机器来同步时间</span><br><span class="line">restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#开启ntpd及查看状态</span><br><span class="line">[root@hadoop001 ~]# systemctl start ntpd</span><br><span class="line">[root@hadoop001 ~]# systemctl status ntpd</span><br><span class="line"> ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago</span><br><span class="line"> Main PID: 18518 (ntpd)</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           └─18518 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line"></span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usec</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabled</span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.</span><br><span class="line"></span><br><span class="line">#验证</span><br><span class="line">[root@hadoop001 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> LOCAL(0)        .LOCL.          10 l  726   64    0    0.000    0.000   0.000</span><br><span class="line"></span><br><span class="line">#其他从节点停止禁用ntpd服务 </span><br><span class="line">[root@hadoop002 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop002 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop002 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001  </span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop004 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop003 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001</span><br></pre></td></tr></table></figure><h3 id="7-部署集群的JDK"><a href="#7-部署集群的JDK" class="headerlink" title="7.部署集群的JDK"></a>7.部署集群的JDK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/</span><br><span class="line">#切记必须修正所属用户及用户组</span><br><span class="line">chown -R root:root /usr/java/jdk1.8.0_45</span><br><span class="line"></span><br><span class="line">echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure><h3 id="8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档"><a href="#8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档" class="headerlink" title="8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)"></a>8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)</h3><ul><li>文档链接:<a href="https://github.com/Hackeruncle/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL</a></li><li>视频链接:<a href="https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ</a> 密码:whic</li></ul><h3 id="9-创建CDH的元数据库和用户、amon服务的数据库及用户"><a href="#9-创建CDH的元数据库和用户、amon服务的数据库及用户" class="headerlink" title="9.创建CDH的元数据库和用户、amon服务的数据库及用户"></a>9.创建CDH的元数据库和用户、amon服务的数据库及用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create database cmf DEFAULT CHARACTER SET utf8;</span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="10-hadoop001节点部署mysql-jdbc-jar"><a href="#10-hadoop001节点部署mysql-jdbc-jar" class="headerlink" title="10.hadoop001节点部署mysql jdbc jar"></a>10.hadoop001节点部署mysql jdbc jar</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java.jar /usr/share/java/</span><br></pre></td></tr></table></figure><h2 id="三-CDH部署"><a href="#三-CDH部署" class="headerlink" title="三.CDH部署"></a>三.CDH部署</h2><h3 id="1-离线部署cm-server及agent"><a href="#1-离线部署cm-server及agent" class="headerlink" title="1.离线部署cm server及agent"></a>1.离线部署cm server及agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1.1.所有节点创建目录及解压</span><br><span class="line">mkdir /opt/cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/</span><br><span class="line"></span><br><span class="line">1.2.所有节点修改agent的配置，指向server的节点hadoop001</span><br><span class="line">sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">1.3.主节点修改server的配置:</span><br><span class="line">vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties </span><br><span class="line">com.cloudera.cmf.db.type=mysql</span><br><span class="line">com.cloudera.cmf.db.host=hadoop001</span><br><span class="line">com.cloudera.cmf.db.name=cmf</span><br><span class="line">com.cloudera.cmf.db.user=cmf</span><br><span class="line">com.cloudera.cmf.db.password=Ruozedata123456!</span><br><span class="line">com.cloudera.cmf.db.setupType=EXTERNAL</span><br><span class="line"></span><br><span class="line">1.4.所有节点创建用户</span><br><span class="line">useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line"></span><br><span class="line">1.5.目录修改用户及用户组</span><br><span class="line">chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager</span><br></pre></td></tr></table></figure><h3 id="2-hadoop001节点部署离线parcel源"><a href="#2-hadoop001节点部署离线parcel源" class="headerlink" title="2.hadoop001节点部署离线parcel源"></a>2.hadoop001节点部署离线parcel源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2.1.部署离线parcel源</span><br><span class="line">$ mkdir -p /opt/cloudera/parcel-repo</span><br><span class="line">$ ll</span><br><span class="line">total 3081664</span><br><span class="line">-rw-r--r-- 1 root root 2127506677 May  9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</span><br><span class="line">-rw-r--r-- 1 root root         41 May  9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</span><br><span class="line">-rw-r--r-- 1 root root  841524318 May  9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  185515842 Aug 10  2017 jdk-8u144-linux-x64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root      66538 May  9 18:03 manifest.json</span><br><span class="line">-rw-r--r-- 1 root root     989495 May 25  2017 mysql-connector-java.jar</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">#切记cp时，重命名去掉1，不然在部署过程CM认为如上文件下载未完整，会持续下载</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha</span><br><span class="line">$ cp manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">2.2.目录修改用户及用户组</span><br><span class="line">$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/</span><br></pre></td></tr></table></figure><h3 id="3-所有节点创建软件安装目录、用户及用户组权限"><a href="#3-所有节点创建软件安装目录、用户及用户组权限" class="headerlink" title="3.所有节点创建软件安装目录、用户及用户组权限"></a>3.所有节点创建软件安装目录、用户及用户组权限</h3><p>mkdir -p /opt/cloudera/parcels<br>chown -R cloudera-scm:cloudera-scm /opt/cloudera/</p><h3 id="4-hadoop001节点启动Server"><a href="#4-hadoop001节点启动Server" class="headerlink" title="4.hadoop001节点启动Server"></a>4.hadoop001节点启动Server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.1.启动server</span><br><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start</span><br><span class="line"></span><br><span class="line">4.2.阿里云web界面，设置该hadoop001节点防火墙放开7180端口</span><br><span class="line">4.3.等待1min，打开 http://hadoop001:7180 账号密码:admin/admin</span><br><span class="line">4.4.假如打不开，去看server的log，根据错误仔细排查错误</span><br></pre></td></tr></table></figure><h3 id="5-所有节点启动Agent"><a href="#5-所有节点启动Agent" class="headerlink" title="5.所有节点启动Agent"></a>5.所有节点启动Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h3 id="6-接下来，全部Web界面操作"><a href="#6-接下来，全部Web界面操作" class="headerlink" title="6.接下来，全部Web界面操作"></a>6.接下来，全部Web界面操作</h3><p><a href="http://hadoop001:7180/" target="_blank" rel="noopener">http://hadoop001:7180/</a><br>账号密码:admin/admin</p><h3 id="7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选"><a href="#7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选" class="headerlink" title="7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选"></a>7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选</h3><p><img src="/assets/blogImg/CDH516_1.png" alt="enter description here"></p><h3 id="8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本"><a href="#8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本" class="headerlink" title="8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本"></a>8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本</h3><p><img src="/assets/blogImg/CDH516_2.png" alt="enter description here"></p><h3 id="9-感谢您选择Cloudera-Manager和CDH"><a href="#9-感谢您选择Cloudera-Manager和CDH" class="headerlink" title="9.感谢您选择Cloudera Manager和CDH"></a>9.感谢您选择Cloudera Manager和CDH</h3><p><img src="/assets/blogImg/CDH516_3.png" alt="enter description here"></p><h3 id="10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选"><a href="#10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选" class="headerlink" title="10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选"></a>10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选</h3><p><img src="/assets/blogImg/CDH516_4.png" alt="enter description here"></p><h3 id="11-选择存储库"><a href="#11-选择存储库" class="headerlink" title="11.选择存储库"></a>11.选择存储库</h3><p><img src="/assets/blogImg/CDH516_5.png" alt="enter description here"></p><h3 id="12-集群安装–正在安装选定Parcel假如"><a href="#12-集群安装–正在安装选定Parcel假如" class="headerlink" title="12.集群安装–正在安装选定Parcel假如"></a>12.集群安装–正在安装选定Parcel假如</h3><p>本地parcel离线源配置正确，则”下载”阶段瞬间完成，其余阶段视节点数与内部网络情况决定。<br><img src="/assets/blogImg/CDH516_6.png" alt="enter description here"></p><h3 id="13-检查主机正确性"><a href="#13-检查主机正确性" class="headerlink" title="13.检查主机正确性"></a>13.检查主机正确性</h3><p><img src="/assets/blogImg/CDH516_7.png" alt="enter description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">13.1.建议将/proc/sys/vm/swappiness设置为最大值10。</span><br><span class="line">swappiness值控制操作系统尝试交换内存的积极；</span><br><span class="line">swappiness=0：表示最大限度使用物理内存，之后才是swap空间；</span><br><span class="line">swappiness=100：表示积极使用swap分区，并且把内存上的数据及时搬迁到swap空间；</span><br><span class="line">如果是混合服务器，不建议完全禁用swap，可以尝试降低swappiness。</span><br><span class="line"></span><br><span class="line">临时调整：</span><br><span class="line">sysctl vm.swappiness=10</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># Adjust swappiness value</span><br><span class="line">vm.swappiness=10</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">13.2.已启用透明大页面压缩，可能会导致重大性能问题，建议禁用此设置。</span><br><span class="line">临时调整：</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local</span><br><span class="line"># Disable transparent_hugepage</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># centos7.x系统，需要为&quot;/etc/rc.d/rc.local&quot;文件赋予执行权限</span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务"><a href="#14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务" class="headerlink" title="14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务"></a>14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务</h3><p><img src="/assets/blogImg/CDH516_8.png" alt="enter description here"></p><h3 id="15-自定义角色分配"><a href="#15-自定义角色分配" class="headerlink" title="15.自定义角色分配"></a>15.自定义角色分配</h3><p><img src="/assets/blogImg/CDH516_9.png" alt="enter description here"></p><h3 id="16-数据库设置"><a href="#16-数据库设置" class="headerlink" title="16.数据库设置"></a>16.数据库设置</h3><p><img src="/assets/blogImg/CDH516_10.png" alt="enter description here"></p><h3 id="17-审改设置，默认即可"><a href="#17-审改设置，默认即可" class="headerlink" title="17.审改设置，默认即可"></a>17.审改设置，默认即可</h3><p><img src="/assets/blogImg/CDH516_11.png" alt="enter description here"></p><h3 id="18-首次运行"><a href="#18-首次运行" class="headerlink" title="18.首次运行"></a>18.首次运行</h3><p><img src="/assets/blogImg/CDH516_12.png" alt="enter description here"></p><h3 id="19-恭喜您"><a href="#19-恭喜您" class="headerlink" title="19.恭喜您!"></a>19.恭喜您!</h3><p><img src="/assets/blogImg/CDH516_13.png" alt="enter description here"></p><h3 id="20-主页"><a href="#20-主页" class="headerlink" title="20.主页"></a>20.主页</h3><p><img src="/assets/blogImg/CDH516_14.png" alt="enter description here"></p><hr><h3 id="CDH全套课程目录，如有buy，加微信-ruoze-star"><a href="#CDH全套课程目录，如有buy，加微信-ruoze-star" class="headerlink" title="CDH全套课程目录，如有buy，加微信(ruoze_star)"></a>CDH全套课程目录，如有buy，加微信(ruoze_star)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">0.青云环境介绍和使用 </span><br><span class="line">1.Preparation        </span><br><span class="line">谈谈怎样入门大数据 </span><br><span class="line">谈谈怎样做好一个大数据平台的运营工作 </span><br><span class="line">Linux机器,各软件版本介绍及安装(录播) </span><br><span class="line">2.Introduction      </span><br><span class="line">Cloudera、CM及CDH介绍 </span><br><span class="line">CDH版本选择 </span><br><span class="line">CDH安装几种方式解读 </span><br><span class="line">3.Install&amp;UnInstall  </span><br><span class="line">集群节点规划,环境准备(NTP,Jdk and etc) </span><br><span class="line">MySQL编译安装及常用命令 </span><br><span class="line">推荐:CDH离线安装(踩坑心得,全面剖析) </span><br><span class="line">解读暴力卸载脚本 </span><br><span class="line"></span><br><span class="line">4.CDH Management      </span><br><span class="line">CDH体系架构剖析 </span><br><span class="line">CDH配置文件深度解析 </span><br><span class="line">CM的常用命令 </span><br><span class="line">CDH集群正确启动和停止顺序 </span><br><span class="line">CDH Tsquery Language </span><br><span class="line">CDH常规管理(监控/预警/配置/资源/日志/安全) </span><br><span class="line"></span><br><span class="line">5.Maintenance Experiment  </span><br><span class="line">HDFS HA 配置 及hadoop/hdfs常规命令 </span><br><span class="line">Yarn HA 配置 及yarn常规命令 </span><br><span class="line">Other CDH Components HA 配置 </span><br><span class="line">CDH动态添加删除服务(hive/spark/hbase) </span><br><span class="line">CDH动态添加删除机器 </span><br><span class="line">CDH动态添加删除及迁移DataNode进程等 </span><br><span class="line">CDH升级(5.10.0--&gt;5.12.0) </span><br><span class="line"></span><br><span class="line">6.Resource Management    </span><br><span class="line">Linux Cgroups </span><br><span class="line">静态资源池 </span><br><span class="line">动态资源池 </span><br><span class="line">多租户案例 </span><br><span class="line"></span><br><span class="line">7.Performance Tunning    </span><br><span class="line">Memory/CPU/Network/Disk及集群规划 </span><br><span class="line">Linux参数 </span><br><span class="line">HDFS参数 </span><br><span class="line">MapReduce及Yarn参数 </span><br><span class="line">其他服务参数 </span><br><span class="line"></span><br><span class="line">8.Cases Share </span><br><span class="line">CDH4&amp;5之Alternatives命令 的研究 </span><br><span class="line">CDH5.8.2安装之Hash verification failed </span><br><span class="line">记录一次CDH4.8.6 配置HDFS HA 坑 </span><br><span class="line">CDH5.0集群IP更改 </span><br><span class="line">CDH的active namenode exit(GC)和彩蛋分享 </span><br><span class="line"></span><br><span class="line">9. Kerberos</span><br><span class="line">Kerberos简介</span><br><span class="line">Kerberos体系结构</span><br><span class="line">Kerberos工作机制</span><br><span class="line">Kerberos安装部署</span><br><span class="line">CDH启用kerberos</span><br><span class="line">Kerberos开发使用(真实代码)</span><br><span class="line"></span><br><span class="line">10.Summary         </span><br><span class="line">总结</span><br></pre></td></tr></table></figure><hr><h4 id="Join-us-if-you-have-a-dream"><a href="#Join-us-if-you-have-a-dream" class="headerlink" title="Join us if you have a dream."></a>Join us if you have a dream.</h4><h5 id="若泽数据官网-http-ruozedata-com"><a href="#若泽数据官网-http-ruozedata-com" class="headerlink" title="若泽数据官网: http://ruozedata.com"></a>若泽数据官网: <a href="http://ruozedata.com" target="_blank" rel="noopener">http://ruozedata.com</a></h5><h5 id="腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com"><a href="#腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com" class="headerlink" title="腾讯课堂，搜若泽数据: http://ruoze.ke.qq.com"></a>腾讯课堂，搜若泽数据: <a href="http://ruoze.ke.qq.com" target="_blank" rel="noopener">http://ruoze.ke.qq.com</a></h5><h5 id="Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323"><a href="#Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323" class="headerlink" title="Bilibili网站,搜若泽数据: https://space.bilibili.com/356836323"></a>Bilibili网站,搜若泽数据: <a href="https://space.bilibili.com/356836323" target="_blank" rel="noopener">https://space.bilibili.com/356836323</a></h5><h5 id="若泽大数据–官方博客"><a href="#若泽大数据–官方博客" class="headerlink" title="若泽大数据–官方博客"></a><a href="https://ruozedata.github.io" target="_blank" rel="noopener">若泽大数据–官方博客</a></h5><h5 id="若泽大数据–博客一览"><a href="#若泽大数据–博客一览" class="headerlink" title="若泽大数据–博客一览"></a><a href="https://github.com/ruozedata/BigData/blob/master/blog/BigDataBlogOverview.md" target="_blank" rel="noopener">若泽大数据–博客一览</a></h5><h5 id="若泽大数据–内部学员面试题"><a href="#若泽大数据–内部学员面试题" class="headerlink" title="若泽大数据–内部学员面试题"></a><a href="https://github.com/ruozedata/BigData/blob/master/interview/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.md" target="_blank" rel="noopener">若泽大数据–内部学员面试题</a></h5><h5 id="扫一扫，学一学"><a href="#扫一扫，学一学" class="headerlink" title="扫一扫，学一学:"></a>扫一扫，学一学:</h5><p><img src="//yoursite.com/2019/05/13/若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)/若泽数据--扫描入口.png" alt="avatar"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cdh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch常用操作解析</title>
      <link href="/2019/05/13/Elasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/05/13/Elasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="创建Maven管理的Java项目"><a href="#创建Maven管理的Java项目" class="headerlink" title="创建Maven管理的Java项目"></a>创建Maven管理的Java项目</h3><p>在pom.xml中添加依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;es.version&gt;6.1.1&lt;/es.version&gt;</span><br><span class="line"> </span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;transport&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;es.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>然后创建一个单元测试类ESApp：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private TransportClient client;</span><br><span class="line"> </span><br><span class="line">    @Before</span><br><span class="line">    public void setUp() throws Exception &#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;mycluster&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, &quot;true&quot;)//增加自动嗅探配置</span><br><span class="line">                .build();</span><br><span class="line"> </span><br><span class="line">        client = new PreBuiltTransportClient(settings);</span><br><span class="line">        client.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;10.8.24.94&quot;), 9300));</span><br><span class="line"> </span><br><span class="line">        System.out.println(client.toString());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行后报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonFactory</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>运行后成功拿到ES的client：</p><p><img src="/assets/blogImg/es.png" alt="enter description here"></p><h3 id="创建一个Index"><a href="#创建一个Index" class="headerlink" title="创建一个Index"></a>创建一个Index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@Test</span><br><span class="line">    public void createIndex() &#123;</span><br><span class="line">        client.admin().indices().prepareCreate(INDEX).get();</span><br><span class="line">        System.out.println(&quot;创建Index成功&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="删除一个Index"><a href="#删除一个Index" class="headerlink" title="删除一个Index"></a>删除一个Index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void deleteIndex() &#123;</span><br><span class="line">        client.admin().indices().prepareDelete(INDEX).get();</span><br><span class="line">        System.out.println(&quot;删除Index成功&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="放入数据的三种方式"><a href="#放入数据的三种方式" class="headerlink" title="放入数据的三种方式"></a>放入数据的三种方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">//不推荐使用，太繁琐拼json格式</span><br><span class="line"> @Test</span><br><span class="line">    public void createDoc() &#123;</span><br><span class="line">        String json = &quot;&#123;\&quot;name\&quot;:\&quot;若泽数据\&quot;&#125;&quot;;</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;100&quot;)</span><br><span class="line">                .setSource(json, XContentType.JSON)</span><br><span class="line">                .get();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    //推荐使用</span><br><span class="line">    @Test</span><br><span class="line">    public void test01() throws Exception &#123;</span><br><span class="line">        Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;();</span><br><span class="line">        json.put(&quot;name&quot;, &quot;ruozedata&quot;);</span><br><span class="line">        json.put(&quot;message&quot;, &quot;trying out Elasticsearch&quot;);</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;101&quot;).setSource(json).get();</span><br><span class="line">        System.out.println(response.getVersion());</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">//推荐使用</span><br><span class="line">    @Test</span><br><span class="line">    public void test02() throws Exception &#123;</span><br><span class="line"> </span><br><span class="line">        XContentBuilder builder = jsonBuilder()</span><br><span class="line">                .startObject()</span><br><span class="line">                .field(&quot;user&quot;, &quot;ruoze&quot;)</span><br><span class="line">                .field(&quot;postDate&quot;, new Date())</span><br><span class="line">                .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;)</span><br><span class="line">                .endObject();</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;102&quot;).setSource(builder).get();</span><br><span class="line">        System.out.println(response.getVersion());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="拿到一条数据"><a href="#拿到一条数据" class="headerlink" title="拿到一条数据"></a>拿到一条数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@Test</span><br><span class="line">    public void getDoc() &#123;</span><br><span class="line">        GetResponse response = client.prepareGet(INDEX, TYPE, &quot;100&quot;).get();</span><br><span class="line">        System.out.println(response.getSourceAsString());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="拿到多条数据"><a href="#拿到多条数据" class="headerlink" title="拿到多条数据"></a>拿到多条数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void getDocsByIds() &#123;</span><br><span class="line"> </span><br><span class="line">        MultiGetResponse responses = client.prepareMultiGet()</span><br><span class="line">                .add(INDEX, TYPE,&quot;100&quot;)</span><br><span class="line">                .add(INDEX, TYPE, &quot;101&quot;, &quot;102&quot;, &quot;1000&quot;)</span><br><span class="line">                .get();</span><br><span class="line"> </span><br><span class="line">        for (MultiGetItemResponse response : responses) &#123;</span><br><span class="line">            GetResponse res = response.getResponse();</span><br><span class="line">            if (res.isExists()) &#123;</span><br><span class="line">                System.out.println(res);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                System.out.println(&quot;没有这条数据&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之三(自定义累加器)</title>
      <link href="/2019/05/10/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)/"/>
      <url>/2019/05/10/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="思路-amp-需求"><a href="#思路-amp-需求" class="headerlink" title="思路 &amp; 需求"></a>思路 &amp; 需求</h2><p>参考IntAccumulatorParam的实现思路（上述文章中有讲）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    // addInPlace有很多具体的实现类</span><br><span class="line">    // 如果想要实现自定义的话，就得实现这个方法</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>自定义也可以通过这个方法去实现，从而兼容我们自定义的累加器</p><h2 id="需求：这里实现一个简单的案例，用分布式的方法去实现随机数"><a href="#需求：这里实现一个简单的案例，用分布式的方法去实现随机数" class="headerlink" title="需求：这里实现一个简单的案例，用分布式的方法去实现随机数"></a>需求：这里实现一个简单的案例，用分布式的方法去实现随机数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">**</span><br><span class="line">  * 自定义的AccumulatorParam</span><br><span class="line">  *</span><br><span class="line">  * Created by lemon on 2018/7/28.</span><br><span class="line">  */</span><br><span class="line">object UniqueKeyAccumulator extends AccumulatorParam[Map[Int, Int]] &#123;</span><br><span class="line">  override def addInPlace(r1: Map[Int, Int], r2: Map[Int, Int]): Map[Int, Int] = &#123;</span><br><span class="line">      // ++用于两个集合相加</span><br><span class="line">      r1++r2</span><br><span class="line">    &#125;</span><br><span class="line">    override def zero(initialValue: Map[Int, Int]): Map[Int, Int] = &#123;</span><br><span class="line">      var data: Map[Int, Int] = Map()</span><br><span class="line">      data</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">  * 使用自定义的累加器，实现随机数</span><br><span class="line">  *</span><br><span class="line">  * Created by lemon on 2018/7/28.</span><br><span class="line">  */</span><br><span class="line">object CustomAccumulator &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;CustomAccumulator&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    val uniqueKeyAccumulator = sc.accumulable(Map[Int, Int]())(UniqueKeyAccumulator)</span><br><span class="line">    val distData = sc.parallelize(1 to 10)</span><br><span class="line">    val mapCount = distData.map(x =&gt; &#123;</span><br><span class="line">      val randomNum = new Random().nextInt(20)</span><br><span class="line">      // 构造一个k-v对</span><br><span class="line">      val map: Map[Int, Int] = Map[Int, Int](randomNum -&gt; randomNum)</span><br><span class="line">      uniqueKeyAccumulator += map</span><br><span class="line">    &#125;)</span><br><span class="line">    println(mapCount.count())</span><br><span class="line">    // 获取到累加器的值 中的key值，并进行打印</span><br><span class="line">    uniqueKeyAccumulator.value.keys.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图：<br><img src="/assets/blogImg/Spark累加器简单案例.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker常用命令以及安装mysql</title>
      <link href="/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/"/>
      <url>/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>Docker是一个开源的应用容器引擎；是一个轻量级容器技术；</p><p>Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；</p><p>运行中的这个镜像称为容器，容器启动是非常快速的。<br><a id="more"></a></p><h3 id="2-核心概念"><a href="#2-核心概念" class="headerlink" title="2.核心概念"></a>2.核心概念</h3><p>docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；</p><p>docker客户端(Client)：连接docker主机进行操作；</p><p>docker仓库(Registry)：用来保存各种打包好的软件镜像；</p><p>docker镜像(Images)：软件打包好的镜像；放在docker仓库中；</p><p>docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用</p><h3 id="3-安装环境"><a href="#3-安装环境" class="headerlink" title="3.安装环境"></a>3.安装环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VM ware Workstation10</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">uname -r</span><br><span class="line">3.10.0-862.el7.x86_64</span><br></pre></td></tr></table></figure><p><strong>检查内核版本，必须是3.10及以上</strong> 查看命令：uname -r</p><h3 id="4-在linux虚拟机上安装docker"><a href="#4-在linux虚拟机上安装docker" class="headerlink" title="4.在linux虚拟机上安装docker"></a>4.在linux虚拟机上安装docker</h3><p>步骤：</p><p>1、检查内核版本，必须是3.10及以上<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -r</span><br></pre></td></tr></table></figure><p></p><p>2、安装docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker</span><br></pre></td></tr></table></figure><p></p><p>3、输入y确认安装<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Dependency Updated:</span><br><span class="line">  audit.x86_64 0:2.8.1-3.el7_5.1                                  audit-libs.x86_64 0:2.8.1-3.el7_5.1                                 </span><br><span class="line"></span><br><span class="line">Complete!</span><br><span class="line">(成功标志)</span><br></pre></td></tr></table></figure><p></p><p>4、启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl start docker</span><br><span class="line">[root@hadoop000 ~]# docker -v</span><br><span class="line">Docker version 1.13.1, build 8633870/1.13.1</span><br></pre></td></tr></table></figure><p></p><p>5、开机启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><p></p><p>6、停止docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl stop docker</span><br></pre></td></tr></table></figure><p></p><h3 id="5-常用命令"><a href="#5-常用命令" class="headerlink" title="5.常用命令"></a>5.常用命令</h3><p>镜像操作</p><table><thead><tr><th>操作</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>检索</td><td>docker search 关键字 eg：docker search redis</td><td>我们经常去docker hub上检索镜像的详细信息，如镜像的TAG</td></tr><tr><td>拉取</td><td>docker pull 镜像名:tag</td><td>:tag是可选的，tag表示标签，多为软件的版本，默认是latest</td></tr><tr><td>列表</td><td>docker images</td><td>查看所有本地镜像</td></tr><tr><td>删除</td><td>docker rmi image-id</td><td>删除指定的本地镜像</td></tr></tbody></table><p>当然大家也可以在官网查找：<a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a></p><p>容器操作<br>软件镜像（QQ安装程序）—-运行镜像—-产生一个容器（正在运行的软件，运行的QQ）；</p><p>步骤：</p><ul><li>1、搜索镜像<br>[root@localhost ~]# docker search tomcat</li><li>2、拉取镜像<br>[root@localhost ~]# docker pull tomcat</li><li>3、根据镜像启动容器<br>docker run –name mytomcat -d tomcat:latest</li><li>4、docker ps<br>查看运行中的容器</li><li>5、 停止运行中的容器<br>docker stop 容器的id</li><li>6、查看所有的容器<br>docker ps -a</li><li>7、启动容器<br>docker start 容器id</li><li>8、删除一个容器<br>docker rm 容器id</li><li><p>9、启动一个做了端口映射的tomcat<br>[root@localhost ~]# docker run -d -p 8888:8080 tomcat<br>-d：后台运行<br>-p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口</p></li><li><p>10、为了演示简单关闭了linux的防火墙<br>service firewalld status ；查看防火墙状态<br>service firewalld stop：关闭防火墙<br>systemctl disable firewalld.service #禁止firewall开机启动</p></li><li>11、查看容器的日志<br>docker logs container-name/container-id</li></ul><p>更多命令参看<br><a href="https://docs.docker.com/engine/reference/commandline/docker/" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/docker/</a><br>可以参考镜像文档</p><h3 id="6-使用docker安装mysql"><a href="#6-使用docker安装mysql" class="headerlink" title="6.使用docker安装mysql"></a>6.使用docker安装mysql</h3><ul><li>docker pull mysql</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">docker pull mysql </span><br><span class="line">Using default tag: latest</span><br><span class="line">Trying to pull repository docker.io/library/mysql ... </span><br><span class="line">latest: Pulling from docker.io/library/mysql</span><br><span class="line">a5a6f2f73cd8: Pull complete </span><br><span class="line">936836019e67: Pull complete </span><br><span class="line">283fa4c95fb4: Pull complete </span><br><span class="line">1f212fb371f9: Pull complete </span><br><span class="line">e2ae0d063e89: Pull complete </span><br><span class="line">5ed0ae805b65: Pull complete </span><br><span class="line">0283dc49ef4e: Pull complete </span><br><span class="line">a7e1170b4fdb: Pull complete </span><br><span class="line">88918a9e4742: Pull complete </span><br><span class="line">241282fa67c2: Pull complete </span><br><span class="line">b0fecf619210: Pull complete </span><br><span class="line">bebf9f901dcc: Pull complete </span><br><span class="line">Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8</span><br><span class="line">Status: Downloaded newer image for docker.io/mysql:latest</span><br><span class="line">[root@hadoop000 ~]# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/mysql     latest              f991c20cb508        10 days ago         486 MB</span><br></pre></td></tr></table></figure><ul><li>启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/mysql     latest              f991c20cb508        10 days ago         486 MB</span><br><span class="line">[root@hadoop000 ~]# docker run --name mysql01 -d mysql</span><br><span class="line">756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a</span><br><span class="line">[root@hadoop000 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS               NAMES</span><br><span class="line">756620c8e583        mysql               &quot;docker-entrypoint...&quot;   9 seconds ago       Exited (1) 7 seconds ago                       mysql01</span><br><span class="line">[root@hadoop000 ~]# docker logs 756620c8e583</span><br><span class="line">error: database is uninitialized and password option is not specified </span><br><span class="line">  You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD</span><br></pre></td></tr></table></figure><p>可以看到上面启动的方式是错误的，提示我们要带上具体的密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysql</span><br><span class="line">eae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc06</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">–name：给新创建的容器命名，此处命名为pwc-mysql</span><br><span class="line">-e：配置信息，此处配置mysql的root用户的登陆密码</span><br><span class="line">-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口</span><br><span class="line">-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...</span><br></pre></td></tr></table></figure><ul><li>查看是否启动成功</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES</span><br><span class="line">eae86796e132        mysql               &quot;docker-entrypoint...&quot;   8 minutes ago       Up 8 minutes        0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql02</span><br></pre></td></tr></table></figure><ul><li>登陆MySQL</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql04 /bin/bash</span><br><span class="line">root@e34aba02c0c3:/# mysql -uroot -p123456 </span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 80</span><br><span class="line">Server version: 8.0.13 MySQL Community Server - GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><ul><li>其他的高级操作<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br><span class="line">把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面</span><br><span class="line">改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）</span><br><span class="line"></span><br><span class="line">docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br><span class="line">指定mysql的一些配置参数</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据课程一览</title>
      <link href="/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/"/>
      <url>/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h1 id="若泽数据课程系列"><a href="#若泽数据课程系列" class="headerlink" title="若泽数据课程系列"></a>若泽数据课程系列</h1><h2 id="基础班"><a href="#基础班" class="headerlink" title="基础班"></a>基础班</h2><h3 id="Liunx"><a href="#Liunx" class="headerlink" title="Liunx"></a>Liunx</h3><ul><li>VM虚拟机安装</li><li>Liunx常用命令（重点）</li><li>开发环境搭</li></ul><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ul><li>源码安装&amp;yum安装</li><li>CRUD编写</li><li>权限控制</li></ul><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>架构介绍&amp;&amp;源码编译</li><li>伪分布式安装&amp;&amp;企业应用</li><li><p>HDFS（重点）</p><ul><li>架构设计</li><li>副本放置策略</li><li>读写流程</li></ul></li><li><p>YARN（重点）</p><ul><li>架构设计</li><li>工作流程</li><li>调度管理&amp;&amp;常见参数配置（调优）</li></ul></li><li><p>MapReduce</p><ul><li>架构设计</li><li>wordcount原理&amp;&amp;join原理和案例<a id="more"></a><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3></li></ul></li><li><p>架构设计</p></li><li>Hive DDL&amp;DML</li><li>join在大数据中的使用</li><li>使用自带UDF和开发自定义UDF</li></ul><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><ul><li>架构设计</li><li>RDBMS导入导出</li></ul><h3 id="整合项目将所有组件合作使用。"><a href="#整合项目将所有组件合作使用。" class="headerlink" title="整合项目将所有组件合作使用。"></a>整合项目将所有组件合作使用。</h3><h3 id="人工智能基础"><a href="#人工智能基础" class="headerlink" title="人工智能基础"></a>人工智能基础</h3><ul><li>python基础</li><li>常用库——pandas、numpy、sklearn、keras</li></ul><h2 id="高级班"><a href="#高级班" class="headerlink" title="高级班"></a>高级班</h2><h3 id="scala编程（重点）"><a href="#scala编程（重点）" class="headerlink" title="scala编程（重点）"></a>scala编程（重点）</h3><h3 id="Spark（五星重点）"><a href="#Spark（五星重点）" class="headerlink" title="Spark（五星重点）"></a>Spark（五星重点）</h3><h3 id="Hadoop高级"><a href="#Hadoop高级" class="headerlink" title="Hadoop高级"></a>Hadoop高级</h3><h3 id="Hive高级"><a href="#Hive高级" class="headerlink" title="Hive高级"></a>Hive高级</h3><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h3 id="调度平台"><a href="#调度平台" class="headerlink" title="调度平台"></a>调度平台</h3><h2 id="线下班"><a href="#线下班" class="headerlink" title="线下班"></a>线下班</h2><p><img src="/assets/blogImg/若泽数据.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kudu与Spark 生产最佳实践</title>
      <link href="/2019/05/07/Kudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/05/07/Kudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> &lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">    &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;</span><br><span class="line">    &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object KuduApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">     //Read a table from Kudu</span><br><span class="line">    val df = spark.read</span><br><span class="line">          .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;))</span><br><span class="line">          .format(&quot;kudu&quot;).load</span><br><span class="line">        df.schema.printTreeString()</span><br><span class="line">//    // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">//    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    // The schema is encoded in a string</span><br><span class="line">//    val schemalString=&quot;id,age,name&quot;</span><br><span class="line">//</span><br><span class="line">//    // Generate the schema based on the string of schema</span><br><span class="line">//    val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))</span><br><span class="line">//    val schema=StructType(fields)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    val KuduTable = kuduContext.createTable(</span><br><span class="line">//     &quot;test_table&quot;, schema, Seq(&quot;id&quot;),</span><br><span class="line">//     new CreateTableOptions()</span><br><span class="line">//       .setNumReplicas(1)</span><br><span class="line">//       .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema</span><br><span class="line">//</span><br><span class="line">//    val  id  = KuduTable.getColumn(&quot;id&quot;)</span><br><span class="line">//    print(id)</span><br><span class="line">//</span><br><span class="line">//    kuduContext.tableExists(&quot;test_table&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现象:通过spark sql 操作报如下错误:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)</span><br><span class="line">    at cn.zhangyu.KuduApp$.main(KuduApp.scala:18)</span><br><span class="line">    at cn.zhangyu.KuduApp.main(KuduApp.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try.orElse(Try.scala:84)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure><p>而通过KuduContext是可以操作的没有报错,代码为上面注解部分</p><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>查询kudu官网:<a href="https://kudu.apache.org/docs/developing.html" target="_blank" rel="noopener">https://kudu.apache.org/docs/developing.html</a></p><p>官网中说出了版本的问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果将Spark 2与Scala 2.11一起使用，请使用kudu-spark2_2.11工件。</span><br><span class="line">kudu-spark版本1.8.0及更低版本的语法略有不同。有关有效示例，请参阅您的版本的文档。可以在发布页面上找到版本化文档。</span><br><span class="line">spark-shell --packages org.apache.kudu:kudu-spark2_2.11:1.9.0</span><br><span class="line">看到了 官网使用的是1.9.0的版本.</span><br></pre></td></tr></table></figure><font size="3" color="red">但是但是但是</font><p>官网下面说到了下面几个集成问题:</p><ol><li><font size="3" color="red"><b>Spark 2.2+在运行时需要Java 8，即使Kudu Spark 2.x集成与Java 7兼容。Spark 2.2是Kudu 1.5.0的默认依赖版本。</b></font></li><li>当注册为临时表时，必须为名称包含大写或非ascii字符的Kudu表分配备用名称。</li><li>包含大写或非ascii字符的列名的Kudu表不能与SparkSQL一起使用。可以在Kudu中重命名列以解决此问题。</li><li>&lt;&gt;并且OR谓词不会被推送到Kudu，而是由Spark任务进行评估。只有LIKE带有后缀通配符的谓词才会被推送到Kudu，这意味着它LIKE “FOO%”被推下但LIKE “FOO%BAR”不是。</li><li>Kudu不支持Spark SQL支持的每种类型。例如， Date不支持复杂类型。</li><li>Kudu表只能在SparkSQL中注册为临时表。使用HiveContext可能无法查询Kudu表。</li></ol><font size="3"><b><br>那就很奇怪了我用的1.5.0版本报错为:找不到类,数据源有问题<br><br>但是把kudu改成1.9.0 问题解决<br></b></font><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = false)</span><br><span class="line"> |-- age: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Spark集成最佳实践"><a href="#Spark集成最佳实践" class="headerlink" title="Spark集成最佳实践"></a>Spark集成最佳实践</h3><p>每个群集避免多个Kudu客户端。</p><p>一个常见的Kudu-Spark编码错误是实例化额外的KuduClient对象。在kudu-spark中，a KuduClient属于KuduContext。Spark应用程序代码不应创建另一个KuduClient连接到同一群集。相反，应用程序代码应使用KuduContext访问KuduClient使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KuduContext#syncClient。</span><br><span class="line">  // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">    val list = kuduContext.syncClient.getTablesList.getTablesList</span><br><span class="line">    if (list.iterator().hasNext)&#123;</span><br><span class="line">      print(list.iterator().next())</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>要诊断KuduClientSpark作业中的多个实例，请查看主服务器的日志中的符号，这些符号会被来自不同客户端的许多GetTableLocations或 GetTabletLocations请求过载，通常大约在同一时间。这种症状特别适用于Spark Streaming代码，其中创建KuduClient每个任务将导致来自新客户端的主请求的周期性波。</p><h3 id="Spark操作kudu-Scala-demo"><a href="#Spark操作kudu-Scala-demo" class="headerlink" title="Spark操作kudu(Scala demo)"></a>Spark操作kudu(Scala demo)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">package cn.zhangyu</span><br><span class="line">import org.apache.kudu.spark.kudu._</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;</span><br><span class="line">import org.slf4j.LoggerFactory</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object SparkTest &#123;</span><br><span class="line">  //kuduMasters and tableName</span><br><span class="line">  val kuduMasters = &quot;192.168.13.130:7051&quot;</span><br><span class="line">  val tableName = &quot;kudu_spark_table&quot;</span><br><span class="line">  //table column</span><br><span class="line">  val idCol = &quot;id&quot;</span><br><span class="line">  val ageCol = &quot;age&quot;</span><br><span class="line">  val nameCol = &quot;name&quot;</span><br><span class="line">  //replication</span><br><span class="line">  val tableNumReplicas = Integer.getInteger(&quot;tableNumReplicas&quot;, 1)</span><br><span class="line">  val logger = LoggerFactory.getLogger(SparkTest.getClass)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //create SparkSession</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    //create kuduContext</span><br><span class="line">    val kuduContext = new KuduContext(kuduMasters,spark.sparkContext)</span><br><span class="line">    //schema</span><br><span class="line">    val schema = StructType(</span><br><span class="line">      List(</span><br><span class="line">        StructField(idCol, IntegerType, false),</span><br><span class="line">        StructField(nameCol, StringType, false),</span><br><span class="line">        StructField(ageCol,StringType,false)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    var tableIsCreated = false</span><br><span class="line">    try&#123;</span><br><span class="line">    // Make sure the table does not exist</span><br><span class="line">    if (kuduContext.tableExists(tableName)) &#123;</span><br><span class="line">      throw new RuntimeException(tableName + &quot;: table already exists&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    //create</span><br><span class="line">    kuduContext.createTable(tableName, schema, Seq(idCol),</span><br><span class="line">      new CreateTableOptions()</span><br><span class="line">        .addHashPartitions(List(idCol).asJava, 3)</span><br><span class="line">        .setNumReplicas(tableNumReplicas))</span><br><span class="line">    tableIsCreated = true</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    //write</span><br><span class="line">    logger.info(s&quot;writing to table &apos;$tableName&apos;&quot;)</span><br><span class="line">        val data = Array(Person(1,&quot;12&quot;,&quot;zhangsan&quot;),Person(2,&quot;20&quot;,&quot;lisi&quot;),Person(3,&quot;30&quot;,&quot;wangwu&quot;))</span><br><span class="line">        val personRDD = spark.sparkContext.parallelize(data)</span><br><span class="line">        val personDF = personRDD.toDF()</span><br><span class="line">        kuduContext.insertRows(personDF,tableName)</span><br><span class="line">    //useing SparkSQL read table</span><br><span class="line">    val sqlDF = spark.sqlContext.read</span><br><span class="line">      .options(Map(&quot;kudu.master&quot; -&gt; kuduMasters, &quot;kudu.table&quot; -&gt; tableName))</span><br><span class="line">      .format(&quot;kudu&quot;).kudu</span><br><span class="line">    sqlDF.createOrReplaceTempView(tableName)</span><br><span class="line">    spark.sqlContext.sql(s&quot;SELECT * FROM $tableName &quot;).show</span><br><span class="line">    //upsert some rows</span><br><span class="line">    val upsertPerson = Array(Person(1,&quot;10&quot;,&quot;jack&quot;))</span><br><span class="line">    val upsertPersonRDD = spark.sparkContext.parallelize(upsertPerson)</span><br><span class="line">    val upsertPersonDF = upsertPersonRDD.toDF()</span><br><span class="line">    kuduContext.updateRows(upsertPersonDF,tableName)</span><br><span class="line">    //useing RDD read table</span><br><span class="line">    val readCols = Seq(idCol,ageCol,nameCol)</span><br><span class="line">    val readRDD = kuduContext.kuduRDD(spark.sparkContext, tableName, readCols)</span><br><span class="line">    val userTuple = readRDD.map &#123; case Row( id: Int,age: String,name: String) =&gt; (id,age,name) &#125;</span><br><span class="line">    println(&quot;count:&quot;+userTuple.count())</span><br><span class="line">    userTuple.collect().foreach(println(_))</span><br><span class="line">    //delete table</span><br><span class="line">    kuduContext.deleteTable(tableName)    </span><br><span class="line">    &#125;catch &#123;</span><br><span class="line">      // Catch, log and re-throw. Not the best practice, but this is a very</span><br><span class="line">      // simplistic example.</span><br><span class="line">      case unknown : Throwable =&gt; logger.error(s&quot;got an exception: &quot; + unknown)</span><br><span class="line">        throw unknown</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      // Clean up.</span><br><span class="line">      if (tableIsCreated) &#123;</span><br><span class="line">        logger.info(s&quot;deleting table &apos;$tableName&apos;&quot;)</span><br><span class="line">        kuduContext.deleteTable(tableName)</span><br><span class="line">      &#125;</span><br><span class="line">      logger.info(s&quot;closing down the session&quot;)</span><br><span class="line">      spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">case class Person(id: Int,age: String,name: String)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> Kudu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019五一-线下项目第13期圆满结束</title>
      <link href="/2019/05/05/2019%E4%BA%94%E4%B8%80-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC13%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/"/>
      <url>/2019/05/05/2019%E4%BA%94%E4%B8%80-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC13%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><center><br>2019年五一，3天2夜北京线下班<br><br>圆满结束<br><br>一句话，北京温度适宜<br><br>小伙伴们来自<br><b><br>北京、上海、深圳<br></b><br><br><br>大家为了一个真实目标<br><br>学习真正企业级大数据生产项目<br><br><br><font color="blue">2个生产项目+3个Topic分享</font><br><br>一年我们只在节假日举办<br><br>错过了就是错过了<br><br>期待@端午节线下项目班第14期<br></center><p><img src="/assets/blogImg/2019-05-05-1.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-05-05-2.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-05-05-3.png" alt></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 线下实战班 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线下实战班 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:连续2周若泽数据第7-12名学员喜捷offer(含蚂蚁金服)</title>
      <link href="/2019/04/29/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD2%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC7-12%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E8%9A%82%E8%9A%81%E9%87%91%E6%9C%8D)/"/>
      <url>/2019/04/29/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD2%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC7-12%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E8%9A%82%E8%9A%81%E9%87%91%E6%9C%8D)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第7个小伙伴，一口气拿了【蚂蚁金服、pulsar开源软件公司、新浪、两家外资】<font color="red">5个offer</font></b></font><p><img src="/assets/blogImg/2019-04-29-1.png" alt="就业1"></p><p><img src="/assets/blogImg/2019-04-29-2.png" alt="就业2"></p><font color="#00CD00" size="4"><b>2.第8个小伙伴，杭州，28K</b></font><p><img src="/assets/blogImg/2019-04-29-3.png" alt="就业3"></p><font color="#00CD00" size="4"><b>3.第9个小伙伴，福建，24K</b></font><p><img src="/assets/blogImg/2019-04-29-4.png" alt="就业4"></p><font color="#00CD00" size="4"><b>4.第10个小伙伴，成都，18K</b></font><p><img src="/assets/blogImg/2019-04-29-5.png" alt="就业5"></p><font color="#00CD00" size="4"><b>5.第11个小伙伴，北京，16K(真正零基础，跨行，之前连Linux不会)</b></font><p><img src="/assets/blogImg/2019-04-29-6.png" alt="就业6"></p><font color="#00CD00" size="4"><b>6.第12个小伙伴，上海，16K(真正零基础)</b></font><p><img src="/assets/blogImg/2019-04-29-7.png" alt="就业7"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之二</title>
      <link href="/2019/04/26/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C/"/>
      <url>/2019/04/26/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font size="4"><b>Driver端</b></font><ol><li>Driver端初始化构建Accumulator并初始化，同时完成了Accumulator注册，Accumulators.register(this)时Accumulator会在序列化后发送到Executor端</li><li>Driver接收到ResultTask完成的状态更新后，会去更新Value的值 然后在Action操作执行后就可以获取到Accumulator的值了</li></ol><font size="4"><b>Executor端</b></font><ol><li>Executor端接收到Task之后会进行反序列化操作，反序列化得到RDD和function。同时在反序列化的同时也去反序列化Accumulator(在readObject方法中完成)，同时也会向TaskContext完成注册</li><li>完成任务计算之后，随着Task结果一起返回给Driver<a id="more"></a></li></ol><h2 id="结合源码分析"><a href="#结合源码分析" class="headerlink" title="结合源码分析"></a>结合源码分析</h2><font size="4"><b>Driver端初始化</b></font><p>&ensp;&ensp;Driver端主要经过以下步骤，完成初始化操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val accum = sparkContext.accumulator(0, “AccumulatorTest”)</span><br><span class="line">val acc = new Accumulator(initialValue, param, Some(name))</span><br><span class="line">Accumulators.register(this)</span><br></pre></td></tr></table></figure><font size="4"><b>Executor端反序列化得到Accumulator</b></font><p>&ensp;&ensp;反序列化是在调用ResultTask的runTask方式时候做的操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 会反序列化出来RDD和自己定义的function</span><br><span class="line">val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](</span><br><span class="line">   ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;在反序列化的过程中，会调用Accumulable中的readObject方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123;</span><br><span class="line">    in.defaultReadObject()</span><br><span class="line">    // value的初始值为zero；该值是会被序列化的</span><br><span class="line">    value_ = zero</span><br><span class="line">    deserialized = true</span><br><span class="line">    // Automatically register the accumulator when it is deserialized with the task closure.</span><br><span class="line">    //</span><br><span class="line">    // Note internal accumulators sent with task are deserialized before the TaskContext is created</span><br><span class="line">    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL</span><br><span class="line">    // metrics, still need to register here.</span><br><span class="line">    val taskContext = TaskContext.get()</span><br><span class="line">    if (taskContext != null) &#123;</span><br><span class="line">      // 当前反序列化所得到的对象会被注册到TaskContext中</span><br><span class="line">      // 这样TaskContext就可以获取到累加器</span><br><span class="line">      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor</span><br><span class="line">      taskContext.registerAccumulator(this)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><font size="4"><b>注意</b></font><p><strong>Accumulable.scala中的value_，是不会被序列化的，@transient关键词修饰了</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@volatile @transient private var value_ : R = initialValue // Current value on master</span><br></pre></td></tr></table></figure><h2 id="累加器在各个节点的累加操作"><a href="#累加器在各个节点的累加操作" class="headerlink" title="累加器在各个节点的累加操作"></a>累加器在各个节点的累加操作</h2><p>针对传入function中不同的操作，对应有不同的调用方法，以下列举几种（在Accumulator.scala中）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def += (term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;</span><br><span class="line">def add(term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;</span><br><span class="line">def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;</span><br></pre></td></tr></table></figure><p>根据不同的累加器参数，有不同实现的AccumulableParam（在Accumulator.scala中）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulableParam[R, T] extends Serializable &#123;</span><br><span class="line">  /**</span><br><span class="line">  def addAccumulator(r: R, t: T): R</span><br><span class="line">  def addInPlace(r1: R, r2: R): R</span><br><span class="line">  def zero(initialValue: R): R</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不同的实现如下图所示：</p><p><img src="/assets/blogImg/累加器在各个节点的操作.png" alt="enter description here"></p><p>以IntAccumulatorParam为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123;</span><br><span class="line">  def addInPlace(t1: Int, t2: Int): Int = t1 + t2</span><br><span class="line">  def zero(initialValue: Int): Int = 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现IntAccumulatorParam实现的是trait AccumulatorParam[T]：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在各个节点上的累加操作完成之后，就会紧跟着返回更新之后的Accumulators的value_值</p><h2 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h2><p>在Task.scala中的run方法，会执行如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 返回累加器，并运行task</span><br><span class="line">// 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map</span><br><span class="line">(runTask(context), context.collectAccumulators())</span><br></pre></td></tr></table></figure><p>在Executor端已经完成了一系列操作，需要将它们的值返回到Driver端进行聚合汇总，整个顺序如图累加器执行流程：</p><p><img src="/assets/blogImg/累加器执行流程图.png" alt="enter description here"></p><p>根据执行流程，我们可以发现，在执行完collectAccumulators方法之后，最终会在DAGScheduler中调用updateAccumulators(event)，而在该方法中会调用Accumulators的add方法，从而完成聚合操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def add(values: Map[Long, Any]): Unit = synchronized &#123;</span><br><span class="line">  // 遍历传进来的值</span><br><span class="line">  for ((id, value) &lt;- values) &#123;</span><br><span class="line">    if (originals.contains(id)) &#123;</span><br><span class="line">      // Since we are now storing weak references, we must check whether the underlying data</span><br><span class="line">      // is valid.</span><br><span class="line">      // 根据id从注册的Map中取出对应的累加器</span><br><span class="line">      originals(id).get match &#123;</span><br><span class="line">        // 将值给累加起来，最终将结果加到value里面</span><br><span class="line">       // ++=是被重载了</span><br><span class="line">        case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value</span><br><span class="line">        case None =&gt;</span><br><span class="line">          throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="获取累加器的值"><a href="#获取累加器的值" class="headerlink" title="获取累加器的值"></a>获取累加器的值</h2><p>通过accum.value方法可以获取到累加器的值</p><font size="3"><b>至此，累加器执行完毕。</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark2.4.2详细介绍</title>
      <link href="/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Spark发布了最新的版本spark-2.4.2<br>根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的</p><h4 id="版本介绍"><a href="#版本介绍" class="headerlink" title="版本介绍"></a>版本介绍</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。<font color="#FF4500"> <strong>我们强烈建议所有2.4用户升级到此稳定版本。</strong></font><br><a id="more"></a></p><h4 id="显著的变化"><a href="#显著的变化" class="headerlink" title="显著的变化"></a>显著的变化</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。</li><li>还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。</li></ul><h4 id="详细更改"><a href="#详细更改" class="headerlink" title="详细更改"></a>详细更改</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>在Spark Driver中发现Java死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>使用选项logConf = true时密码将以conf的明文形式记录</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>用Snappy 1.1.7.1解压、压缩空序列化数据时失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferTo中的潜在损坏</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointData因文件系统已缓存而无法清理</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>在仅使用空值列的AggregateEstimation之后的错误outputRows估计</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>修复包名称不匹配</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>修复updateTableStats以使用新统计信息或无更新表统计信息</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1静默删除DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>松开在ExpressionInfo的’examples’字段中换行断言条件</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>将jquery更新为1.12.x以获取安全修复程序</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>隐藏“org.apache.spark.util.kvstore”的API文档</td></tr></tbody></table><h6 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:上周若泽数据6名学员喜捷offer(含腾讯)</title>
      <link href="/2019/04/22/%E6%8D%B7%E6%8A%A5_%E4%B8%8A%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE6%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E8%85%BE%E8%AE%AF)/"/>
      <url>/2019/04/22/%E6%8D%B7%E6%8A%A5_%E4%B8%8A%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE6%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E8%85%BE%E8%AE%AF)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.腾讯offer 3.2W*16(往届学长)</b></font><p><img src="/assets/blogImg/2019-04-22-1.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-04-22-2.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.第二个学员18K*13</b></font><p><img src="/assets/blogImg/2019-04-22-3.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>3.第三个学员15K*14</b></font><p><img src="/assets/blogImg/2019-04-22-4.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>4.第四个学员21K</b></font><br><img src="/assets/blogImg/2019-04-22-5.png" alt="enter description here"><br><br><br><font color="#00CD00" size="4"><b>5.第5个学员也是18K*15</b></font><p><img src="/assets/blogImg/2019-04-22-6.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>6.第6个小伙伴是16K北京的</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之一</title>
      <link href="/2019/04/19/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80/"/>
      <url>/2019/04/19/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p><strong>由于最近在项目中需要用到Spark的累加器，同时需要自己去自定义实现Spark的累加器，从而满足生产上的需求。对此，对Spark的累加器实现机制进行了追踪学习。</strong></p><p>本系列文章，将从以下几个方面入手，对Spark累加器进行剖析：</p><ol><li>Spark累加器的基本概念</li><li>累加器的重点类构成</li><li>累加器的源码解析</li><li>累加器的执行过程</li><li>累加器使用中的坑</li><li>自定义累加器的实现<a id="more"></a><h2 id="Spark累加器基本概念"><a href="#Spark累加器基本概念" class="headerlink" title="Spark累加器基本概念"></a>Spark累加器基本概念</h2></li></ol><p>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，只能累加，不能减少累加器只能在Driver端构建，并只能从Driver端读取结果，在Task端只能进行累加。</p><p>至于这里为什么只能在Task累加呢？下面的内容将会进行详细的介绍，先简单介绍下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Task节点，准确的就是说在executor上；</span><br><span class="line">每个Task都会有一个累加器的变量，被序列化传输到executor端运行之后再返回过来都是独立运行的；</span><br><span class="line">如果在Task端去获取值的话，只能获取到当前Task的，Task与Task之间不会有影响</span><br></pre></td></tr></table></figure><p>累加器不会改变Spark lazy计算的特点，只会在Job触发的时候进行相关的累加操作</p><p>现有累加器类型:</p><p><img src="/assets/blogImg/Spark累加器类型_1.png" alt="enter description here"></p><h2 id="累加器的重点类介绍"><a href="#累加器的重点类介绍" class="headerlink" title="累加器的重点类介绍"></a>累加器的重点类介绍</h2><font size="4"><b>class Accumulator extends Accumulable</b></font><p>源码（源码中已经对这个类的作用做了十分详细的解释）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * A simpler value of [[Accumulable]] where the result type being accumulated is the same</span><br><span class="line"> * as the types of elements being merged, i.e. variables that are only &quot;added&quot; to through an</span><br><span class="line"> * associative operation and can therefore be efficiently supported in parallel. They can be used</span><br><span class="line"> * to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric</span><br><span class="line"> * value types, and programmers can add support for new types.</span><br><span class="line"> *</span><br><span class="line"> * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].</span><br><span class="line"> * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.</span><br><span class="line"> * However, they cannot read its value. Only the driver program can read the accumulator&apos;s value,</span><br><span class="line"> * using its value method.</span><br><span class="line"> *</span><br><span class="line"> * @param initialValue initial value of accumulator</span><br><span class="line"> * @param param helper object defining how to add elements of type `T`</span><br><span class="line"> * @tparam T result type</span><br><span class="line"> */</span><br><span class="line">class Accumulator[T] private[spark] (</span><br><span class="line">    @transient private[spark] val initialValue: T,</span><br><span class="line">    param: AccumulatorParam[T],</span><br><span class="line">    name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Accumulable[T, T](initialValue, param, name, internal) &#123;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123;</span><br><span class="line">    this(initialValue, param, name, false)</span><br><span class="line">  &#125;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T]) = &#123;</span><br><span class="line">    this(initialValue, param, None, false)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>主要实现了累加器的初始化及封装了相关的累加器操作方法同时在类对象构建的时候向Accumulators注册累加器累加器的add操作的返回值类型和传入进去的值类型可以不一样所以一定要定义好两步操作（即add方法）：累加操作/合并操作</code></pre><font size="4"><b>object Accumulators</b></font><pre><code>该方法在Driver端管理着累加器，也包含了累加器的聚合操作</code></pre><font size="4"><b>trait AccumulatorParam[T] extends AccumulableParam[T, T]</b></font><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add</span><br><span class="line"> * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be</span><br><span class="line"> * available when you create Accumulators of a specific type.</span><br><span class="line"> *</span><br><span class="line"> * @tparam T type of value to accumulate</span><br><span class="line"> */</span><br><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>AccumulatorParam的addAccumulator操作的泛型封装具体的实现还是需要在具体实现类里面实现addInPlace方法自定义实现累加器的关键</code></pre><font size="4"><b>object AccumulatorParam</b></font><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object AccumulatorParam &#123;</span><br><span class="line">  // The following implicit objects were in SparkContext before 1.2 and users had to</span><br><span class="line">  // `import SparkContext._` to enable them. Now we move them here to make the compiler find</span><br><span class="line">  // them automatically. However, as there are duplicate codes in SparkContext for backward</span><br><span class="line">  // compatibility, please update them accordingly if you modify the following implicit objects.</span><br><span class="line">  implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] &#123;</span><br><span class="line">    def addInPlace(t1: Double, t2: Double): Double = t1 + t2</span><br><span class="line">    def zero(initialValue: Double): Double = 0.0</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123;</span><br><span class="line">    def addInPlace(t1: Int, t2: Int): Int = t1 + t2</span><br><span class="line">    def zero(initialValue: Int): Int = 0</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object LongAccumulatorParam extends AccumulatorParam[Long] &#123;</span><br><span class="line">    def addInPlace(t1: Long, t2: Long): Long = t1 + t2</span><br><span class="line">    def zero(initialValue: Long): Long = 0L</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object FloatAccumulatorParam extends AccumulatorParam[Float] &#123;</span><br><span class="line">    def addInPlace(t1: Float, t2: Float): Float = t1 + t2</span><br><span class="line">    def zero(initialValue: Float): Float = 0f</span><br><span class="line">  &#125;</span><br><span class="line">  // TODO: Add AccumulatorParams for other types, e.g. lists and strings</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>从源码中大量的implicit关键词，可以发现该类主要进行隐式类型转换的操作</code></pre><font size="4"><b>TaskContextImpl</b></font><pre><code>在Executor端管理着我们的累加器，累加器是通过该类进行返回的</code></pre><h2 id="累加器的源码解析"><a href="#累加器的源码解析" class="headerlink" title="累加器的源码解析"></a>累加器的源码解析</h2><font size="4"><b>Driver端</b></font><p>&ensp;&ensp;<font size="3"><b>accumulator方法</b></font></p><p>以下列这段代码中的accumulator方法为入口点，进入到相应的源码中去</p><p><code>val acc = new Accumulator(initialValue, param, Some(name))</code></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Accumulator[T] private[spark] (</span><br><span class="line">    @transient private[spark] val initialValue: T,</span><br><span class="line">    param: AccumulatorParam[T],</span><br><span class="line">    name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Accumulable[T, T](initialValue, param, name, internal) &#123;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123;</span><br><span class="line">    this(initialValue, param, name, false)</span><br><span class="line">  &#125;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T]) = &#123;</span><br><span class="line">    this(initialValue, param, None, false)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>继承的Accumulable[T, T]</b></font></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Accumulable[R, T] private[spark] (</span><br><span class="line">    initialValue: R,</span><br><span class="line">    param: AccumulableParam[R, T],</span><br><span class="line">    val name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Serializable &#123;</span><br><span class="line">…</span><br><span class="line">// 这里的_value并不支持序列化</span><br><span class="line">// 注：有@transient的都不会被序列化</span><br><span class="line">@volatile @transient private var value_ : R = initialValue // Current value on master</span><br><span class="line">  …</span><br><span class="line">  // 注册了当前的累加器</span><br><span class="line">  Accumulators.register(this)</span><br><span class="line">  …,</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators.register()</b></font></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 传入参数，注册累加器</span><br><span class="line">def register(a: Accumulable[_, _]): Unit = synchronized &#123;</span><br><span class="line">// 构造成WeakReference</span><br><span class="line">originals(a.id) = new WeakReference[Accumulable[_, _]](a)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font size="3"><b>至此，Driver端的初始化已经完成</b></font> <font size="4"><b>Executor端</b></font><pre><code>Executor端的反序列化是一个得到我们的对象的过程初始化是在反序列化的时候就完成的，同时反序列化的时候还完成了Accumulator向TaskContextImpl的注册</code></pre><p>&ensp;&ensp;<font size="3"><b>TaskRunner中的run方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// 在计算的过程中，会将RDD和function经过序列化之后传给Executor端</span><br><span class="line">private[spark] class Executor(</span><br><span class="line">    executorId: String,</span><br><span class="line">    executorHostname: String,</span><br><span class="line">    env: SparkEnv,</span><br><span class="line">    userClassPath: Seq[URL] = Nil,</span><br><span class="line">    isLocal: Boolean = false)</span><br><span class="line">  extends Logging &#123;</span><br><span class="line">...</span><br><span class="line">  class TaskRunner(</span><br><span class="line">      execBackend: ExecutorBackend,</span><br><span class="line">      val taskId: Long,</span><br><span class="line">      val attemptNumber: Int,</span><br><span class="line">      taskName: String,</span><br><span class="line">      serializedTask: ByteBuffer)</span><br><span class="line">    extends Runnable &#123;</span><br><span class="line">…</span><br><span class="line">override def run(): Unit = &#123;</span><br><span class="line">    …</span><br><span class="line">val (value, accumUpdates) = try &#123;</span><br><span class="line">         // 调用TaskRunner中的task.run方法，触发task的运行</span><br><span class="line">         val res = task.run(</span><br><span class="line">           taskAttemptId = taskId,</span><br><span class="line">           attemptNumber = attemptNumber,</span><br><span class="line">           metricsSystem = env.metricsSystem)</span><br><span class="line">         threwException = false</span><br><span class="line">         res</span><br><span class="line">       &#125; finally &#123;</span><br><span class="line">        …</span><br><span class="line">       &#125;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Task中的collectAccumulators()方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private[spark] abstract class Task[T](</span><br><span class="line">final def run(</span><br><span class="line">    taskAttemptId: Long,</span><br><span class="line">    attemptNumber: Int,</span><br><span class="line">    metricsSystem: MetricsSystem)</span><br><span class="line">  : (T, AccumulatorUpdates) = &#123;</span><br><span class="line">  …</span><br><span class="line">    try &#123;</span><br><span class="line">      // 返回累加器，并运行task</span><br><span class="line">      // 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map</span><br><span class="line">      (runTask(context), context.collectAccumulators())</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">  …</span><br><span class="line"> &#125;</span><br><span class="line"> …</span><br><span class="line"> &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>ResultTask中的runTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">override def runTask(context: TaskContext): U = &#123;</span><br><span class="line">  // Deserialize the RDD and the func using the broadcast variables.</span><br><span class="line">  val deserializeStartTime = System.currentTimeMillis()</span><br><span class="line">  val ser = SparkEnv.get.closureSerializer.newInstance()</span><br><span class="line">  // 反序列化是在调用ResultTask的runTask方法的时候做的</span><br><span class="line">  // 会反序列化出来RDD和自己定义的function</span><br><span class="line">  val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](</span><br><span class="line">    ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br><span class="line">  _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime</span><br><span class="line">  metrics = Some(context.taskMetrics)</span><br><span class="line">  func(context, rdd.iterator(partition, context))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulable中的readObject方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 在反序列化的过程中会调用Accumulable.readObject方法</span><br><span class="line">  // Called by Java when deserializing an object</span><br><span class="line">  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123;</span><br><span class="line">    in.defaultReadObject()</span><br><span class="line">    // value的初始值为zero；该值是会被序列化的</span><br><span class="line">    value_ = zero</span><br><span class="line">    deserialized = true</span><br><span class="line">    // Automatically register the accumulator when it is deserialized with the task closure.</span><br><span class="line">    //</span><br><span class="line">    // Note internal accumulators sent with task are deserialized before the TaskContext is created</span><br><span class="line">    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL</span><br><span class="line">    // metrics, still need to register here.</span><br><span class="line">    val taskContext = TaskContext.get()</span><br><span class="line">    if (taskContext != null) &#123;</span><br><span class="line">      // 当前反序列化所得到的对象会被注册到TaskContext中</span><br><span class="line">      // 这样TaskContext就可以获取到累加器</span><br><span class="line">      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor</span><br><span class="line">      taskContext.registerAccumulator(this)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Executor.scala</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 在executor端拿到accumuUpdates值之后，会去构造一个DirectTaskResult</span><br><span class="line">val directResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.orNull)</span><br><span class="line">val serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">val resultSize = serializedDirectResult.limit</span><br><span class="line">…</span><br><span class="line">// 最终由ExecutorBackend的statusUpdate方法发送至Driver端</span><br><span class="line">// ExecutorBackend为一个Trait，有多种实现</span><br><span class="line">execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>CoarseGrainedExecutorBackend中的statusUpdate方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 通过ExecutorBackend的一个实现类：CoarseGrainedExecutorBackend 中的statusUpdate方法</span><br><span class="line">// 将数据发送至Driver端</span><br><span class="line">override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) &#123;</span><br><span class="line">    val msg = StatusUpdate(executorId, taskId, state, data)</span><br><span class="line">    driver match &#123;</span><br><span class="line">      case Some(driverRef) =&gt; driverRef.send(msg)</span><br><span class="line">      case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>CoarseGrainedSchedulerBackend中的receive方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// Driver端在接收到消息之后，会调用CoarseGrainedSchedulerBackend中的receive方法</span><br><span class="line">override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">      case StatusUpdate(executorId, taskId, state, data) =&gt;</span><br><span class="line">        // 会在DAGScheduler的handleTaskCompletion方法中将结果返回</span><br><span class="line">        scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskSchedulerImpl的statusUpdate方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) &#123;</span><br><span class="line">  …</span><br><span class="line">            if (state == TaskState.FINISHED) &#123;</span><br><span class="line">              taskSet.removeRunningTask(tid)</span><br><span class="line">              // 将成功的Task入队</span><br><span class="line">              taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)</span><br><span class="line">            &#125; else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) &#123;</span><br><span class="line">              taskSet.removeRunningTask(tid)</span><br><span class="line">              taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)</span><br><span class="line">            &#125;</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskResultGetter的enqueueSuccessfulTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) &#123;</span><br><span class="line">…</span><br><span class="line">          result.metrics.setResultSize(size)</span><br><span class="line">          scheduler.handleSuccessfulTask(taskSetManager, tid, result)</span><br><span class="line">…</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskSchedulerImpl的handleSuccessfulTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def handleSuccessfulTask(</span><br><span class="line">      taskSetManager: TaskSetManager,</span><br><span class="line">      tid: Long,</span><br><span class="line">      taskResult: DirectTaskResult[_]): Unit = synchronized &#123;</span><br><span class="line">    taskSetManager.handleSuccessfulTask(tid, taskResult)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的taskEnded方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def taskEnded(</span><br><span class="line">     task: Task[_],</span><br><span class="line">     reason: TaskEndReason,</span><br><span class="line">     result: Any,</span><br><span class="line">     accumUpdates: Map[Long, Any],</span><br><span class="line">     taskInfo: TaskInfo,</span><br><span class="line">     taskMetrics: TaskMetrics): Unit = &#123;</span><br><span class="line"> eventProcessLoop.post(</span><br><span class="line">     // 给自身的消息循环体发了个CompletionEvent</span><br><span class="line">     // 这个CompletionEvent会被handleTaskCompletion方法所接收到</span><br><span class="line">     CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的handleTaskCompletion方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// 与上述CoarseGrainedSchedulerBackend中的receive方法章节对应</span><br><span class="line">// 在handleTaskCompletion方法中，接收CompletionEvent</span><br><span class="line">// 不论是ResultTask还是ShuffleMapTask都会去调用updateAccumulators方法，更新累加器的值</span><br><span class="line">private[scheduler] def handleTaskCompletion(event: CompletionEvent) &#123;</span><br><span class="line">    …</span><br><span class="line">    event.reason match &#123;</span><br><span class="line">      case Success =&gt;</span><br><span class="line">        listenerBus.post(SparkListenerTaskEnd(stageId, stage.latestInfo.attemptId, taskType,</span><br><span class="line">          event.reason, event.taskInfo, event.taskMetrics))</span><br><span class="line">        stage.pendingPartitions -= task.partitionId</span><br><span class="line">        task match &#123;</span><br><span class="line">          case rt: ResultTask[_, _] =&gt;</span><br><span class="line">            // Cast to ResultStage here because it&apos;s part of the ResultTask</span><br><span class="line">            // TODO Refactor this out to a function that accepts a ResultStage</span><br><span class="line">            val resultStage = stage.asInstanceOf[ResultStage]</span><br><span class="line">            resultStage.activeJob match &#123;</span><br><span class="line">              case Some(job) =&gt;</span><br><span class="line">                if (!job.finished(rt.outputId)) &#123;</span><br><span class="line">                  updateAccumulators(event)</span><br><span class="line">          case smt: ShuffleMapTask =&gt;</span><br><span class="line">            val shuffleStage = stage.asInstanceOf[ShuffleMapStage]</span><br><span class="line">            updateAccumulators(event)</span><br><span class="line">&#125;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的updateAccumulators方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private def updateAccumulators(event: CompletionEvent): Unit = &#123;</span><br><span class="line">   val task = event.task</span><br><span class="line">   val stage = stageIdToStage(task.stageId)</span><br><span class="line">   if (event.accumUpdates != null) &#123;</span><br><span class="line">     try &#123;</span><br><span class="line">       // 调用了累加器的add方法</span><br><span class="line">       Accumulators.add(event.accumUpdates)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的add方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def add(values: Map[Long, Any]): Unit = synchronized &#123;</span><br><span class="line">    // 遍历传进来的值</span><br><span class="line">    for ((id, value) &lt;- values) &#123;</span><br><span class="line">      if (originals.contains(id)) &#123;</span><br><span class="line">        // Since we are now storing weak references, we must check whether the underlying data</span><br><span class="line">        // is valid.</span><br><span class="line">        // 根据id从注册的Map中取出对应的累加器</span><br><span class="line">        originals(id).get match &#123;</span><br><span class="line">          // 将值给累加起来，最终将结果加到value里面</span><br><span class="line">          // ++=是被重载了</span><br><span class="line">          case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value</span><br><span class="line">          case None =&gt;</span><br><span class="line">            throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的++=方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的value方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def value: R = &#123;</span><br><span class="line">   if (!deserialized) &#123;</span><br><span class="line">     value_</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     throw new UnsupportedOperationException(&quot;Can&apos;t read accumulator value in task&quot;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><font size="4"><b>此时我们的应用程序就可以通过 .value 的方式去获取计数器的值了</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark2.4.0如何Debug源代码</title>
      <link href="/2019/04/17/%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81/"/>
      <url>/2019/04/17/%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="源码获取与编译"><a href="#源码获取与编译" class="headerlink" title="源码获取与编译"></a>源码获取与编译</h3><ol><li>直接从Spark官网获取源码或者从GitHub获取<br><img src="/assets/blogImg/2019-04-17-1.png" alt="enter description here"></li></ol><p><img src="/assets/blogImg/2019-04-17-2.png" alt="enter description here"></p><p>下载源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0.tgz</span><br></pre></td></tr></table></figure><p>解压源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf spark-2.4.0.tgz</span><br></pre></td></tr></table></figure><a id="more"></a><ol start="2"><li>Spark源码编译<br>此处不再啰嗦，直接去腾讯课堂，搜索“若泽大数据”即可找到编译视频。</li></ol><h3 id="源码导入IDEA"><a href="#源码导入IDEA" class="headerlink" title="源码导入IDEA"></a>源码导入IDEA</h3><p><img src="/assets/blogImg/2019-04-17-3.png" alt="enter description here"></p><h3 id="运行hive-thriftserver2"><a href="#运行hive-thriftserver2" class="headerlink" title="运行hive-thriftserver2"></a>运行hive-thriftserver2</h3><p>从spark-2.4.0-bin-2.6.0-cdh5.7.0/sbin/start-thriftserver.sh 脚本中找到 hive-thriftserver2 的入口类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.hive.thriftserver.HiveThriftServer2</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-4.png" alt="enter description here"></p><h3 id="配置运行环境"><a href="#配置运行环境" class="headerlink" title="配置运行环境"></a>配置运行环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Menu -&gt; Run -&gt; Edit Configurations -&gt; 选择 + -&gt; Application</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-5.png" alt="enter description here"></p><p>-Dspark.master=local[2] 代表使用本地模式运行Spark代码</p><p>运行之前需要做一件很重要的事情，将 hive-thriftserver 这个子项目的pom依赖全部由provided改为compile：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;</span><br><span class="line">    &lt;scope&gt;compile&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt;</span><br><span class="line">    &lt;scope&gt;compile&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="添加运行依赖的jars"><a href="#添加运行依赖的jars" class="headerlink" title="添加运行依赖的jars"></a>添加运行依赖的jars</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Menu -&gt; File -&gt; Project Structure -&gt; Modules -&gt; spark-hive-thriftserver_2.11 -&gt; Dependencies 添加依赖 jars -&gt; &#123;Spark_home&#125;/assembly/target/scala-2.11/jars/</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-6.png" alt="enter description here"></p><h3 id="中间遇到的问题"><a href="#中间遇到的问题" class="headerlink" title="中间遇到的问题"></a>中间遇到的问题</h3><p>问题一</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark\sql\hive-thriftserver\src\main\java\org\apache\hive\service\cli\thrift\ThriftCLIService.java</span><br><span class="line"></span><br><span class="line">Error:(52, 75) not found: value TCLIService</span><br><span class="line"></span><br><span class="line">public abstract class ThriftCLIService extends AbstractService implements TCLIService.Iface, Runnable &#123;………..</span><br></pre></td></tr></table></figure><p>解决办法： 在spark\sql\hive-thriftserver\src\gen\java右键中点Mark Directory as -&gt; Sources Root即可</p><p>问题二</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal  </span><br><span class="line">    at java.lang.ClassLoader.defineClass1(Native Method)</span><br></pre></td></tr></table></figure><p>解决办法：在 hive-thriftserve 子项目的pom文件中添加依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;xml-apis&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;xml-apis&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.4.01&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>问题三</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.BindException: Cannot assign requested address: Service &apos;sparkDriver&apos; failed after 16 retries (starting from 0)! Consider explicitly setting the appropriate port for the service &apos;sparkDriver&apos; (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.</span><br></pre></td></tr></table></figure><p>解决办法： 在 /etc/hosts 文件中配置相应的地址映射。</p><h3 id="成功运行"><a href="#成功运行" class="headerlink" title="成功运行"></a>成功运行</h3><p>在 HiveThriftServer2 中打断点进行调试源码即可。</p><p>打一个断点如下所示：<br><img src="/assets/blogImg/2019-04-17-7.png" alt="enter description here"><br>就能看到断点所打印出来的信息。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之三 UnifiedMemoryManager分析</title>
      <link href="/2019/04/16/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90/"/>
      <url>/2019/04/16/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="acquireExecutionMemory方法"><a href="#acquireExecutionMemory方法" class="headerlink" title="acquireExecutionMemory方法"></a>acquireExecutionMemory方法</h3><p>UnifiedMemoryManager中的accquireExecutionMemory方法：</p><p><img src="/assets/blogImg/2019-04-16内存管理1.png" alt="enter description here"></p><p>当前的任务尝试从executor中<font size="3"><strong>获取numBytes这么大的内存</strong></font></p><p>该方法直接向ExecutionMemoryPool索要所需内存，索要内存有以下几个关注点：</p><ul><li>当ExecutionMemory 内存充足，则不会触发向Storage申请内存</li><li>每个Task能够被使用的内存是被限制的</li><li>索要内存的大小</li></ul><p>我们通过源码来进行分析<br><a id="more"></a><br><strong><code>UnifiedMemoryManager.scala中</code></strong><br><img src="/assets/blogImg/2019-04-16内存管理2.png" alt="enter description here"></p><p>我们点进去后会发现，会调用ExecutionMemoryPool.acquireMemory()方法</p><p><strong><code>ExecutionMemoryPool.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理3.png" alt="enter description here"></p><p>我们可以发现每Task能够被使用的内存被限制在：</p><p>poolSize / (2 * numActiveTasks) ~ maxPoolSize / numActiveTasks 之间</p><p>val maxMemoryPerTask = maxPoolSize /numActiveTasks</p><p>val minMemoryPerTask = poolSize / (2 * numActiveTasks)</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理4.png" alt="enter description here"></p><p>其中maxPoolSize = maxMemory - math.min(storageMemoryUsed, storageRegionSize)</p><p>maxMemory = storage + execution的最大内存</p><p>poolSize = 当前这个pool的大小</p><p>maxPoolSize = execution pool的最大内存</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理5.png" alt="enter description here"></p><p>从上述代码中我们可以知道索要内存的大小：</p><p>val memoryReclaimableFromStorage=math.max(storageMemoryPool.memoryFree, storageMemoryPool.poolSize -storageRegionSize)</p><p>取决于StorageMemoryPool的剩余内存和 storageMemoryPool 从ExecutionMemory借来的内存哪个大，取最大的那个，作为可以重新归还的最大内存</p><p>用公式表达出来就是这一个样子：</p><p>ExecutionMemory 能借到的最大内存 = StorageMemory 借的内存 + StorageMemory 空闲内存</p><p><strong>注意：</strong>如果实际需要的小于能够借到的最大值，则以实际需要值为准</p><p>能回收的内存大小为：</p><p>val spaceToReclaim =storageMemoryPool.freeSpaceToShrinkPool ( math.min(extraMemoryNeeded,memoryReclaimableFromStorage))</p><h3 id="ExecutionMemoryPool-acquireMemory-解析"><a href="#ExecutionMemoryPool-acquireMemory-解析" class="headerlink" title="ExecutionMemoryPool.acquireMemory()解析"></a>ExecutionMemoryPool.acquireMemory()解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">while (true) &#123;</span><br><span class="line">  val numActiveTasks = memoryForTask.keys.size</span><br><span class="line">  val curMem = memoryForTask(taskAttemptId)</span><br><span class="line">  maybeGrowPool(numBytes - memoryFree)</span><br><span class="line">  val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">  val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">  val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br><span class="line">  val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem))</span><br><span class="line">  val toGrant = math.min(maxToGrant, memoryFree)</span><br><span class="line">  if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;</span><br><span class="line">    logInfo(s&quot;TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free&quot;)</span><br><span class="line">    lock.wait()</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    memoryForTask(taskAttemptId) += toGrant</span><br><span class="line">    return toGrant</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>整体流程解析：</p><p>程序一直处理该task的请求，直到系统判定无法满足该请求或者已经为该请求分配到足够的内存为止；如果当前execution内存池剩余内存不足以满足此次请求时，会向storage部分请求释放出被借走的内存以满足此次请求</p><p>根据此刻execution内存池的总大小maxPoolSize，以及从memoryForTask中统计出的处于active状态的task的个数计算出：</p><p>每个task能够得到的最大内存数 maxMemoryPerTask = maxPoolSize / numActiveTasks</p><p>每个task能够得到的最少内存数 minMemoryPerTask = poolSize /(2 * numActiveTasks)</p><p>根据申请内存的task当前使用的execution内存大小决定分配给该task多少内存，总的内存不能超过maxMemoryPerTask；但是如果execution内存池能够分配的最大内存小于numBytes，并且如果把能够分配的内存分配给当前task，但是该task最终得到的execution内存还是小于minMemoryPerTask时，该task进入等待状态，等其他task申请内存时再将其唤醒，唤醒之后如果此时满足，就会返回能够分配的内存数，并且更新memoryForTask，将该task使用的内存调整为分配后的值</p><font size="3"><strong>一个Task最少需要minMemoryPerTask才能开始执行</strong></font><h3 id="acquireStorageMemory方法"><a href="#acquireStorageMemory方法" class="headerlink" title="acquireStorageMemory方法"></a>acquireStorageMemory方法</h3><p>流程和acquireExecutionMemory类似，当storage的内存不足时，同样会向execution借内存，但区别是当且仅当ExecutionMemory有空闲内存时，StorageMemory 才能借走该内存</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理6.png" alt="enter description here"></p><p>从上述代码中我们可以知道能借到的内存数为：</p><p>val memoryBorrowedFromExecution = Math.min(onHeapExecutionMemoryPool.memoryFree,numBytes)</p><p>所以StorageMemory从ExecutionMemory借走的内存，完全取决于当时ExecutionMemory是不是有空闲内存；借到内存后，storageMemoryPool增加借到的这部分内存，之后同上一样，会调用StorageMemoryPool的acquireMemory()方法</p><h3 id="StorageMemoryPool-acquireMemory"><a href="#StorageMemoryPool-acquireMemory" class="headerlink" title="StorageMemoryPool.acquireMemory"></a>StorageMemoryPool.acquireMemory</h3><p><img src="/assets/blogImg/2019-04-16内存管理7.png" alt="enter description here"></p><p>整体流程解析：</p><p>在申请内存时，如果numBytes大于此刻storage内存池的剩余内存，即if (numBytesToFree &gt; 0)，那么需要storage内存池释放一部分内存以满足申请需求</p><p><strong>注意：</strong>这里的numBytesToFree可以理解为numBytes大小减去Storage内存池剩余大小，大于0，即所需要申请的numBytes大于Storage内存池剩余的内存</p><p>释放内存后如果memoryFree &gt;= numBytes，就会把这部分内存分配给申请内存的task，并且更新storage内存池的使用情况</p><p>同时StorageMemoryPool与ExecutionMemoryPool不同的是，他不会像前者那样分不到资源就进行等待，acquireStorageMemory只会返回一个true或是false，告知内存分配是否成功</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:线下班学员年薪35W的offer及面试题</title>
      <link href="/2019/04/11/%E6%8D%B7%E6%8A%A5_%E7%BA%BF%E4%B8%8B%E7%8F%AD%E5%AD%A6%E5%91%98%E5%B9%B4%E8%96%AA35W%E7%9A%84offer%E5%8F%8A%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/04/11/%E6%8D%B7%E6%8A%A5_%E7%BA%BF%E4%B8%8B%E7%8F%AD%E5%AD%A6%E5%91%98%E5%B9%B4%E8%96%AA35W%E7%9A%84offer%E5%8F%8A%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了）<br></font><font color="red" size="4"><br>直接看线下班学员的offer（3天2个）及刚出炉的面试题，难吗？<br></font><a id="more"></a> <font color="#00CD00" size="4"><b>1.第一个学员</b></font><p>线下班结束后第一天面试，年薪&lt;font color=”red”size=4&gt;<strong>35W</strong>的offer</p><p><img src="/assets/blogImg/2019-04-11-1.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.第二个学员</b></font><p>线下班结束后第三天面试，&lt;font color=”red”size=4&gt;<strong>20.5K*15=30.75W</strong>的offer</p><p><img src="/assets/blogImg/2019-04-11-2.png" alt="enter description here"></p><p><font color="#00CD00" size="4"><b>3.面试题，刚出炉的，你会吗？</b></font><br><img src="/assets/blogImg/2019-04-11-3.png" alt><br><img src="/assets/blogImg/2019-04-11-4.png" alt="enter description here"><br><img src="/assets/blogImg/2019-04-11-5.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之二 统一内存管理及设计理念</title>
      <link href="/2019/04/10/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/"/>
      <url>/2019/04/10/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>Spark 1.6之后引入的统一内存管理机制，与静态内存管理的区别在于Storage和Execution共享同一块内存空间，可以动态占用对方的空闲区域</p><p><img src="/assets/blogImg/2019-04-10-内存管理1.png" alt="enter description here"></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><ul><li>设定基本的Storage内存和Execution内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘，若己方空间不足而对方空余时，可借用对方的空间（存储空间不足是指不足以放下一个完整的 Block）</li><li>Execution的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li><li>Storage的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle过程中的很多因素，实现起来较为复杂<a id="more"></a><h3 id="动态内存占用机制"><a href="#动态内存占用机制" class="headerlink" title="动态内存占用机制"></a>动态内存占用机制</h3></li></ul><p>动态占用机制如下图所示：</p><p><img src="/assets/blogImg/2019-04-10-内存管理2.png" alt="enter description here"></p><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧</p><p>譬如：如果Storage的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理</p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>如下图所示，相较于静态内存管理，引入了动态占用机制</p><p><img src="/assets/blogImg/2019-04-10-内存管理3.png" alt="enter description here"></p><h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>spark从1.6版本以后，默认的内存管理方式就调整为统一内存管理模式</p><p>由UnifiedMemoryManager实现</p><p>Unified MemoryManagement模型，重点是打破运行内存和存储内存之间的界限，使spark在运行时，不同用途的内存之间可以实现互相的拆借</p><h3 id="Reserved-Memory"><a href="#Reserved-Memory" class="headerlink" title="Reserved Memory"></a>Reserved Memory</h3><p>这部分内存是预留给系统使用,在1.6.1默认为300MB，这一部分内存不计算在Execution和Storage中；可通过spark.testing.reservedMemory进行设置；然后把实际可用内存减去这个reservedMemor得到usableMemory</p><p>ExecutionMemory 和 StorageMemory 会共享usableMemory * spark.memory.fraction(默认0.75)</p><font color="red" size="3"><b>注意：</b></font><ul><li>在Spark 1.6.1 中spark.memory.fraction默认为0.75</li><li>在Spark 2.2.0 中spark.memory.fraction默认为0.6</li></ul><h3 id="User-Memory"><a href="#User-Memory" class="headerlink" title="User Memory"></a>User Memory</h3><p>分配Spark Memory剩余的内存，用户可以根据需要使用</p><p>在Spark 1.6.1中，默认占(Java Heap - Reserved Memory) * 0.25</p><p>在Spark 2.2.0中，默认占(Java Heap - Reserved Memory) * 0.4</p><h3 id="Spark-Memory"><a href="#Spark-Memory" class="headerlink" title="Spark Memory"></a>Spark Memory</h3><p>计算方式为：<code>(Java Heap – ReservedMemory) * spark.memory.fraction</code></p><p>在Spark 1.6.1中，默认为(Java Heap - 300M) * 0.75</p><p>在Spark 2.2.0中，默认为(Java Heap - 300M) * 0.6</p><p>Spark Memory又分为Storage Memory和Execution Memory两部分</p><p>两个边界由spark.memory.storageFraction设定，默认为0.5</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>相对于静态内存模型（即Storage和Execution相互隔离、彼此不可拆借），动态内存实现了存储和计算内存的动态拆借：</p><ul><li>当计算内存超了，它会从空闲的存储内存中借一部分内存使用</li><li>存储内存不够用的时候，也会向空闲的计算内存中拆借</li></ul><p>值得注意的地方是：</p><ul><li>被借走用来执行运算的内存，在执行完任务之前是不会释放内存的</li><li>通俗的讲，运行任务会借存储的内存，但是它直到执行完以后才能归还内存</li></ul><h3 id="和动态内存相关的参数"><a href="#和动态内存相关的参数" class="headerlink" title="和动态内存相关的参数"></a>和动态内存相关的参数</h3><ul><li><p>spark.memory.fraction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Spark 1.6.1 默认0.75，Spark 2.2.0 默认0.6 </span><br><span class="line">这个参数用来配置存储和计算内存占整个可用内存的比例 </span><br><span class="line">这个参数设置的越低，也就是存储和计算内存占可用的比例越低，就越可能频繁的发生内存的释放（将内存中的数据写磁盘或者直接丢弃掉） </span><br><span class="line">反之，如果这个参数越高，发生释放内存的可能性就越小 </span><br><span class="line">这个参数的目的是在jvm中留下一部分空间用来保存spark内部数据，用户数据结构，并且防止对数据的错误预估可能造成OOM的风险，这就是Other部分</span><br></pre></td></tr></table></figure></li><li><p>spark.memory.storageFraction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认 0.5；在统一内存中存储内存所占的比例，默认是0.5，如果使用的存储内存超过了这个范围，缓存的数据会被驱赶</span><br></pre></td></tr></table></figure></li><li><p>spark.memory.useLegacyMode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">默认false；设置是否使用saprk1.5及以前遗留的内存管理模型，即静态内存模型，前面的文章介绍过这个，主要是设置以下几个参数：</span><br><span class="line">spark.storage.memoryFraction</span><br><span class="line">spark.storage.safetyFraction</span><br><span class="line">spark.storage.unrollFraction</span><br><span class="line">spark.shuffle.memoryFraction</span><br><span class="line">spark.shuffle.safetyFraction</span><br></pre></td></tr></table></figure></li></ul><h3 id="动态内存设计中的取舍"><a href="#动态内存设计中的取舍" class="headerlink" title="动态内存设计中的取舍"></a>动态内存设计中的取舍</h3><p>因为内存可以被Execution和Storage拆借，我们必须明确在这种机制下，当内存压力上升的时候，该如何进行取舍？</p><p>从三个角度进行分析：</p><ul><li>倾向于优先释放计算内存</li><li>倾向于优先释放存储内存</li><li>不偏不倚，平等竞争</li></ul><h4 id="释放内存的代价"><a href="#释放内存的代价" class="headerlink" title="释放内存的代价"></a>释放内存的代价</h4><p>释放存储内存的代价取决于Storage Level.：</p><ul><li>如果数据的存储level是MEMORY_ONLY的话代价最高，因为当你释放在内存中的数据的时候，你下次再复用的话只能重新计算了</li><li>如果数据的存储level是MEMORY_AND_DIS_SER的时候，释放内存的代价最低，因为这种方式，当内存不够的时候，它会将数据序列化后放在磁盘上，避免复用的时候再计算，唯一的开销在I/O</li></ul><p><strong>综述：</strong></p><p>释放计算内存的代价不是很显而易见：</p><ul><li>这里没有复用数据重计算的代价，因为计算内存中的任务数据会被移到硬盘，最后再归并起来（后面会有文章介绍到这点）</li><li>最近的spark版本将计算的中间数据进行压缩使得序列化的代价降到了最低</li></ul><p>值得注意的是：</p><ul><li>移到硬盘的数据总会再重新读回来</li><li>从存储内存移除的数据也许不会被用到，所以当没有重新计算的风险时，释放计算的内存要比释放存储内存的代价更高（假使计算内存部分刚好用于计算任务的时候）</li></ul><h4 id="实现复杂度"><a href="#实现复杂度" class="headerlink" title="实现复杂度"></a>实现复杂度</h4><ul><li>实现释放存储内存的策略很简单：我们只需要用目前的内存释放策略释放掉存储内存中的数据就好了</li><li>实现释放计算内存却相对来说很复杂</li></ul><p>这里有2个释放计算内存的思路：</p><ul><li>当运行任务要拆借存储内存的时候，给所有这些任务注册一个回调函数以便日后调这个函数来回收内存</li><li>协同投票来进行内存的释放</li></ul><p>值得我们注意的一个地方是，以上无论哪种方式，都需要考虑一种特殊情况：</p><ul><li>即如果我要释放正在运行的计算任务的内存，同时我们想要cache到存储内存的一部分数据恰巧是由这个计算任务产生的</li><li>此时，如果我们现在释放掉正在运行的任务的计算内存，就需要考虑在这种环境下会造成的饥饿情况：即生成cache的数据的计算任务没有足够的内存空间来跑出cache的数据，而一直处于饥饿状态（因为计算内存已经不够了，再释放计算内存更加不可取）</li><li><p>此外，我们还需要考虑：一旦我们释放掉计算内存，那么那些需要cache的数据应该怎么办？有2种方案：</p><ol><li>最简单的方式就是等待，直到计算内存有足够的空闲，但是这样就可能会造成死锁，尤其是当新的数据块依赖于之前的计算内存中的数据块的时候</li><li>另一个可选的操作就是丢掉那些最新的正准备写入到磁盘中的块并且一旦当计算内存够了又马上加载回来。为了避免总是丢掉那些等待中的块，我们可以设置一个小的内存空间(比如堆内存的5%)去确保内存中至少有一定的比例的的数据块</li></ol></li></ul><p><strong>综述：</strong></p><p>所给的两种方法都会增加额外的复杂度，这两种方式在第一次的实现中都被排除了</p><p>综上目前看来，释放掉存储内存中的计算任务在实现上比较繁琐，目前暂不考虑</p><p>即计算内存借了存储内存用来计算任务，然后释放，这种不考虑；计算内存借来内存之后，是可以不还的</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>我们倾向于<font color="red">优先释放掉存储内存</font></p><p>即如果存储内存拆借了计算内存，当计算内存需要进行计算并且内存空间不足的时候，<font color="red">优先把计算内存中这部分被用来存储的内存释放掉</font></p><h3 id="可选设计"><a href="#可选设计" class="headerlink" title="可选设计"></a>可选设计</h3><h4 id="1-设计方案"><a href="#1-设计方案" class="headerlink" title="1.设计方案"></a><font size="4"><b>1.设计方案</b></font></h4><p>结合我们前面的描述，针对在内存压力下释放存储内存有以下几个可选设计：</p><ul><li><p>设计1：释放存储内存数据块，完全平滑</p><p>计算和存储内存共享一片统一的区域，没有进行统一的划分</p><ol><li>内存压力上升，优先释放掉存储内存部分中的数据</li><li>如果压力没有缓解，开始将计算内存中运行的任务数据进行溢写磁盘</li></ol></li><li><p>设计2：释放存储内存数据块，静态存储空间预留，存储空间的大小是定死的</p><p>这种设计和1设计很像，不同的是会专门划分一个预留存储内存区域：在这个内存区域内，存储内存不会被释放，只有当存储内存超出这个预留区域，才会被释放（即超过50%了就被释放，当然50%为默认值）。这个参数由spark.memory.storageFraction（默认值为0.5，即计算和存储内存的分割线）配置</p></li><li><p>设计3：释放存储内存数据块，动态存储空间预留</p><p>这种设计于设计2很相似，但是存储空间的那一部分区域不再是静态设置的了，而是动态分配；这样设置带来的不同是计算内存可以尽可能借走存储内存中可用的部分，因为存储内存是动态分配的</p></li></ul><p><strong>结论：最终采用的的是设计3</strong></p><h4 id="2-各个方案的优劣"><a href="#2-各个方案的优劣" class="headerlink" title="2.各个方案的优劣"></a><font size="4"><b>2.各个方案的优劣</b></font></h4><ul><li><p>设计1被拒绝的原因</p><p>设计1不适合那些对cache内存重度依赖的saprk任务，因为设计1中只要内存压力上升就释放存储内存</p></li><li><p>设计2被拒绝的原因</p><p>设计2在很多情况下需要用户去设置存储内存中那部分最小的区域<br>另外无论我们设置一个具体值，只要它非0，那么计算内存最终也会达到一个上限，比如，如果我们将存储内存设置为0.6，那么有效的执行内存就是：</p><ul><li>Spark 1.6.1 可用内存0.40.75</li><li><p>Spark 2.2.0 可用内存0.40.6</p><p>那么如果用户没有cache数据，或是cache的数据达不到设置的0.6，那么这种情况就又回到了静态内存模型那种情况，并没有改善什么</p></li></ul></li><li><p>最终选择设计3的原因</p><p>设计3就避免了2中的问题只要存储内存有空余的情况，那么计算内存就可以借用</p><p>需要关注的问题是：</p><ul><li>当计算内存已经使用了存储内存中的所有可用内存但是又需要cache数据的时候应该怎么处理</li><li>最早的版本中直接释放最新的block来避免引入执行驱赶策略（eviction策略，上述章节中有介绍）的复杂性</li></ul></li></ul><font size="4"><b>设计3是唯一一个同时满足下列条件的：</b></font><ol><li>存储内存没有上限</li><li>计算内存没有上限</li><li>保障了存储空间有一个小的保留区域</li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019清明-线下项目第12期圆满结束</title>
      <link href="/2019/04/09/2019%E6%B8%85%E6%98%8E-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC12%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/"/>
      <url>/2019/04/09/2019%E6%B8%85%E6%98%8E-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC12%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><center><br>2019年清明3天<br><br>一句话，上海温度适宜<br><br>小伙伴们来自五湖四海<br><b><br>北京、成都、深圳、广州<br><br>杭州、山东、齐齐哈尔<br></b><br><br>大家为了一个真实目标<br><br>学习真正企业级大数据生产项目<br><br><font color="blue">2个生产项目+3个Topic分享</font><br><br>一年我们只在节假日举办<br><br>清明3天+2夜，错过了就是错过了<br><br>期待@端午节线下项目班第13期<br></center><p><img src="/assets/blogImg/2019-04-09-1.png" alt="enter description here"><br><img src="/assets/blogImg/2019-04-09-2.png" alt="enter description here"><br><img src="/assets/blogImg/2019-04-09-3.png" alt="enter description here"><br><img src="/assets/blogImg/2019-04-09-4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 线下实战班 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线下实战班 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之一 静态内存管理</title>
      <link href="/2019/04/03/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2019/04/03/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="Spark内存管理简介"><a href="#Spark内存管理简介" class="headerlink" title="Spark内存管理简介"></a>Spark内存管理简介</h3><p>Spark从<font color="blue"><strong>1.6</strong></font>开始引入了动态内存管理模式，即执行内存和存储内存之间可以相互抢占</p><p>Spark提供了2种内存分配模式：</p><ul><li>静态内存管理</li><li>统一内存管理</li></ul><p>本系列文章将分别对这两种内存管理模式的优缺点以及设计原理进行分析（主要基于Spark 1.6.1的内存管理进行分析）</p><font color="blue">在本篇文章中，将先对<font size="3" color="red">静态内存管理</font>进行介绍</font><a id="more"></a><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>在Spark最初采用的静态内存管理机制下，存储内存、执行内存和其它内存的大小在Spark应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置，堆内内存的分配如下图所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理1.png" alt="enter description here"></p><p>默认情况下，spark内存管理采用unified模式，如果要开启静态内存管理模式，需要将spark.memory.useLegacyMode参数调为true（默认为false），1.6.1版本的官网配置如下所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理2.png" alt="enter description here"></p><p>将参数调整为true之后，就会进入到静态内存管理中来，可以通过SparkEnv.scala中发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理3.png" alt="enter description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果spark.memory.useLegacyMode为true，就进入到StaticMemoryManager（静态内存管理）；</span><br><span class="line">如果为false，就进入到UnifiedMemoryManager（统一内存管理）；</span><br><span class="line">同时我们可以发现该参数的默认值为false，即默认情况下就会调用统一内存管理类。</span><br></pre></td></tr></table></figure><h3 id="Execution内存"><a href="#Execution内存" class="headerlink" title="Execution内存"></a>Execution内存</h3><p>####可用的Execution内存</p><p>用于shuffle操作的内存，取决于join、sort、aggregation等过程频繁的IO需要的Buffer临时数据存储</p><p>简单来说，spark在shuffle write的过程中，每个executor会将数据写到该executor的物理磁盘上，下一个stage的task会去上一个stage拉取其所需要处理的数据，并且是边拉取边进行处理的（和MapReduce的拉取合并数据基本一样），这个时候就会用到一个aggregate的数据结构，比如hashmap这种边拉取数据边进行聚合。这部分内存就被称为execution内存</p><p>从StaticMemoryManager.scala中的getMaxExecutionMemory方法中，我们可以发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理4.png" alt="enter description here"></p><p>每个executor分配给execution的内存为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ExecutionMemory = systemMaxMemory * memoryFraction * safetyFraction </span><br><span class="line">默认情况下为：systemMaxMemory * 0.2 * 0.8 = 0.16 * systemMaxMemory </span><br><span class="line">即默认为executor最大可用内存 * 0.16</span><br></pre></td></tr></table></figure><p>Execution内存再运行的时候会被分配给运行在JVM上的task；这里不同的是，分配给每个task的内存并不是固定的，而是动态的；spark不是一上来就分配固定大小的内存块给task，而是允许一个task占据JVM所有execution内存</p><p>每个JVM上的task可以最多申请至多1/N的execution内存，其中N为active task的个数，由spark.executor.cores指定；如果task的申请没有被批准，它会释放一部分内存，并且下次申请的时候，它会申请更小的一部分内存</p><p><strong>注：</strong></p><ul><li>每个Executor单独运行在一个JVM进程中，每个Task则是运行在Executor中的线程</li><li>spark.executor.cores设置的是每个executor的core数量</li><li>task的数量就是partition的数量</li><li>一般来说，一个core设置2~4个partition</li></ul><font color="red" size="3"><b>注意：</b></font> <font color="red"><br>为了防止过多的spilling数据，只有当一个task分配到的内存达到execution内存1/2N的时候才会spill，如果目前空闲的内存达不到1/2N的时候，内存申请会被阻塞直到其它的task spill掉它们的内存；<br><br>如果不这样限制，假设当前一个任务占据了绝大部分内存，那么新来的task会一直往硬盘spill数据，这样就会导致比较严重的I/O问题；而我们做了一定程度的限制，会进行一定程度的阻塞等待，对于频繁的小数据集的I/O会有一定的减缓<br><br>例子：某executor先启动一个task A，并在task B启动前快速占用了所有可用的内存；在B启用之后N变成了2，task B会阻塞直到task A spill，自己可以获得1/2N=1/4的execution内存的时候；而一大task B获取到了1/4的内存，A和B就都有可能spill了<br></font><h3 id="预留内存"><a href="#预留内存" class="headerlink" title="预留内存"></a>预留内存</h3><p>Spark之所以会有一个SafetyFraction这样的参数，是为了避免潜在的OOM。例如，进行计算时，有一个提前未预料到的比较大的数据，会导致计算时间延长甚至OOM，safetyFraction为storage和execution都提供了额外的buffer以防止此类的数据倾斜；这部分内存叫作预留内存</p><p>####Storage内存</p><p>####可用的Storage内存</p><p>该部分内存用作对RDD的缓存（如调用cache、persist等方法），节点间传输的广播变量</p><p>StaticMemoryManager.scala中的getMaxStorageMemory方法发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理5.png" alt="enter description here"></p><p>最后为每个executor分配到的storage的内存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">StorageMemory = systemMaxMemory * memoryFraction * safetyFraction </span><br><span class="line">默认情况下为：systemMaxMemory * 0.6 * 0.9 = 0.54 * systemMaxMemory </span><br><span class="line">即默认分配executor最大可用内存的0.54</span><br></pre></td></tr></table></figure><h4 id="预留内存-1"><a href="#预留内存-1" class="headerlink" title="预留内存"></a>预留内存</h4><p>同Execution内存中的预留部分</p><h3 id="Unroll"><a href="#Unroll" class="headerlink" title="Unroll"></a>Unroll</h3><p>Unroll是storage中比较特殊的一部分，它默认占据storage总内存的20%</p><p>BlockManager是spark自己实现的内部分布式文件系统，BlockManager接受数据（可能从本地或者其他节点）的时候是以iterator的形式，并且这些数据是有序列化和非序列化的，因此需要注意以下两点：</p><ul><li>Iterator在物理内存上是不连续的，如果后续spark要把数据装载进内存的话，就需要把这些数据放进一个array（物理上连续）</li><li>另外，序列化数据需要进行展开，如果直接展开序列化的数据，会造成OOM，所以BlockManager会逐渐的展开这个iterator，并逐渐检查内存里是否还有足够的空间用来展开数据放进array里</li></ul><p>StaticMemoryManager.scala中的maxUnrollMemory方法：</p><p><img src="/assets/blogImg/2019-04-03-内存管理6.png" alt="enter description here"></p><p>Unroll的优先级别还是比较高的，它使用的内存空间是可以从storage中借用的，如果在storage中没有现存的数据block，它甚至可以占据整个storage空间；如果storage中有数据block，它可以最大drop掉内存的数据是通过spark.storage.unrollFraction来控制的，通过源码可知这部分的默认值为0.2</p><font color="red" size="3"><b>注意：</b></font> <font color="red"><br>这个20%的空间并不是静态保留的，而是通过drop掉内存中的数据block来分配的（动态的分配过程）；如果unroll失败了，spark会把这部分数据evict到硬盘中去<br></font><h3 id="eviction策略"><a href="#eviction策略" class="headerlink" title="eviction策略"></a>eviction策略</h3><p>在spark技术文档中，eviction一词经常出现，eviction并不是单纯字面上驱逐的意思。说句题外话，spark通常被我们叫做内存计算框架，但是从严格意义上说，spark并不是内存计算的新技术；无论是cache还是persist这类算子，spark在内存安排上，绝大多数用的都是LRU策略（LRU可以说是一种算法，也可以算是一种原则，用来判断如何从Cache中清除对象，而LRU就是“近期最少使用”原则，当Cache溢出时，最近最少使用的对象将被从Cache中清除）。即当内存不够的时候，会evict掉最远使用过的内存数据block；当evict的时候，spark会将该数据块evict到硬盘，而不是单纯的抛弃掉</p><p>无论是storage还是execution的内存空间，当内存区域的空间不够用的时候，spark都会evict数据到硬盘</p><h3 id="Other部分"><a href="#Other部分" class="headerlink" title="Other部分"></a>Other部分</h3><p>这部分的内存用于程序本身运行所需要的内存，以及用户定义的数据结构和创建的对象，此内存由上面两部分：storage、execution决定的，默认为0.2</p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>Spark1.6开始引入了Off-heap memory（详见SPARK-11389）</p><p>堆外的空间分配较为简单，只有存储内存和执行内存，如图所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理7.png" alt="enter description here"></p><p>可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定（默认为0.5），由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域</p><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>在Spark的设计文档中，指出了静态内存管理的局限性：</p><p>没有适用于所有应用的默认配置，通常需要开发人员针对不同的应用进行不同的参数进行配置：比如根据任务的执行逻辑，调整shuffle和storage的内存占比来适应任务的需求</p><p>这样需要开发人员具备较高的spark原理知识</p><p>那些不cache数据的应用在运行的时候只会占用一小部分可用内存，而默认的内存配置中storage就用去了60%，造成了浪费</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别有洞天之Hive作业无法申请资源</title>
      <link href="/2019/03/21/%E5%88%AB%E6%9C%89%E6%B4%9E%E5%A4%A9%E4%B9%8BHive%E4%BD%9C%E4%B8%9A%E6%97%A0%E6%B3%95%E7%94%B3%E8%AF%B7%E8%B5%84%E6%BA%901/"/>
      <url>/2019/03/21/%E5%88%AB%E6%9C%89%E6%B4%9E%E5%A4%A9%E4%B9%8BHive%E4%BD%9C%E4%B8%9A%E6%97%A0%E6%B3%95%E7%94%B3%E8%AF%B7%E8%B5%84%E6%BA%901/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>在使用Hive Client跑job时，一直提示job被kill，然后观察YARN的WebUI进行查看，如图：</p><p><img src="/assets/blogImg/2019-03-21.png" alt="enter description here"></p><p>然后观察Hive Client的控制台输出，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1552895066408_0001, Tracking URL = http://localhost:8088/proxy/application_1552895066408_0001/</span><br><span class="line">Kill Command = /wangqingguo/bigdata/hadoop-2.6.0-cdh5.7.0/bin/hadoop job  -kill job_1552895066408_0001</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0</span><br><span class="line">2019-03-18 15:44:40,758 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">Ended Job = job_1552895066408_0001 with errors</span><br><span class="line">Error during job, obtaining debugging information...</span><br><span class="line">FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br></pre></td></tr></table></figure><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>通过YARN的WebUI看到，发现YARN没有Core和Memory，按照常理讲，如果不配置Core和Memeory，yarn-site.xml文件会有默认的值。</p><p>为了保险起见，我添加以下参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>重启HDFS的进程后，重新提交job，发现还是报这个错，然后通过仔细观察WebUI的log发现一句话：</p><p><code>Hadoop MapReduce Error - /bin/bash: /bin/java: is a directory</code></p><p>终于找到错误的所在，原来是找不到Java。</p><p>最后我在etc/hadoop/hadoop.env.sh中配置了java_home，问题解决。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka+SparkStreaming+MySQL经典案例源代码</title>
      <link href="/2019/03/18/Kafka+SparkStreaming+MySQL%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%E6%BA%90%E4%BB%A3%E7%A0%81/"/>
      <url>/2019/03/18/Kafka+SparkStreaming+MySQL%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%E6%BA%90%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p><strong>步骤：</strong></p><ul><li>安装部署单机Kafka</li><li>创建MySQL表</li><li>SparkStreaming实时消费</li></ul><h2 id="安装Kafka"><a href="#安装Kafka" class="headerlink" title="安装Kafka"></a>安装Kafka</h2><p>注：出于方便以及机器问题，使用单机部署，并不需要另外安装zookeeper，使用kafka自带的zookeeper。</p><ol><li>下载<a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">https://kafka.apache.org/downloads</a>（使用版本：kafka_2.11-0.10.0.1.tgz）</li><li><p>编辑<code>server.properties</code>文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">host.name=内网地址               # kafka绑定的interface</span><br><span class="line">advertised.listeners=PLAINTEXT://外网映射地址:9092    # 注册到zookeeper的地址和端口#添加如上两个地址（云主机！）</span><br><span class="line">log.dirs=/opt/software/kafka/logs            #配置log日志地址</span><br></pre></td></tr></table></figure></li><li><p>bin/zookeeper-server-start.sh ../config/zookeeper.properties</p><p>如果使用bin/zookeeper-server-start.sh config/zookeeper.properties会导致无法找到config目录而报错。所以最好将kafka配置到全局环境变量中</p><p>使用<code>nohup /zookeeper-server-start.sh ../config/zookeeper.properties &amp;</code> 启动后台服务</p></li><li><p>bin/kafka-server-start.sh ../config/server.properties</p><p>可以用使用<code>nohup kafka-server-start.sh config/server.properties &amp;</code>启动后台服务</p></li><li><p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test #创建topic</p></li><li>bin/kafka-topics.sh –list –zookeeper localhost:2181 测试是否创建成功</li><li>启动生产者：bin/kafka-console-producer.sh –broker-list localhost/或者云主机外网ip:9092 –topic test</li></ol><h2 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h2><ol><li><p>使用yum安装mysql5.6</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">安装:</span><br><span class="line">   cat &gt;/etc/yum.repos.d/MySQL5.6.repo&lt;&lt;EOF</span><br><span class="line">   # Enable to use MySQL 5.6</span><br><span class="line">   [mysql56-community]</span><br><span class="line">   name=MySQL 5.6 Community Server</span><br><span class="line">   baseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/6/\$basearch/</span><br><span class="line">   enabled=1</span><br><span class="line">   gpgcheck=0</span><br><span class="line">   EOF</span><br><span class="line">   yum -y install mysql-community-server</span><br><span class="line">   # 使用已经下载好的rpm包安装：</span><br><span class="line">   # yum -y localinstall \</span><br><span class="line">   # mysql-community-common-5.6.39-2.el6.x86_64.rpm \</span><br><span class="line">   # mysql-community-client-5.6.39-2.el6.x86_64.rpm \</span><br><span class="line">   # mysql-community-libs-compat-5.6.39-2.el6.x86_64.rpm \</span><br><span class="line">   # mysql-community-libs-5.6.39-2.el6.x86_64.rpm \</span><br><span class="line">   # mysql-community-server-5.6.39-2.el6.x86_64.rpm</span><br><span class="line">   chkconfig mysqld on</span><br><span class="line">   /etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">   启动:</span><br><span class="line">   mysqladmin -u root password root</span><br><span class="line">   mysql -uroot -proot</span><br><span class="line">   use mysql;</span><br><span class="line">   GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;</span><br><span class="line">   GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;</span><br><span class="line">   GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;</span><br><span class="line">   update user set password=password(&apos;root&apos;) where user=&apos;root&apos;;</span><br><span class="line">   delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line">   delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line">   drop database test;</span><br><span class="line">   DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line">   flush privileges;</span><br><span class="line">   exit;</span><br></pre></td></tr></table></figure></li><li><p>创建测试表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database ruozedata;</span><br><span class="line">use ruozedata;</span><br><span class="line">grant all privileges on ruozedata.* to ruoze@&apos;%&apos; identified by &apos;123456&apos;;</span><br><span class="line">CREATE TABLE `test` ( `ip` varchar(255) NOT NULL, `total` int(11) NOT NULL, PRIMARY KEY (`ip`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1;</span><br></pre></td></tr></table></figure></li></ol><h2 id="KafkaAndSparkStreamingToMySQL"><a href="#KafkaAndSparkStreamingToMySQL" class="headerlink" title="KafkaAndSparkStreamingToMySQL"></a>KafkaAndSparkStreamingToMySQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.G5</span><br><span class="line">import org.apache.log4j.Level</span><br><span class="line">import org.apache.log4j.Logger</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">import kafka.serializer.StringDecoder</span><br><span class="line">import org.apache.spark.streaming.kafka.KafkaUtils</span><br><span class="line">import java.sql.DriverManager</span><br><span class="line">import java.sql.PreparedStatement</span><br><span class="line">import java.sql.Connection</span><br><span class="line">object KafkaAndSparkStreamingToMySQL &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // 减少日志输出</span><br><span class="line">  Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)</span><br><span class="line">  val sparkConf = new SparkConf().setAppName(&quot;www.ruozedata.com&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">  val sparkStreaming = new StreamingContext(sparkConf, Seconds(10))</span><br><span class="line">  // 创建topic名称</span><br><span class="line">  val topic = Set(&quot;test&quot;)</span><br><span class="line">  // 制定Kafka的broker地址</span><br><span class="line">  val kafkaParams = Map[String, String](&quot;metadata.broker.list&quot; -&gt; &quot;139.198.189.141:9092&quot;)</span><br><span class="line">  // 创建DStream，接受kafka数据irectStream[String, String, StringDecoder,StringDecoder](sparkStreaming, kafkaParams, topic)</span><br><span class="line">  val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](sparkStreaming, kafkaParams, topic)</span><br><span class="line">  val line = kafkaStream.map(e =&gt; &#123;</span><br><span class="line">    new String(e.toString())</span><br><span class="line">  &#125;)</span><br><span class="line">  // 获取数据</span><br><span class="line">  val logRDD = kafkaStream.map(_._2)</span><br><span class="line">  // 将数据打印在屏幕</span><br><span class="line">  logRDD.print()</span><br><span class="line">  // 对接受的数据进行分词处理</span><br><span class="line">  val datas = logRDD.map(line =&gt; &#123;</span><br><span class="line">    // 201.105.101.108,productid=1  输入数据</span><br><span class="line">    val index: Array[String] = line.split(&quot;,&quot;)</span><br><span class="line">    val ip = index(0);</span><br><span class="line">    (ip, 1)</span><br><span class="line">  &#125;)</span><br><span class="line">  // 打印在屏幕</span><br><span class="line">  datas.print()</span><br><span class="line">  // 将数据保存在mysql数据库</span><br><span class="line">  datas.foreachRDD(cs =&gt; &#123;</span><br><span class="line">    var conn: Connection = null;</span><br><span class="line">    var ps: PreparedStatement = null;</span><br><span class="line">    try &#123;</span><br><span class="line">      Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();</span><br><span class="line">      cs.foreachPartition(f =&gt; &#123;</span><br><span class="line">        conn = DriverManager.getConnection(</span><br><span class="line">          &quot;jdbc:mysql://128.12.xx.xx:3306/ruozedata?useUnicode=true&amp;characterEncoding=utf8&quot;,</span><br><span class="line">          &quot;ruoze&quot;,</span><br><span class="line">          &quot;123456&quot;);</span><br><span class="line">        ps = conn.prepareStatement(&quot;insert into result values(?,?)&quot;);</span><br><span class="line">        f.foreach(s =&gt; &#123;</span><br><span class="line">          ps.setString(1, s._1);</span><br><span class="line">          ps.setInt(2, s._2);</span><br><span class="line">          ps.executeUpdate();</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case t: Throwable =&gt; t.printStackTrace() // TODO: handle error</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      if (ps != null) &#123;</span><br><span class="line">        ps.close()</span><br><span class="line">      &#125;</span><br><span class="line">      if (conn != null) &#123;</span><br><span class="line">        conn.close();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  sparkStreaming.start()</span><br><span class="line">  sparkStreaming.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="POM文件"><a href="#POM文件" class="headerlink" title="POM文件"></a>POM文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line">    &lt;groupId&gt;com.ruozedata&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;train-scala&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">    &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt;</span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">        &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;</span><br><span class="line">        &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;scala-tools.org&lt;/id&gt;</span><br><span class="line">            &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;</span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">            &lt;name&gt;cloudera&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">    &lt;/repositories&gt;</span><br><span class="line">    &lt;pluginRepositories&gt;</span><br><span class="line">        &lt;pluginRepository&gt;</span><br><span class="line">            &lt;id&gt;scala-tools.org&lt;/id&gt;</span><br><span class="line">            &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;</span><br><span class="line">        &lt;/pluginRepository&gt;</span><br><span class="line">    &lt;/pluginRepositories&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;!--Scala 依赖--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.5&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!--</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spark-streaming-flume_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;5.1.28&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;</span><br><span class="line">        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;/goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;/scalaVersion&gt;</span><br><span class="line">                    &lt;args&gt;</span><br><span class="line">                        &lt;arg&gt;-target:jvm-1.5&lt;/arg&gt;</span><br><span class="line">                    &lt;/args&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;downloadSources&gt;true&lt;/downloadSources&gt;</span><br><span class="line">                    &lt;buildcommands&gt;</span><br><span class="line">                        &lt;buildcommand&gt;ch.epfl.lamp.sdt.core.scalabuilder&lt;/buildcommand&gt;</span><br><span class="line">                    &lt;/buildcommands&gt;</span><br><span class="line">                    &lt;additionalProjectnatures&gt;</span><br><span class="line">                        &lt;projectnature&gt;ch.epfl.lamp.sdt.core.scalanature&lt;/projectnature&gt;</span><br><span class="line">                    &lt;/additionalProjectnatures&gt;</span><br><span class="line">                    &lt;classpathContainers&gt;</span><br><span class="line">                        &lt;classpathContainer&gt;org.eclipse.jdt.launching.JRE_CONTAINER&lt;/classpathContainer&gt;</span><br><span class="line">                        &lt;classpathContainer&gt;ch.epfl.lamp.sdt.launching.SCALA_CONTAINER&lt;/classpathContainer&gt;</span><br><span class="line">                    &lt;/classpathContainers&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                        &lt;/manifest&gt;</span><br><span class="line">                    &lt;/archive&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">                    &lt;/descriptorRefs&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">    &lt;reporting&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;/scalaVersion&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/reporting&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><font color="#FF4500">样本数据模拟kafka生产消费的:</font><p><img src="/assets/pic/2019-03-18-1.png" alt="生产消费"></p><font color="#FF4500">样本数据写入到kafka,spark streaming消费，写入到mysql:</font><p><img src="/assets/pic/2019-03-18-2.png" alt="写入到mysql"></p><font color="#FF4500">spark streaming打印结果:</font><p><img src="/assets/pic/2019-03-18-3.png" alt="打印结果"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark Streaming </tag>
            
            <tag> Kafka </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:高级班学员年薪37.4W的offer及3家面试题</title>
      <link href="/2019/03/14/%E6%8D%B7%E6%8A%A5_%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E5%B9%B4%E8%96%AA37.4W%E7%9A%84offer%E5%8F%8A3%E5%AE%B6%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/03/14/%E6%8D%B7%E6%8A%A5_%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E5%B9%B4%E8%96%AA37.4W%E7%9A%84offer%E5%8F%8A3%E5%AE%B6%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了）<br><br>直接看学员offer及刚出炉的面试题，难吗？<br></font><a id="more"></a> <font color="#00CD00" size="4"><b>1.先看第一个高级班小伙伴，月薪24K，没有明确年终奖几个月。</b></font><p><img src="/assets/blogImg/2019-03-14-1.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.高级班小伙伴，年薪22*17=37.4W</b></font><p><img src="/assets/blogImg/2019-03-14-2.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>刚出炉的面试题，你会吗？</b></font> <font color="red">假如会了可以私信找我们，可以内推大数据开发哟</font> <font color="blue" size="3"><b>1.第一家面试题:</b></font><ul><li>数据源有哪些？指标是什么</li><li>在做项目时，约定和配置你们是怎么选的</li><li>datax到数据到hdfs</li><li>datax的并发条件</li><li>使用linux查看hive的表</li><li>hive命令查看内部表外部表</li><li>kafka的分区</li><li>hbase批量写数据出现行级事务锁</li><li>linux查看文件的前10行</li><li>sqoop的并发操作以及并发条件</li><li>对于hadoop节点如何动态上下线</li><li>分组求和：用mapreduce实现</li><li>kafka redis怎么选型 用在什么场景</li><li>hive 表结构经常变化 怎么处理（除了外部表 有什么其他解决方案）</li><li>mongodb即作为元数据信息和规则信息，还存储一些图片文章，马上mongodb出现了瓶颈了，如何解决</li><li>集群一台 dstanode挂了怎么复制的</li><li>kafka 不丢数据怎么做</li><li>spark-streaming数据倾斜的两种原因，以及解决办法</li><li>mysql的单例和联合索引</li><li>mysql数据库备份方式</li><li>设计一个责任链</li></ul><font color="blue" size="3"><b>2.第二家面试题:</b></font><ul><li>scala和java的区别</li><li>反射在spark中的应用</li><li>反射在hadoop中的应用</li><li>反射和泛型</li><li>sql中的left join 和 join区别</li><li>在a.sh中执行b.sh</li><li>hive的数据倾斜怎么解决</li><li>文件大小1G，每一行就是一个单词，求频率最高的单词的top100 ， 要求只用1M内存</li><li>spark和sparkStraming中的常用代码</li><li>hive的新用户和老用户查询登录的问题</li><li>使用hive求日留存和pvuv</li><li>使用hive求连续登陆的用户</li><li>Kafka原理</li></ul><font color="blue" size="3"><b>3.第三家面试题:</b></font><ul><li>hive的外部表与内部表区别</li><li>hive的分区和分桶的区别</li><li>hive的索引</li><li>hdfs的ha中，zookeeper所起到的作用</li><li>监控，服务发现</li><li>zookeeper平时有哪些作用</li><li>mr的shuffle和spark的shuffle的区别</li><li>flink和sparkStreaming的区别</li><li>机架感知（上传文件后是怎么存储的）</li><li>hdfs的写数据流程</li><li>spark2.0新特性</li><li>实现sparkStreaming的HA</li><li>checkpoint的缺陷</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:刚出炉年薪30w的offer和面试题</title>
      <link href="/2019/03/13/%E6%8D%B7%E6%8A%A5_%E5%88%9A%E5%87%BA%E7%82%89%E5%B9%B4%E8%96%AA30w%E7%9A%84offer%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/03/13/%E6%8D%B7%E6%8A%A5_%E5%88%9A%E5%87%BA%E7%82%89%E5%B9%B4%E8%96%AA30w%E7%9A%84offer%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了）<br><br>直接看学员offer及刚出炉的面试题，难吗？<br></font><a id="more"></a> <font color="#00CD00" size="4"><b>1.上海小伙伴拿了月薪25K的offer(其实手里有3个offer)</b></font><p><img src="/assets/blogImg/2019-03-13-1_1.png" alt="enter description here"></p><font size="3"><b>面试题(其中1家完整面试题记录)</b></font><ol><li>自我介绍</li><li><p>讲一下实时数仓那个项目</p><ul><li>架构设计，节点数量，进程内存分配</li><li>Kafka的多次消费及消费紊乱，你们怎么保证的</li><li>你们每天的数据量多少</li><li>你们的rowkey怎么设计的避免热点问题</li></ul></li></ol><p>之后 围绕笔试题</p><ol start="3"><li>Spark ReduceByKey 和 GroupByKey什么区别 会不会有shuffle</li><li>宽窄依赖区别</li><li>left join 以及 inner join 以及 笛卡尔积什么区别</li><li>笔试题第一题里面的聚合怎么做（考了个outer full join</li><li>聚集索引 和 非聚集索引什么区别</li><li>Spark的map 与 mappatition区别</li><li>什么情况下使用广播变量 数据量大 还是 数据量小 为什么</li><li>Spark累加器你们是怎么用的</li><li>SQL优化说说怎么做的</li><li>like在什么情况下索引不会命中</li><li>如果第一题用shell做，会吗（考验awk 和 sed）</li><li>笔试题：</li></ol><p><img src="/assets/blogImg/2019-03-13-2.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-03-13-3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark Executor Dead快速剖析</title>
      <link href="/2019/03/12/%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90/"/>
      <url>/2019/03/12/%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>通过Spark UI查看Executors，发现存在Executor Dead的情况</p><p><img src="/assets/blogImg/2019-03-12-1.png" alt="enter description here"></p><p>进一步查看dead Executor stderr日志，发现如下报错信息：</p><p><img src="/assets/blogImg/2019-03-12-2.png" alt="enter description here"></p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><a id="more"></a> <font color="#FF4500">打开GC日志，配置如下</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf &quot;spark.executor.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;</span><br><span class="line">--conf &quot;spark.driver.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;</span><br></pre></td></tr></table></figure><font color="#FF4500">打开exeutor gc日志，发现一直在<strong>full gc</strong>，几乎每秒1次，基本处于拒绝服务状态</font><p><img src="//yoursite.com/2019/03/12/生产Spark_Executor_Dead快速剖析/blogImg/2019-03-12-3.png" alt="enter description here"></p><font size="4"><b>至此找到问题原因，executor内存不够导致dead，调大executor内存即可 ，所以排错方法定位很重要！</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark Streaming 黑名单过滤案例</title>
      <link href="/2019/03/08/%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B/"/>
      <url>/2019/03/08/%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>测试数据(通过Socket传入)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">20180808,zs</span><br><span class="line">20180808,ls</span><br><span class="line">20180808,ww</span><br></pre></td></tr></table></figure><p>黑名单列表(生产存在表)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zs</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ol><li>原始日志可以通过Streaming直接读取成一个DStream</li><li><p>名单通过RDD来模拟一份</p><a id="more"></a><h3 id="逻辑实现"><a href="#逻辑实现" class="headerlink" title="逻辑实现"></a>逻辑实现</h3></li><li><p>将DStream转成以下格式(黑名单只有名字)</p><p><code>(zs,(20180808,zs))(ls,(20180808,ls))(ww,( 20180808,ww))</code></p></li><li><p>将黑名单转成</p><p><code>(zs, true)(ls, true)</code></p></li><li><p>DStram与RDD进行LeftJoin(DStream能与RDD进行Join就是借用的transform算子)</p></li></ol><h3 id="具体代码实现及注释"><a href="#具体代码实现及注释" class="headerlink" title="具体代码实现及注释"></a>具体代码实现及注释</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.spark.Streaming</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">/**</span><br><span class="line">  * @author soulChun</span><br><span class="line">  * @create 2019-01-10-16:12</span><br><span class="line">  */</span><br><span class="line">object TransformApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatafulFunApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf,Seconds(10))</span><br><span class="line">    //构建黑名单</span><br><span class="line">    val blacks = List(&quot;zs&quot;, &quot;ls&quot;)</span><br><span class="line">    //通过map操作将黑名单结构转换成(zs, true)(ls, true)</span><br><span class="line">    val blackRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, true))</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;localhost&quot;, 8769)</span><br><span class="line">    //lines (20180808,zs)</span><br><span class="line">    //lines 通过map.split(1)之后取得就是zs,然后加一个x就转成了(zs,(20180808,zs)).就可以和blackRDD进行Join了</span><br><span class="line">    val clicklog = lines.map(x =&gt; (x.split(&quot;,&quot;)(1), x)).transform(rdd =&gt; &#123;</span><br><span class="line">      //Join之后数据结构就变成了(zs,[(20180808,zs),true]),过滤掉第二个元素中的第二个元素等于true的</span><br><span class="line">      rdd.leftOuterJoin(blackRDD).filter(x =&gt; x._2._2.getOrElse(false) != true)</span><br><span class="line">        //我们最后要输出的格式是(20180808,zs)，所以取Join之后的第二个元素中的第一个元素</span><br><span class="line">        .map(x =&gt; x._2._1)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后输出：<br><img src="/assets/blogImg/2019-03-08-1.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> sparkstreaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>刚出炉的3家大数据面试题(含高级),你会吗？</title>
      <link href="/2019/03/07/%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97/"/>
      <url>/2019/03/07/%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="第一家大数据开发的面试题"><a href="#第一家大数据开发的面试题" class="headerlink" title="第一家大数据开发的面试题:"></a>第一家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试1.png" alt="enter description here"></p><h3 id="第二家大数据开发的面试题"><a href="#第二家大数据开发的面试题" class="headerlink" title="第二家大数据开发的面试题:"></a>第二家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试2.png" alt="enter description here"></p><h3 id="第三家大数据开发的面试题"><a href="#第三家大数据开发的面试题" class="headerlink" title="第三家大数据开发的面试题:"></a>第三家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkShuffle详解剖析</title>
      <link href="/2019/03/06/SparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90/"/>
      <url>/2019/03/06/SparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="HashShuffle"><a href="#HashShuffle" class="headerlink" title="HashShuffle"></a>HashShuffle</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>所谓Shuffle就是将不同节点上相同的Key拉取到一个节点的过程。这之中涉及到各种IO，所以执行时间势必会较长，<font color="#FF4500">Spark的Shuffle在1.2之前默认的计算引擎是HashShuffleManager</font>，不过HashShuffleManager有一个十分严重的弊端，就是会产生大量的中间文件。<font color="#FF4500">在1.2之后默认Shuffle改为SortShuffleManager</font>，相对于之前，在每个Task虽然也会产生大量中间文件，但是最后会将所有的临时文件合并（merge）成一个文件。因此Shuffle read只需要读取时，根据索引拿到每个磁盘的部分数据就可以了<br><a id="more"></a></p><h3 id="测试条件"><a href="#测试条件" class="headerlink" title="测试条件"></a>测试条件</h3><p><code>每个Executor只有一个CUP（core），同一时间每个Executor只能执行一个task</code></p><h3 id="HashShuffleManager未优化版本"><a href="#HashShuffleManager未优化版本" class="headerlink" title="HashShuffleManager未优化版本"></a>HashShuffleManager未优化版本</h3><p>首先从shuffle write阶段，主要是在一个stage结束后，为了下一个stage可以执行shuffle，将每一个task的数据按照key进行分类，对key进行hash算法，从而使相同的key写入同一个文件，每个磁盘文件都由下游stage的一个task读取。在写入磁盘时，先将数据写入内存缓冲，当内存缓冲填满后，才会溢写到磁盘文件（似乎所以写文件都需要写入先写入缓冲区，然后再溢写，防止频繁IO）</p><p>我们可以先算一下当前stage的一个task会为下一个stage创建多少个磁盘文件。若下一个stage有100个task，则当前stage的每一个task都将创建100个文件，若当前stage要处理的task为50个，共有10个Executor，也就是说每个Executor共执行5个task，5x100x10=1000。也就是说这么一个小规模的操作会生产5000个文件。这是相当可观的。</p><p>而shuffle read 通常是一个stage一开始要做的事情。此时stage的每一个task去将上一个stage的计算结果的所有相同的key从不同节点拉到自己所在节点。进行聚合或join操作。在shuffle write过程，每个task给下游的每个task都创建了一个磁盘文件。在read过程task只需要去上游stage的task中拉取属于自己的磁盘文件。</p><p>shuffle read是边拉取边聚合。每一个read task都有一个buffer缓冲，然后通过内存中的Map进行聚合，每次只拉取buffer大小的数据，放到缓冲区中聚合，直到所有数据都拉取完。</p><p><img src="/source/assets/blogImg/2019-03-06-1.png" alt="HashShuffleManager未优化版本"></p><h3 id="优化版本"><a href="#优化版本" class="headerlink" title="优化版本"></a>优化版本</h3><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启这个机制之后，在shuffle write时，task并不是为下游的每一个task创建一个磁盘文件。引入了shuffleFileGroup的概念，每个shuffleFileGroup都对应一批磁盘文件。磁盘文件数量与下游task相同。只是仅仅第一批执行的task会创建一个shuffleFIleGroup，将数据写入到对应磁盘文件。</p><p>在执行下一批的task时，会复用已经创建好的shuffleFIleGroup和磁盘文件，即数据会继续写入到已有的磁盘文件。该机制会允许不同task复用同一个磁盘文件，对于多个task进行了一定程度的合并，大幅度减少shuffle write时，文件的数量，提升性能。</p><p>相对于优化前，每个Executor之前需要创建五百个磁盘文件，因为之前需要5个task线性执行，而使用参数优化之后，就每个Executor只需要100个就可以了，这样10个Executor就是1000个文件，这比优化前整整减少了4000个文件。</p><p><img src="/source/assets/blogImg/2019-03-06-2.png" alt="优化版本"></p><h2 id="SortShuffle"><a href="#SortShuffle" class="headerlink" title="SortShuffle"></a>SortShuffle</h2><p>在<font color="#FF4500">Spark1.2版本之后，出现了SortShuffle</font>，这种方式以更少的中间磁盘文件产生而远远优于HashShuffle。而它的运行机制主要分为两种。一种为普通机制，另一种为bypass机制。而bypass机制的启动条件为，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。即当read task不是那么多的时候，采用bypass机制是更好的选择。</p><h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一遍写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值，如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。</p><p>在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件。</p><p>最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引，start offset和end offset。</p><p>这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50个磁盘文件。</p><p><img src="/source/assets/blogImg/2019-03-06-3.png" alt="普通运行机制"></p><h3 id="bypass机制"><a href="#bypass机制" class="headerlink" title="bypass机制"></a>bypass机制</h3><font size="4"><b>bypass机制运行条件：</b></font><ol><li><strong>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</strong></li><li><strong>不是聚合类的shuffle算子（比如reduceByKey）。</strong></li></ol><p>在这种机制下，当前stage的task会为每个下游的task都创建临时磁盘文件。将数据按照key值进行hash，然后根据hash值，将key写入对应的磁盘文件中（个人觉得这也相当于一次另类的排序，将相同的key放在一起了）。最终，同样会将所有临时文件依次合并成一个磁盘文件，建立索引。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>该机制与未优化的hashshuffle相比，没有那么多磁盘文件，下游task的read操作相对性能会更好。</p><p>该机制与sortshuffle的普通机制相比，在readtask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。</p><p><img src="/source/assets/blogImg/2019-03-06-4.png" alt="优点"></p><font color="#FF4500"><br></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最佳实践之Spark写入Hfile经典案例</title>
      <link href="/2019/03/01/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/"/>
      <url>/2019/03/01/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font size="5" color="blue"><b>本文由小伙伴提供</b></font><p>将HDFS上的数据解析出来，然后通过hfile方式批量写入Hbase(需要多列写入) 写⼊数据的关键api:<br><a id="more"></a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">saveAsNewAPIHadoopFile(</span><br><span class="line">        stagingFolder,</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[KeyValue],</span><br><span class="line">        classOf[HFileOutputFormat2],</span><br><span class="line">        job.getConfiguration)</span><br></pre></td></tr></table></figure><p></p><font size="4" color="red"><b>特殊地方：</b></font><ol><li><p><strong>最初写hfile警告</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Does it contain files in subdirectories that correspond to column family names</span><br></pre></td></tr></table></figure></li></ol><pre><code>这个原因大概三种* 代码问题* 数据源问题* setMapOutputKeyClass 和 saveAsNewAPIHadoopFile中的Class不不⼀一致这里是我的是数据源问题</code></pre><ol start="2"><li><p><strong>正常写put操作的时候，服务端自动帮助排序，因此在使用put操作的时候没有涉及到这样的错误</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure></li></ol><pre><code>但是在写hfile的时候如果出现报错:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure>这样的错误，一般会认为rowkey没有做好排序，然后傻fufu的去验证了一下，rowkey的确做了排序。真正原因:`spark写hfile时候是按照rowkey+列族+列名进⾏行排序的，因此在写⼊数据的时候，要做到整体有序 (事情还没完)`</code></pre><ol start="3"><li><p><strong>因为需要多列写入，最好的⽅式:要么反射来动态获取列名称和列值 、 要么通过datafame去获取(df.columns)</strong><br>反射方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] = rdd.map</span><br><span class="line">&#123;</span><br><span class="line">        line =&gt;</span><br><span class="line">          val rowkey = line.vintime</span><br><span class="line">          val clazz = Class.forName(XXXXXXXXXXXXXXXX)</span><br><span class="line">          val fields = clazz.getDeclaredFields</span><br><span class="line">          var list = new ListBuffer[String]()</span><br><span class="line">          var kvlist = new ListBuffer[KeyValue]()//</span><br><span class="line">          if (fields != null &amp;&amp; fields.size &gt; 0) &#123;</span><br><span class="line">            for (field &lt;- fields) &#123;</span><br><span class="line">              field.setAccessible(true)</span><br><span class="line">              val column = field.getName</span><br><span class="line">              list.append(column)</span><br><span class="line">&#125; &#125;</span><br><span class="line">          val newList = list.sortWith(_ &lt; _)</span><br><span class="line">          val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">          for(column &lt;- newList)&#123;</span><br><span class="line">            val declaredField: Field =</span><br><span class="line">line.getClass.getDeclaredField(column)</span><br><span class="line">&#125;</span><br><span class="line">  declaredField.setAccessible(true)</span><br><span class="line">  val value = declaredField.get(line).toString</span><br><span class="line">  val kv: KeyValue = new KeyValue(</span><br><span class="line">    Bytes.toBytes(rowkey),</span><br><span class="line">    Bytes.toBytes(columnFamily),</span><br><span class="line">    Bytes.toBytes(column),</span><br><span class="line">    Bytes.toBytes(value))</span><br><span class="line">  kvlist.append(kv)</span><br><span class="line">&#125;</span><br><span class="line">(ik, kvlist)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><pre><code>datafame的方式:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">val tmpData: RDD[(ImmutableBytesWritable, util.LinkedList[KeyValue])] =</span><br><span class="line">df.rdd.map(</span><br><span class="line">      line =&gt;&#123;</span><br><span class="line">        val rowkey = line.getAs[String](&quot;vintime&quot;)</span><br><span class="line">        val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">        var linkedList = new util.LinkedList[KeyValue]()</span><br><span class="line">        for (column &lt;- columns) &#123;</span><br><span class="line">        val kv: KeyValue = new KeyValue(</span><br><span class="line">            Bytes.toBytes(rowkey),</span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            Bytes.toBytes(column),</span><br><span class="line">            Bytes.toBytes(line.getAs[String](column)))</span><br><span class="line">          linkedList.add(kv)</span><br><span class="line">        &#125;</span><br><span class="line">        (ik, linkedList)</span><br><span class="line">      &#125;)</span><br><span class="line">    val result: RDD[(ImmutableBytesWritable, KeyValue)] =</span><br><span class="line">tmpData.flatMapValues(</span><br><span class="line">      s =&gt; &#123;</span><br><span class="line">        val values: Iterator[KeyValue] =</span><br><span class="line">JavaConverters.asScalaIteratorConverter(s.iterator()).asScala</span><br><span class="line">        values</span><br><span class="line">      &#125;</span><br><span class="line">    ).sortBy(x =&gt;x._1 , true)</span><br></pre></td></tr></table></figure>仔细观察可以发现，其实两者都做了排序操作，但是即便经过(1)步骤后仍然报错:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure>那么再回想⼀下之前写hfile的要求:rowkey+列族+列都要有序，那么如果出现数据的重复，也不算是有序的操作! 因为，做一下数据的去重:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val key: RDD[(String, TransferTime)] = data.reduceByKey((x, y) =&gt; y)</span><br><span class="line">val unitData: RDD[TransferTime] = key.map(line =&gt; line._2)</span><br></pre></td></tr></table></figure>果然，这样解决了:Added a key not lexically larger than previous这个异常 但是会报如下另⼀个异常:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kryo serialization failed: Buffer overflow</span><br></pre></td></tr></table></figure>这个是因为在对⼀些类做kryo序列化时候，数据量的缓存⼤小超过了默认值，做⼀下调整即可<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(&quot;spark.kryoserializer.buffer.max&quot; , &quot;256m&quot;)</span><br><span class="line">sparkConf.set(&quot;spark.kryoserializer.buffer&quot; , &quot;64m&quot;)</span><br></pre></td></tr></table></figure></code></pre><font size="5"><b>完整代码</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">object WriteTransferTime extends WriteToHbase&#123;</span><br><span class="line">  /**</span><br><span class="line">* @param data 要插⼊入的数据 * @param tableName 表名</span><br><span class="line">**/</span><br><span class="line">  override def bulkLoadData(data: RDD[Any], tableName: String ,</span><br><span class="line">columnFamily:String): Unit = &#123;</span><br><span class="line">    val bean: RDD[TransferTime] = data.map(line =&gt;</span><br><span class="line">line.asInstanceOf[TransferTime])</span><br><span class="line">    val map: RDD[(String, TransferTime)] = bean.map(line =&gt; (line.vintime ,</span><br><span class="line">line))</span><br><span class="line">    val key: RDD[(String, TransferTime)] = map.reduceByKey((x, y) =&gt; y)</span><br><span class="line">    val map1: RDD[TransferTime] = key.map(line =&gt; line._2)</span><br><span class="line">    val by1: RDD[TransferTime] = map1.sortBy(f =&gt; f.vintime)</span><br><span class="line">    val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] =</span><br><span class="line">by1.map &#123;</span><br><span class="line">      line =&gt;</span><br><span class="line">        val rowkey = line.vintime</span><br><span class="line">        val clazz =</span><br><span class="line">Class.forName(&quot;com.dongfeng.code.Bean.message.TransferTime&quot;)</span><br><span class="line">        val fields = clazz.getDeclaredFields</span><br><span class="line">        var list = new ListBuffer[String]()</span><br><span class="line">        var kvlist = new ListBuffer[KeyValue]()//</span><br><span class="line">        if (fields != null &amp;&amp; fields.size &gt; 0) &#123;</span><br><span class="line">          for (field &lt;- fields) &#123;</span><br><span class="line">            field.setAccessible(true)</span><br><span class="line">            val column = field.getName</span><br><span class="line">            list.append(column)</span><br><span class="line">&#125; &#125;</span><br><span class="line">        val newList = list.sortWith(_ &lt; _)</span><br><span class="line">        val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">        for(column &lt;- newList)&#123;</span><br><span class="line">          val declaredField: Field = line.getClass.getDeclaredField(column)</span><br><span class="line">          declaredField.setAccessible(true)</span><br><span class="line">          val value = declaredField.get(line).toString</span><br><span class="line">          val kv: KeyValue = new KeyValue(</span><br><span class="line">            Bytes.toBytes(rowkey),</span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            Bytes.toBytes(column),</span><br><span class="line">            Bytes.toBytes(value))</span><br><span class="line">          kvlist.append(kv)</span><br><span class="line">        &#125;</span><br><span class="line">        (ik, kvlist)</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">       val result: RDD[(ImmutableBytesWritable, KeyValue)] =</span><br><span class="line">listData.flatMapValues(</span><br><span class="line">      s =&gt; &#123;</span><br><span class="line">        val values: Iterator[KeyValue] = s.iterator</span><br><span class="line">        values</span><br><span class="line">&#125; )</span><br><span class="line">    val resultDD: RDD[(ImmutableBytesWritable, KeyValue)] = result.sortBy(x</span><br><span class="line">=&gt;x._1 , true)</span><br><span class="line">    WriteToHbaseDB.hfile_load(result , TableName.valueOf(tableName) ,</span><br><span class="line">columnFamily)</span><br><span class="line">&#125; &#125;</span><br><span class="line">    def hfile_load(rdd:RDD[(ImmutableBytesWritable , KeyValue)] , tableName:</span><br><span class="line">TableName , columnFamily:String): Unit =&#123;</span><br><span class="line">//声明表的信息</span><br><span class="line">var table: Table = null try&#123;</span><br><span class="line">val startTime = System.currentTimeMillis() println(s&quot;开始时间:--------&gt;$&#123;startTime&#125;&quot;) //⽣生成的HFile的临时保存路路径</span><br><span class="line">val stagingFolder = &quot;hdfs://cdh1:9000/hfile/&quot;+tableName+new</span><br><span class="line">Date().getTime//</span><br><span class="line">table = connection.getTable(tableName) //如果表不不存在，则创建表 if(!admin.tableExists(tableName))&#123;</span><br><span class="line">        createTable(tableName , columnFamily)</span><br><span class="line">      &#125;</span><br><span class="line">//开始导⼊</span><br><span class="line">val job = Job.getInstance(config) job.setJobName(&quot;DumpFile&quot;) job.setMapOutputKeyClass(classOf[ImmutableBytesWritable]) job.setMapOutputValueClass(classOf[KeyValue])</span><br><span class="line">      rdd.sortBy(x =&gt; x._1, true).saveAsNewAPIHadoopFile(</span><br><span class="line">        stagingFolder,</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[KeyValue],</span><br><span class="line">        classOf[HFileOutputFormat2],</span><br><span class="line">        job.getConfiguration)</span><br><span class="line">      val load = new LoadIncrementalHFiles(config)</span><br><span class="line">      val regionLocator = connection.getRegionLocator(tableName)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">HFileOutputFormat2.configureIncrementalLoad(job, table,</span><br><span class="line">regionLocator)</span><br><span class="line">      load.doBulkLoad(new Path(stagingFolder), table.asInstanceOf[HTable])</span><br><span class="line">//      load.doBulkLoad(new Path(stagingFolder) , connection.getAdmin ,</span><br><span class="line">table , regionLocator)</span><br><span class="line">val endTime = System.currentTimeMillis() println(s&quot;结束时间:--------&gt;$&#123;endTime&#125;&quot;) println(s&quot;花费的时间:-----------------&gt;$&#123;(endTime - startTime)&#125;ms&quot;)</span><br><span class="line">    &#125;catch&#123;</span><br><span class="line">      case e:IOException =&gt;</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">    &#125;finally &#123;</span><br><span class="line">      if (table != null) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">// 关闭HTable对象 table.close(); &#125; catch &#123;</span><br><span class="line">          case e: IOException =&gt;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">&#125; &#125;</span><br><span class="line">if (connection != null) &#123; try &#123; //关闭hbase连接. connection.close();</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: IOException =&gt;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">&#125; &#125;</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
            <tag> Hfile </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产SparkSQL如何读写本地外部数据源及排错</title>
      <link href="/2019/03/01/%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99/"/>
      <url>/2019/03/01/%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p><a href="https://spark-packages.org/" target="_blank" rel="noopener">https://spark-packages.org/</a>里有很多third-party数据源的package，spark把包加载进来就可以使用了</p><p><img src="/assets/blogImg/2019-03-01-1.png" alt="enter description here"></p><p>csv格式在spark2.0版本之后是内置的，2.0之前属于第三方数据源<br><a id="more"></a></p><h3 id="读取本地外部数据源"><a href="#读取本地外部数据源" class="headerlink" title="读取本地外部数据源"></a>读取本地外部数据源</h3><h4 id="直接读取一个json文件"><a href="#直接读取一个json文件" class="headerlink" title="直接读取一个json文件"></a>直接读取一个json文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.27.jar </span><br><span class="line">scala&gt; spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;).show</span><br></pre></td></tr></table></figure><p>运行报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: file:/home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [49, 57, 125, 10]</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)</span><br><span class="line">  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)</span><br><span class="line">  ... 32 more</span><br></pre></td></tr></table></figure><p>查看load方法的源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by</span><br><span class="line">   * a local or distributed file system).</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def load(path: String): DataFrame = &#123;</span><br><span class="line">    option(&quot;path&quot;, path).load(Seq.empty: _*) // force invocation of `load(...varargs...)`</span><br><span class="line">  &#125;</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">/**</span><br><span class="line">   * Loads input in as a `DataFrame`, for data sources that support multiple paths.</span><br><span class="line">   * Only works if the source is a HadoopFsRelationProvider.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def load(paths: String*): DataFrame = &#123;</span><br><span class="line">    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) &#123;</span><br><span class="line">      throw new AnalysisException(&quot;Hive data source can only be used with tables, you can not &quot; +</span><br><span class="line">        &quot;read files of Hive data source directly.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)</span><br><span class="line">    if (classOf[DataSourceV2].isAssignableFrom(cls)) &#123;</span><br><span class="line">      val ds = cls.newInstance()</span><br><span class="line">      val options = new DataSourceOptions((extraOptions ++</span><br><span class="line">        DataSourceV2Utils.extractSessionConfigs(</span><br><span class="line">          ds = ds.asInstanceOf[DataSourceV2],</span><br><span class="line">          conf = sparkSession.sessionState.conf)).asJava)</span><br><span class="line">      // Streaming also uses the data source V2 API. So it may be that the data source implements</span><br><span class="line">      // v2, but has no v2 implementation for batch reads. In that case, we fall back to loading</span><br><span class="line">      // the dataframe as a v1 source.</span><br><span class="line">      val reader = (ds, userSpecifiedSchema) match &#123;</span><br><span class="line">        case (ds: ReadSupportWithSchema, Some(schema)) =&gt;</span><br><span class="line">          ds.createReader(schema, options)</span><br><span class="line">        case (ds: ReadSupport, None) =&gt;</span><br><span class="line">          ds.createReader(options)</span><br><span class="line">        case (ds: ReadSupportWithSchema, None) =&gt;</span><br><span class="line">          throw new AnalysisException(s&quot;A schema needs to be specified when using $ds.&quot;)</span><br><span class="line">        case (ds: ReadSupport, Some(schema)) =&gt;</span><br><span class="line">          val reader = ds.createReader(options)</span><br><span class="line">          if (reader.readSchema() != schema) &#123;</span><br><span class="line">            throw new AnalysisException(s&quot;$ds does not allow user-specified schemas.&quot;)</span><br><span class="line">          &#125;</span><br><span class="line">          reader</span><br><span class="line">        case _ =&gt; null // fall back to v1</span><br><span class="line">      &#125;</span><br><span class="line">      if (reader == null) &#123;</span><br><span class="line">        loadV1Source(paths: _*)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Dataset.ofRows(sparkSession, DataSourceV2Relation(reader))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      loadV1Source(paths: _*)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  private def loadV1Source(paths: String*) = &#123;</span><br><span class="line">    // Code path for data source v1.</span><br><span class="line">    sparkSession.baseRelationToDataFrame(</span><br><span class="line">      DataSource.apply(</span><br><span class="line">        sparkSession,</span><br><span class="line">        paths = paths,</span><br><span class="line">        userSpecifiedSchema = userSpecifiedSchema,</span><br><span class="line">        className = source,</span><br><span class="line">        options = extraOptions.toMap).resolveRelation())</span><br><span class="line">  &#125;</span><br><span class="line">------------------------------------------------------</span><br><span class="line">private var source: String = sparkSession.sessionState.conf.defaultDataSourceName</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">def defaultDataSourceName: String = getConf(DEFAULT_DATA_SOURCE_NAME)</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">// This is used to set the default data source</span><br><span class="line">  val DEFAULT_DATA_SOURCE_NAME = buildConf(&quot;spark.sql.sources.default&quot;)</span><br><span class="line">    .doc(&quot;The default data source to use in input/output.&quot;)</span><br><span class="line">    .stringConf</span><br><span class="line">    .createWithDefault(&quot;parquet&quot;)</span><br></pre></td></tr></table></figure><p>从源码中可以看出，如果不指定format，load默认读取的是parquet文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val users = spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">scala&gt; users.show()</span><br><span class="line">+------+--------------+----------------+                                        </span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><p>读取其他格式的文件，必须通过format指定文件格式，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//windows idea环境下</span><br><span class="line">val df1 = spark.read.format(&quot;json&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).load(&quot;hdfs://192.168.137.141:9000/data/people.json&quot;)</span><br><span class="line">df1.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><font color="#FF4500">option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)必须带上，不然报错</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX</span><br></pre></td></tr></table></figure><h4 id="读取CSV格式文件"><a href="#读取CSV格式文件" class="headerlink" title="读取CSV格式文件"></a>读取CSV格式文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">//源文件内容如下：</span><br><span class="line">[hadoop@hadoop001 ~]$ hadoop fs -text /data/people.csv</span><br><span class="line">name;age;job</span><br><span class="line">Jorge;30;Developer</span><br><span class="line">Bob;32;Developer</span><br><span class="line"></span><br><span class="line">//windows idea环境下</span><br><span class="line">val df2 = spark.read.format(&quot;csv&quot;)</span><br><span class="line">      .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">      .option(&quot;header&quot;,&quot;true&quot;)     //use first line of all files as header</span><br><span class="line">      .option(&quot;inferSchema&quot;,&quot;true&quot;)</span><br><span class="line">      .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)</span><br><span class="line">df2.show()</span><br><span class="line">df2.printSchema()</span><br><span class="line">//输出结果：</span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| 30|Developer|</span><br><span class="line">|  Bob| 32|Developer|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- job: string (nullable = true)</span><br><span class="line">-----------------------------------------------------------</span><br><span class="line">//如果不指定option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">+------------------+</span><br><span class="line">|      name;age;job|</span><br><span class="line">+------------------+</span><br><span class="line">|Jorge;30;Developer|</span><br><span class="line">|  Bob;32;Developer|</span><br><span class="line">+------------------+</span><br><span class="line">//如果不指定option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|  _c0|_c1|      _c2|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">|Jorge| 30|Developer|</span><br><span class="line">|  Bob| 32|Developer|</span><br><span class="line">+-----+---+---------+</span><br></pre></td></tr></table></figure><p>读取csv格式文件还可以自定义schema</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">val peopleschema = StructType(Array(</span><br><span class="line">StructField(&quot;hlwname&quot;,StringType,true), </span><br><span class="line">StructField(&quot;hlwage&quot;,IntegerType,true), </span><br><span class="line">StructField(&quot;hlwjob&quot;,StringType,true)))</span><br><span class="line">val df2 = spark.read.format(&quot;csv&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">        .option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">        .schema(peopleschema)</span><br><span class="line">        .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)</span><br><span class="line">      //打印测试</span><br><span class="line">      df2.show()</span><br><span class="line">      df2.printSchema()</span><br><span class="line">输出结果：</span><br><span class="line">+-------+------+---------+</span><br><span class="line">|hlwname|hlwage|   hlwjob|</span><br><span class="line">+-------+------+---------+</span><br><span class="line">|  Jorge|    30|Developer|</span><br><span class="line">|    Bob|    32|Developer|</span><br><span class="line">+-------+------+---------+</span><br><span class="line">root</span><br><span class="line"> |-- hlwname: string (nullable = true)</span><br><span class="line"> |-- hlwage: integer (nullable = true)</span><br><span class="line"> |-- hlwjob: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="将读取的文件以其他格式写出"><a href="#将读取的文件以其他格式写出" class="headerlink" title="将读取的文件以其他格式写出"></a>将读取的文件以其他格式写出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//将上文读取的users.parquet以json格式写出</span><br><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br><span class="line">[hadoop@hadoop000 ~]$ cd /home/hadoop/tmp/parquet2json</span><br><span class="line">[hadoop@hadoop000 parquet2json]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 56 Sep 24 10:15 part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Sep 24 10:15 _SUCCESS</span><br><span class="line">[hadoop@hadoop000 parquet2json]$ cat part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Alyssa&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Ben&quot;,&quot;favorite_color&quot;:&quot;red&quot;&#125;</span><br><span class="line"></span><br><span class="line">//将上文读取的people.json以csv格式写出</span><br><span class="line">df1.write.format(&quot;csv&quot;)</span><br><span class="line">     .mode(&quot;overwrite&quot;)</span><br><span class="line">     .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;)</span><br><span class="line">     .save(&quot;hdfs://192.168.137.141:9000/data/formatconverttest/&quot;)</span><br><span class="line">------------------------------------------</span><br><span class="line">[hadoop@hadoop001 ~]$ hadoop fs -text /data/formatconverttest/part-00000-6fd65eff-d0d3-43e5-9549-2b11bc3ca9de-c000.csv</span><br><span class="line">,Michael</span><br><span class="line">30,Andy</span><br><span class="line">19,Justin</span><br><span class="line">//发现若没有.option(&quot;header&quot;,&quot;true&quot;)，写出的csv丢失了首行的age,name信息</span><br><span class="line">//若不指定.option(&quot;sep&quot;,&quot;;&quot;)，默认逗号为分隔符</span><br></pre></td></tr></table></figure><p>此操作的目的在于学会类型转换，生产上最开始进来的数据大多都是text，json等行式存储的文件，一般都要转成ORC，parquet列式存储的文件，加上压缩，能把文件大小减小到10%左右，大幅度减小IO和数据处理量，提高性能</p><p>此时如果再执行一次save，路径不变，则会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br><span class="line">org.apache.spark.sql.AnalysisException: path file:/home/hadoop/tmp/parquet2json already exists.;</span><br><span class="line">  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)</span><br><span class="line">  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)</span><br><span class="line">.........................................................</span><br></pre></td></tr></table></figure><p>可以通过设置savemode来解决这个问题</p><p><img src="/assets/blogImg/2019-03-01-2.png" alt="enter description here"></p><p>默认是errorifexists</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark开发读取云主机HDFS异常剖析流程</title>
      <link href="/2019/02/26/%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/02/26/%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="问题背景："><a href="#问题背景：" class="headerlink" title="问题背景："></a>问题背景：</h3><h4 id="云主机是-Linux-环境，搭建-Hadoop-伪分布式"><a href="#云主机是-Linux-环境，搭建-Hadoop-伪分布式" class="headerlink" title="云主机是 Linux 环境，搭建 Hadoop 伪分布式"></a>云主机是 Linux 环境，搭建 Hadoop 伪分布式</h4><ul><li>公网 IP：139.198.xxx.xxx</li><li>内网 IP：192.168.137.2</li><li>主机名：hadoop001</li></ul><h4 id="本地的core-site-xml配置如下："><a href="#本地的core-site-xml配置如下：" class="headerlink" title="本地的core-site.xml配置如下："></a>本地的core-site.xml配置如下：</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:9001&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:9001/hadoop/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="本地的hdfs-site-xml配置如下："><a href="#本地的hdfs-site-xml配置如下：" class="headerlink" title="本地的hdfs-site.xml配置如下："></a>本地的hdfs-site.xml配置如下：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="云主机hosts文件配置："><a href="#云主机hosts文件配置：" class="headerlink" title="云主机hosts文件配置："></a>云主机hosts文件配置：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"># hostname loopback address</span><br><span class="line">  192.168.137.2   hadoop001</span><br></pre></td></tr></table></figure><p>云主机将内网IP和主机名hadoop001做了映射</p><h4 id="本地hosts文件配置"><a href="#本地hosts文件配置" class="headerlink" title="本地hosts文件配置"></a>本地hosts文件配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">139.198.18.XXX     hadoop001</span><br></pre></td></tr></table></figure><p>本地已经将公网IP和域名hadoop001做了映射</p><h3 id="问题症状"><a href="#问题症状" class="headerlink" title="问题症状"></a>问题症状</h3><ol><li><strong>在云主机上开启 HDFS，JPS 查看进程都没有异常，通过 Shell 操作 HDFS 文件也没有问题</strong></li><li><strong>通过浏览器访问 50070 端口管理界面也没有问题</strong></li><li><p><strong>在本地机器上使用 Java API 操作远程 HDFS 文件，URI 使用公网 IP，代码如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val uri = new URI(&quot;hdfs://hadoop001:9001&quot;)</span><br><span class="line">val fs = FileSystem.get(uri,conf)</span><br><span class="line">val listfiles = fs.listFiles(new Path(&quot;/data&quot;),true)</span><br><span class="line">    while (listfiles.hasNext) &#123;</span><br><span class="line">    val nextfile = listfiles.next()</span><br><span class="line">    println(&quot;get file path:&quot; + nextfile.getPath().toString())</span><br><span class="line">    &#125;</span><br><span class="line">------------------------------运行结果---------------------------------</span><br><span class="line">get file path:hdfs://hadoop001:9001/data/infos.txt</span><br></pre></td></tr></table></figure></li><li><p>在本地机器使用SparkSQL读取hdfs上的文件并转换为DF的过程中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">object SparkSQLApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">  val spark = SparkSession.builder().appName(&quot;SparkSQLApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">  val info = spark.sparkContext.textFile(&quot;/data/infos.txt&quot;)</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val infoDF = info.map(_.split(&quot;,&quot;)).map(x=&gt;Info(x(0).toInt,x(1),x(2).toInt)).toDF()</span><br><span class="line">  infoDF.show()</span><br><span class="line">  spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Info(id:Int,name:String,age:Int)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><pre><code>出现如下报错信息：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">   ....</span><br><span class="line">   ....</span><br><span class="line">   19/02/23 16:07:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)</span><br><span class="line">   19/02/23 16:07:00 INFO HadoopRDD: Input split: hdfs://hadoop001:9001/data/infos.txt:0+17</span><br><span class="line">   19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   .....</span><br><span class="line">   ....</span><br><span class="line">   19/02/23 16:07:21 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">   19/02/23 16:07:21 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 272.617680460432 msec.</span><br><span class="line">   19/02/23 16:07:42 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">       at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">       at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3499)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:12 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">       at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">       at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:12 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">   19/02/23 16:08:12 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11918.913311370841 msec.</span><br><span class="line">   19/02/23 16:08:45 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: DFS Read</span><br><span class="line">   org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:648)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job</span><br><span class="line">   19/02/23 16:08:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool </span><br><span class="line">   19/02/23 16:08:45 INFO TaskSchedulerImpl: Cancelling stage 0</span><br><span class="line">   19/02/23 16:08:45 INFO DAGScheduler: ResultStage 0 (show at SparkSQLApp.scala:30) failed in 105.618 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure></code></pre><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><ol><li><strong>本地 Shell 可以正常操作，排除集群搭建和进程没有启动的问题</strong></li><li><strong>云主机没有设置防火墙，排除防火墙没关的问题</strong></li><li><strong>云服务器防火墙开放了 DataNode 用于数据传输服务端口 默认是 50010</strong></li><li><strong>我在本地搭建了另一台虚拟机，该虚拟机和本地在同一局域网，本地可以正常操作该虚拟机的hdfs，基本确定了是由于内外网的原因。</strong></li><li><strong>查阅资料发现 HDFS 中的文件夹和文件名都是存放在 NameNode 上，操作不需要和 DataNode 通信，因此可以正常创建文件夹和创建文件说明本地和远程 NameNode 通信没有问题。那么很可能是本地和远程 DataNode 通信有问题</strong></li></ol><h3 id="问题猜想"><a href="#问题猜想" class="headerlink" title="问题猜想"></a>问题猜想</h3><p>由于本地测试和云主机不在一个局域网，hadoop配置文件是以内网ip作为机器间通信的ip。在这种情况下,我们能够访问到namenode机器，namenode会给我们数据所在机器的ip地址供我们访问数据传输服务，但是当写数据的时候，NameNode 和DataNode 是通过内网通信的，返回的是datanode内网的ip,我们无法根据该IP访问datanode服务器。</p><p>我们来看一下其中一部分报错信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">...</span><br><span class="line">19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue....</span><br></pre></td></tr></table></figure><p>从报错信息中可以看出，连接不到192.168.137.2:50010，也就是datanode的地址，因为外网必须访问“139.198.18.XXX:50010”才能访问到datanode。</p><p>为了能够让开发机器访问到hdfs，我们可以通过域名访问hdfs，让namenode返回给我们datanode的域名。</p><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><h4 id="尝试一："><a href="#尝试一：" class="headerlink" title="尝试一："></a>尝试一：</h4><p>在开发机器的hosts文件中配置datanode对应的外网ip和域名（上文已经配置），并且在与hdfs交互的程序中添加如下代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val conf = new Configuration()</span><br><span class="line">conf.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure><p>报错依旧</p><h4 id="尝试二："><a href="#尝试二：" class="headerlink" title="尝试二："></a>尝试二：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;SparkSQLApp&quot;)</span><br><span class="line">       .master(&quot;local[2]&quot;)</span><br><span class="line">      .config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure><p>报错依旧</p><h4 id="尝试三："><a href="#尝试三：" class="headerlink" title="尝试三："></a>尝试三：</h4><p>在hdfs-site.xml中添加如下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>运行成功</p><font color="#FF4500">通过查阅资料，建议在<strong>hdfs-site.xml</strong>中增加<strong><em>dfs.datanode.use.datanode.hostname</em></strong>属性，表示datanode之间的通信也通过域名方式</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这样能够使得更换内网IP变得十分简单、方便，而且可以让特定datanode间的数据交换变得更容易。但与此同时也<font color="#FF4500">存在一个副作用</font>，当DNS解析失败时会导致整个Hadoop不能正常工作，所以要保证DNS的可靠</p><font size="5"><b>总结：将默认的通过IP访问，改为通过域名方式访问。</b></font><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://blog.csdn.net/vaf714/article/details/82996860" target="_blank" rel="noopener">https://blog.csdn.net/vaf714/article/details/82996860</a></p><p><a href="https://www.cnblogs.com/krcys/p/9146329.html" target="_blank" rel="noopener">https://www.cnblogs.com/krcys/p/9146329.html</a></p><p><a href="https://blog.csdn.net/dominic_tiger/article/details/71773656" target="_blank" rel="noopener">https://blog.csdn.net/dominic_tiger/article/details/71773656</a></p><p><a href="https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/" target="_blank" rel="noopener">https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>捷报:高级班学员月薪22K和面试题</title>
      <link href="/2019/02/25/%E6%8D%B7%E6%8A%A5_%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E6%9C%88%E8%96%AA22K%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/02/25/%E6%8D%B7%E6%8A%A5_%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E6%9C%88%E8%96%AA22K%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="3"><br>我们不做过多宣传，<br><br>因为我们是若泽数据，企业在职培训。<br><br>直接看高级班学员offer及offer的刚出炉的面试题，3份面试题，难吗？<br></font><a id="more"></a> <font color="#00CD00" size="4"><b>1.若泽数据高级班，第3期学员22K的offer</b></font><p><img src="/assets/blogImg/2019-02-25-1.png" alt="enter description here"><br>让我们看看22k面试题</p><p><img src="/assets/blogImg/2019-02-25-2.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark UI界面实现原理</title>
      <link href="/2019/02/22/Spark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>/2019/02/22/Spark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>当Spark程序在运行时，会提供一个Web页面查看Application运行状态信息。是否开启UI界面由参数spark.ui.enabled(默认为true)来确定。下面列出Spark UI一些相关配置参数，默认值，以及其作用。</p><p><img src="/assets/blogImg/2019-02-22-1.png" alt="enter description here"></p><p>本文接下来分成两个部分，第一部分基于Spark-1.6.0的源码，结合第二部分的图片内容来描述UI界面在Spark中的实现方式。第二部分以实例展示Spark UI界面显示的内容。</p><a id="more"></a><h2 id="Spark-UI界面实现方式"><a href="#Spark-UI界面实现方式" class="headerlink" title="Spark UI界面实现方式"></a>Spark UI界面实现方式</h2><h3 id="UI组件结构"><a href="#UI组件结构" class="headerlink" title="UI组件结构"></a>UI组件结构</h3><p>这部分先讲UI界面的实现方式，UI界面的实例在本文最后一部分。如果对这部分中的某些概念不清楚，那么最好先把第二部分了解一下。</p><p>从下面UI界面的实例可以看出，不同的内容以Tab的形式展现在界面上，对应每一个Tab在下方显示具体内容。基本上Spark UI界面也是按这个层次关系实现的。</p><p>以SparkUI类为容器，各个Tab，如JobsTab, StagesTab, ExecutorsTab等镶嵌在SparkUI上，对应各个Tab，有页面内容实现类JobPage, StagePage, ExecutorsPage等页面。这些类的继承和包含关系如下图所示：</p><p><img src="/assets/blogImg/2019-02-22-2.png" alt="enter description here"></p><h3 id="初始化过程"><a href="#初始化过程" class="headerlink" title="初始化过程"></a>初始化过程</h3><p>从上面可以看出，SparkUI类型的对象是UI界面的根对象，它是在SparkContext类中构造出来的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">private var _ui: Option[SparkUI] = None //定义</span><br><span class="line">_ui = //SparkUI对象的生成</span><br><span class="line">  if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123;</span><br><span class="line">    Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,</span><br><span class="line">      _env.securityManager, appName, startTime = startTime))</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // For tests, do not enable the UI</span><br><span class="line">    None</span><br><span class="line">  &#125;</span><br><span class="line">_ui.foreach(_.bind())  //启动jetty。bind方法继承自WebUI，该类负责和真实的Jetty Server API打交道</span><br></pre></td></tr></table></figure><p>上面这段代码中可以看到SparkUI对象的生成过程，结合上面的类结构图，可以看到bind方法继承自WebUI类，进入WebUI类中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">protected val handlers = ArrayBuffer[ServletContextHandler]() // 这个对象在下面bind方法中会使用到。</span><br><span class="line">  protected val pageToHandlers = new HashMap[WebUIPage, ArrayBuffer[ServletContextHandler]] // 将page绑定到handlers上</span><br><span class="line">  /** 将Http Server绑定到这个Web页面 */</span><br><span class="line">  def bind() &#123;</span><br><span class="line">    assert(!serverInfo.isDefined, &quot;Attempted to bind %s more than once!&quot;.format(className))</span><br><span class="line">    try &#123;</span><br><span class="line">      serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name))</span><br><span class="line">      logInfo(&quot;Started %s at http://%s:%d&quot;.format(className, publicHostName, boundPort))</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logError(&quot;Failed to bind %s&quot;.format(className), e)</span><br><span class="line">        System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>上面代码中handlers对象维持了WebUIPage和Jetty之间的关系，org.eclipse.jetty.servlet.ServletContextHandler是标准jetty容器的handler。而对象pageToHandlers维持了WebUIPage到ServletContextHandler的对应关系。</p><p>各Tab页以及该页内容的实现，基本上大同小异。接下来以AllJobsPage页面为例仔细梳理页面展示的过程。</p><h3 id="SparkUI中Tab的绑定"><a href="#SparkUI中Tab的绑定" class="headerlink" title="SparkUI中Tab的绑定"></a>SparkUI中Tab的绑定</h3><p>从上面的类结构图中看到WebUIPage提供了两个重要的方法，render和renderJson用于相应页面请求，在WebUIPage的实现类中，具体实现了这两个方法。在SparkContext中构造出SparkUI的实例后，会执行SparkUI#initialize方法进行初始化。如下面代码中，调用SparkUI从WebUI继承的attacheTab方法，将各Tab页面绑定到UI上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def initialize() &#123;</span><br><span class="line">    attachTab(new JobsTab(this))</span><br><span class="line">    attachTab(stagesTab)</span><br><span class="line">    attachTab(new StorageTab(this))</span><br><span class="line">    attachTab(new EnvironmentTab(this))</span><br><span class="line">    attachTab(new ExecutorsTab(this))</span><br><span class="line">    attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;))</span><br><span class="line">    attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath))</span><br><span class="line">    attachHandler(ApiRootResource.getServletHandler(this))</span><br><span class="line">    // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs</span><br><span class="line">    attachHandler(createRedirectHandler(</span><br><span class="line">      &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest,</span><br><span class="line">      httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="页面内容绑定到Tab"><a href="#页面内容绑定到Tab" class="headerlink" title="页面内容绑定到Tab"></a>页面内容绑定到Tab</h3><p>在上一节中，JobsTab标签绑定到SparkUI上之后，在JobsTab上绑定了AllJobsPage和JobPage类。AllJobsPage页面即访问SparkUI页面时列举出所有Job的那个页面，JobPage页面则是点击单个Job时跳转的页面。通过调用JobsTab从WebUITab继承的attachPage方法与JobsTab进行绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123;</span><br><span class="line">  val sc = parent.sc</span><br><span class="line">  val killEnabled = parent.killEnabled</span><br><span class="line">  val jobProgresslistener = parent.jobProgressListener</span><br><span class="line">  val executorListener = parent.executorsListener</span><br><span class="line">  val operationGraphListener = parent.operationGraphListener</span><br><span class="line">  def isFairScheduler: Boolean =</span><br><span class="line">    jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR)</span><br><span class="line">  attachPage(new AllJobsPage(this))</span><br><span class="line">  attachPage(new JobPage(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="页面内容的展示"><a href="#页面内容的展示" class="headerlink" title="页面内容的展示"></a>页面内容的展示</h3><p>知道了AllJobsPage页面如何绑定到SparkUI界面后，接下来分析这个页面的内容是如何显示的。进入AllJobsPage类，主要观察render方法。在页面展示上Spark直接利用了Scala对html/xml的语法支持，将页面的Html代码嵌入Scala程序中。具体的页面生成过程可以查看下面源码中的注释。这里可以结合第二部分的实例进行查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">def render(request: HttpServletRequest): Seq[Node] = &#123;</span><br><span class="line">    val listener = parent.jobProgresslistener //获取jobProgresslistener对象，页面展示的数据都是从这里读取</span><br><span class="line">    listener.synchronized &#123;</span><br><span class="line">      val startTime = listener.startTime // 获取application的开始时间，默认值为-1L</span><br><span class="line">      val endTime = listener.endTime // 获取application的结束时间，默认值为-1L</span><br><span class="line">      val activeJobs = listener.activeJobs.values.toSeq // 获取当前application中处于active状态的job</span><br><span class="line">      val completedJobs = listener.completedJobs.reverse.toSeq // 获取当前application中完成状态的job</span><br><span class="line">      val failedJobs = listener.failedJobs.reverse.toSeq  // 获取当前application中失败状态的job</span><br><span class="line">      val activeJobsTable =</span><br><span class="line">        jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val completedJobsTable =</span><br><span class="line">        jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val failedJobsTable =</span><br><span class="line">        jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val shouldShowActiveJobs = activeJobs.nonEmpty</span><br><span class="line">      val shouldShowCompletedJobs = completedJobs.nonEmpty</span><br><span class="line">      val shouldShowFailedJobs = failedJobs.nonEmpty</span><br><span class="line">      val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123;</span><br><span class="line">        s&quot;$&#123;completedJobs.size&#125;&quot;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot;</span><br><span class="line">      &#125;</span><br><span class="line">      val summary: NodeSeq =</span><br><span class="line">        &lt;div&gt;</span><br><span class="line">          &lt;ul class=&quot;unstyled&quot;&gt;</span><br><span class="line">            &lt;li&gt;</span><br><span class="line">              &lt;strong&gt;Total Uptime:&lt;/strong&gt; // 显示当前Spark应用运行时间</span><br><span class="line">              &#123;// 如果还没有结束，就用系统当前时间减开始时间。如果已经结束，就用结束时间减开始时间</span><br><span class="line">                if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123;</span><br><span class="line">                  UIUtils.formatDuration(System.currentTimeMillis() - startTime)</span><br><span class="line">                &#125; else if (endTime &gt; 0) &#123;</span><br><span class="line">                  UIUtils.formatDuration(endTime - startTime)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &lt;/li&gt;</span><br><span class="line">            &lt;li&gt;</span><br><span class="line">              &lt;strong&gt;Scheduling Mode: &lt;/strong&gt; // 显示调度模式，FIFO或FAIR</span><br><span class="line">              &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125;</span><br><span class="line">            &lt;/li&gt;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowActiveJobs) &#123; // 如果有active状态的job，则显示Active Jobs有多少个</span><br><span class="line">                &lt;li&gt;</span><br><span class="line">                  &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;activeJobs.size&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowCompletedJobs) &#123; // 如果有完成状态的job，则显示Completed Jobs的个数</span><br><span class="line">                &lt;li id=&quot;completed-summary&quot;&gt;</span><br><span class="line">                  &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;completedJobNumStr&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowFailedJobs) &#123; // 如果有失败状态的job，则显示Failed Jobs的个数</span><br><span class="line">                &lt;li&gt;</span><br><span class="line">                  &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;listener.numFailedJobs&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &lt;/ul&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">      var content = summary // 将上面的html代码写入content变量，在最后统一显示content中的内容</span><br><span class="line">      val executorListener = parent.executorListener // 这里获取EventTimeline中的信息</span><br><span class="line">      content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs,</span><br><span class="line">          executorListener.executorIdToData, startTime)</span><br><span class="line">// 然后根据当前application中是否存在active， failed， completed状态的job，将这些信息显示在页面上。</span><br><span class="line">      if (shouldShowActiveJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++</span><br><span class="line">          activeJobsTable // 生成active状态job的展示表格，具体形式可参看第二部分。按提交时间倒序排列</span><br><span class="line">      &#125;</span><br><span class="line">      if (shouldShowCompletedJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++</span><br><span class="line">          completedJobsTable</span><br><span class="line">      &#125;</span><br><span class="line">      if (shouldShowFailedJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++</span><br><span class="line">          failedJobsTable</span><br><span class="line">      &#125;</span><br><span class="line">      val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; +</span><br><span class="line">        &quot; Click on a job to see information about the stages of tasks inside it.&quot;</span><br><span class="line">      UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText)) // 最后将content中的所有内容全部展示在页面上</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>接下来以activeJobsTable代码为例分析Jobs信息展示表格的生成。这里主要的方法是makeRow，接收的是上面代码中的activeJobs, completedJobs, failedJobs。这三个对象都是包含在JobProgressListener对象中的，在JobProgressListener中的定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 这三个对象用于存储数据的主要是JobUIData类型，</span><br><span class="line">  val activeJobs = new HashMap[JobId, JobUIData]</span><br><span class="line">  val completedJobs = ListBuffer[JobUIData]()</span><br><span class="line">  val failedJobs = ListBuffer[JobUIData]()</span><br></pre></td></tr></table></figure><p>将上面三个对象传入到下面这段代码中，继续执行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123;</span><br><span class="line">    val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined)</span><br><span class="line">    val columns: Seq[Node] = &#123; // 显示的信息包括，Job Id(Job Group)以及Job描述，Job提交时间，Job运行时间，总的Stage/Task数，成功的Stage/Task数，以及一个进度条</span><br><span class="line">      &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Description&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Submitted&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Duration&lt;/th&gt;</span><br><span class="line">      &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt;</span><br><span class="line">      &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    def makeRow(job: JobUIData): Seq[Node] = &#123;</span><br><span class="line">      val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job)</span><br><span class="line">      val duration: Option[Long] = &#123;</span><br><span class="line">        job.submissionTime.map &#123; start =&gt; // Job运行时长为系统时间，或者结束时间减去开始时间</span><br><span class="line">          val end = job.completionTime.getOrElse(System.currentTimeMillis())</span><br><span class="line">          end - start</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      val formattedDuration = duration.map(d =&gt;  // 格式化任务运行时间，显示为a h:b m:c s格式UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;)</span><br><span class="line">      val formattedSubmissionTime = // 获取Job提交时间job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;)</span><br><span class="line">      val jobDescription = UIUtils.makeDescription(lastStageDescription, parent.basePath) // 获取任务描述</span><br><span class="line">      val detailUrl = // 点击单个Job下面链接跳转到JobPage页面，传入参数为jobId</span><br><span class="line">        &quot;%s/jobs/job?id=%s&quot;.format(UIUtils.prependBaseUri(parent.basePath), job.jobId)</span><br><span class="line">      &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt;</span><br><span class="line">          &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td&gt;</span><br><span class="line">          &#123;jobDescription&#125;</span><br><span class="line">          &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt;</span><br><span class="line">          &#123;formattedSubmissionTime&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt;</span><br><span class="line">        &lt;td class=&quot;stage-progress-cell&quot;&gt;</span><br><span class="line">          &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125;</span><br><span class="line">          &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125;</span><br><span class="line">          &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td class=&quot;progress-cell&quot;&gt; // 进度条</span><br><span class="line">          &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks,</span><br><span class="line">           failed = job.numFailedTasks, skipped = job.numSkippedTasks,</span><br><span class="line">           total = job.numTasks - job.numSkippedTasks)&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">      &lt;/tr&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt;</span><br><span class="line">      &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt; // 显示列名</span><br><span class="line">      &lt;tbody&gt;</span><br><span class="line">        &#123;jobs.map(makeRow)&#125; // 调用上面的row生成方法，具体显示Job信息</span><br><span class="line">      &lt;/tbody&gt;</span><br><span class="line">    &lt;/table&gt;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>从上面这些代码中可以看到，Job页面显示的所有数据，都是从JobProgressListener对象中获得的。SparkUI可以理解成一个JobProgressListener对象的消费者，页面上显示的内容都是JobProgressListener内在的展现。</p><h2 id="Spark-UI界面实例"><a href="#Spark-UI界面实例" class="headerlink" title="Spark UI界面实例"></a>Spark UI界面实例</h2><p>默认情况下，当一个Spark Application运行起来后，可以通过访问hostname:4040端口来访问UI界面。hostname是提交任务的Spark客户端ip地址，端口号由参数spark.ui.port(默认值4040，如果被占用则顺序往后探查)来确定。由于启动一个Application就会生成一个对应的UI界面，所以如果启动时默认的4040端口号被占用，则尝试4041端口，如果还是被占用则尝试4042，一直找到一个可用端口号为止。</p><p>下面启动一个Spark ThriftServer服务，并用beeline命令连接该服务，提交sql语句运行。则ThriftServer对应一个Application，每个sql语句对应一个Job，按照Job的逻辑划分Stage和Task。</p><h3 id="Jobs页面"><a href="#Jobs页面" class="headerlink" title="Jobs页面"></a>Jobs页面</h3><p><img src="/assets/blogImg/2019-02-22-3.png" alt="enter description here"></p><p>连接上该端口后，显示的就是上面的页面，也是Job的主页面。这里会显示所有Active，Completed, Cancled以及Failed状态的Job。默认情况下总共显示1000条Job信息，这个数值由参数spark.ui.retainedJobs(默认值1000)来确定。</p><p>从上面还看到，除了Jobs选项卡之外，还可显示Stages, Storage, Enviroment, Executors, SQL以及JDBC/ODBC Server选项卡。分别如下图所示。</p><h3 id="Stages页面"><a href="#Stages页面" class="headerlink" title="Stages页面"></a>Stages页面</h3><p><img src="/assets/blogImg/2019-02-22-4.png" alt="enter description here"></p><h3 id="Storage页面"><a href="#Storage页面" class="headerlink" title="Storage页面"></a>Storage页面</h3><p><img src="/assets/blogImg/2019-02-22-5.png" alt="enter description here"></p><h3 id="Enviroment页面"><a href="#Enviroment页面" class="headerlink" title="Enviroment页面"></a>Enviroment页面</h3><p><img src="/assets/blogImg/2019-02-22-6.png" alt="enter description here"></p><h3 id="Executors页面"><a href="#Executors页面" class="headerlink" title="Executors页面"></a>Executors页面</h3><p><img src="/assets/blogImg/2019-02-22-7.png" alt="enter description here"></p><h3 id="单个Job包含的Stages页面"><a href="#单个Job包含的Stages页面" class="headerlink" title="单个Job包含的Stages页面"></a>单个Job包含的Stages页面</h3><p><img src="/assets/blogImg/2019-02-22-8.png" alt="enter description here"></p><h3 id="Task页面"><a href="#Task页面" class="headerlink" title="Task页面"></a>Task页面</h3><p><img src="/assets/blogImg/2019-02-22-9.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高级班学员高薪offer32w，你比他高吗？</title>
      <link href="/2019/02/21/%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E9%AB%98%E8%96%AAoffer32w%EF%BC%8C%E4%BD%A0%E6%AF%94%E4%BB%96%E9%AB%98%E5%90%97%EF%BC%9F/"/>
      <url>/2019/02/21/%E9%AB%98%E7%BA%A7%E7%8F%AD%E5%AD%A6%E5%91%98%E9%AB%98%E8%96%AAoffer32w%EF%BC%8C%E4%BD%A0%E6%AF%94%E4%BB%96%E9%AB%98%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽数据，创办三年来，高级班课表已经更新<font color="red"><b>V3</b></font>版本，和老师的企业生产项目同步更新！<font color="red"><b>官网课表已经更新！</b></font></p><p>官网 <a href="http://www.ruozedata.com" target="_blank" rel="noopener">http://www.ruozedata.com</a></p><a id="more"></a><p>本次课程以6大生产项目为驱动：</p><ol><li>生产上怎么开发、怎么debug源代码、怎么调优、怎么故障排查；</li><li>高级班就怎么教学，让我们实实在在学习真的大数据技术；</li><li>不套路，不搞pv、uv、demo级别、不读PPT；</li><li>全程官网(更新最快)学习、剖析、跟踪、教会大家怎样去学习。</li></ol><p>其实有个同学有句话说的很好，</p><font color="#CD69C9" size="3"><b>若泽数据的培训，更像是企业的老同事带领新同事，进行开发战斗！</b></font> <font color="#CD3333" size="4"><b>接下来看2个刚出炉的offer，高吗？高吗？高吗？</b></font> <font color="blue" size="4"><b>1.高级班第5期–某北京学员，课程才学完60%，年薪32W。</b></font><p><img src="/assets/blogImg/2019-02-21-1.png" alt="enter description here"></p><p>来看看面试题：</p><ul><li>hive1.x和hive2.x的区别？</li><li>yarn如何做资源隔离？</li><li>Spark作业上线流程？</li><li>MySQL事务隔离级别</li><li>MySQL调优流程？</li><li>Oracle逻辑读大能调整吗？</li><li>Spark作业上线资源评估</li><li>讲述一下你在项目中做了哪些东西？</li><li>每天处理的数据量</li><li>会产生小文件吗？小文件在项目的那个环节处理？</li></ul><font color="blue" size="4"><b>2.高级班第5期–某成都学员，课程才学完60%，年薪25.2W。</b></font><p><img src="/assets/blogImg/2019-02-21-2.png" alt="enter description here"></p><p>来看看面试题：</p><ul><li>数据倾斜</li><li>Spark应用中遇到的问题(说了OOM和序列化问题)</li><li>hbase解决数据热点</li><li>hbase region如何切分？做预切分吗？</li><li>Spark on yarn</li><li>小文件问题(他们做的是每天归档)</li><li>SparkStreaming和Kafka对接时，kafka的分区和SparkStreaming分区是怎么对应的</li><li>工作中用第三方工具来监控kafka吗</li><li>会kafka调优吗</li><li>会Spark调优吗？工作中调优了哪些参数</li><li>Spark宽依赖和窄依赖</li><li>hive底层存储格式</li><li>了不了解flink，它和spark的区别在哪里</li><li>yarn资源队列</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 高薪就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高薪 </tag>
            
            <tag> 就业 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark监控报错javax.servlet.http.HttpServletRequest.isAsyncStarted</title>
      <link href="/2019/02/16/Spark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted/"/>
      <url>/2019/02/16/Spark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h4><ul><li>Spark2.2.1</li><li>Hadoop2.6</li><li>Intellj</li><li>Scala2.11</li></ul><h4 id="pom文件"><a href="#pom文件" class="headerlink" title="pom文件"></a>pom文件</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.common.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.hdfs.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.client.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h4 id="报错信息如下所示："><a href="#报错信息如下所示：" class="headerlink" title="报错信息如下所示："></a>报错信息如下所示：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">at org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)</span><br><span class="line">at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)</span><br><span class="line">at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)</span><br><span class="line">at org.spark_project.jetty.server.Server.handle(Server.java:499)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN HttpChannel: Could not send response error 500: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">16/11/08 21:37:43 WARN HttpChannel: /jobs/</span><br><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">at org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)</span><br><span class="line">at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)</span><br><span class="line">at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)</span><br><span class="line">at org.spark_project.jetty.server.Server.handle(Server.java:499)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN QueuedThreadPool: </span><br><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletResponse.getStatus()I</span><br><span class="line">at org.spark_project.jetty.server.handler.ErrorHandler.handle(ErrorHandler.java:112)</span><br><span class="line">at org.spark_project.jetty.server.Response.sendError(Response.java:597)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handleException(HttpChannel.java:487)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection$HttpChannelOverHttp.handleException(HttpConnection.java:594)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:387)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN QueuedThreadPool: Unexpected thread death: org.spark_project.jetty.util.thread.QueuedThreadPool$3@3ec5063f in SparkUI&#123;STARTED,8&lt;=8&lt;=200,i=4,q=0&#125;</span><br></pre></td></tr></table></figure><h4 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h4><h5 id="查看报错信息"><a href="#查看报错信息" class="headerlink" title="查看报错信息"></a>查看报错信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br></pre></td></tr></table></figure><p>未找到HttpServletRequest类中的isAsyncStarted方法。</p><h5 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h5><p>使用搜索功能，查看该类存在于哪些包下。</p><p><img src="/source/assets/blogImg/2019-02-16-Spark监控报错.png" alt="Spark监控报错"></p><h5 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h5><p><img src="/source/assets/blogImg/2019-02-16-Spark监控问题解决1.png" alt="Spark监控问题解决1"></p><p><img src="/source/assets/blogImg/2019-02-16-Spark监控问题解决2.png" alt="Spark监控问题解决2"></p><p>所有涉及到该类jar文件且版本低于3.0的均需要进行删除。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每天起床第一句，看看Spark调度器</title>
      <link href="/2019/01/18/Spark%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
      <url>/2019/01/18/Spark%E8%B0%83%E5%BA%A6%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>之前呢，我们详细地分析了DAGScheduler的执行过程，我们知道，RDD形成的DAG经过DAGScheduler，依据shuffle将DAG划分为若干个stage，再由taskScheduler提交task到executor中执行，那么执行task的过程，就需要调度器来参与了。</p><p>Spark调度器主要有两种模式，也是大家耳熟能详的FIFO和FAIR模式。默认情况下，Spark是FIFO（先入先出）模式，即谁先提交谁先执行。而FAIR（公平调度）模式会在调度池中为任务进行分组，可以有不同的权重，根据权重来决定执行顺序。</p><p>那么源码中是怎么实现的呢？<br><a id="more"></a><br>首先，当Stage划分好，会调用TaskSchedulerImpl.submitTasks()方法，以TaskSet的形式提交给TaskScheduler，并创建一个TaskSetManger对象添加进调度池。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">override def submitTasks(taskSet: TaskSet) &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    //....</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">    //.....</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br></pre></td></tr></table></figure><p>SchedulerBulider通过TaskSchedulerImpl.initialize()进行了实例化，并调用了SchedulerBulider.buildPools()方法。具体怎么个build，就要看用户选择的schedulingMode了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def initialize(backend: SchedulerBackend) &#123;</span><br><span class="line">    this.backend = backend</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode match &#123;</span><br><span class="line">        case SchedulingMode.FIFO =&gt;</span><br><span class="line">          new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">        case SchedulingMode.FAIR =&gt;</span><br><span class="line">          new FairSchedulableBuilder(rootPool, conf)</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; +</span><br><span class="line">          s&quot;$schedulingMode&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>然后我们来看一下两个调度器的buildPools()方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">override def buildPools() &#123;</span><br><span class="line">    // nothing</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>FIFO什么也没干~~</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">override def buildPools() &#123;</span><br><span class="line">    var fileData: Option[(InputStream, String)] = None</span><br><span class="line">    try &#123;</span><br><span class="line">      fileData = schedulerAllocFile.map &#123; f =&gt;</span><br><span class="line">        val fis = new FileInputStream(f)</span><br><span class="line">        logInfo(s&quot;Creating Fair Scheduler pools from $f&quot;)</span><br><span class="line">        Some((fis, f))</span><br><span class="line">      &#125;.getOrElse &#123;</span><br><span class="line">        val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE)</span><br><span class="line">        if (is != null) &#123;</span><br><span class="line">          logInfo(s&quot;Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE&quot;)</span><br><span class="line">          Some((is, DEFAULT_SCHEDULER_FILE))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          logWarning(&quot;Fair Scheduler configuration file not found so jobs will be scheduled in &quot; +</span><br><span class="line">            s&quot;FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or &quot; +</span><br><span class="line">            s&quot;set $SCHEDULER_ALLOCATION_FILE_PROPERTY to a file that contains the configuration.&quot;)</span><br><span class="line">          None</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; buildFairSchedulerPool(is, fileName) &#125;</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(t) =&gt;</span><br><span class="line">        val defaultMessage = &quot;Error while building the fair scheduler pools&quot;</span><br><span class="line">        val message = fileData.map &#123; case (is, fileName) =&gt; s&quot;$defaultMessage from $fileName&quot; &#125;</span><br><span class="line">          .getOrElse(defaultMessage)</span><br><span class="line">        logError(message, t)</span><br><span class="line">        throw t</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; is.close() &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // finally create &quot;default&quot; pool</span><br><span class="line">    buildDefaultPool()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再谈，某头条公司Spark结构化流的SQL实现</title>
      <link href="/2019/01/10/%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/01/10/%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>前面介绍了大概的使用语句，接下来讲解基本的功能点的实现。</p><h2 id="SQL语句的解析-解析部分为开源项目flinkStreamSQL内容，直接拿过来用"><a href="#SQL语句的解析-解析部分为开源项目flinkStreamSQL内容，直接拿过来用" class="headerlink" title="SQL语句的解析(解析部分为开源项目flinkStreamSQL内容，直接拿过来用)"></a>SQL语句的解析(解析部分为开源项目flinkStreamSQL内容，直接拿过来用)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SocketTable(</span><br><span class="line">    word String,</span><br><span class="line">    valuecount int</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;socket&apos;,</span><br><span class="line">    host=&apos;hadoop-sh1-core1&apos;,</span><br><span class="line">    port=&apos;9998&apos;,</span><br><span class="line">    delimiter=&apos; &apos;</span><br><span class="line">);</span><br><span class="line">create SINK console(</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;console&apos;,</span><br><span class="line">    outputmode=&apos;complete&apos;</span><br><span class="line">);</span><br><span class="line">insert into console select word,count(*) from SocketTable group by word;</span><br></pre></td></tr></table></figure><a id="more"></a><p>将create的内容根据正则解析出来，将field和配置相关的内容解析出来。</p><p>insert into部分的内容则使用calsite解析出insert部分的target表和已经create的source表内容。</p><p>因为spark没有定义好表之后直接可以insert的内容，所以要将需要sink的target解析出来另外处理。</p><h2 id="创建source输入"><a href="#创建source输入" class="headerlink" title="创建source输入"></a>创建source输入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SocketTable(</span><br><span class="line">    word String,</span><br><span class="line">    valuecount int</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;socket&apos;,</span><br><span class="line">    host=&apos;hadoop-sh1-core1&apos;,</span><br><span class="line">    port=&apos;9998&apos;,</span><br><span class="line">    delimiter=&apos; &apos;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>解析出type中的内容，使用反射寻找到对应的处理类，解析各个参数是否合法，获得默认参数等。</p><p>这里就会使用format(‘socket’)的方式，option中分别是host和port，分隔符是’ ‘空格。</p><h2 id="schema的定义"><a href="#schema的定义" class="headerlink" title="schema的定义"></a>schema的定义</h2><p>schema的定义<br>spark.readStream创建的是dataframe，比如socket，它创建的df只有一个列，schema是value，如果是kafka的话就更多了。</p><p>接下来就是将定义的表中的field赋给df。</p><p>本项目中采用的是json的方式传schema，具体原因也很简单，tuple不行，case class的话需要动态变化，难度大，rdd方式在里面行不通，就通过json来做了。</p><h2 id="窗口的定义"><a href="#窗口的定义" class="headerlink" title="窗口的定义"></a>窗口的定义</h2><p>flink中其实也有在sql中添加窗口相关的字段，比如group by proctime 之类的。</p><p>在StructuredStreamingInSQL中添加，eventtime或者processtime的window sql，看源码中，其实定义一个窗口，就是为这个df添加了一个window的字段，window中有start、end等字段，知道了这个，我们在df中只要定义窗口的字段覆盖掉默认的window字段，就能使用processtime和eventtime的sql语句啦！</p><h2 id="sink的处理"><a href="#sink的处理" class="headerlink" title="sink的处理"></a>sink的处理</h2><p>将create的source加上定义field，加上window字段之后，就是将insert into的sql解析，把target的表拿出来，select后的内容是逻辑的主体，sql执行的内容结束之后，就和前面一样，根据type中的内容，找到对应的sink内容，执行writeStream。</p><h2 id="动态添加"><a href="#动态添加" class="headerlink" title="动态添加"></a>动态添加</h2><p>在处理中可能有这样的情况，想要更新执行的sql，但又不希望spark程序停止，这个时候就可以通过在zk上创建监听器的方式来实现sql的动态添加。</p><p>动态的替换的实现方式是，结构化流把所有的查询存在一个map中，key是jobid，value是query，通过获取旧的query的id，将其stop，新的query就会无缝对接，由于是新的query，bachid等内容都会从头开始计算。</p><h2 id="后续监控、自定义函数、压测、调优等功能-待分享"><a href="#后续监控、自定义函数、压测、调优等功能-待分享" class="headerlink" title="后续监控、自定义函数、压测、调优等功能(待分享)"></a>后续监控、自定义函数、压测、调优等功能(待分享)</h2><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 结构化流 </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019元旦-线下项目第11期圆满结束</title>
      <link href="/2019/01/02/2019%E5%85%83%E6%97%A6-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC11%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/"/>
      <url>/2019/01/02/2019%E5%85%83%E6%97%A6-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC11%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><center><br>2019年元旦3天<br><br>一句话，上海太冷<br><br>小伙伴们来自五湖四海<br><br><b>北京、成都、深圳、天津、广州、重庆等</b><br><br>大家为了一个目标<br><br>学习真正企业级大数据生产项目<br><br><br><br>一年我们只在节假日举办<br><br>元旦3天，错过了就是错过了<br><br>期待线下项目班第12期<br></center><p><img src="/assets/blogImg/2019-01-02-1.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-01-02-2.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-01-02-3.png" alt></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 线下实战班 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线下实战班 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我司Kafka+Flink+MySQL生产完整案例代码</title>
      <link href="/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
      <url>/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h6 id="1-版本信息："><a href="#1-版本信息：" class="headerlink" title="1.版本信息："></a>1.版本信息：</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flink实战</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="3-工程pom-xml"><a href="#3-工程pom-xml" class="headerlink" title="3.工程pom.xml"></a>3.工程pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConf类 定义与MySQL连接的JDBC的参数<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="5-MySQLSlink类"><a href="#5-MySQLSlink类" class="headerlink" title="5.MySQLSlink类"></a>5.MySQLSlink类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-数据清洗日期工具类"><a href="#6-数据清洗日期工具类" class="headerlink" title="6.数据清洗日期工具类"></a>6.数据清洗日期工具类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //测试一下</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQL建表"><a href="#7-MySQL建表" class="headerlink" title="7.MySQL建表"></a>7.MySQL建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-主程序："><a href="#8-主程序：" class="headerlink" title="8.主程序："></a>8.主程序：</h6><p>主要是将time的格式转成yyyyMMddHHmmss,</p><p>还有取URL中的课程ID，将不是/class开头的过滤掉。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="9-启动主程序，查看MySQL表数据在递增"><a href="#9-启动主程序，查看MySQL表数据在递增" class="headerlink" title="9.启动主程序，查看MySQL表数据在递增"></a>9.启动主程序，查看MySQL表数据在递增</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafka过来的消息是我模拟的，一分钟产生100条。</p><p>以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark在携程的实践（二）</title>
      <link href="/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>以下内容来自第三届携程大数据沙龙</p><h3 id="七、遇到的问题"><a href="#七、遇到的问题" class="headerlink" title="七、遇到的问题"></a>七、遇到的问题</h3><h5 id="orc-split"><a href="#orc-split" class="headerlink" title="orc split"></a>orc split</h5><p>Spark读取Hive表用的各个文件格式的InuptFormat，计算读取表需要的task数量依赖于InputFormat#getSplits<br>由于大部分表的存储格式主要使用的是orc，当一个orc文件超过256MB，split算法并行去读取orc元数据，有时候Driver内存飙升，OOM crash，Full GC导致network timeout，spark context stop<br>Hive读这些大表为何没有问题？因为Hive默认使用的是CombineHiveInputFormat，split是基于文件大小的。<br>Spark也需要实现类似于Hive的CombineInputFormat，还能解决小文件过多导致提交task数量过多的问题。<br>Executor Container killed<br>Executor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead<br><a id="more"></a></p><h5 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h5><p>1.Shuffle Read时netty堆外内存的使用<br>2.Window function spill threshold过小，导致每4096条或者64MB为一个文件写到磁盘<br>外部排序同时打开每个文件，每个文件占用1MB的堆外内存，导致container使用的内存远超过申请的内存，遂被yarn kill。<br>解决：<br>Patch：<br>[SPARK-19659] Fetch big blocks to disk when shuffle-read<br>[SPARK-21369][CORE] Don’t use Scala Tuple2 in common/network-<em><br>参数：spark.reducer.maxReqSizeShuffleToMem=209715200<br>Patch：<br>[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArray<br>参数：<br>spark.sql.windowExec.buffer.in.memory.threshold=4096<br>spark.sql.windowExec.buffer.spill.threshold= 1024 </em>1024 * 1024 / 2</p><h5 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h5><p>Spark写数据时生成很多小文件，对NameNode产生巨大的压力，在一开始Spark灰度上线的时候，文件数和Block数飙升，文件变小导致压缩率降低，容量也跟着上去。</p><h5 id="移植Hive-MergeFileTask的实现"><a href="#移植Hive-MergeFileTask的实现" class="headerlink" title="移植Hive MergeFileTask的实现"></a>移植Hive MergeFileTask的实现</h5><p>在Spark最后写目标表的阶段追加入了一个MergeFileTask，参考了Hive的实现<br>org.apache.hadoop.hive.ql.io.merge.MergeFileTask<br>org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator</p><h5 id="无数据的情况下不创建空文件"><a href="#无数据的情况下不创建空文件" class="headerlink" title="无数据的情况下不创建空文件"></a>无数据的情况下不创建空文件</h5><p>[SPARK-21435][SQL]<br>Empty files should be skipped while write to file</p><h3 id="八、优化"><a href="#八、优化" class="headerlink" title="八、优化"></a>八、优化</h3><p>1.查询分区表时支持broadcast join，加速查询<br>2.减少Broadcast join的内存压力 SPARK-22170<br>3.Fetch失败后能快速失败，以免作业卡几个小时 SPARK-19753<br>4.Spark Thrift Server稳定性<br>经常挂掉，日志里异常，more than one active taskSet for stage<br>Apply SPARK-23433仍有少数挂掉的情况，<br>提交SPARK-24677到社区，修复之<br>5.作业hang住 SPARK-21834 SPARK-19326 SPARK-11334</p><h3 id="九、未来计划"><a href="#九、未来计划" class="headerlink" title="九、未来计划"></a>九、未来计划</h3><h5 id="自动调优内存"><a href="#自动调优内存" class="headerlink" title="自动调优内存"></a>自动调优内存</h5><p>手机spark driver和executor内存使用情况<br>根据作业历史的内存使用情况，在调度系统端自动设置合适的内存<br><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener">https://github.com/uber-common/jvm-profiler</a></p><h5 id="spark-adaptive"><a href="#spark-adaptive" class="headerlink" title="spark adaptive"></a>spark adaptive</h5><p>动态调整执行计划 SortMergeJoin转化为BroadcastHashJoin<br>动态处理数据倾斜<br><a href="https://issues.apache.org/jira/browse/SPARK-23128" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-23128</a><br><a href="https://github.com/Intel-bigdata/spark-adaptive" target="_blank" rel="noopener">https://github.com/Intel-bigdata/spark-adaptive</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark在携程的实践（一）</title>
      <link href="/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="一、Spark在携程应用的现状"><a href="#一、Spark在携程应用的现状" class="headerlink" title="一、Spark在携程应用的现状"></a>一、Spark在携程应用的现状</h3><h6 id="集群规模："><a href="#集群规模：" class="headerlink" title="集群规模："></a>集群规模：</h6><p>平均每天MR任务数：30W+</p><h6 id="开发平台："><a href="#开发平台：" class="headerlink" title="开发平台："></a>开发平台：</h6><p>调度系统运行的任务数：10W+<br>每天运行任务实例数：23W+<br>ETL/计算任务：~58%</p><h6 id="查询平台"><a href="#查询平台" class="headerlink" title="查询平台:"></a>查询平台:</h6><p>adhoc查询：2W+<br>支持Spark/Hive/Presto<br><img src="/assets/blogImg/1209_1.png" alt="enter description here"></p><h3 id="二、Hive与Spark的区别"><a href="#二、Hive与Spark的区别" class="headerlink" title="二、Hive与Spark的区别"></a>二、Hive与Spark的区别</h3><h6 id="Hive："><a href="#Hive：" class="headerlink" title="Hive："></a>Hive：</h6><p>优点：运行稳定，客户端内存消耗小。<br>存在问题：生成多个MapReduce作业；中间结果落地，IO开销大；频繁申请和释放container，资源没有合理充分利用</p><h6 id="Spark："><a href="#Spark：" class="headerlink" title="Spark："></a>Spark：</h6><p>快：高效的DAG执行引擎，可以基于内存来高效的处理数据流，节省大量IO开销<br>通用性：SparkSQL能直接使用HiveQL语法，Hive Metastore，Serdes，UDFs<br><img src="/assets/blogImg/1209_2.png" alt="enter description here"></p><h3 id="三、迁移SparkSQL的挑战"><a href="#三、迁移SparkSQL的挑战" class="headerlink" title="三、迁移SparkSQL的挑战"></a>三、迁移SparkSQL的挑战</h3><h6 id="兼容性："><a href="#兼容性：" class="headerlink" title="兼容性："></a>兼容性：</h6><p>Hive原先的权限控制<br>SQL语法，UDF和Hive的兼容性</p><h6 id="稳定性："><a href="#稳定性：" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明，低优先级用户无感知<br>监控作业迁移后成功率及运行时长对比</p><h6 id="准确性："><a href="#准确性：" class="headerlink" title="准确性："></a>准确性：</h6><p>数据一致<br>功能增强：<br>用户体验，是否易用，报错信息是否可读<br>潜在Bug<br>周边系统配合改造<br>血缘收集</p><h3 id="四、兼容性改造"><a href="#四、兼容性改造" class="headerlink" title="四、兼容性改造"></a>四、兼容性改造</h3><h6 id="移植hive权限"><a href="#移植hive权限" class="headerlink" title="移植hive权限"></a>移植hive权限</h6><p>Spark没有权限认证模块，可对任意表进行查询，有安全隐患<br>需要与Hive共享同一套权限</p><h6 id="方案："><a href="#方案：" class="headerlink" title="方案："></a>方案：</h6><p>执行SQL时，对SQL解析得到LogicalPlan，对LogicalPlan进行遍历，提取读取的表及写入的表，调用Hvie的认证方法进行检查，如果有权限则继续执行，否则拒绝该用户的操作。<br>SQL语法和hive兼容<br>Spark创建的某些视图，在Hive查询时报错，Spark创建的视图不会对SQL进行展开，视图定义没有当前的DB信息，Hive不兼容读取这样的视图</p><h6 id="方案：、"><a href="#方案：、" class="headerlink" title="方案：、"></a>方案：、</h6><p>保持与Hive一致，在Spark创建和修改视图时，使用hive cli driver去执行create/alter view sql<br>UDF与hive兼容<br>UDF计算结果不一样，即使是正常数据，Spark返回null，Hive结果正确；异常数据，Spark抛exception导致作业失败，Hive返回的null。</p><h6 id="方案：-1"><a href="#方案：-1" class="headerlink" title="方案："></a>方案：</h6><p>Spark函数修复，比如round函数<br>将hive一些函数移植，并注册成永久函数<br>整理Spark和Hive语法和UDF差异<br>五、稳定性和准确性</p><h6 id="稳定性：-1"><a href="#稳定性：-1" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明：调度系统对低优先级作业，按作业粒度切换成Spark执行，失败后再切换成hive<br>灰度变更，多种变更规则：支持多版本Spark，自动切换引擎，Spark v2 -&gt; Spark v1 -&gt; Hive；灰度推送参数，调优参数，某些功能<br>监控：每日统计spark和hive运行对比，每时收集作业粒度失败的Spark作业，分析失败原因<br>准确性：<br>数据质量系统：校验任务，检查数据准确性</p><h3 id="六、功能增强"><a href="#六、功能增强" class="headerlink" title="六、功能增强"></a>六、功能增强</h3><h6 id="Spark-Thrift-Server："><a href="#Spark-Thrift-Server：" class="headerlink" title="Spark Thrift Server："></a>Spark Thrift Server：</h6><ul><li>1.基于delegation token的impersontion<br>Driver：<br>为不同的用户拿delegation token，写到staging目录，记录User-&gt;SQL-&gt;Job映射关系，分发task带上对应的username<br>Executor：<br>根据task信息带的username找到staging目录下的token，加到当前proxy user的ugi，实现impersonate</li><li>2.基于zookeeper的服务发现，支持多台server<br>这一块主要移植了Hive zookeeper的实现</li><li>3.限制大查询作业，防止driver OOM<br>限制每个job产生的task最大数量<br>限制查询SQL的最大行数，客户端查询大批量数据，数据挤压在Thrift Server，堆内内存飙升，强制在只有查的SQL加上limit<br>限制查询SQL的结果集数据大小</li><li>4.监控<br>对每个server定时查询，检测是否可用<br>多运行时长较久的作业，主动kill<h6 id="用户体验"><a href="#用户体验" class="headerlink" title="用户体验"></a>用户体验</h6>用户看到的是类似Hive MR进度的日志，INFO级别日志收集到ES，可供日志的分析和排查问题<br>收集生成的表或者分区的numRows numFile totalSize，输出到日志<br>对简单的语句，如DDL语句，自动使用–master=local方式启动<h6 id="Combine-input-Format"><a href="#Combine-input-Format" class="headerlink" title="Combine input Format"></a>Combine input Format</h6>在HadoopTableReader#makeRDDForTable，拿到对应table的InputFormatClass，转换成对应格式的CombineInputFormat<br>通过开关来决定是否启用这个特性<br>set spark.sql.combine.input.splits.enable=true<br>通过参数来调整每个split的total input size<br>mapreduce.input.fileinputformat.split.maxsize=256MB <em>1024</em>1024<br>之前driver读大表高峰时段split需要30分钟不止，才把任务提交上，现在只要几分钟就算好split的数量并提交任务，也解决了一些表不大，小文件多，能合并到同一个task进行读取</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码 | Spark读取mongoDB数据写入Hive普通表和分区表</title>
      <link href="/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/"/>
      <url>/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="版本："><a href="#版本：" class="headerlink" title="版本："></a>版本：</h3><p>spark 2.2.0<br>hive 1.1.0<br>scala 2.11.8<br>hadoop-2.6.0-cdh5.7.0<br>jdk 1.8<br>MongoDB 3.6.4</p><h3 id="一-原始数据及Hive表"><a href="#一-原始数据及Hive表" class="headerlink" title="一 原始数据及Hive表"></a>一 原始数据及Hive表</h3><h5 id="MongoDB数据格式"><a href="#MongoDB数据格式" class="headerlink" title="MongoDB数据格式"></a>MongoDB数据格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;),</span><br><span class="line">    &quot;id&quot; : &quot;1&quot;,</span><br><span class="line">    &quot;name&quot; : &quot;lisi&quot;,</span><br><span class="line">    &quot;age&quot; : &quot;18&quot;,</span><br><span class="line">    &quot;deptno&quot; : &quot;01&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="Hive普通表"><a href="#Hive普通表" class="headerlink" title="Hive普通表"></a>Hive普通表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table mg_hive_test(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string,</span><br><span class="line">deptno string</span><br><span class="line">)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h5 id="Hive分区表"><a href="#Hive分区表" class="headerlink" title="Hive分区表"></a>Hive分区表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table  mg_hive_external(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string</span><br><span class="line">)</span><br><span class="line">partitioned by (deptno string)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h3 id="二-IDEA-Maven-Java"><a href="#二-IDEA-Maven-Java" class="headerlink" title="二 IDEA+Maven+Java"></a>二 IDEA+Maven+Java</h3><h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.6.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.2.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/2 21:00</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import com.mongodb.spark.MongoSpark;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import org.bson.Document;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class sparkreadmgtohive &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        //spark 2.x</span><br><span class="line">        String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath();</span><br><span class="line">        SparkSession spark = SparkSession.builder()</span><br><span class="line">                .master(&quot;local[2]&quot;)</span><br><span class="line">                .appName(&quot;SparkReadMgToHive&quot;)</span><br><span class="line">                .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">                .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line"></span><br><span class="line">        //spark 1.x</span><br><span class="line">//        JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);</span><br><span class="line">//        SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);</span><br><span class="line">//        conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);</span><br><span class="line">//        conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);</span><br><span class="line">//        HiveContext sqlContext = new HiveContext(sc);</span><br><span class="line">//        //create df from mongo</span><br><span class="line">//        Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();</span><br><span class="line">//        df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">        String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;;</span><br><span class="line">        String opType =&quot;P&quot;;</span><br><span class="line"></span><br><span class="line">        SQLUtils sqlUtils = new SQLUtils();</span><br><span class="line">        List&lt;String&gt; column = sqlUtils.getColumns(querysql);</span><br><span class="line"></span><br><span class="line">        //create rdd from mongo</span><br><span class="line">        JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc);</span><br><span class="line">        //将Document转成Object</span><br><span class="line">        JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123;</span><br><span class="line">            public Object call(Document document)&#123;</span><br><span class="line">                List list = new ArrayList();</span><br><span class="line">                for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">                    list.add(String.valueOf(document.get(column.get(i))));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line"></span><br><span class="line">//                return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(Ordd.first());</span><br><span class="line">        //通过编程方式将RDD转成DF</span><br><span class="line">        List ls= new ArrayList();</span><br><span class="line">        for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">            ls.add(column.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">        String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;);</span><br><span class="line">        System.out.println(schemaString);</span><br><span class="line"></span><br><span class="line">        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();</span><br><span class="line">        for (String fieldName : schemaString.split(&quot;,&quot;)) &#123;</span><br><span class="line">            StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);</span><br><span class="line">            fields.add(field);</span><br><span class="line">        &#125;</span><br><span class="line">        StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123;</span><br><span class="line">            List fileds = (List) record;</span><br><span class="line">//            String[] attributes = record.toString().split(&quot;,&quot;);</span><br><span class="line">            return RowFactory.create(fileds.toArray());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema);</span><br><span class="line"></span><br><span class="line">        //将DF写入到Hive中</span><br><span class="line">        //选择Hive数据库</span><br><span class="line">        spark.sql(&quot;use datalake&quot;);</span><br><span class="line">        //注册临时表</span><br><span class="line">        df.registerTempTable(&quot;mgtable&quot;);</span><br><span class="line"></span><br><span class="line">        if (&quot;O&quot;.equals(opType.trim())) &#123;</span><br><span class="line">            System.out.println(&quot;数据插入到Hive ordinary table&quot;);</span><br><span class="line">            Long t1 = System.currentTimeMillis();</span><br><span class="line">            spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;);</span><br><span class="line">            Long t2 = System.currentTimeMillis();</span><br><span class="line">            System.out.println(&quot;共耗时：&quot; + (t2 - t1) / 60000 + &quot;分钟&quot;);</span><br><span class="line">        &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123;</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;数据插入到Hive  dynamic partition table&quot;);</span><br><span class="line">        Long t3 = System.currentTimeMillis();</span><br><span class="line">        //必须设置以下参数 否则报错</span><br><span class="line">        spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;);</span><br><span class="line">        //depton为分区字段   select语句最后一个字段必须是deptno</span><br><span class="line">        spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;);</span><br><span class="line">        Long t4 = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;共耗时：&quot;+(t4 -t3)/60000+ &quot;分钟&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/3 23:20</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class SQLUtils &#123;</span><br><span class="line"></span><br><span class="line">    public List&lt;String&gt; getColumns(String querysql)&#123;</span><br><span class="line">        List&lt;String&gt; column = new ArrayList&lt;String&gt;();</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6,</span><br><span class="line">                querysql.indexOf(&quot;from&quot;)).trim();</span><br><span class="line">        if (tmp.indexOf(&quot;*&quot;) == -1)&#123;</span><br><span class="line">            String cols[] = tmp.split(&quot;,&quot;);</span><br><span class="line">            for (String c:cols)&#123;</span><br><span class="line">                column.add(c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return column;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTBname(String querysql)&#123;</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim();</span><br><span class="line">        int sx = tmp.indexOf(&quot; &quot;);</span><br><span class="line">        if(sx == -1)&#123;</span><br><span class="line">            return tmp;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return tmp.substring(0,sx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三-错误解决办法"><a href="#三-错误解决办法" class="headerlink" title="三 错误解决办法"></a>三 错误解决办法</h3><p>1 IDEA会获取不到Hive的数据库和表，将hive-site.xml放入resources文件中。并且将resources设置成配置文件(设置成功文件夹是蓝色否则是灰色)<br>file–&gt;Project Structure–&gt;Modules–&gt;Source<br><img src="/assets/blogImg/1120_1.png" alt="enter description here"><br>2 上面错误处理完后如果报JDO类型的错误，那么检查HIVE_HOME/lib下时候否mysql驱动，如果确定有，那么就是IDEA获取不到。解决方法如下：</p><p>将mysql驱动拷贝到jdk1.8.0_171.jdk/Contents/Home/jre/lib/ext路径下(jdk/jre/lib/ext)<br>在IDEA项目External Libraries下的&lt;1.8&gt;里面添加mysql驱动<br><img src="/assets/blogImg/1120_2.png" alt="enter description here"></p><h3 id="四-注意点"><a href="#四-注意点" class="headerlink" title="四 注意点"></a>四 注意点</h3><p>由于将MongoDB数据表注册成了临时表和Hive表进行了关联，所以要将MongoDB中的id字段设置成索引字段，否则性能会很慢。<br>MongoDB设置索引方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;</span><br></pre></td></tr></table></figure><p></p><p>查看索引：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).getIndexes()</span><br><span class="line">MongoSpark网址：https://docs.mongodb.com/spark-connector/current/java-api/</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)</title>
      <link href="/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
      <url>/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h5 id="1-下载Flink安装包"><a href="#1-下载Flink安装包" class="headerlink" title="1.下载Flink安装包"></a>1.下载Flink安装包</h5><p>flink下载地址</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可</p><p>上传至机器的/opt目录下<br><a id="more"></a></p><h5 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-配置master节点"><a href="#3-配置master节点" class="headerlink" title="3.配置master节点"></a>3.配置master节点</h5><p>选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。</p><p>jobmanager.rpc.address: node1</p><p>(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)</p><p>rest.port: 8088</p><p>本次安装 master节点为node1，因为单机，slave节点也为node1</p><h5 id="4-配置slaves"><a href="#4-配置slaves" class="headerlink" title="4.配置slaves"></a>4.配置slaves</h5><p>将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。</p><h5 id="5-启动flink集群"><a href="#5-启动flink集群" class="headerlink" title="5.启动flink集群"></a>5.启动flink集群</h5><p>bin/start-cluster.sh</p><p>打开 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 查看web页面<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managers代表当前的flink只有一个节点，每个task还有两个slots</p><h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6.测试"></a>6.测试</h5><p><strong>依赖</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p></p><h5 id="7-Socket测试代码"><a href="#7-Socket测试代码" class="headerlink" title="7.Socket测试代码"></a>7.Socket测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)</p><p>在命令行set MAVEN_OPTS= -Xms128m -Xmx512m</p><p>继续执行mvn clean install</p><p>生成FlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>找到打成的jar，并upload，开始上传<br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>运行参数介绍<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个</p><p>发送数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>点开running的job，你可以看见接收的字节数等信息</p><p>到log目录下可以清楚的看见输出<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure><p></p><p>除了可以在界面提交，还可以将jar上传的linux中进行提交任务</p><p>运行flink上传的jar<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure><p></p><p>其他步骤一致。</p><h5 id="8-使用kafka作为source"><a href="#8-使用kafka作为source" class="headerlink" title="8.使用kafka作为source"></a>8.使用kafka作为source</h5><p>加上依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //可以通过正则表达式来匹配合适的topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //配置从最新的地方开始消费</span><br><span class="line">        kafkaSource.setStartFromLatest();        //使用addsource，将kafka的输入转变为datastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-使用mysql作为sink"><a href="#9-使用mysql作为sink" class="headerlink" title="9.使用mysql作为sink"></a>9.使用mysql作为sink</h5><p>flink本身并没有提供datastream输出到mysql，需要我们自己去实现</p><p>首先，导入依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。</p><p>使用这个mysqlsink也非常简单<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure><p></p><h5 id="10-总结"><a href="#10-总结" class="headerlink" title="10.总结"></a>10.总结</h5><p>本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>19生产预警平台项目之sparkdemo.jar运行在yarn上过程</title>
      <link href="/2018/09/28/19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/09/28/19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-将之前打包的jar包上传"><a href="#1-将之前打包的jar包上传" class="headerlink" title="1.将之前打包的jar包上传"></a>1.将之前打包的jar包上传</h4><p>[root@sht-sgmhadoopnn-01 spark]# pwd<br>/root/learnproject/app/spark<br>[root@sht-sgmhadoopnn-01 spark]# rz<br>rz waiting to receive.<br>Starting zmodem transfer. Press Ctrl+C to cancel.<br>Transferring sparkdemo.jar…<br>100% 164113 KB 421 KB/sec 00:06:29 0 Errors</p><h5 id="2-以下是错误"><a href="#2-以下是错误" class="headerlink" title="2.以下是错误"></a>2.以下是错误</h5><h6 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR1: Exception in thread &quot;main&quot; </span><br><span class="line">java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</span><br></pre></td></tr></table></figure><p>IDEA打包的jar包,需要使用zip删除指定文件<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -d sparkdemo.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</span><br></pre></td></tr></table></figure><p></p><h6 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR2: Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/learn/java/main/OnLineLogAnalysis2 : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure><p>yarn环境的jdk版本低于编译jar包的jdk版本(需要一致或者高于;每个节点需要安装jdk,同时修改每个节点的hadoop-env.sh文件的JAVA_HOME参数指向)</p><h6 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ERROR3: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line"> 17/02/15 17:30:35 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line"> java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line">  at org.influxdb.impl.InfluxDBImpl.ping(InfluxDBImpl.java:178)</span><br><span class="line">  at org.influxdb.impl.InfluxDBImpl.version(InfluxDBImpl.java:201)</span><br><span class="line">  at com.learn.java.main.OnLineLogAnalysis2.main(OnLineLogAnalysis2.java:69)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">  at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">  at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)</span><br></pre></td></tr></table></figure><p>抛错信息为NoSuchMethodError，表示 guava可能有多版本，则低版本<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# pwd</span><br><span class="line"> /root/learnproject/app</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# ll</span><br><span class="line"> total 470876</span><br><span class="line"> -rw-r--r--  1 root root   7509833 Jan 16 22:11 AdminLTE.zip</span><br><span class="line"> drwxr-xr-x 12 root root      4096 Feb 14 11:21 hadoop</span><br><span class="line"> -rw-r--r--  1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz</span><br><span class="line"> drwxr-xr-x  7 root root      4096 Feb  7 11:16 kafka-manager-1.3.2.1</span><br><span class="line"> -rw-r--r--  1 root root  59682993 Dec 26 14:44 kafka-manager-1.3.2.1.zip</span><br><span class="line"> drwxr-xr-x  2 root root      4096 Jan  7 16:21 kafkaoffsetmonitor</span><br><span class="line"> drwxr-xr-x  2  777 root      4096 Feb 14 14:48 pid</span><br><span class="line"> drwxrwxr-x  4 1000 1000      4096 Oct 29 01:46 sbt</span><br><span class="line"> -rw-r--r--  1 root root   1049906 Dec 25 21:29 sbt-0.13.13.tgz</span><br><span class="line"> drwxrwxr-x  6 root root      4096 Mar  4  2016 scala</span><br><span class="line"> -rw-r--r--  1 root root  28678231 Mar  4  2016 scala-2.11.8.tgz</span><br><span class="line"> drwxr-xr-x 13 root root      4096 Feb 15 17:01 spark</span><br><span class="line"> -rw-r--r--  1 root root 187426587 Nov 12 06:54 spark-2.0.2-bin-hadoop2.7.tgz</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# </span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# find ./ -name *guava*</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/yarn/lib/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# mv ./spark/jars/guava-14.0.1.jar ./spark/jars/guava-14.0.1.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./spark/jars/</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/common/lib/</span><br></pre></td></tr></table></figure><p></p><h5 id="3-后台提交jar包运行"><a href="#3-后台提交jar包运行" class="headerlink" title="3.后台提交jar包运行"></a>3.后台提交jar包运行</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# nohup /root/learnproject/app/spark/bin/spark-submit \</span><br><span class="line">&gt; --name onlineLogsAnalysis \</span><br><span class="line">&gt; --master yarn    \</span><br><span class="line">&gt; --deploy-mode cluster     \</span><br><span class="line">&gt; --conf &quot;spark.scheduler.mode=FAIR&quot; \</span><br><span class="line">&gt; --conf &quot;spark.sql.codegen=true&quot; \</span><br><span class="line">&gt; --driver-memory 2G \</span><br><span class="line">&gt; --executor-memory 2G \</span><br><span class="line">&gt; --executor-cores 1 \</span><br><span class="line">&gt; --num-executors 3 \</span><br><span class="line">&gt; --class com.learn.java.main.OnLineLogAnalysis2     \</span><br><span class="line">&gt; /root/learnproject/app/spark/sparkdemo.jar &amp;</span><br><span class="line">[1] 22926</span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# nohup: ignoring input and appending output to `nohup.out&apos;</span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# tail -f nohup.out</span><br></pre></td></tr></table></figure><h5 id="4-yarn-web界面查看运行log"><a href="#4-yarn-web界面查看运行log" class="headerlink" title="4.yarn web界面查看运行log"></a>4.yarn web界面查看运行log</h5><p><img src="/assets/blogImg/928_1.jpg" alt="enter description here"><br>ApplicationMaster：打开为spark history server web界面</p><p>logs： 查看stderr 和 stdout日志 (system.out.println方法输出到stdout日志中)<br><img src="/assets/blogImg/928_2.jpg" alt="enter description here"><br><img src="/assets/blogImg/928_3.jpg" alt="enter description here"><br><img src="/assets/blogImg/928_4.jpg" alt="enter description here"></p><h5 id="5-查看spark-history-web"><a href="#5-查看spark-history-web" class="headerlink" title="5.查看spark history web"></a>5.查看spark history web</h5><p><img src="/assets/blogImg/928_5.jpg" alt="enter description here"></p><h5 id="6-查看DashBoard-实时可视化"><a href="#6-查看DashBoard-实时可视化" class="headerlink" title="6.查看DashBoard ,实时可视化"></a>6.查看DashBoard ,实时可视化</h5><p><img src="/assets/blogImg/928_6.jpg" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>18Spark on Yarn配置日志Web UI(HistoryServer服务)</title>
      <link href="/2018/09/26/18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)/"/>
      <url>/2018/09/26/18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 ~]# cd /root/learnproject/app/spark/conf</span><br><span class="line">[root@sht-sgmhadoopnn-01 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><h4 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /</span><br><span class="line"> Found 3 items</span><br><span class="line"> drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line"> drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line"> drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line"> You have new mail in /var/spool/mail/root</span><br><span class="line"> [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /spark</span><br><span class="line"> Found 1 items</span><br><span class="line"> drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@sht-sgmhadoopnn-01 conf]# hdfs dfs -mkdir /spark/historylog</span><br><span class="line">#在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息</span><br></pre></td></tr></table></figure><h4 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir               hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br><span class="line">#spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建</span><br><span class="line">#spark.yarn.historyServer.address : Spark history server的地址(不加http://).</span><br><span class="line">这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</span><br></pre></td></tr></table></figure><h4 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 conf]# vi spark-env.sh</span><br><span class="line"> #!/usr/bin/env bash</span><br><span class="line"> </span><br><span class="line"> export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line"> export JAVA_HOME=/usr/java/jdk1.8.0_111</span><br><span class="line"> export SPARK_MASTER_IP=172.16.101.55</span><br><span class="line"> export SPARK_WORKER_MEMORY=1g</span><br><span class="line"> export SPARK_PID_DIR=/root/learnproject/app/pid</span><br><span class="line"> export HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoop</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h4 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 spark]# ./sbin/start-history-server.sh </span><br><span class="line"> starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out</span><br><span class="line"> [root@sht-sgmhadoopnn-01 ~]# jps</span><br><span class="line"> 28905 HistoryServer</span><br><span class="line"> 30407 ProdServerStart</span><br><span class="line"> 30373 ResourceManager</span><br><span class="line"> 30957 NameNode</span><br><span class="line"> 16949 Jps</span><br><span class="line"> 30280 DFSZKFailoverController</span><br><span class="line">31445 JobHistoryServer</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# ps -ef|grep spark</span><br><span class="line">root     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java          </span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>17生产预警平台项目之使用IDEA将工程Build成jar包</title>
      <link href="/2018/09/25/17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85/"/>
      <url>/2018/09/25/17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-File–-gt-Project-Structure"><a href="#1-File–-gt-Project-Structure" class="headerlink" title="1.File–&gt;Project Structure"></a>1.File–&gt;Project Structure</h4><p><img src="/assets/blogImg/0925_1.png" alt="enter description here"></p><h4 id="2-Artifacts–-gt-–-gt-JAR–-gt-From-modules-with-dependencies"><a href="#2-Artifacts–-gt-–-gt-JAR–-gt-From-modules-with-dependencies" class="headerlink" title="2.Artifacts–&gt;+–&gt;JAR–&gt;From modules with dependencies"></a>2.Artifacts–&gt;+–&gt;JAR–&gt;From modules with dependencies</h4><p><img src="/assets/blogImg/0925_2.png" alt="enter description here"></p><h4 id="3-单击…-–-gt-选择OnLineLogAnalysis2"><a href="#3-单击…-–-gt-选择OnLineLogAnalysis2" class="headerlink" title="3. 单击… –&gt;选择OnLineLogAnalysis2"></a>3. 单击… –&gt;选择OnLineLogAnalysis2</h4><p><img src="/assets/blogImg/0925_3.png" alt="enter description here"><br><img src="/assets/blogImg/0925_4.png" alt="enter description here"></p><h4 id="4-选择项目的根目录"><a href="#4-选择项目的根目录" class="headerlink" title="4.选择项目的根目录"></a>4.选择项目的根目录</h4><p><img src="/assets/blogImg/0925_5.png" alt="enter description here"></p><h4 id="5-修改Name–-gt-选择输出目录–-gt-选择Output-directory–-gt-Apply–-gt-OK"><a href="#5-修改Name–-gt-选择输出目录–-gt-选择Output-directory–-gt-Apply–-gt-OK" class="headerlink" title="5.修改Name–&gt;选择输出目录–&gt;选择Output directory–&gt;Apply–&gt;OK"></a>5.修改Name–&gt;选择输出目录–&gt;选择Output directory–&gt;Apply–&gt;OK</h4><p><img src="/assets/blogImg/0925_6.png" alt="enter description here"></p><h4 id="6-Build–-gt-Build-Artifacts–-gt-Build"><a href="#6-Build–-gt-Build-Artifacts–-gt-Build" class="headerlink" title="6.Build–&gt;Build Artifacts–&gt;Build"></a>6.Build–&gt;Build Artifacts–&gt;Build</h4><p><img src="/assets/blogImg/0925_7.png" alt="enter description here"><br><img src="/assets/blogImg/0925_8.png" alt="enter description here"><br><img src="/assets/blogImg/0925_9.png" alt="enter description here"></p><p>===================================<br>说明:<br>1.打包方式很多，大家自行google.<br>2.由于我是引用influxdb的源码包,需要引入许多依赖jar包,所以我需要将相关依赖jar包全部打包到本程序的jar包,故该jar包大概160M。<br>(当然也可以只需要打本程序的jar包，只不过需要事先将相关的所有或者部分依赖jar包，前提上传到集群，然后spark-submit使用–jars引用即可)</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>16生产预警平台项目之grafana-4.1.1 Install和新建日志分析的DashBoard</title>
      <link href="/2018/09/19/16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard/"/>
      <url>/2018/09/19/16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h4><p>wget <a href="https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz" target="_blank" rel="noopener">https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz</a></p><h4 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h4><p>tar -zxvf grafana-4.1.1-1484211277.linux-x64.tar.gz</p><h4 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3.配置文件"></a>3.配置文件</h4><p>cd grafana-4.1.1-1484211277<br>cp conf/sample.ini conf/custom.ini</p><p>#make changes to conf/custom.ini then start grafana-server</p><h4 id="4-后台启动"><a href="#4-后台启动" class="headerlink" title="4.后台启动"></a>4.后台启动</h4><p>./bin/grafana-server &amp;</p><h4 id="5-打开web"><a href="#5-打开web" class="headerlink" title="5.打开web"></a>5.打开web</h4><p><a href="http://172.16.101.66:3000/" target="_blank" rel="noopener">http://172.16.101.66:3000/</a> admin/admin</p><h4 id="6-配置数据源influxdb"><a href="#6-配置数据源influxdb" class="headerlink" title="6.配置数据源influxdb"></a>6.配置数据源influxdb</h4><p><img src="/assets/blogImg/919_1.png" alt="enter description here"><br>还要填写Database 为 online_log_analysis</p><h4 id="7-IDEA本机运行OnLineLogAanlysis2-class，实时计算存储到influxdb"><a href="#7-IDEA本机运行OnLineLogAanlysis2-class，实时计算存储到influxdb" class="headerlink" title="7.IDEA本机运行OnLineLogAanlysis2.class，实时计算存储到influxdb"></a>7.IDEA本机运行OnLineLogAanlysis2.class，实时计算存储到influxdb</h4><h4 id="8-新建dashboard和-cdh-hdfs-warn曲线图"><a href="#8-新建dashboard和-cdh-hdfs-warn曲线图" class="headerlink" title="8.新建dashboard和 cdh_hdfs_warn曲线图"></a>8.新建dashboard和 cdh_hdfs_warn曲线图</h4><p><img src="/assets/blogImg/919_2.png" alt="enter description here"><br>参考:<br><a href="http://grafana.org/download/" target="_blank" rel="noopener">http://grafana.org/download/</a><br><a href="http://docs.grafana.org/" target="_blank" rel="noopener">http://docs.grafana.org/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15生产预警平台项目之基于Spark Streaming+Saprk SQL开发OnLineLogAanlysis2</title>
      <link href="/2018/09/18/15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming+Saprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2/"/>
      <url>/2018/09/18/15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming+Saprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h5 id="1-influxdb创建database"><a href="#1-influxdb创建database" class="headerlink" title="1.influxdb创建database"></a>1.influxdb创建database</h5><p>[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339<br>Connected to <a href="http://localhost:8086" target="_blank" rel="noopener">http://localhost:8086</a> version 1.2.0<br>InfluxDB shell version: 1.2.0</p><blockquote><p>create database online_log_analysis</p></blockquote><h5 id="2-导入源代码"><a href="#2-导入源代码" class="headerlink" title="2.导入源代码"></a>2.导入源代码</h5><p>项目中原本想将 influxdb-java <a href="https://github.com/influxdata/influxdb-java的InfluxDBTest.java" target="_blank" rel="noopener">https://github.com/influxdata/influxdb-java的InfluxDBTest.java</a> 文件的加到项目中，所以必须要引入 influxdb-java 的包；<br>但是由于GitHub的上的class文件的某些方法，是版本是2.6，而maven中的最高也就2.5版本，所以将Github的源代码下载导入到idea中，编译导出2.6.jar包；<br>可是 引入2.6jar包，其在InfluxDBTest.class文件的 无法import org.influxdb（百度谷歌很长时间，尝试很多方法不行）。<br>最后索性将 influx-java的源代码全部添加到项目中即可，如下图所示。</p><h5 id="3-运行OnLineLogAanlysis2-java"><a href="#3-运行OnLineLogAanlysis2-java" class="headerlink" title="3.运行OnLineLogAanlysis2.java"></a>3.运行OnLineLogAanlysis2.java</h5><p><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.java</a><br><img src="/assets/blogImg/0918_1.png" alt="enter description here"><br><strong>比如 logtype_count,host_service_logtype=hadoopnn-01_namenode_WARN</strong> count=12<br>logtype_count 是表<br>host_service_logtype=hadoopnn-01_namenode_WARN 是 tag–标签，在InfluxDB中，tag是一个非常重要的部分，表名+tag一起作为数据库的索引，是“key-value”的形式。<br>count=12 是 field–数据，field主要是用来存放数据的部分，也是“key-value”的形式。<br>tag、field 中间是要有空格的</p><h5 id="4-influxdb查询数据"><a href="#4-influxdb查询数据" class="headerlink" title="4.influxdb查询数据"></a>4.influxdb查询数据</h5><p><img src="/assets/blogImg/0918_2.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>13生产预警平台项目之舍弃Redis+echarts3,选择InfluxDB+Grafana</title>
      <link href="/2018/09/17/13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis+echarts3,%E9%80%89%E6%8B%A9InfluxDB+Grafana/"/>
      <url>/2018/09/17/13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis+echarts3,%E9%80%89%E6%8B%A9InfluxDB+Grafana/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-最初选择Redis作为存储，是主要有4个原因"><a href="#1-最初选择Redis作为存储，是主要有4个原因" class="headerlink" title="1.最初选择Redis作为存储，是主要有4个原因:"></a>1.最初选择Redis作为存储，是主要有4个原因:</h4><p>a.redis是一个key-value的存储系统，数据是存储在内存中，读写性能很高；<br>b.支持多种数据类型，如set,zset,list,hash,string；<br>c.key过期策略；<br>d.最主要是网上的博客全是sparkstreaming+redis，都互相模仿；<br>至于缺点，当时还没考虑到。</p><h4 id="2-然后开始添加CDHRolelog-class类和将redis模块加入代码中，"><a href="#2-然后开始添加CDHRolelog-class类和将redis模块加入代码中，" class="headerlink" title="2.然后开始添加CDHRolelog.class类和将redis模块加入代码中，"></a>2.然后开始添加CDHRolelog.class类和将redis模块加入代码中，</h4><p>使计算结果（本次使用spark streaming+spark sql，之前仅仅是spark streaming，具体看代码）存储到redis中，当然存储到redis中，有两种存储格式。</p><h5 id="2-1-key为机器名称-服务名称-日志级别拼接的字符串，"><a href="#2-1-key为机器名称-服务名称-日志级别拼接的字符串，" class="headerlink" title="2.1 key为机器名称,服务名称,日志级别拼接的字符串，"></a>2.1 key为机器名称,服务名称,日志级别拼接的字符串，</h5><p><strong>如hadoopnn-01_namenode_WARN，</strong><br>value为数据类型list，其存储为json格式的 [{“timeStamp”: “2017-02-09 17:16:14.249”,”hostName”: “hadoopnn-01”,”serviceName”: “namenode”,”logType”:”WARN”,”count”:”12” }]<br>代码url,下载导入idea,运行即可:<br><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java</a><br><img src="/assets/blogImg/917_1.png" alt="enter description here"></p><h4 id="2-2-key为timestamp"><a href="#2-2-key为timestamp" class="headerlink" title="2.2 key为timestamp"></a>2.2 key为timestamp</h4><p><strong>如 2017-02-09 18:09:02.462,</strong><br>value 为 [ {“host_service_logtype”: “hadoopnn-01_namenode_INFO”,”count”:”110” }, {“host_service_logtype”: “hadoopnn-01_namenode_DEBUG”,”count”:”678” }, {“host_service_logtype”: “hadoopnn-01_namenode_WARN”,”count”:”12” }]<br>代码url,下载导入idea,运行即可:<br><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java</a><br><img src="/assets/blogImg/917_2.png" alt="enter description here"></p><h4 id="3-做可视化这块，我们选择adminLTE-flask-echarts3-计划和编程开发尝试去从redis实时读取数据，动态绘制图表；"><a href="#3-做可视化这块，我们选择adminLTE-flask-echarts3-计划和编程开发尝试去从redis实时读取数据，动态绘制图表；" class="headerlink" title="3.做可视化这块，我们选择adminLTE+flask+echarts3, 计划和编程开发尝试去从redis实时读取数据，动态绘制图表；"></a>3.做可视化这块，我们选择adminLTE+flask+echarts3, 计划和编程开发尝试去从redis实时读取数据，动态绘制图表；</h4><p>后来开发调研大概1周，最终2.1 和2.2方法的存储格式都不能有效适合我们，进行开发可视化Dashboard，<br>所以我们最终调研采取InfluxDB+Grafana来做存储和可视化展示及预警。</p><h4 id="4-InfluxDB是时序数据库"><a href="#4-InfluxDB是时序数据库" class="headerlink" title="4.InfluxDB是时序数据库"></a>4.InfluxDB是时序数据库</h4><p><a href="https://docs.influxdata.com/influxdb/v1.2/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/</a></p><h4 id="5-Grafana是可视化组件"><a href="#5-Grafana是可视化组件" class="headerlink" title="5.Grafana是可视化组件"></a>5.Grafana是可视化组件</h4><p><a href="http://grafana.org/" target="_blank" rel="noopener">http://grafana.org/</a><br><a href="https://github.com/grafana/grafana" target="_blank" rel="noopener">https://github.com/grafana/grafana</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>14生产预警平台项目之influxdb-1.2.0 Install和概念，语法等学习</title>
      <link href="/2018/09/17/14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0/"/>
      <url>/2018/09/17/14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-下载rpm"><a href="#1-下载rpm" class="headerlink" title="1.下载rpm"></a>1.下载rpm</h4><p><a href="https://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpm" target="_blank" rel="noopener">https://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpm</a><br>我选择用window7 浏览器下载，然后rz上传到linux机器上</p><h4 id="2-安装"><a href="#2-安装" class="headerlink" title="2.安装"></a>2.安装</h4><p>yum install influxdb-1.2.0.x86_64.rpm</p><h4 id="3-启动"><a href="#3-启动" class="headerlink" title="3.启动"></a>3.启动</h4><p>service influxdb start</p><p>参考:<br><a href="https://docs.influxdata.com/influxdb/v1.2/introduction/installation/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/introduction/installation/</a><br>编译安装:<br><a href="https://anomaly.io/compile-influxdb/" target="_blank" rel="noopener">https://anomaly.io/compile-influxdb/</a></p><h4 id="4-进入"><a href="#4-进入" class="headerlink" title="4.进入"></a>4.进入</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339</span><br><span class="line">Connected to http://localhost:8086 version 1.2.0</span><br><span class="line">InfluxDB shell version: 1.2.0</span><br></pre></td></tr></table></figure><p>语法参考:<br><a href="https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/</a></p><p>学习url:<br><a href="http://www.linuxdaxue.com/influxdb-study-series-manual.html" target="_blank" rel="noopener">http://www.linuxdaxue.com/influxdb-study-series-manual.html</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12生产预警平台项目之RedisLive监控工具的详细安装</title>
      <link href="/2018/09/14/12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85/"/>
      <url>/2018/09/14/12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GitHub: https://github.com/nkrode/RedisLive</span><br></pre></td></tr></table></figure><h4 id="1-安装python2-7-5-和pip"><a href="#1-安装python2-7-5-和pip" class="headerlink" title="1.安装python2.7.5 和pip"></a>1.安装python2.7.5 和pip</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://blog.itpub.net/30089851/viewspace-2132450/</span><br></pre></td></tr></table></figure><h4 id="2-下载RedisLive"><a href="#2-下载RedisLive" class="headerlink" title="2.下载RedisLive"></a>2.下载RedisLive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# wget https://github.com/nkrode/RedisLive/archive/master.zip</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# unzip master </span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# mv RedisLive-master RedisLive</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# cd RedisLive/</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 design</span><br><span class="line">-rw-r--r-- 1 root root 1067 Aug 20  2015 MIT-LICENSE.txt</span><br><span class="line">-rw-r--r-- 1 root root  902 Aug 20  2015 README.md</span><br><span class="line">-rw-r--r-- 1 root root   58 Aug 20  2015 requirements.txt</span><br><span class="line">drwxr-xr-x 7 root root 4096 Aug 20  2015 src</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]#</span><br></pre></td></tr></table></figure><h4 id="3-查看版本要求-刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程"><a href="#3-查看版本要求-刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程" class="headerlink" title="3.查看版本要求(刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程)"></a>3.查看版本要求(刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cat requirements.txt</span><br><span class="line">argparse==1.2.1</span><br><span class="line">python-dateutil==1.5</span><br><span class="line">redis</span><br><span class="line">tornado==2.1.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cd ../</span><br></pre></td></tr></table></figure><h4 id="4-pip安装环境要求"><a href="#4-pip安装环境要求" class="headerlink" title="4.pip安装环境要求"></a>4.pip安装环境要求</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install python-dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install argparse</span><br></pre></td></tr></table></figure><h4 id="5-进入-root-learnproject-app-RedisLive-src目录-配置redis-live-conf文件"><a href="#5-进入-root-learnproject-app-RedisLive-src目录-配置redis-live-conf文件" class="headerlink" title="5.进入 /root/learnproject/app/RedisLive/src目录,配置redis-live.conf文件"></a>5.进入 /root/learnproject/app/RedisLive/src目录,配置redis-live.conf文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# cd -</span><br><span class="line">/root/learnproject/app/RedisLive</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cd src</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# ll</span><br><span class="line">total 40</span><br><span class="line">drwxr-xr-x 4 root root 4096 Aug 20  2015 api</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 dataprovider</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 db</span><br><span class="line">-rw-r--r-- 1 root root    0 Aug 20  2015 __init__.py</span><br><span class="line">-rw-r--r-- 1 root root  381 Aug 20  2015 redis-live.conf.example</span><br><span class="line">-rwxr-xr-x 1 root root 1343 Aug 20  2015 redis-live.py</span><br><span class="line">-rwxr-xr-x 1 root root 9800 Aug 20  2015 redis-monitor.py</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 util</span><br><span class="line">drwxr-xr-x 4 root root 4096 Aug 20  2015 www</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp redis-live.conf.example redis-live.conf</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# vi redis-live.conf</span><br><span class="line">&#123;</span><br><span class="line">        &quot;RedisServers&quot;:</span><br><span class="line">        [</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;server&quot;: &quot;172.16.101.66&quot;,</span><br><span class="line">                        &quot;port&quot; : 6379</span><br><span class="line">                &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;DataStoreType&quot; : &quot;redis&quot;,</span><br><span class="line">        &quot;RedisStatsServer&quot;:</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;server&quot; : &quot;172.16.101.66&quot;,</span><br><span class="line">          &quot;port&quot; : 6379</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-第一次尝试启动redis-monitor-py抛错-sqlite3"><a href="#6-第一次尝试启动redis-monitor-py抛错-sqlite3" class="headerlink" title="6.第一次尝试启动redis-monitor.py抛错 _sqlite3"></a>6.第一次尝试启动redis-monitor.py抛错 _sqlite3</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">ImportError: No module named _sqlite3</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite-devel</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# find / -name _sqlite3.so</span><br><span class="line">/usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so</span><br><span class="line">/usr/local/Python-2.7.5/build/lib.linux-x86_64-2.7/_sqlite3.so</span><br><span class="line">/usr/lib64/python2.6/lib-dynload/_sqlite3.so</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# cp /usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# python</span><br><span class="line">Python 2.7.5 (default, Sep 17 2016, 15:34:31) </span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import sqlite3</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">参考: http://ju.outofmemory.cn/entry/97658</span><br></pre></td></tr></table></figure><h4 id="7-第二次尝试启动redis-monitor-py抛错-redis"><a href="#7-第二次尝试启动redis-monitor-py抛错-redis" class="headerlink" title="7.第二次尝试启动redis-monitor.py抛错 redis"></a>7.第二次尝试启动redis-monitor.py抛错 redis</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">ImportError: No module named redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name redis</span><br><span class="line">/etc/rc.d/init.d/redis</span><br><span class="line">/root/learnproject/app/redis</span><br><span class="line">/root/learnproject/app/redis-monitor/src/main/java/sun/redis</span><br><span class="line">/root/learnproject/app/redis-monitor/src/test/java/sun/redis</span><br><span class="line">/usr/local/redis</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r  /usr/local/python27/lib/python2.7/site-packages/redis  /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# python </span><br><span class="line">Python 2.7.5 (default, Sep 17 2016, 15:34:31) </span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import redis</span><br></pre></td></tr></table></figure><h4 id="8-第三次尝试启动redis-monitor-py，成功；按ctrl-c中断掉"><a href="#8-第三次尝试启动redis-monitor-py，成功；按ctrl-c中断掉" class="headerlink" title="8.第三次尝试启动redis-monitor.py，成功；按ctrl+c中断掉"></a>8.第三次尝试启动redis-monitor.py，成功；按ctrl+c中断掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">^Cshutting down...</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]#</span><br></pre></td></tr></table></figure><h4 id="9-尝试第一次启动redis-live-py-，tornado-ioloop"><a href="#9-尝试第一次启动redis-live-py-，tornado-ioloop" class="headerlink" title="9.尝试第一次启动redis-live.py ，tornado.ioloop"></a>9.尝试第一次启动redis-live.py ，tornado.ioloop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 3, in &lt;module&gt;</span><br><span class="line">    import tornado.ioloop</span><br><span class="line">ImportError: No module named tornado.ioloop</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name  tornado</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/tornado  /usr/local/lib/python2.7/lib-dynload/</span><br></pre></td></tr></table></figure><h4 id="10-尝试第二次启动redis-live-py-，singledispatch"><a href="#10-尝试第二次启动redis-live-py-，singledispatch" class="headerlink" title="10.尝试第二次启动redis-live.py ，singledispatch"></a>10.尝试第二次启动redis-live.py ，singledispatch</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    import tornado.web</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/web.py&quot;, line 84, in &lt;module&gt;</span><br><span class="line">    from tornado import gen</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/gen.py&quot;, line 98, in &lt;module&gt;</span><br><span class="line">    from singledispatch import singledispatch  # backport</span><br><span class="line">ImportError: No module named singledispatch</span><br></pre></td></tr></table></figure><p>这个 singledispatch 错误，其实就是在tornado里的，谷歌和思考过后，怀疑是版本问题，于是果断卸载tornado<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# rm -rf  /usr/local/lib/python2.7/lib-dynload/tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">假如find有的话 ，就要手工删除掉</span><br></pre></td></tr></table></figure><p></p><h4 id="11-于是想想其他也是要卸载掉"><a href="#11-于是想想其他也是要卸载掉" class="headerlink" title="11.于是想想其他也是要卸载掉"></a>11.于是想想其他也是要卸载掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall argparse</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall python-dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name argparse</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name python-dateutil</span><br><span class="line">假如find有的话 ，就要手工删除掉</span><br></pre></td></tr></table></figure><h4 id="12-关键一步-根据step3的指定版本来安装"><a href="#12-关键一步-根据step3的指定版本来安装" class="headerlink" title="12.关键一步: 根据step3的指定版本来安装"></a>12.关键一步: 根据step3的指定版本来安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v tornado==2.1.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v argparse==1.2.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v python-dateutil==1.5</span><br></pre></td></tr></table></figure><h4 id="13-再次尝试启动redis-live-py-，抛错dateutil-parser"><a href="#13-再次尝试启动redis-live-py-，抛错dateutil-parser" class="headerlink" title="13.再次尝试启动redis-live.py ，抛错dateutil.parser"></a>13.再次尝试启动redis-live.py ，抛错dateutil.parser</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 10, in &lt;module&gt;</span><br><span class="line">    from api.controller.ServerListController import ServerListController</span><br><span class="line">  File &quot;/root/learnproject/app/RedisLive/src/api/controller/ServerListController.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from BaseController import BaseController</span><br><span class="line">  File &quot;/root/learnproject/app/RedisLive/src/api/controller/BaseController.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    import dateutil.parser</span><br><span class="line">ImportError: No module named dateutil.parser</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name dateutil</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/dateutil  /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">You have mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure><h4 id="14-再在尝试启动redis-live-py-，成功了，然后按ctrl-c中断掉"><a href="#14-再在尝试启动redis-live-py-，成功了，然后按ctrl-c中断掉" class="headerlink" title="14.再在尝试启动redis-live.py ，成功了，然后按ctrl+c中断掉"></a>14.再在尝试启动redis-live.py ，成功了，然后按ctrl+c中断掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">^CTraceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 36, in &lt;module&gt;</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/ioloop.py&quot;, line 283, in start</span><br><span class="line">    event_pairs = self._impl.poll(poll_timeout)</span><br><span class="line">KeyboardInterrupt</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]#</span><br></pre></td></tr></table></figure><h4 id="15-启动"><a href="#15-启动" class="headerlink" title="15.启动"></a>15.启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py  &amp;</span><br></pre></td></tr></table></figure><p>打开web界面</p><p><a href="http://172.16.101.66:8888/index.html" target="_blank" rel="noopener">http://172.16.101.66:8888/index.html</a></p><p><img src="/assets/blogImg/914_1.png" alt="enter description here"></p><h4 id="16-总结"><a href="#16-总结" class="headerlink" title="16.总结"></a>16.总结</h4><p><strong>a.安装 python2.7+pip</strong></p><p><strong>b.pip指定版本去安装那几个组件</strong></p><h4 id="17-说明"><a href="#17-说明" class="headerlink" title="17.说明:"></a>17.说明:</h4><p><strong>redis live 实时redis监控面板</strong></p><p>可以同时监控多个redis实例 , 包括 内存使用 、分db显示的key数、客户端连接数、 命令处理数、 系统运行时间 , 以及各种直观的折线图柱状图.<br>缺点是使用了monitor 命令监控 , 对性能有影响 ,最好不要长时间启动 .</p><p><strong>redis-monitor.py:</strong></p><p>用来调用redis的monitor命令来收集redis的命令来进行统计</p><p><strong>redis-live.py:</strong></p><p>启动web服务</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11生产预警平台项目之redis-3.2.5 install(单节点)</title>
      <link href="/2018/09/12/11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)/"/>
      <url>/2018/09/12/11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1-安装jdk1-8"><a href="#1-安装jdk1-8" class="headerlink" title="1.安装jdk1.8"></a>1.安装jdk1.8</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 ~]# cd /usr/java/</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot;  http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# tar -zxvf jdk-8u111-linux-x64.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_111</span><br><span class="line">export path=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# java -version</span><br><span class="line">java version &quot;1.8.0_111&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_111-b14)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]#</span><br></pre></td></tr></table></figure><h4 id="2-安装-redis-3-2-5"><a href="#2-安装-redis-3-2-5" class="headerlink" title="2.安装 redis 3.2.5"></a>2.安装 redis 3.2.5</h4><h5 id="2-1-安装编绎所需包gcc-tcl"><a href="#2-1-安装编绎所需包gcc-tcl" class="headerlink" title="2.1 安装编绎所需包gcc,tcl"></a>2.1 安装编绎所需包gcc,tcl</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# yum install gcc</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# yum install tcl</span><br></pre></td></tr></table></figure><h5 id="2-2-下载redis-3-2-5"><a href="#2-2-下载redis-3-2-5" class="headerlink" title="2.2 下载redis-3.2.5"></a>2.2 下载redis-3.2.5</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# wget http://download.redis.io/releases/redis-3.2.5.tar.gz</span><br><span class="line">--2016-11-12 20:16:40--  http://download.redis.io/releases/redis-3.2.5.tar.gz</span><br><span class="line">Resolving download.redis.io (download.redis.io)... 109.74.203.151</span><br><span class="line">Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1544040 (1.5M) [application/x-gzip]</span><br><span class="line">Saving to: ‘redis-3.2.5.tar.gz’</span><br><span class="line">100%[==========================================================================================================================&gt;] 1,544,040    221KB/s   in 6.8s  </span><br><span class="line">2016-11-12 20:16:47 (221 KB/s) - ‘redis-3.2.5.tar.gz’ saved [1544040/1544040]</span><br></pre></td></tr></table></figure><h5 id="2-3-安装redis"><a href="#2-3-安装redis" class="headerlink" title="2.3 安装redis"></a>2.3 安装redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# mkdir /usr/local/redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# tar xzvf redis-3.2.5.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# cd redis-3.2.5</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# make PREFIX=/usr/local/redis install</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# cd ../</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# ll /usr/local/redis/bin/</span><br><span class="line">total 15056</span><br><span class="line">-rwxr-xr-x 1 root root 2431728 Nov 12 20:45 redis-benchmark</span><br><span class="line">-rwxr-xr-x 1 root root   25165 Nov 12 20:45 redis-check-aof</span><br><span class="line">-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-check-rdb</span><br><span class="line">-rwxr-xr-x 1 root root 2584443 Nov 12 20:45 redis-cli</span><br><span class="line">lrwxrwxrwx 1 root root      12 Nov 12 20:45 redis-sentinel -&gt; redis-server</span><br><span class="line">-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-server</span><br></pre></td></tr></table></figure><h5 id="2-4-配置redis为服务"><a href="#2-4-配置redis为服务" class="headerlink" title="2.4 配置redis为服务"></a>2.4 配置redis为服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@server redis-3.2.5]# cp utils/redis_init_script /etc/rc.d/init.d/redis</span><br><span class="line">[root@server redis-3.2.5]# vi /etc/rc.d/init.d/redis </span><br><span class="line">在第二行添加：#chkconfig: 2345 80 90</span><br><span class="line">EXEC=/usr/local/bin/redis-server  修改成 EXEC=/usr/local/redis/bin/redis-server</span><br><span class="line">CLIEXEC=/usr/local/bin/redis-cli  修改成 CLIEXEC=/usr/local/redis/bin/redis-cli</span><br><span class="line">CONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot; 修改成 CONF=&quot;/usr/local/redis/conf/$&#123;REDISPORT&#125;.conf&quot;</span><br><span class="line">$EXEC $CONF 修改成  $EXEC $CONF &amp;</span><br><span class="line">[root@server redis-3.2.5]# mkdir /usr/local/redis/conf/</span><br><span class="line">[root@server redis-3.2.5]# chkconfig --add redis</span><br><span class="line">[root@server redis-3.2.5]# cp redis.conf /usr/local/redis/conf/6379.conf </span><br><span class="line">[root@server redis-3.2.5]# vi /usr/local/redis/conf/6379.conf </span><br><span class="line">daemonize yes</span><br><span class="line">pidfile /var/run/redis_6379.pid</span><br><span class="line">bind 172.16.101.66</span><br></pre></td></tr></table></figure><h5 id="2-5-启动redis"><a href="#2-5-启动redis" class="headerlink" title="2.5 启动redis"></a>2.5 启动redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@server redis-3.2.5]# cd ../redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis start</span><br><span class="line">Starting Redis server...</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# netstat -tnlp|grep redis</span><br><span class="line">tcp        0      0 172.16.100.79:6379      0.0.0.0:*               LISTEN      30032/redis-server  </span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]#</span><br></pre></td></tr></table></figure><h5 id="2-6-添加环境变量"><a href="#2-6-添加环境变量" class="headerlink" title="2.6 添加环境变量"></a>2.6 添加环境变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 redis]# vi /etc/profile</span><br><span class="line">export REDIS_HOME=/usr/local/redis</span><br><span class="line">export PATH=$REDIS_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# which redis-cli</span><br><span class="line">/usr/local/redis/bin/redis-cli</span><br></pre></td></tr></table></figure><h5 id="2-7-测试-和-设置密码-本次实验未设置密码"><a href="#2-7-测试-和-设置密码-本次实验未设置密码" class="headerlink" title="2.7 测试 和 设置密码(本次实验未设置密码)"></a>2.7 测试 和 设置密码(本次实验未设置密码)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# red</span><br><span class="line">is-cli -h sht-sgmhadoopdn-04</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; </span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set testkey testvalue </span><br><span class="line">OK</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get test</span><br><span class="line">(nil)</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get testkey</span><br><span class="line">&quot;testvalue&quot;</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt;</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# vi /usr/local/redis/conf/6379.conf </span><br><span class="line">/*添加一个验证密码*/</span><br><span class="line">requirepass 123456</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis stop</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis start</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set key ss</span><br><span class="line">(error) NOAUTH Authentication required.  </span><br><span class="line">[root@server redis-3.2.5]# redis-cli -h sht-sgmhadoopdn-04 -a 123456</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set a b</span><br><span class="line">OK</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get a</span><br><span class="line">&quot;b&quot;</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; exit;</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10生产预警平台项目之基于Spark Streaming开发OnLineLogAanlysis1</title>
      <link href="/2018/09/11/10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1/"/>
      <url>/2018/09/11/10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-GitHub"><a href="#1-GitHub" class="headerlink" title="1.GitHub"></a>1.GitHub</h4><p><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java</a></p><h4 id="2-使用IDEA-本地运行测试（未打jar包）"><a href="#2-使用IDEA-本地运行测试（未打jar包）" class="headerlink" title="2.使用IDEA 本地运行测试（未打jar包）"></a>2.使用IDEA 本地运行测试（未打jar包）</h4><p><img src="/assets/blogImg/0911.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09生产预警平台项目之基于Spark Streaming Direct方式的WordCount最详细案例(java版)</title>
      <link href="/2018/09/10/09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)/"/>
      <url>/2018/09/10/09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-前提"><a href="#1-前提" class="headerlink" title="1.前提"></a>1.前提</h4><ul><li><p>a. flume 收集–》flume 聚合–》kafka ，启动进程和启动kafka manager监控，参考08【在线日志分析】之Flume Agent(聚合节点) sink to kafka cluster</p></li><li><p>b.window7 安装jdk1.7 或者1.8(本次环境是1.8)</p></li><li><p>c.window7 安装IDEA开发工具(以下仅供参考)<br>使用IntelliJ IDEA 配置Maven（入门）:<br><a href="http://blog.csdn.net/qq_32588349/article/details/51461182" target="_blank" rel="noopener">http://blog.csdn.net/qq_32588349/article/details/51461182</a><br>IDEA Java/Scala混合项目Maven打包:<br><a href="http://blog.csdn.net/rongyongfeikai2/article/details/51404611" target="_blank" rel="noopener">http://blog.csdn.net/rongyongfeikai2/article/details/51404611</a><br>Intellij idea使用java编写并执行spark程序:<br><a href="http://blog.csdn.net/yhao2014/article/details/44239021" target="_blank" rel="noopener">http://blog.csdn.net/yhao2014/article/details/44239021</a></p></li></ul><h4 id="2-源代码"><a href="#2-源代码" class="headerlink" title="2.源代码"></a>2.源代码</h4><p>（可下载单个java文件，加入projet 或者 整个工程下载，IDEA选择open 即可）<br>GitHub: <a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java</a></p><h4 id="3-使用IDEA-本地运行测试（未打jar包）"><a href="#3-使用IDEA-本地运行测试（未打jar包）" class="headerlink" title="3.使用IDEA 本地运行测试（未打jar包）"></a>3.使用IDEA 本地运行测试（未打jar包）</h4><p><img src="/assets/blogImg/0910.png" alt="enter description here"></p><h4 id="海康威视校招电话面试："><a href="#海康威视校招电话面试：" class="headerlink" title="海康威视校招电话面试："></a>海康威视校招电话面试：</h4><p>1.数据倾斜的解决，怎么知道哪里倾斜<br>2.自定义类的广播<br>3.cache机制，rdd和df的cache什么区别<br>4.spark动态内存，堆内和堆外<br>5.rdd算子，map,mappartitions,foreach，union<br>6.宽依赖，窄依赖<br>7.spark DAG过程，doOnrecive，eventloop执行过程<br>8.stage和task怎么分类<br>9.spark调优<br>10.概念，executor，worker，job,task和partition的关系<br>11.用没用过spark什么log，没记住<br>12.讲讲sparkSQL数据清洗过程<br>13.捎带一点项目</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08生产预警平台项目之Flume Agent(聚合节点) sink to kafka cluster</title>
      <link href="/2018/09/07/08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster/"/>
      <url>/2018/09/07/08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1-创建logtopic"><a href="#1-创建logtopic" class="headerlink" title="1.创建logtopic"></a>1.创建logtopic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic logtopic</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="2-创建avro-memory-kafka-properties-kafka-sink"><a href="#2-创建avro-memory-kafka-properties-kafka-sink" class="headerlink" title="2.创建avro_memory_kafka.properties (kafka sink)"></a>2.创建avro_memory_kafka.properties (kafka sink)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/conf</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# cp avro_memory_hdfs.properties avro_memory_kafka.properties</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_kafka.properties </span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 172.16.101.54</span><br><span class="line">a1.sources.r1.port = 4545</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = logtopic</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 6000</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1</span><br><span class="line">a1.sinks.ki.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90</span><br><span class="line">a1.channels.c1.capacity = 2000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h4 id="3-后台启动-flume-ng-agent-聚合节点-和查看nohup-out"><a href="#3-后台启动-flume-ng-agent-聚合节点-和查看nohup-out" class="headerlink" title="3.后台启动 flume-ng agent(聚合节点)和查看nohup.out"></a>3.后台启动 flume-ng agent(聚合节点)和查看nohup.out</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 ~]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# nohup  flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_kafka.properties  -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 4971</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# nohup: ignoring input and appending output to `nohup.out&apos;</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# </span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# </span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# cat nohup.out</span><br></pre></td></tr></table></figure><h4 id="4-检查log收集的三台-收集节点-开启没"><a href="#4-检查log收集的三台-收集节点-开启没" class="headerlink" title="4.检查log收集的三台(收集节点)开启没"></a>4.检查log收集的三台(收集节点)开启没</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br></pre></td></tr></table></figure><h4 id="5-打开kafka-manager监控"><a href="#5-打开kafka-manager监控" class="headerlink" title="5.打开kafka manager监控"></a>5.打开kafka manager监控</h4><p><a href="http://172.16.101.55:9999" target="_blank" rel="noopener">http://172.16.101.55:9999</a><br><img src="/assets/blogImg/0609_1.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07生产预警平台项目之kafka-manager监控工具的搭建(sbt安装与编译)</title>
      <link href="/2018/09/06/07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)/"/>
      <url>/2018/09/06/07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1-下载sbt"><a href="#1-下载sbt" class="headerlink" title="1.下载sbt"></a>1.下载sbt</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">http://www.scala-sbt.org/download.html</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# rz</span><br><span class="line">rz waiting to receive.</span><br><span class="line">Starting zmodem transfer.  Press Ctrl+C to cancel.</span><br><span class="line">Transferring sbt-0.13.13.tgz...</span><br><span class="line">  100%    1025 KB    1025 KB/sec    00:00:01       0 Errors</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# tar -zxvf sbt-0.13.13.tgz</span><br><span class="line">sbt-launcher-packaging-0.13.13/</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/sbtconfig.txt</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/sbtopts</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt.bat</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt-launch.jar</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt-launch-lib.bash</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# mv sbt-launcher-packaging-0.13.13 sbt</span><br></pre></td></tr></table></figure><h4 id="3-添加脚本文件"><a href="#3-添加脚本文件" class="headerlink" title="3.添加脚本文件"></a>3.添加脚本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 bin]# vi sbt</span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">BT_OPTS=&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;</span><br><span class="line">java $SBT_OPTS -jar /root/learnproject/app/sbt/bin/sbt-launch.jar &quot;$@&quot;</span><br></pre></td></tr></table></figure><h4 id="4-修改权限和环境变量"><a href="#4-修改权限和环境变量" class="headerlink" title="4.修改权限和环境变量"></a>4.修改权限和环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 bin]# chmod u+x sbt</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# vi /etc/profile</span><br><span class="line">export SBT_HOME=/root/learnproject/app/sbt</span><br><span class="line">export PATH=$SBT_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">&quot;/etc/profile&quot; 94L, 2265C written</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="5-测试"><a href="#5-测试" class="headerlink" title="5.测试"></a>5.测试</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/*第一次执行时，会下载一些文件包，然后才能正常使用，要确保联网了，安装成功后显示如下*/</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# sbt sbt-version</span><br><span class="line">[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)</span><br><span class="line">[info] 0.13.13</span><br><span class="line">[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)</span><br><span class="line">[info] 0.13.13</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06生产预警平台项目之 KafkaOffsetMonitor监控工具的搭建</title>
      <link href="/2018/09/05/06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/09/05/06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h3><p>在window7 手工下载好下面的链接<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/quantifind/KafkaOffsetMonitor/releases/tag/v0.2.1</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# mkdir kafkaoffsetmonitor</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# cd kafkaoffsetmonitor</span><br><span class="line">#使用rz命令上传</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# rz</span><br><span class="line">rz waiting to receive.</span><br><span class="line">Starting zmodem transfer.  Press Ctrl+C to cancel.</span><br><span class="line">Transferring KafkaOffsetMonitor-assembly-0.2.1.jar...</span><br><span class="line">  100%   51696 KB    12924 KB/sec    00:00:04       0 Errors </span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="2-新建一个kafkaMonitor-sh文件，文件内容如下："><a href="#2-新建一个kafkaMonitor-sh文件，文件内容如下：" class="headerlink" title="2.新建一个kafkaMonitor.sh文件，文件内容如下："></a>2.新建一个kafkaMonitor.sh文件，文件内容如下：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# vi kafkaoffsetmonitor.sh</span><br><span class="line">! /bin/bash</span><br><span class="line">java -cp KafkaOffsetMonitor-assembly-0.2.1.jar \</span><br><span class="line">com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line">--zk 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka \</span><br><span class="line">--port 8089 \</span><br><span class="line">--refresh 5.seconds \</span><br><span class="line">--retain 7.days</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# chmod +x *.sh</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#</span><br></pre></td></tr></table></figure><p>参数说明：<br>–zk 这里写的地址和端口，是zookeeper集群的各个地址和端口。应和kafka/bin文件夹中的zookeeper.properties中的host.name和clientPort一致。<br>–port 这个是本软件KafkaOffsetMonitor的端口。注意不要使用那些著名的端口号，例如80,8080等。我采用了8089.<br>–refresh 这个是软件刷新间隔时间，不要太短也不要太长。<br>–retain 这个是数据在数据库中保存的时间。</p><h3 id="3-后台启动"><a href="#3-后台启动" class="headerlink" title="3.后台启动"></a>3.后台启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# nohup ./kafkaoffsetmonitor.sh &amp;</span><br><span class="line"> 2serving resources from: jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp</span><br><span class="line"> 3SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line"> 4SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line"> 5SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br><span class="line"> 6log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection).</span><br><span class="line"> 7log4j:WARN Please initialize the log4j system properly.</span><br><span class="line"> 8log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line"> 9log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).</span><br><span class="line">10log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">11log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">122016-12-25 22:00:24.252:INFO:oejs.Server:jetty-7.x.y-SNAPSHOT</span><br><span class="line">132016-12-25 22:00:24.319:INFO:oejsh.ContextHandler:started o.e.j.s.ServletContextHandler&#123;/,jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp&#125;</span><br><span class="line">142016-12-25 22:00:24.328:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:8089</span><br></pre></td></tr></table></figure><h3 id="4-IE浏览器打开"><a href="#4-IE浏览器打开" class="headerlink" title="4.IE浏览器打开"></a>4.IE浏览器打开</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://172.16.101.55:8089</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05生产预警平台项目之Kafka 0.10.1.0 Cluster的搭建和Topic简单操作实验</title>
      <link href="/2018/09/04/05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C/"/>
      <url>/2018/09/04/05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>【kafka cluster机器】:<br>机器名称 用户名称<br>sht-sgmhadoopdn-01/02/03 root<br>【安装目录】: /root/learnproject/app<br><a id="more"></a></p><h4 id="1-将scala文件夹同步到集群其他机器-scala-2-11版本，可单独下载解压"><a href="#1-将scala文件夹同步到集群其他机器-scala-2-11版本，可单独下载解压" class="headerlink" title="1.将scala文件夹同步到集群其他机器(scala 2.11版本，可单独下载解压)"></a>1.将scala文件夹同步到集群其他机器(scala 2.11版本，可单独下载解压)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-01:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-02:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line"></span><br><span class="line">#环境变量</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-03 app]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="2-下载基于Scala-2-11的kafka版本为0-10-1-0"><a href="#2-下载基于Scala-2-11的kafka版本为0-10-1-0" class="headerlink" title="2.下载基于Scala 2.11的kafka版本为0.10.1.0"></a>2.下载基于Scala 2.11的kafka版本为0.10.1.0</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# pwd</span><br><span class="line">/root/learnproject/app</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# wget http://www-eu.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# tar xzvf kafka_2.11-0.10.1.0.tgz </span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# mv kafka_2.11-0.10.1.0 kafka</span><br></pre></td></tr></table></figure><h4 id="3-创建logs目录和修改server-properties-前提zookeeper-cluster部署好，见“03【在线日志分析】之hadoop-2-7-3编译和搭建集群环境-HDFS-HA-Yarn-HA-”"><a href="#3-创建logs目录和修改server-properties-前提zookeeper-cluster部署好，见“03【在线日志分析】之hadoop-2-7-3编译和搭建集群环境-HDFS-HA-Yarn-HA-”" class="headerlink" title="3.创建logs目录和修改server.properties(前提zookeeper cluster部署好，见“03【在线日志分析】之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)” )"></a>3.创建logs目录和修改server.properties(前提zookeeper cluster部署好，见“03【在线日志分析】之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)” )</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# cd kafka</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# mkdir logs</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# cd config/</span><br><span class="line">[root@sht-sgmhadoopdn-01 config]# vi server.properties</span><br><span class="line">broker.id=1</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.58</span><br><span class="line">log.dirs=/root/learnproject/app/kafka/logs</span><br><span class="line">zookeeper.connect=172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka</span><br></pre></td></tr></table></figure><h4 id="4-同步到02-03服务器，更改broker-id-及host-name"><a href="#4-同步到02-03服务器，更改broker-id-及host-name" class="headerlink" title="4.同步到02/03服务器，更改broker.id 及host.name"></a>4.同步到02/03服务器，更改broker.id 及host.name</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 config]# vi server.properties </span><br><span class="line">broker.id=2</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.59</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-03 config]# vi server.properties </span><br><span class="line">broker.id=3</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.60</span><br></pre></td></tr></table></figure><h4 id="5-环境变量"><a href="#5-环境变量" class="headerlink" title="5.环境变量"></a>5.环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# vi /etc/profile</span><br><span class="line">export KAFKA_HOME=/root/learnproject/app/kafka</span><br><span class="line">export PATH=$KAFKA_HOME/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-02:/etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-03:/etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]#</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-03 kafka]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="6-启动-停止"><a href="#6-启动-停止" class="headerlink" title="6.启动/停止"></a>6.启动/停止</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-03 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line"></span><br><span class="line">###停止</span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h4 id="7-topic相关的操作"><a href="#7-topic相关的操作" class="headerlink" title="7.topic相关的操作"></a>7.topic相关的操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">a.创建topic，如能成功创建topic则表示集群安装完成，也可以用jps命令查看kafka进程是否存在。</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic test</span><br><span class="line"></span><br><span class="line">b.通过list命令查看创建的topic:</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --list --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka</span><br><span class="line"></span><br><span class="line">c.查看创建的Topic</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:1        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# </span><br><span class="line">第一行列出了这个topic的总体情况，如topic名称，分区数量，副本数量等。</span><br><span class="line">第二行开始，每一行列出了一个分区的信息，如它是第几个分区，这个分区的leader是哪个broker，副本位于哪些broker，有哪些副本处理同步状态。</span><br><span class="line"></span><br><span class="line">Partition： 分区</span><br><span class="line">Leader ：   负责读写指定分区的节点</span><br><span class="line">Replicas ： 复制该分区log的节点列表</span><br><span class="line">Isr ：      “in-sync” replicas，当前活跃的副本列表（是一个子集），并且可能成为Leader</span><br><span class="line">我们可以通过Kafka自带的bin/kafka-console-producer.sh和bin/kafka-console-consumer.sh脚本，来验证演示如果发布消息、消费消息。</span><br><span class="line"></span><br><span class="line">d.删除topic</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh  --delete --zookeeper  172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka  --topic test</span><br><span class="line"></span><br><span class="line">e.修改topic</span><br><span class="line">使用—-alert原则上可以修改任何配置，以下列出了一些常用的修改选项：</span><br><span class="line">（1）改变分区数量</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]#bin/kafka-topics.sh --alter  --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test --partitions 3</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]#</span><br><span class="line"></span><br><span class="line">（2）增加、修改或者删除一个配置参数</span><br><span class="line"> bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka  --topic my_topic_name --config key=value</span><br><span class="line"> bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka  --topic my_topic_name --deleteConfig key</span><br></pre></td></tr></table></figure><h4 id="8-模拟实验1"><a href="#8-模拟实验1" class="headerlink" title="8.模拟实验1"></a>8.模拟实验1</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在一个终端，启动Producer，并向我们上面创建的名称为my-replicated-topic5的Topic中生产消息，执行如下脚本：</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-console-producer.sh --broker-list 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092 --topic test</span><br><span class="line"></span><br><span class="line">在另一个终端，启动Consumer，并订阅我们上面创建的名称为my-replicated-topic5的Topic中生产的消息，执行如下脚本：</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --from-beginning --topic test</span><br><span class="line"></span><br><span class="line">可以在Producer终端上输入字符串消息行，就可以在Consumer终端上看到消费者消费的消息内容。</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03生产预警平台项目之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)</title>
      <link href="/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/"/>
      <url>/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="1-下载hadoop2-7-3最新源码"><a href="#1-下载hadoop2-7-3最新源码" class="headerlink" title="1.下载hadoop2.7.3最新源码"></a>1.下载hadoop2.7.3最新源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt </span><br><span class="line">Build instructions for Hadoop</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Requirements:</span><br><span class="line">* Unix System</span><br><span class="line">* JDK 1.7+</span><br><span class="line">* Maven 3.0 or later</span><br><span class="line">* Findbugs 1.3.9 (if running findbugs)</span><br><span class="line">* ProtocolBuffer 2.5.0</span><br><span class="line">* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac</span><br><span class="line">* Zlib devel (if compiling native code)</span><br><span class="line">* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )</span><br><span class="line">* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )</span><br><span class="line">* Internet connection for first build (to fetch all Maven and Hadoop dependencies)</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:</span><br><span class="line">* Oracle JDK 1.7 (preferred)</span><br><span class="line">  $ sudo apt-get purge openjdk*</span><br><span class="line">  $ sudo apt-get install software-properties-common</span><br><span class="line">  $ sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">  $ sudo apt-get update</span><br><span class="line">  $ sudo apt-get install oracle-java7-installer</span><br><span class="line">* Maven</span><br><span class="line">  $ sudo apt-get -y install maven</span><br><span class="line">* Native libraries</span><br><span class="line">  $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev</span><br><span class="line">* ProtocolBuffer 2.5.0 (required)</span><br><span class="line">  $ sudo apt-get -y install libprotobuf-dev protobuf-compiler</span><br><span class="line">Optional packages:</span><br><span class="line">* Snappy compression</span><br><span class="line">  $ sudo apt-get install snappy libsnappy-dev</span><br><span class="line">* Bzip2</span><br><span class="line">  $ sudo apt-get install bzip2 libbz2-dev</span><br><span class="line">* Jansson (C Library for JSON)</span><br><span class="line">  $ sudo apt-get install libjansson-dev</span><br><span class="line">* Linux FUSE</span><br><span class="line">  $ sudo apt-get install fuse libfuse-dev</span><br></pre></td></tr></table></figure><h3 id="2-安装依赖包"><a href="#2-安装依赖包" class="headerlink" title="2.安装依赖包"></a>2.安装依赖包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*</span><br></pre></td></tr></table></figure><h3 id="3-安装jdk"><a href="#3-安装jdk" class="headerlink" title="3.安装jdk"></a>3.安装jdk</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# java -version</span><br><span class="line">java version &quot;1.7.0_67&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_67-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#</span><br></pre></td></tr></table></figure><h3 id="4-安装maven"><a href="#4-安装maven" class="headerlink" title="4.安装maven"></a>4.安装maven</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">#在编译过程中为了防止Java内存溢出，需要加入以下环境变量</span><br><span class="line">export MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;</span><br><span class="line">export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# mvn -version</span><br><span class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</span><br><span class="line">Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">Java version: 1.7.0_67, vendor: Oracle Corporation</span><br><span class="line">Java home: /usr/java/jdk1.7.0_67-cloudera/jre</span><br><span class="line">Default locale: en_US, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#</span><br></pre></td></tr></table></figure><h3 id="5-编译安装protobuf"><a href="#5-编译安装protobuf" class="headerlink" title="5.编译安装protobuf"></a>5.编译安装protobuf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make install</span><br><span class="line">#查看protobuf版本以测试是否安装成功</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">libprotoc 2.5.0</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]#</span><br></pre></td></tr></table></figure><h3 id="6-安装snappy"><a href="#6-安装snappy" class="headerlink" title="6.安装snappy"></a>6.安装snappy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz</span><br><span class="line">#用root用户执行以下命令</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install</span><br><span class="line">#查看snappy库文件</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy</span><br><span class="line">-rw-r--r--  1 root root 229K Jun 21 15:46 libsnappy.a</span><br><span class="line">-rwxr-xr-x  1 root root  953 Jun 21 15:46 libsnappy.la</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0</span><br><span class="line">-rwxr-xr-x  1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]#</span><br></pre></td></tr></table></figure><h3 id="7-编译"><a href="#7-编译" class="headerlink" title="7.编译"></a>7.编译</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">mvn clean package -Pdist,native -DskipTests -Dtar</span><br><span class="line">或</span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package –Pdist,native –DskipTests –Dtar</span><br><span class="line">[INFO] Executing tasks</span><br><span class="line">main:</span><br><span class="line">     [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3</span><br><span class="line">     [exec] $ gzip -f hadoop-2.7.3.tar</span><br><span class="line">     [exec] </span><br><span class="line">     [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz</span><br><span class="line">     [exec] </span><br><span class="line">[INFO] Executed tasks</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---</span><br><span class="line">[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s]</span><br><span class="line">[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  6.832 s]</span><br><span class="line">[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s]</span><br><span class="line">[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s]</span><br><span class="line">[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.411 s]</span><br><span class="line">[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  4.814 s]</span><br><span class="line">[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s]</span><br><span class="line">[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min]</span><br><span class="line">[INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s]</span><br><span class="line">[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s]</span><br><span class="line">[INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min]</span><br><span class="line">[INFO] Apache Hadoop NFS .................................. SUCCESS [  9.285 s]</span><br><span class="line">[INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s]</span><br><span class="line">[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.049 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min]</span><br><span class="line">[INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  4.599 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.044 s]</span><br><span class="line">[INFO] hadoop-yarn ........................................ SUCCESS [  0.043 s]</span><br><span class="line">[INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min]</span><br><span class="line">[INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s]</span><br><span class="line">[INFO] hadoop-yarn-server ................................. SUCCESS [  0.041 s]</span><br><span class="line">[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s]</span><br><span class="line">[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s]</span><br><span class="line">[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  6.415 s]</span><br><span class="line">[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s]</span><br><span class="line">[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s]</span><br><span class="line">[INFO] hadoop-yarn-server-tests ........................... SUCCESS [  7.751 s]</span><br><span class="line">[INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s]</span><br><span class="line">[INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [  5.612 s]</span><br><span class="line">[INFO] hadoop-yarn-applications ........................... SUCCESS [  0.038 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  4.029 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  2.611 s]</span><br><span class="line">[INFO] hadoop-yarn-site ................................... SUCCESS [  0.077 s]</span><br><span class="line">[INFO] hadoop-yarn-registry ............................... SUCCESS [  8.045 s]</span><br><span class="line">[INFO] hadoop-yarn-project ................................ SUCCESS [  5.456 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.226 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  6.697 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  9.328 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  2.412 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  8.876 s]</span><br><span class="line">[INFO] hadoop-mapreduce ................................... SUCCESS [  4.237 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s]</span><br><span class="line">[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s]</span><br><span class="line">[INFO] Apache Hadoop Archives ............................. SUCCESS [  3.069 s]</span><br><span class="line">[INFO] Apache Hadoop Rumen ................................ SUCCESS [  7.446 s]</span><br><span class="line">[INFO] Apache Hadoop Gridmix .............................. SUCCESS [  5.765 s]</span><br><span class="line">[INFO] Apache Hadoop Data Join ............................ SUCCESS [  3.752 s]</span><br><span class="line">[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  2.771 s]</span><br><span class="line">[INFO] Apache Hadoop Extras ............................... SUCCESS [  5.612 s]</span><br><span class="line">[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s]</span><br><span class="line">[INFO] Apache Hadoop OpenStack support .................... SUCCESS [  7.131 s]</span><br><span class="line">[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min]</span><br><span class="line">[INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s]</span><br><span class="line">[INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s]</span><br><span class="line">[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.142 s]</span><br><span class="line">[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  7.354 s]</span><br><span class="line">[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s]</span><br><span class="line">[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.035 s]</span><br><span class="line">[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 26:29 min</span><br><span class="line">[INFO] Finished at: 2016-12-24T21:07:09+08:00</span><br><span class="line">[INFO] Final Memory: 214M/740M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]# ll</span><br><span class="line">total 193152</span><br><span class="line">drwxr-xr-x 5 root root      4096 Dec 24 20:24 compilesoft</span><br><span class="line">-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]#</span><br></pre></td></tr></table></figure><h3 id="8-搭建HDFS-HA-YARN-HA集群（5个节点）"><a href="#8-搭建HDFS-HA-YARN-HA集群（5个节点）" class="headerlink" title="8.搭建HDFS HA,YARN HA集群（5个节点）"></a>8.搭建HDFS HA,YARN HA集群（5个节点）</h3><p>参考:<br><a href="http://blog.itpub.net/30089851/viewspace-1994585/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-1994585/</a><br><a href="https://github.com/Hackeruncle/Hadoop" target="_blank" rel="noopener">https://github.com/Hackeruncle/Hadoop</a></p><h3 id="9-搭建集群-验证版本和支持的压缩信息"><a href="#9-搭建集群-验证版本和支持的压缩信息" class="headerlink" title="9.搭建集群,验证版本和支持的压缩信息"></a>9.搭建集群,验证版本和支持的压缩信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop version</span><br><span class="line">Hadoop 2.7.3</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2016-12-24T12:45Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4</span><br><span class="line">This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop checknative</span><br><span class="line">16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">zlib:    true /lib64/libz.so.1</span><br><span class="line">snappy:  true /usr/local/lib/libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   true /lib64/libbz2.so.1</span><br><span class="line">openssl: true /usr/lib64/libcrypto.so</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]#</span><br></pre></td></tr></table></figure><p>[参考]</p><ul><li><a href="http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html" target="_blank" rel="noopener">http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html</a></li><li><a href="http://blog.csdn.net/haohaixingyun/article/details/52800048" target="_blank" rel="noopener">http://blog.csdn.net/haohaixingyun/article/details/52800048</a></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04生产预警平台项目之Flume Agent的3台收集+1台聚合到hdfs的搭建</title>
      <link href="/2018/09/03/04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86+1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/09/03/04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86+1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>【log收集】:<br>机器名称 服务名称 用户<br>flume-agent-01: namenode hdfs<br>flume-agent-02: datanode hdfs<br>flume-agent-03: datanode hdfs</p><p>【log聚合】:<br>机器名称 用户<br>sht-sgmhadoopcm-01(172.16.101.54) root</p><p>【sink到hdfs】:<br>hdfs://172.16.101.56:8020/testwjp/</p><a id="more"></a><h5 id="1-下载apache-flume-1-7-0-bin-tar-gz"><a href="#1-下载apache-flume-1-7-0-bin-tar-gz" class="headerlink" title="1.下载apache-flume-1.7.0-bin.tar.gz"></a>1.下载apache-flume-1.7.0-bin.tar.gz</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ wget http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz</span><br><span class="line">--2017-01-04 20:40:10--  http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz</span><br><span class="line">Resolving www-eu.apache.org... 88.198.26.2, 2a01:4f8:130:2192::2</span><br><span class="line">Connecting to www-eu.apache.org|88.198.26.2|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 55711670 (53M) [application/x-gzip]</span><br><span class="line">Saving to: “apache-flume-1.7.0-bin.tar.gz”</span><br><span class="line"></span><br><span class="line">100%[===============================================================================================================================================================================================&gt;] 55,711,670   473K/s   in 74s    </span><br><span class="line"></span><br><span class="line">2017-01-04 20:41:25 (733 KB/s) - “apache-flume-1.7.0-bin.tar.gz” saved [55711670/55711670]</span><br></pre></td></tr></table></figure><h5 id="2-解压重命名"><a href="#2-解压重命名" class="headerlink" title="2.解压重命名"></a>2.解压重命名</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ </span><br><span class="line">[hdfs@flume-agent-01 tmp]$ tar -xzvf apache-flume-1.7.0-bin.tar.gz </span><br><span class="line">[hdfs@flume-agent-01 tmp]$ mv apache-flume-1.7.0-bin flume-ng</span><br><span class="line">[hdfs@flume-agent-01 tmp]$ cd flume-ng/conf</span><br></pre></td></tr></table></figure><h5 id="3-拷贝flume环境配置和agent配置文件"><a href="#3-拷贝flume环境配置和agent配置文件" class="headerlink" title="3.拷贝flume环境配置和agent配置文件"></a>3.拷贝flume环境配置和agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ cp flume-env.sh.template flume-env.sh</span><br><span class="line">[hdfs@flume-agent-01 tmp]$ cp flume-conf.properties.template exec_memory_avro.properties</span><br></pre></td></tr></table></figure><h5 id="4-添加hdfs用户的环境变量文件"><a href="#4-添加hdfs用户的环境变量文件" class="headerlink" title="4.添加hdfs用户的环境变量文件"></a>4.添加hdfs用户的环境变量文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ cd</span><br><span class="line">[hdfs@flume-agent-01 ~]$ ls -la</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x   3 hdfs hadoop 4096 Jul  8 14:05 .</span><br><span class="line">drwxr-xr-x. 35 root root   4096 Dec 10  2015 ..</span><br><span class="line">-rw-------   1 hdfs hdfs   4471 Jul  8 17:22 .bash_history</span><br><span class="line">drwxrwxrwt   2 hdfs hadoop 4096 Nov 19  2014 cache</span><br><span class="line">-rw-------   1 hdfs hdfs   3131 Jul  8 14:05 .viminfo</span><br><span class="line">[hdfs@flume-agent-01 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-01 ~]$ ls -la</span><br><span class="line">total 36</span><br><span class="line">drwxr-xr-x   3 hdfs hadoop 4096 Jan  4 20:49 .</span><br><span class="line">drwxr-xr-x. 35 root root   4096 Dec 10  2015 ..</span><br><span class="line">-rw-------   1 hdfs hdfs   4471 Jul  8 17:22 .bash_history</span><br><span class="line">-rw-r--r--   1 hdfs hdfs     18 Jan  4 20:49 .bash_logout</span><br><span class="line">-rw-r--r--   1 hdfs hdfs    176 Jan  4 20:49 .bash_profile</span><br><span class="line">-rw-r--r--   1 hdfs hdfs    124 Jan  4 20:49 .bashrc</span><br><span class="line">drwxrwxrwt   2 hdfs hadoop 4096 Nov 19  2014 cache</span><br><span class="line">-rw-------   1 hdfs hdfs   3131 Jul  8 14:05 .viminfo</span><br></pre></td></tr></table></figure><h5 id="5-添加flume的环境变量"><a href="#5-添加flume的环境变量" class="headerlink" title="5.添加flume的环境变量"></a>5.添加flume的环境变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 ~]$ vi .bash_profile</span><br><span class="line"></span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-01 ~]$ . .bash_profile</span><br></pre></td></tr></table></figure><h5 id="6-修改flume环境配置文件"><a href="#6-修改flume环境配置文件" class="headerlink" title="6.修改flume环境配置文件"></a>6.修改flume环境配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_25</span><br></pre></td></tr></table></figure><h5 id="7-将基于Flume-ng-Exec-Source开发自定义插件AdvancedExecSource的AdvancedExecSource-jar包上传到-FLUME-HOME-lib"><a href="#7-将基于Flume-ng-Exec-Source开发自定义插件AdvancedExecSource的AdvancedExecSource-jar包上传到-FLUME-HOME-lib" class="headerlink" title="7.将基于Flume-ng Exec Source开发自定义插件AdvancedExecSource的AdvancedExecSource.jar包上传到$FLUME_HOME/lib/"></a>7.将基于Flume-ng Exec Source开发自定义插件AdvancedExecSource的AdvancedExecSource.jar包上传到$FLUME_HOME/lib/</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://blog.itpub.net/30089851/viewspace-2131995/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$ pwd</span><br><span class="line">/tmp/flume-ng/lib</span><br><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$ ll AdvancedExecSource.jar </span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 10618 Jan  5 23:50 AdvancedExecSource.jar</span><br><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$</span><br></pre></td></tr></table></figure><h5 id="8-修改flume的agent配置文件"><a href="#8-修改flume的agent配置文件" class="headerlink" title="8.修改flume的agent配置文件"></a>8.修改flume的agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 conf]$ vi exec_memory_avro.properties</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#Describe/configure the custom exec source</span><br><span class="line">a1.sources.r1.type = com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-NAMENODE-flume-agent-01.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-01</span><br><span class="line">a1.sources.r1.servicename = namenode</span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = 172.16.101.54</span><br><span class="line">a1.sinks.k1.port = 4545</span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 60 </span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h5 id="9-将flume-agent-01的flume-ng打包-scp到flume-agent-02-03-和-sht-sgmhadoopcm-01-172-16-101-54"><a href="#9-将flume-agent-01的flume-ng打包-scp到flume-agent-02-03-和-sht-sgmhadoopcm-01-172-16-101-54" class="headerlink" title="9.将flume-agent-01的flume-ng打包,scp到flume-agent-02/03 和 sht-sgmhadoopcm-01(172.16.101.54)"></a>9.将flume-agent-01的flume-ng打包,scp到flume-agent-02/03 和 sht-sgmhadoopcm-01(172.16.101.54)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ zip -r flume-ng.zip flume-ng/*</span><br><span class="line"></span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-02:/tmp/</span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-03:/tmp/</span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip sht-sgmhadoopcm-01:/tmp/</span><br></pre></td></tr></table></figure><h5 id="10-在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件"><a href="#10-在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件" class="headerlink" title="10.在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件"></a>10.在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-02 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-02 ~]$ vi .bash_profile</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-02 ~]$ . .bash_profile</span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-02 tmp]$ unzip flume-ng.zip</span><br><span class="line">[hdfs@flume-agent-02 tmp]$ cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">##修改以下参数即可</span><br><span class="line">[hdfs@flume-agent-02 conf]$ vi exec_memory_avro.properties</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-02.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-02</span><br><span class="line">a1.sources.r1.servicename = datanode</span><br><span class="line"></span><br><span class="line">###要检查flume-env.sh的JAVA_HOME目录是否存在</span><br></pre></td></tr></table></figure><h5 id="11-在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件"><a href="#11-在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件" class="headerlink" title="11.在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件"></a>11.在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-03 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-03 ~]$ vi .bash_profile</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-03 ~]$ . .bash_profile</span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-03 tmp]$ unzip flume-ng.zip</span><br><span class="line">[hdfs@flume-agent-03 tmp]$ cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">##修改以下参数即可</span><br><span class="line">[hdfs@flume-agent-03 conf]$ vi exec_memory_avro.properties</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-03.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-03</span><br><span class="line">a1.sources.r1.servicename = datanode</span><br><span class="line"></span><br><span class="line">###要检查flume-env.sh的JAVA_HOME目录是否存在</span><br></pre></td></tr></table></figure><h5 id="12-聚合端-sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件"><a href="#12-聚合端-sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件" class="headerlink" title="12.聚合端 sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件"></a>12.聚合端 sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 tmp]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line"></span><br><span class="line">export PATH=$FLUME_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]#</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# unzip flume-ng.zip</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">###测试: 先聚合, sink到hdfs端</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_hdfs.properties</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 172.16.101.54</span><br><span class="line">a1.sources.r1.port = 4545</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://172.16.101.56:8020/testwjp/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = logs</span><br><span class="line">a1.sinks.k1.hdfs.inUsePrefix = .</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">###roll 16 m = 16777216 bytes</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 1048576</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 6000</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = text</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90 </span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h5 id="13-后台启动"><a href="#13-后台启动" class="headerlink" title="13.后台启动"></a>13.后台启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopnn-01 flume-ng]# nohup  flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_hdfs.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br></pre></td></tr></table></figure><h5 id="14-校验：将集群的日志下载到本地，打开查看即可-略"><a href="#14-校验：将集群的日志下载到本地，打开查看即可-略" class="headerlink" title="14.校验：将集群的日志下载到本地，打开查看即可(略)"></a>14.校验：将集群的日志下载到本地，打开查看即可(略)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">【备注】: </span><br><span class="line">1.错误1 flume-ng安装的机器上没有hadoop环境，所以假如sink到hdfs话，需要用到hdfs的jar包</span><br><span class="line">[ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:146)] Failed to start agent </span><br><span class="line">because dependencies were not found in classpath. Error follows.</span><br><span class="line">java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType</span><br><span class="line"></span><br><span class="line">只需在其他安装hadoop机器上搜索以下5个jar包，拷贝到$FLUME_HOME/lib目录即可。</span><br><span class="line">搜索方法: find $HADOOP_HOME/ -name commons-configuration*.jar</span><br><span class="line"></span><br><span class="line">commons-configuration-1.6.jar</span><br><span class="line">hadoop-auth-2.7.3.jar</span><br><span class="line">hadoop-common-2.7.3.jar</span><br><span class="line">hadoop-hdfs-2.7.3.jar</span><br><span class="line">hadoop-mapreduce-client-core-2.7.3.jar</span><br><span class="line">protobuf-java-2.5.0.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.错误2 无法加载自定义插件的类 Unable to load source type: com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line">2017-01-06 21:10:48,278 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:142)] Failed to load configuration data. Exception follows.</span><br><span class="line">org.apache.flume.FlumeException: Unable to load source type: com.onlinelog.analysis.AdvancedExecSource, class: com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line"></span><br><span class="line">执行hdfs或者root用户的环境变量即可</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02生产预警平台项目之Flume-1.7.0源码编译导入eclipse</title>
      <link href="/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/"/>
      <url>/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="【前提】"><a href="#【前提】" class="headerlink" title="【前提】:"></a>【前提】:</h3><p>1.windows 7 安装maven-3.3.9<br>其中在conf/setting.xml文件添加<br>D:\software\apache-maven-3.3.9\repository<br><a href="http://blog.csdn.net/defonds/article/details/41957287" target="_blank" rel="noopener">http://blog.csdn.net/defonds/article/details/41957287</a><br>2.windows 7 安装eclipse 64位(百度下载，解压即可)<br>3.eclipse安装maven插件，选择第二种方式link<br><a href="http://blog.csdn.net/lfsfxy9/article/details/9397937" target="_blank" rel="noopener">http://blog.csdn.net/lfsfxy9/article/details/9397937</a><br>其中 eclipse-maven3-plugin.7z 这个包可以加群258669058找我，分享给你</p><h3 id="【flume-ng-1-7-0源码的编译导入eclipse】"><a href="#【flume-ng-1-7-0源码的编译导入eclipse】" class="headerlink" title="【flume-ng 1.7.0源码的编译导入eclipse】:"></a>【flume-ng 1.7.0源码的编译导入eclipse】:</h3><h4 id="1-下载官网的源码-不要下载GitHub上源码，因为这时pom文件中版本为1-8-0，编译会有问题"><a href="#1-下载官网的源码-不要下载GitHub上源码，因为这时pom文件中版本为1-8-0，编译会有问题" class="headerlink" title="1.下载官网的源码(不要下载GitHub上源码，因为这时pom文件中版本为1.8.0，编译会有问题)"></a>1.下载官网的源码(不要下载GitHub上源码，因为这时pom文件中版本为1.8.0，编译会有问题)</h4><p><a href="http://archive.apache.org/dist/flume/1.7.0/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/1.7.0/</a><br>a.下载apache-flume-1.7.0-src.tar.gz<br>b.解压重命名为flume-1.7.0</p><h4 id="2-修改pom-xml-大概在621行，将自带的repository注释掉，添加以下的"><a href="#2-修改pom-xml-大概在621行，将自带的repository注释掉，添加以下的" class="headerlink" title="2.修改pom.xml (大概在621行，将自带的repository注释掉，添加以下的)"></a>2.修改pom.xml (大概在621行，将自带的repository注释掉，添加以下的)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;repository&gt;</span><br><span class="line">       &lt;id&gt;maven.tempo-db.com&lt;/id&gt;</span><br><span class="line">       &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt;</span><br><span class="line"> &lt;/repository&gt;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/0828_1.png" alt="enter description here"></p><h4 id="3-打开cmd-编译"><a href="#3-打开cmd-编译" class="headerlink" title="3.打开cmd,编译"></a>3.打开cmd,编译</h4><p>cd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0<br>mvn compile<br><img src="/assets/blogImg/0828_2.png" alt="enter description here"></p><h4 id="4-打开eclipse-单击Window–-gt-Perferences–-gt-左侧的Maven–-gt-User-Settings"><a href="#4-打开eclipse-单击Window–-gt-Perferences–-gt-左侧的Maven–-gt-User-Settings" class="headerlink" title="4.打开eclipse,单击Window–&gt;Perferences–&gt;左侧的Maven–&gt;User Settings"></a>4.打开eclipse,单击Window–&gt;Perferences–&gt;左侧的Maven–&gt;User Settings</h4><p>然后设置自己的mvn的setting.xml路径和Local Repository<br>(最好使用Maven3.3.x版本以上，我是3.3.9)<br><img src="/assets/blogImg/0828_3.png" alt="enter description here"></p><h4 id="5-关闭eclipse的-Project–-gt-Buid-Automatically"><a href="#5-关闭eclipse的-Project–-gt-Buid-Automatically" class="headerlink" title="5.关闭eclipse的 Project–&gt;Buid Automatically"></a>5.关闭eclipse的 Project–&gt;Buid Automatically</h4><p><img src="/assets/blogImg/0828_4.png" alt="enter description here"></p><h4 id="6-关闭eclipse的Download-repository-index-updates-on-startup"><a href="#6-关闭eclipse的Download-repository-index-updates-on-startup" class="headerlink" title="6.关闭eclipse的Download repository index updates on startup"></a>6.关闭eclipse的Download repository index updates on startup</h4><p><img src="/assets/blogImg/0828_5.png" alt="enter description here"></p><h4 id="7-导入flume1-7-0源码"><a href="#7-导入flume1-7-0源码" class="headerlink" title="7.导入flume1.7.0源码"></a>7.导入flume1.7.0源码</h4><p>a.File–&gt;Import–&gt;Maven–&gt;Existing Maven Projects–&gt;Next<br>b.选择目录–&gt; Finish</p><h4 id="8-检查源码，没有抛任何错误"><a href="#8-检查源码，没有抛任何错误" class="headerlink" title="8.检查源码，没有抛任何错误"></a>8.检查源码，没有抛任何错误</h4><p><img src="/assets/blogImg/0828_6.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是一篇热腾腾的面经</title>
      <link href="/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/"/>
      <url>/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>伟梦：<br>1.主要还是项目？<br>基本上没问什么技术，我就说了一遍项目流程，<br>然后说几个优化点，比如上次讲的血案，我也顺带提了一下。<br>2.在大数据中，有没有什么是不足的，遇到过什么问题？<br><a id="more"></a></p><p>微盟：<br>1.SparkStreaming处理完一批次的数据，写偏移量之前挂了，数据怎么保证不重？<br>2.Maxwell的底层原理？<br>3.手写Spring？<br>4.遍历二叉树？<br>5.用过什么算法？<br>6.多线程方面，怎么实现一个主线程，等待其他子线程完成后再运行？<br>7.Maxwell和Cannal的比较？<br>8.direct比较receiver的优势？<br>9.原来是把数据传入到Hive，之后改了架构，怎么把Hive的数据导入到Hbase？<br>10.为什么用Kafka自己存储offset来替代checkpoint，怎么防止了数据双份落地，数据双份是指什么？<br>11.单例用过吗？</p><p>平安：<br>1.问项目，流程，业务？<br>2.数据量，增量？<br>3.几个人开发的，代码量多少？<br>4.你主要做什么的？<br>5.什么场景，用SparkSql分析什么东西？</p><p>总结：<br>基本上都是围绕项目来面，第一家问的比较少，而且都是关于项目；微盟的面试官做的项目，<br>跟简历上的项目，架构上基本一样，所以问的比较深，问我Maxwell的底层原理，对比Cannal有什么优势，<br>为什么选择它，这个我没回答上来，后来让手写Spring，算法，后来就让我走了；<br>平安也是基本围绕项目，业务，数据量，没问什么技术，而且我说了关于优化的点(面试官说不要说网上都有的东西)。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01生产预警平台项目之项目概述</title>
      <link href="/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/"/>
      <url>/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="1-前期基本架构图"><a href="#1-前期基本架构图" class="headerlink" title="1.前期基本架构图"></a>1.前期基本架构图</h3><p><img src="/assets/blogImg/0827_1.png" alt="enter description here"></p><h3 id="2-最终基本架构图"><a href="#2-最终基本架构图" class="headerlink" title="2.最终基本架构图"></a>2.最终基本架构图</h3><p><img src="/assets/blogImg/0827_2.png" alt="enter description here"><br><a id="more"></a></p><h3 id="3-版本"><a href="#3-版本" class="headerlink" title="3.版本"></a>3.版本</h3><table><thead><tr><th>组件</th><th>版本</th></tr></thead><tbody><tr><td>Flume:</td><td>1.7</td></tr><tr><td>Hadoop:</td><td>2.7.3</td></tr><tr><td>Scala:</td><td>2.11</td></tr><tr><td>Kafka:</td><td>0.10.1.0</td></tr><tr><td>Spark:</td><td>2.0.2</td></tr><tr><td>InfluxDB:</td><td>1.2.0</td></tr><tr><td>Grafana:</td><td>4.1.1</td></tr><tr><td>maven:</td><td>3.3.9</td></tr></tbody></table><h3 id="4-主要目的"><a href="#4-主要目的" class="headerlink" title="4.主要目的"></a>4.主要目的</h3><p>主要是想基于Exec Source开发自定义插件AdvancedExecSource，将机器名称 和 服务名称 添加到cdh 服务的角色log数据的每一行前面，则格式为：机器名称 服务名称 年月日 时分秒.毫秒 日志级别 日志信息 ；<br>然后在后面的spark streaming 实时计算我们所需求：比如统计每台机器的服务的每秒出现的error次数 、统计每5秒的warn，error次数等等；<br>来实时可视化展示和邮件短信、微信企业号通知。</p><p>其实主要我们现在的很多监控服务基本达不到秒级的通知，都为5分钟等等，为了方便我们自己的维护；<br>其实对一些即将出现的问题可以提前预知；<br>其实最主要可以有效扩展到实时计算数据库级别日志，比如MySQL慢查询日志，nginx，tomcat，linux的系统级别日志等等。</p><h3 id="5-大概流程"><a href="#5-大概流程" class="headerlink" title="5.大概流程"></a>5.大概流程</h3><p>1.搭建hadoop cluster<br>2.eclipse 导入flume源代码（window7 安装maven，eclipse，eclipse与maven集成）<br>3.开发flume-ng 自定义插件<br>4.flume 收集，汇聚到hdfs(主要测试是否汇聚成功，后期也可以做离线处理)<br>5.flume 收集，汇聚到kafka<br>6.搭建kafka monitor<br>7.搭建 spark client<br>8.window7装ieda开发工具<br>9.idea开发 spark streaming 的wc<br>10.读取kafka日志，开发spark streaming的这块日志分析<br>11.写入influxdb<br>12.grafana可视化展示<br>13.集成邮件</p><p>###说明：<br>针对自身情况，自行选择，步骤如上，但不是固定的，有些顺序是可以打乱的，例如开发工具的安装，可以一起操作的，再如这几个组件的下载编译，如果不<br>想编译可以直接下tar包的，自行选择就好，但是建议还是自己编译，遇到坑才能更好的记住这个东西，本身这个项目就是学习提升的过程，要是什么都是现成的，<br>那就没什么意义了</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark中配置启用LZO压缩</title>
      <link href="/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/"/>
      <url>/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Spark中配置启用LZO压缩，步骤如下：</p><h3 id="一、spark-env-sh配置"><a href="#一、spark-env-sh配置" class="headerlink" title="一、spark-env.sh配置"></a>一、spark-env.sh配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h3 id="二、spark-defaults-conf配置"><a href="#二、spark-defaults-conf配置" class="headerlink" title="二、spark-defaults.conf配置"></a>二、spark-defaults.conf配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br></pre></td></tr></table></figure><p><font color="#FF4500">注：指向编译生成lzo的jar包</font><br><a id="more"></a></p><h3 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h3><h4 id="1、读取Lzo文件"><a href="#1、读取Lzo文件" class="headerlink" title="1、读取Lzo文件"></a>1、读取Lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)</span><br></pre></td></tr></table></figure><h4 id="2、写出lzo文件"><a href="#2、写出lzo文件" class="headerlink" title="2、写出lzo文件"></a>2、写出lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val lzoTest = sc.parallelize(1 to 10)</span><br><span class="line">scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])</span><br></pre></td></tr></table></figure><p>结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2018-03-16 23:24 /input/test_lzo/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         61 2018-03-16 23:24 /input/test_lzo/part-00001.lzo</span><br></pre></td></tr></table></figure><p></p><p>至此配置与测试完成。</p><h3 id="四、配置与测试中存问题"><a href="#四、配置与测试中存问题" class="headerlink" title="四、配置与测试中存问题"></a>四、配置与测试中存问题</h3><h4 id="1、引用native，缺少LD-LIBRARY-PATH"><a href="#1、引用native，缺少LD-LIBRARY-PATH" class="headerlink" title="1、引用native，缺少LD_LIBRARY_PATH"></a>1、引用native，缺少LD_LIBRARY_PATH</h4><h5 id="1-1、错误提示："><a href="#1-1、错误提示：" class="headerlink" title="1.1、错误提示："></a>1.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: native-lzo library not available</span><br><span class="line">  at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120)</span><br><span class="line">  at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178)</span><br><span class="line">  at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111)</span><br><span class="line">  at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)</span><br><span class="line">  at org.apache.spark.scheduler.Task.run(Task.scala:108)</span><br><span class="line">  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><h5 id="1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容："><a href="#1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容：" class="headerlink" title="1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容："></a>1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h4 id="2、无法找到LzopCodec类"><a href="#2、无法找到LzopCodec类" class="headerlink" title="2、无法找到LzopCodec类"></a>2、无法找到LzopCodec类</h4><h5 id="2-1、错误提示："><a href="#2-1、错误提示：" class="headerlink" title="2.1、错误提示："></a>2.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found.</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)</span><br><span class="line">    at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found</span><br><span class="line">    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)</span><br></pre></td></tr></table></figure><h5 id="2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容："><a href="#2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容：" class="headerlink" title="2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容："></a>2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS之垃圾回收箱配置及使用</title>
      <link href="/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>HDFS为每个用户创建一个回收站:<br>目录:/user/用户/.Trash/Current, 系统回收站都有一个周期,周期过后hdfs会彻底删除清空,周期内可以恢复。<br><a id="more"></a></p><h4 id="一、HDFS删除文件-无法恢复"><a href="#一、HDFS删除文件-无法恢复" class="headerlink" title="一、HDFS删除文件,无法恢复"></a>一、HDFS删除文件,无法恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">Deleted /123.log</span><br></pre></td></tr></table></figure><h4 id="二、-启用回收站功能"><a href="#二、-启用回收站功能" class="headerlink" title="二、 启用回收站功能"></a>二、 启用回收站功能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vim core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多长时间创建CheckPoint NameNode节点上运行的CheckPointer </span><br><span class="line">从Current文件夹创建CheckPoint; 默认: 0 由fs.trash.interval项指定 --&gt;</span><br><span class="line">&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多少分钟.Trash下的CheckPoint目录会被删除,</span><br><span class="line">该配置服务器设置优先级大于客户端，默认:不启用 --&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;  -- 清除周期分钟(24小时)</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="1、重启hdfs服务"><a href="#1、重启hdfs服务" class="headerlink" title="1、重启hdfs服务"></a>1、重启hdfs服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sbin]$ ./stop-dfs.sh</span><br><span class="line">[hadoop@hadoop001 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure><h5 id="2、测试回收站功能"><a href="#2、测试回收站功能" class="headerlink" title="2、测试回收站功能"></a>2、测试回收站功能</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /123.log</span><br></pre></td></tr></table></figure><h5 id="文件删除成功存放回收站路径下"><a href="#文件删除成功存放回收站路径下" class="headerlink" title="文件删除成功存放回收站路径下"></a>文件删除成功存放回收站路径下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="恢复文件"><a href="#恢复文件" class="headerlink" title="恢复文件"></a>恢复文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /456.log</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="删除文件跳过回收站"><a href="#删除文件跳过回收站" class="headerlink" title="删除文件跳过回收站"></a>删除文件跳过回收站</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.log</span><br><span class="line">Deleted /456.log</span><br></pre></td></tr></table></figure><p>源码参考：<br><a href="https://blog.csdn.net/tracymkgld/article/details/17557655" target="_blank" rel="noopener">https://blog.csdn.net/tracymkgld/article/details/17557655</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark序列化，你了解吗</title>
      <link href="/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h3 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h3><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p><h4 id="测试代码："><a href="#测试代码：" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="测试结果："><a href="#测试结果：" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_1.png" alt="enter description here"></p><h3 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h3><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p><h4 id="测试代码：-1"><a href="#测试代码：-1" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/716_2.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">      .registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="测试结果：-1"><a href="#测试结果：-1" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_3.png" alt="enter description here"></p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line"> * Component which configures serialization, compression and encryption for various Spark</span><br><span class="line"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span><br><span class="line"> */</span><br><span class="line">private[spark] class SerializerManager(</span><br><span class="line">    defaultSerializer: Serializer,</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    encryptionKey: Option[Array[Byte]]) &#123;</span><br><span class="line"></span><br><span class="line">  def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None)</span><br><span class="line"></span><br><span class="line">  private[this] val kryoSerializer = new KryoSerializer(conf)</span><br><span class="line"></span><br><span class="line">  private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]]</span><br><span class="line">  private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123;</span><br><span class="line">    val primitiveClassTags = Set[ClassTag[_]](</span><br><span class="line">      ClassTag.Boolean,</span><br><span class="line">      ClassTag.Byte,</span><br><span class="line">      ClassTag.Char,</span><br><span class="line">      ClassTag.Double,</span><br><span class="line">      ClassTag.Float,</span><br><span class="line">      ClassTag.Int,</span><br><span class="line">      ClassTag.Long,</span><br><span class="line">      ClassTag.Null,</span><br><span class="line">      ClassTag.Short</span><br><span class="line">    )</span><br><span class="line">    val arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">    primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 状态管理函数，你了解吗</title>
      <link href="/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>Spark Streaming 状态管理函数包括updateStateByKey和mapWithState</p><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p><p>统计全局的key的状态，但是就算没有数据输入，他也会在每一个批次的时候返回之前的key的状态。</p><p>这样的缺点：如果数据量太大的话，我们需要checkpoint数据会占用较大的存储。而且效率也不高<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//[root@bda3 ~]# nc -lk 9999  </span><br><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start()  // 一定要写  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">  /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">  def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="二、mapWithState-效率更高，生产中建议使用"><a href="#二、mapWithState-效率更高，生产中建议使用" class="headerlink" title="二、mapWithState  (效率更高，生产中建议使用)"></a>二、mapWithState (效率更高，生产中建议使用)</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。</p><p>这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储。</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/**  </span><br><span class="line"> * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every  </span><br><span class="line"> * second starting with initial value of word count.  </span><br><span class="line"> * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;  </span><br><span class="line"> *   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive  </span><br><span class="line"> *   data.  </span><br><span class="line"> *  </span><br><span class="line"> * To run this on your local machine, you need to first run a Netcat server  </span><br><span class="line"> *    `$ nc -lk 9999`  </span><br><span class="line"> * and then run the example  </span><br><span class="line"> *    `$ bin/run-example  </span><br><span class="line"> *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`  </span><br><span class="line"> */  </span><br><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;)  </span><br><span class="line">    // Create the context with a 1 second batch size  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)  </span><br><span class="line"></span><br><span class="line">    // Initial state RDD for mapWithState operation  </span><br><span class="line">    val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1)))  </span><br><span class="line"></span><br><span class="line">    // Create a ReceiverInputDStream on target ip:port and count the  </span><br><span class="line">    // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;)  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    // Update the cumulative count using mapWithState  </span><br><span class="line">    // This will give a DStream made of state (which is the cumulative count of the words)  </span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">      StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark和DL/AI结合，谁与争锋? 期待Spark3.0的到来！</title>
      <link href="/2018/06/22/AI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/"/>
      <url>/2018/06/22/AI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p><img src="/assets/blogImg/0622_1.png" alt="enter description here"><br><a id="more"></a></p><p>不知各位，是否关注社区的发展？关注Spark呢？</p><p>官网的Spark图标和解释语已经发生变化了。</p><p>然而在6-18号，社区提出Spark and DL/AI相结合，这无比再一次说明，Spark在大数据的地位是无法撼动的！期待Spark3.0的到来！</p><p>接下来对SPARK-24579的翻译:</p><p>在大数据和人工智能的十字路口，我们看到了Apache Spark作为一个统一的分析引擎以及AI框架如TensorFlow和Apache MXNet (正在孵化中)的兴起及这两大块的巨大成功 。</p><p>大数据和人工智能都是推动企业创新的不可或缺的组成部分， 两个社区的多次尝试，使他们结合在一起。</p><p>我们看到AI社区的努力，为AI框架实现数据解决方案，如TF.DATA和TF.Tror。然而，50+个数据源和内置SQL、数据流和流特征，Spark仍然是对于大数据社区选择。</p><p>这就是为什么我们看到许多努力,将DL/AI框架与Spark结合起来，以利用它的力量，例如，Spark数据源TFRecords、TensorFlowOnSpark, TensorFrames等。作为项目Hydrogen的一部分，这个SPIP将Spark+AI从不同的角度统一起来。</p><p>没有在Spark和外部DL/AI框架之间交换数据，这些集成都是不可能的,也有性能问题。然而，目前还没有一种标准的方式来交换数据，因此实现和性能优化就陷入了困境。例如，在Python中，TensorFlowOnSpark使用Hadoop InputFormat/OutputFormat作为TensorFlow的TFRecords，来加载和保存数据，并将RDD数据传递给TensorFlow。TensorFrames使用TensorFlow的Java API，转换为 Spark DataFrames Rows to/from TensorFlow Tensors 。我们怎样才能降低复杂性呢?</p><p>这里的建议是标准化Spark和DL/AI框架之间的数据交换接口(或格式)，并优化从/到这个接口的数据转换。因此，DL/AI框架可以利用Spark从任何地方加载数据，而无需花费额外的精力构建复杂的数据解决方案，比如从生产数据仓库读取特性或流模型推断。Spark用户可以使用DL/AI框架，而无需学习那里实现的特定数据api。而且双方的开发人员都可以独立地进行性能优化，因为接口本身不会带来很大的开销。</p><p>ISSUE: <a href="https://issues.apache.org/jira/browse/SPARK-24579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24579</a><br>若泽数据，星星本人水平有限，翻译多多包涵。</p><p>对了忘记说了，本ISSUE有个PDF文档，赶快去下载吧。<br><a href="https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf" target="_blank" rel="noopener">https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最前沿！带你读Structured Streaming重量级论文！</title>
      <link href="/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/"/>
      <url>/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-论文下载地址"><a href="#1-论文下载地址" class="headerlink" title="1.论文下载地址"></a>1.论文下载地址</h4><p><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf</a></p><h4 id="2-前言"><a href="#2-前言" class="headerlink" title="2.前言"></a>2.前言</h4><p>建议首先阅读Structured Streaming官网：<a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a><br>以及这两篇Databricks在2016年关于Structured Streaming的文章：</p><p><a href="https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html</a></p><p><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</a></p><p>言归正传<br>该论文收录自2018年ACM SIGMOD会议，是由美国计算机协会（ACM）发起的、在数据库领域具有最高学术地位的国际性学术会议。论文的作者为Databricks的工程师及Spark的开发者，其权威性、重要程度不言而喻。文章开头为该论文的下载地址，供读者阅读交流。本文对该论文进行简要的总结，希望大家能够下载原文细细品读，了解最前沿的大数据技术。</p><h4 id="3-论文简要总结"><a href="#3-论文简要总结" class="headerlink" title="3.论文简要总结"></a>3.论文简要总结</h4><p>题目：Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark</p><h5 id="3-1-摘要"><a href="#3-1-摘要" class="headerlink" title="3.1 摘要"></a>3.1 摘要</h5><p>摘要是一篇论文的精髓，这里给出摘要完整的翻译。<br>随着实时数据的普遍存在，我们需要可扩展的、易用的、易于集成的流式处理系统。结构化流是基于我们对Spark Streaming的经验开发出来的高级别的Spark流式API。结构化流与其他现有的流式API，如谷歌的Dataflow，主要有两点不同。第一，它是一个基于自动增量化的关系型查询API，无需用户自己构建DAG；第二，结构化流旨在于支持端到端的实时应用并整合流与批处理的交互分析。在实践中，我们发现这种整合是一个关键的挑战。结构化流通过Spark SQL的代码生成引擎实现了很高的性能，是Apache Flink的两倍以及Apache Kafka的90倍。它还提供了丰富的运行特性，如回滚、代码更新以及流/批混合执行。最后我们描述了系统的设计以及部署在Databricks几百个生产节点的一个用例。</p><h5 id="3-2-流式处理面临的挑战"><a href="#3-2-流式处理面临的挑战" class="headerlink" title="3.2 流式处理面临的挑战"></a>3.2 流式处理面临的挑战</h5><p>(1) 复杂、低级别的API<br>(2) 端到端应用的集成<br>(3) 运行时挑战：容灾，代码更新，监控等<br>(4) 成本和性能挑战</p><h5 id="3-3-结构化流基本概念"><a href="#3-3-结构化流基本概念" class="headerlink" title="3.3 结构化流基本概念"></a>3.3 结构化流基本概念</h5><p><img src="/assets/blogImg/614_1.png" alt="enter description here"><br>图1 结构化流的组成部分</p><p>(1) Input and Output<br>Input sources 必须是 replayable 的，支持节点宕机后从当前输入继续读取。例如：Apache Kinesis和Apache Kafka。<br>Output sinks 必须支持 idempotent （幂等），确保在节点宕机时可靠的恢复。<br>(2) APIs<br>编写结构化流程序时，可以使用Spark SQL的APIs：DataFrame和SQL来查询streams和tables，该查询定义了一个output table（输出表），用来接收来自steam的数据。engine决定如何计算并将输出表 incrementally（增量地）写入sink。不同的sinks支持不同的output modes（输出模式，后面会提到）。<br>为了处理流式数据，结构化流还增加了一些APIs与已有的Spark SQL API相配合：</p><ul><li>a. Triggers 控制engine多久执行一次计算</li><li>b. event time 是数据源的时间戳；watermark 策略，与event time 相差一段时间后不再接收数据。</li><li>c.Stateful operator（状态算子），类似于Spark Streaming 的updateStateByKey。</li></ul><p>(3) 执行<br>一旦接收到了查询，结构化流就会进行优化递增，并开始执行。结构化流使用两种持久化存储的方式实现容错：</p><ul><li>a.write-ahead log （WAL：预写日志）持续追踪哪些数据已被执行，确保数据的可靠写入。</li><li>b.系统采用大规模的 state store（状态存储）来保存长时间运行的聚合算子的算子状态快照。</li></ul><h5 id="3-4-编程模型"><a href="#3-4-编程模型" class="headerlink" title="3.4 编程模型"></a>3.4 编程模型</h5><p>结构化流将谷歌的Dataflow、增量查询和Spark Streaming 结合起来，以便在Spark SQL下实现流式处理。</p><ul><li>a. A Short Example<pre><code>首先从一个批处理作业开始，统计一个web应用在不同国家的点击数。假设输入数据是一个JSON文件，输出一个Parquet文件，该作业可以通过DataFrame来完成：</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read from static data</span><br><span class="line">2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a static data sink</span><br><span class="line">6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure></li></ul><p>把该作业变成使用结构化流仅仅需要改变输入和输出源，例如，如果新的JSON文件continually（持续地）上传，我们只需要改变第一行和最后一行。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read streaming data</span><br><span class="line">2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a streaming data sink</span><br><span class="line">6counts . writeStream . format (&quot; parquet &quot;)</span><br><span class="line">7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure><p></p><p>结构化流也支持 windowing（窗口）和通过Spark SQL已存在的聚合算子处理event time。例如：我们可以通过修改中间的代码，计算1小时的滑动窗口，每五分钟前进一次：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1// Count events by windows on the &quot; time &quot; field</span><br><span class="line">2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()</span><br></pre></td></tr></table></figure><p></p><ul><li>b. 编程模型语义<br><img src="/assets/blogImg/614_2.png" alt="enter description here"></li></ul><p>图 2 两种输出模式</p><ul><li>i. 每一个输入源提供了一个基于时间的部分有序的记录集（set of records），例如，Kafka将流式数据分为各自有序的partitions。</li><li>ii. 用户提供跨输入数据执行的查询，该输入数据可以在任意给定的处理时间点输出一个 result table（结果表）。结构化流总会产生与所有输入源的数据的前缀上（prefix of the data in all input sources）查询相一致的结果。</li><li>iii. Triggers 告诉系统何时去运行一个新的增量计算，何时更新result table。例如，在微批处理模式，用户希望会每分钟触发一次增量计算。</li><li>iiii. engine支持三种output mode：<pre><code>  Complete：engine一次写所有result table。  Append：engine仅仅向sink增加记录。  Update：engine基于key更新每一个record，更新值改变的keys。该模型有两个特性：第一，结果表的内容独立于输出模式。第二，该模型具有很强的语义一致性，被称为prefix consistency。</code></pre>c.流式算子<pre><code>加入了两种类型的算子：watermarking算子告诉系统何时关闭event time window和输出结果；结构化流允许用户通过withWatermark算子来设置一个watermark，该算子给系统设置一个给定时间戳C的延迟阈值tC，在任意时间点，C的watermark是max（C）-tC。stateful operators允许用户编写自定义逻辑来实现复杂的功能。</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1// Define an update function that simply tracks the</span><br><span class="line"> 2// number of events for each key as its state , returns</span><br><span class="line"> 3// that as its result , and times out keys after 30 min.</span><br><span class="line"> 4def updateFunc (key: UserId , newValues : Iterator [ Event ],</span><br><span class="line"> 5state : GroupState [Int ]): Int = &#123;</span><br><span class="line"> 6val totalEvents = state .get () + newValues . size ()</span><br><span class="line"> 7state . update ( totalEvents )</span><br><span class="line"> 8state . setTimeoutDuration (&quot;30 min&quot;)</span><br><span class="line"> 9return totalEvents</span><br><span class="line">10&#125;</span><br><span class="line">11// Use this update function on a stream , returning a</span><br><span class="line">12// new table lens that contains the session lengths .</span><br><span class="line">13lens = events . groupByKey ( event =&gt; event . userId )</span><br><span class="line">14. mapGroupsWithState ( updateFunc )</span><br></pre></td></tr></table></figure></li></ul><p>用mapGroupWithState算子来追踪每个会话的事件数量，30分钟后关闭会话。</p><h5 id="3-5-运行特性"><a href="#3-5-运行特性" class="headerlink" title="3.5 运行特性"></a>3.5 运行特性</h5><p>(1) 代码更新（code update）<br>开发者能够在编程过程中更新UDF，并且可以简单的重启以使用新版本的代码。<br>(2) 手动回滚（manual rollback）<br>有时在用户发现之前，程序会输出错误的结果，因此回滚至关重要。结构化流很容易定位问题所在。同时手动回滚与前面提到的prefix consistency有很好的交互。<br>(3) 流式和批次混合处理<br>这是结构化流最显而易见的好处，用户能够共用流式处理和批处理作业的代码。<br>(4) 监控<br>结构化流使用Spark已有的API和结构化日志来报告信息，例如处理过的记录数量，跨网络的字节数等。这些接口被Spark开发者所熟知，并易于连接到不同的UI工具。</p><h5 id="3-6-生产用例与总结"><a href="#3-6-生产用例与总结" class="headerlink" title="3.6 生产用例与总结"></a>3.6 生产用例与总结</h5><p>给出简要架构图，篇幅原因不再赘述，希望详细了解的下载论文自行阅读。本文只挑选了部分关键点进行了浅层次的叙述，希望读者能够将论文下载下来认真品读，搞懂开发者的开发思路，跟上大数据的前沿步伐。<br><img src="/assets/blogImg/614_3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产开发必用-Spark RDD转DataFrame的两种方法</title>
      <link href="/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
      <url>/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:<br><a href="https://blog.csdn.net/lemonzhaotao/article/details/80211231" target="_blank" rel="noopener">https://blog.csdn.net/lemonzhaotao/article/details/80211231</a></p><p>在本篇文章中，将介绍RDD转换为DataFrame的2种方式</p><p>官网之RDD转DF:<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds</a><br><a id="more"></a><br>DataFrame 与 RDD 的交互</p><p>Spark SQL它支持两种不同的方式转换已经存在的RDD到DataFrame</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>第一种方式是使用反射的方式，用反射去推倒出来RDD里面的schema。这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。<br>对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常非常多的，是不可能只有20来个字段的。<br>示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 1</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runInferSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  val df = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">              .map(x =&gt; People(x(0), x(1).trim.toInt))  //将rdd的每一行都转换成了一个people</span><br><span class="line">              .toDF         //必须先导入import spark.implicits._  不然这个方法会报错</span><br><span class="line">  df.show()</span><br><span class="line">  df.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">  // 这个DF包含了两个字段name和age</span><br><span class="line">  val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">  // teenager(0)代表第一个字段</span><br><span class="line">  // 取值的第一种方式：index from zero</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line">  // 取值的第二种方式：byName</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()</span><br><span class="line">&#125;</span><br><span class="line">// 注意：case class必须定义在main方法之外；否则会报错</span><br><span class="line">case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p></p><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>创建一个DataFrame，使用编程的方式 这个方式用的非常多。通过编程方式指定schema ，对于第一种方式的schema其实定义在了case class里面了。<br>官网解读：<br>当我们的case class不能提前定义(因为业务处理的过程当中，你的字段可能是在变化的),因此使用case class很难去提前定义。<br>使用该方式创建DF的三大步骤：</p><ul><li>Create an RDD of Rows from the original RDD;</li><li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li><li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 2</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  // 1.转成RDD</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  // 2.定义schema，带有StructType的</span><br><span class="line">  // 定义schema信息</span><br><span class="line">  val schemaString = &quot;name age&quot;</span><br><span class="line">  // 对schema信息按空格进行分割</span><br><span class="line">  // 最终fileds里包含了2个StructField</span><br><span class="line">  val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">                            // 字段类型，字段名称判断是不是为空</span><br><span class="line">                           .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">  val schema = StructType(fields)</span><br><span class="line">  // 3.把我们的schema信息作用到RDD上</span><br><span class="line">  //   这个RDD里面包含了一些行</span><br><span class="line">  // 形成Row类型的RDD</span><br><span class="line">  val rowRDD = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">                  .map(x =&gt; Row(x(0), x(1).trim))</span><br><span class="line">  // 通过SparkSession创建一个DataFrame</span><br><span class="line">  // 传进来一个rowRDD和schema，将schema作用到rowRDD上</span><br><span class="line">  val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="扩展-生产上创建DataFrame的代码举例"><a href="#扩展-生产上创建DataFrame的代码举例" class="headerlink" title="[扩展]生产上创建DataFrame的代码举例"></a>[扩展]生产上创建DataFrame的代码举例</h4><p>在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，这里将展示部分代码：</p><h4 id="Schema的定义"><a href="#Schema的定义" class="headerlink" title="Schema的定义"></a>Schema的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">object AccessConvertUtil &#123;</span><br><span class="line">  val struct = StructType(</span><br><span class="line">    Array(</span><br><span class="line">      StructField(&quot;url&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsType&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsId&quot;,LongType),</span><br><span class="line">      StructField(&quot;traffic&quot;,LongType),</span><br><span class="line">      StructField(&quot;ip&quot;,StringType),</span><br><span class="line">      StructField(&quot;city&quot;,StringType),</span><br><span class="line">      StructField(&quot;time&quot;,StringType),</span><br><span class="line">      StructField(&quot;day&quot;,StringType)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  /**</span><br><span class="line">    * 根据输入的每一行信息转换成输出的样式</span><br><span class="line">    */</span><br><span class="line">  def parseLog(log:String) = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val splits = log.split(&quot;\t&quot;)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      val traffic = splits(2).toLong</span><br><span class="line">      val ip = splits(3)</span><br><span class="line">      val domain = &quot;http://www.imooc.com/&quot;</span><br><span class="line">      val cms = url.substring(url.indexOf(domain) + domain.length)</span><br><span class="line">      val cmsTypeId = cms.split(&quot;/&quot;)</span><br><span class="line">      var cmsType = &quot;&quot;</span><br><span class="line">      var cmsId = 0l</span><br><span class="line">      if (cmsTypeId.length &gt; 1) &#123;</span><br><span class="line">        cmsType = cmsTypeId(0)</span><br><span class="line">        cmsId = cmsTypeId(1).toLong</span><br><span class="line">      &#125;</span><br><span class="line">      val city = IpUtils.getCity(ip)</span><br><span class="line">      val time = splits(0)</span><br><span class="line">      val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;)</span><br><span class="line">      //这个Row里面的字段要和struct中的字段对应上</span><br><span class="line">      Row(url, cmsType, cmsId, traffic, ip, city, time, day)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt; Row(0)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object SparkStatCleanJob &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;)</span><br><span class="line">    //accessRDD.take(10).foreach(println)</span><br><span class="line">    //RDD ==&gt; DF，创建生成DataFrame</span><br><span class="line">    val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)),</span><br><span class="line">      AccessConvertUtil.struct)</span><br><span class="line">    accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite)</span><br><span class="line">            .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你大爷永远是你大爷，RDD血缘关系源码详解！</title>
      <link href="/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/"/>
      <url>/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="一、RDD的依赖关系"><a href="#一、RDD的依赖关系" class="headerlink" title="一、RDD的依赖关系"></a>一、RDD的依赖关系</h4><p>RDD的依赖关系分为两类：宽依赖和窄依赖。我们可以这样认为：</p><ul><li><p>（1）窄依赖：每个parent RDD 的 partition 最多被 child RDD 的一个partition 使用。</p></li><li><p>（2）宽依赖：每个parent RDD partition 被多个 child RDD 的partition 使用。</p></li></ul><p>窄依赖每个 child RDD 的 partition 的生成操作都是可以并行的，而宽依赖则需要所有的 parent RDD partition shuffle 结果得到后再进行。<br><a id="more"></a></p><h4 id="二、org-apache-spark-Dependency-scala-源码解析"><a href="#二、org-apache-spark-Dependency-scala-源码解析" class="headerlink" title="二、org.apache.spark.Dependency.scala 源码解析"></a>二、org.apache.spark.Dependency.scala 源码解析</h4><p>Dependency是一个抽象类：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">  def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。</p><h5 id="（1）NarrowDependency也是一个抽象类"><a href="#（1）NarrowDependency也是一个抽象类" class="headerlink" title="（1）NarrowDependency也是一个抽象类"></a>（1）NarrowDependency也是一个抽象类</h5><p>定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;  </span><br><span class="line">/**</span><br><span class="line">   * Get the parent partitions for a child partition.</span><br><span class="line">   * @param partitionId a partition of the child RDD</span><br><span class="line">   * @return the partitions of the parent RDD that the child partition depends upon</span><br><span class="line">   */</span><br><span class="line">  def getParents(partitionId: Int): Seq[Int]</span><br><span class="line"></span><br><span class="line">  override def rdd: RDD[T] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>窄依赖又有两个具体的实现：OneToOneDependency和RangeDependency。<br>（a）OneToOneDependency指child RDD的partition只依赖于parent RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = List(partitionId)</span><br><span class="line">&#125;</span><br><span class="line">        （b）RangeDependency指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。</span><br><span class="line"></span><br><span class="line">// Denpendency.scala</span><br><span class="line">class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)  </span><br><span class="line">  extends NarrowDependency[T](rdd) &#123;//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = &#123;    </span><br><span class="line">    if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      List(partitionId - outStart + inStart)//表示于当前索引的相对位置</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      Nil</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="（2）ShuffleDependency指宽依赖"><a href="#（2）ShuffleDependency指宽依赖" class="headerlink" title="（2）ShuffleDependency指宽依赖"></a>（2）ShuffleDependency指宽依赖</h5><p>表示一个parent RDD的partition会被child RDD的partition使用多次。需要经过shuffle才能形成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</span><br><span class="line">    @transient private val _rdd: RDD[_ &lt;: Product2[K, V]],    </span><br><span class="line">    val partitioner: Partitioner,    </span><br><span class="line">    val serializer: Serializer = SparkEnv.get.serializer,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = false)</span><br><span class="line">  extends Dependency[Product2[K, V]] &#123;  //shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的</span><br><span class="line">  override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"></span><br><span class="line">  private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName</span><br><span class="line">  private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName</span><br><span class="line">  private[spark] val combinerClassName: Option[String] =</span><br><span class="line">    Option(reflect.classTag[C]).map(_.runtimeClass.getName)  //获取shuffleId</span><br><span class="line">  val shuffleId: Int = _rdd.context.newShuffleId()  //向shuffleManager注册shuffle信息</span><br><span class="line">  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, this)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。</p><h4 id="三、两种依赖的区分"><a href="#三、两种依赖的区分" class="headerlink" title="三、两种依赖的区分"></a>三、两种依赖的区分</h4><p>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java可扩展线程池之ThreadPoolExecutor</title>
      <link href="/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/"/>
      <url>/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1、ThreadPoolExecutor"><a href="#1、ThreadPoolExecutor" class="headerlink" title="1、ThreadPoolExecutor"></a>1、ThreadPoolExecutor</h4><p>我们知道ThreadPoolExecutor是可扩展的,它提供了几个可以在子类中改写的空方法如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;</span><br><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;  </span><br><span class="line">protected void terminated() &#123; &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="2、为什么要进行扩展？"><a href="#2、为什么要进行扩展？" class="headerlink" title="2、为什么要进行扩展？"></a>2、为什么要进行扩展？</h4><p>因为在实际应用中，可以对线程池运行状态进行跟踪，输出一些有用的调试信息，以帮助故障诊断。</p><h4 id="3、ThreadPoolExecutor-Worker的run方法实现"><a href="#3、ThreadPoolExecutor-Worker的run方法实现" class="headerlink" title="3、ThreadPoolExecutor.Worker的run方法实现"></a>3、ThreadPoolExecutor.Worker的run方法实现</h4><p>通过看源码我们发现 ThreadPoolExecutor的工作线程其实就是Worker实例，Worker.runTask()会被线程池以多线程模式异步调用，则以上三个方法也将被多线程同时访问。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">1// 基于jdk1.8.0_161final void runWorker(Worker w) &#123;</span><br><span class="line"> 2         Thread wt = Thread.currentThread();</span><br><span class="line"> 3         Runnable task = w.firstTask;</span><br><span class="line"> 4         w.firstTask = null;</span><br><span class="line"> 5         w.unlock(); // allow interrupts</span><br><span class="line"> 6         boolean completedAbruptly = true;        </span><br><span class="line"> 7             try &#123;            </span><br><span class="line"> 8             while (task != null || (task = getTask()) != null) &#123;</span><br><span class="line"> 9                  w.lock();                </span><br><span class="line">10              if ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class="line">11                     (Thread.interrupted() &amp;&amp;</span><br><span class="line">12                      runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class="line">13                    !wt.isInterrupted())</span><br><span class="line">14                    wt.interrupt();               </span><br><span class="line">15              try &#123;</span><br><span class="line">16                    beforeExecute(wt, task);</span><br><span class="line">17                    Throwable thrown = null;                   </span><br><span class="line">18              try &#123;</span><br><span class="line">19                        task.run();</span><br><span class="line">20                    &#125; catch (RuntimeException x) &#123;</span><br><span class="line">21                        thrown = x; throw x;</span><br><span class="line">22                    &#125; catch (Error x) &#123;</span><br><span class="line">23                        thrown = x; throw x;</span><br><span class="line">24                    &#125; catch (Throwable x) &#123;</span><br><span class="line">25                        thrown = x; throw new Error(x);</span><br><span class="line">26                    &#125; finally &#123;</span><br><span class="line">27                        afterExecute(task, thrown);</span><br><span class="line">28                    &#125;</span><br><span class="line">29                &#125; finally &#123;</span><br><span class="line">30                    task = null;</span><br><span class="line">31                    w.completedTasks++;</span><br><span class="line">32                    w.unlock();</span><br><span class="line">33                &#125;</span><br><span class="line">34            &#125;</span><br><span class="line">35            completedAbruptly = false;</span><br><span class="line">36        &#125; finally &#123;</span><br><span class="line">37            processWorkerExit(w, completedAbruptly);</span><br><span class="line">38        &#125;</span><br><span class="line">39    &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="4、扩展线程池实现"><a href="#4、扩展线程池实现" class="headerlink" title="4、扩展线程池实现"></a>4、扩展线程池实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1public class ExtThreadPool &#123;</span><br><span class="line"> 2    public static class MyTask implements Runnable &#123;</span><br><span class="line"> 3        public String name;        </span><br><span class="line"> 4        public MyTask(String name) &#123;            </span><br><span class="line"> 5          this.name = name;</span><br><span class="line"> 6        &#125;       </span><br><span class="line"> 7        public void run() &#123;</span><br><span class="line"> 8            System.out.println(&quot;正在执行:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name);            try &#123;</span><br><span class="line"> 9                Thread.sleep(100);</span><br><span class="line">10            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">11                e.printStackTrace();</span><br><span class="line">12            &#125;</span><br><span class="line">13        &#125;</span><br><span class="line">14    &#125;    </span><br><span class="line">15public static void main(String args[]) throws InterruptedException &#123;</span><br><span class="line">16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,</span><br><span class="line">17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123;            </span><br><span class="line">18protected void beforeExecute(Thread t, Runnable r) &#123;</span><br><span class="line">19 System.out.println(&quot;准备执行：&quot; + ((MyTask) r).name);</span><br><span class="line">20&#125;            </span><br><span class="line">21protected void afterExecute(Thread t, Runnable r) &#123;</span><br><span class="line">22  System.out.println(&quot;执行完成&quot; + ((MyTask) r).name);</span><br><span class="line">23&#125;            </span><br><span class="line">24protected void terminated() &#123;</span><br><span class="line">25  System.out.println(&quot;线程池退出！&quot;);</span><br><span class="line">26&#125;</span><br><span class="line">27&#125;;        </span><br><span class="line">28for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">29 MyTask task = new MyTask(&quot;TASK--&quot; + i);</span><br><span class="line">30            executorService.execute(task);</span><br><span class="line">31            Thread.sleep(10);</span><br><span class="line">32        &#125;</span><br><span class="line">33 executorService.shutdown();</span><br><span class="line">34    &#125;</span><br><span class="line">35&#125;</span><br></pre></td></tr></table></figure><p>输出结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">准备执行：TASK–0 </span><br><span class="line">正在执行:Thread ID:10,Task Name:TASK–0 </span><br><span class="line">准备执行：TASK–1 </span><br><span class="line">正在执行:Thread ID:11,Task Name:TASK–1 </span><br><span class="line">准备执行：TASK–2 </span><br><span class="line">正在执行:Thread ID:12,Task Name:TASK–2 </span><br><span class="line">准备执行：TASK–3 </span><br><span class="line">正在执行:Thread ID:13,Task Name:TASK–3 </span><br><span class="line">准备执行：TASK–4 </span><br><span class="line">正在执行:Thread ID:14,Task Name:TASK–4 </span><br><span class="line">线程池退出！</span><br></pre></td></tr></table></figure><p></p><p>这样就实现了在执行前后进行的一些控制，除此之外我们还可以输出每个线程的执行时间，或者一些其他增强操作。</p><h4 id="5、思考？"><a href="#5、思考？" class="headerlink" title="5、思考？"></a>5、思考？</h4><p>请读者思考shutdownNow和shutdown方法的区别？<br>如何优雅的关闭线程池？</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark 技术团队开源机器学习平台 MLflow</title>
      <link href="/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/"/>
      <url>/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。<br><a id="more"></a><br><img src="/assets/blogImg/612_1.png" alt="enter description here"><br>Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。<br>MLflow 正是受现有的 ML 平台启发，主打开放性：</p><ul><li>开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。</li><li>开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。<br>MLflow 目前的 alpha 版本包含三个组件：<br><img src="/assets/blogImg/612_2.png" alt="enter description here"><br>其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow</span><br><span class="line"></span><br><span class="line"># Log parameters (key-value pairs)</span><br><span class="line">mlflow.log_param(&quot;num_dimensions&quot;, 8)</span><br><span class="line">mlflow.log_param(&quot;regularization&quot;, 0.1)</span><br><span class="line"></span><br><span class="line"># Log a metric; metrics can be updated throughout the run</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.1)</span><br><span class="line">...</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.45)</span><br><span class="line"></span><br><span class="line"># Log artifacts (output files)</span><br><span class="line">mlflow.log_artifact(&quot;roc.png&quot;)</span><br><span class="line">mlflow.log_artifact(&quot;model.pkl&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name: My Project</span><br><span class="line">conda_env: conda.yaml</span><br><span class="line">entry_points:</span><br><span class="line">  main:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">      regularization: &#123;type: float, default: 0.1&#125;</span><br><span class="line">    command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot;</span><br><span class="line">  validate:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">    command: &quot;python validate.py &#123;data_file&#125;&quot;</span><br></pre></td></tr></table></figure><p></p><p>MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_created: 2018-02-21T13:21:34.12</span><br><span class="line">flavors:</span><br><span class="line">  sklearn:</span><br><span class="line">    sklearn_version: 0.19.1</span><br><span class="line">    pickled_model: model.pkl</span><br><span class="line">  python_function:</span><br><span class="line">    loader_module: mlflow.sklearn</span><br><span class="line">    pickled_model: model.pkl</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStreaming 状态管理函数的选择比较</title>
      <link href="/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/"/>
      <url>/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</span><br></pre></td></tr></table></figure><p></p><p>也即是说它会统计全局的key的状态，就算没有数据输入，它也会在每一个批次的时候返回之前的key的状态。</p><p>缺点：若数据量太大的话，需要checkpoint的数据会占用较大的存储，效率低下。</p><p>程序示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：要使用updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start() </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;   </span><br><span class="line"> /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">def updateFunction(currentValues: Seq[Int], preValues: Option[Int]):                                Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="二、mapWithState"><a href="#二、mapWithState" class="headerlink" title="二、mapWithState"></a>二、mapWithState</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。效率更高，生产中建议使用</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount </span><br><span class="line">      &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;StatefulNetworkWordCount&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)   </span><br><span class="line">    val initialRDD = ssc.sparkContext</span><br><span class="line">      .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1)))  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], </span><br><span class="line">     state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">    StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="三、源码分析"><a href="#三、源码分析" class="headerlink" title="三、源码分析"></a>三、源码分析</h4><h5 id="upateStateByKey："><a href="#upateStateByKey：" class="headerlink" title="upateStateByKey："></a>upateStateByKey：</h5><ul><li>map返回的是MappedDStream，而MappedDStream并没有updateStateByKey方法，并且它的父类DStream中也没有该方法。但是DStream的伴生对象中有一个隐式转换函数：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">object DStream &#123;</span><br><span class="line">  implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)])</span><br><span class="line">      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null):</span><br><span class="line">    PairDStreamFunctions[K, V] = &#123;</span><br><span class="line">    new PairDStreamFunctions[K, V](stream)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>跟进去 PairDStreamFunctions ，发现最终调用的是自己的updateStateByKey。<br>其中updateFunc就要传入的参数，他是一个函数，Seq[V]表示当前key对应的所有值，<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Option[S] 是当前key的历史状态，返回的是新的状态。</span><br><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Seq[V], Option[S]) =&gt; Option[S]</span><br><span class="line">  ): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>最终调用：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],</span><br><span class="line">    partitioner: Partitioner,</span><br><span class="line">    rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  val cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">  val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123;</span><br><span class="line">    cleanedFunc(it)</span><br><span class="line">  &#125;</span><br><span class="line">  new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>再跟进去 new StateDStream:<br>在这里面new出了一个StateDStream对象。在其compute方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中StateDStream的父类计算出的RDD（本次batch的单词计数）分别是prevStateRDD和parentRDD，然后在调用 computeUsingPreviousRDD 方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private [this] def computeUsingPreviousRDD(</span><br><span class="line">    batchTime: Time,</span><br><span class="line">    parentRDD: RDD[(K, V)],</span><br><span class="line">    prevStateRDD: RDD[(K, S)]) = &#123;</span><br><span class="line">  // Define the function for the mapPartition operation on cogrouped RDD;</span><br><span class="line">  // first map the cogrouped tuple to tuples of required type,</span><br><span class="line">  // and then apply the update function</span><br><span class="line">  val updateFuncLocal = updateFunc</span><br><span class="line">  val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123;</span><br><span class="line">    val i = iterator.map &#123; t =&gt;</span><br><span class="line">      val itr = t._2._2.iterator</span><br><span class="line">      val headOption = if (itr.hasNext) Some(itr.next()) else None</span><br><span class="line">      (t._1, t._2._1.toSeq, headOption)</span><br><span class="line">    &#125;</span><br><span class="line">    updateFuncLocal(batchTime, i)</span><br><span class="line">  &#125;</span><br><span class="line">  val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</span><br><span class="line">  val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</span><br><span class="line">  Some(stateRDD)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在这里两个RDD进行cogroup然后应用updateStateByKey传入的函数。我们知道cogroup的性能是比较低下，参考<a href="http://lxw1234.com/archives/2015/07/384.htm。" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/07/384.htm。</a></p><h5 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState:"></a>mapWithState:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Experimental</span><br><span class="line">def mapWithState[StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    spec: StateSpec[K, V, StateType, MappedType]</span><br><span class="line">  ): MapWithStateDStream[K, V, StateType, MappedType] = &#123;</span><br><span class="line">  new MapWithStateDStreamImpl[K, V, StateType, MappedType](</span><br><span class="line">    self,</span><br><span class="line">    spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>说明：StateSpec 封装了状态管理函数，并在该方法中创建了MapWithStateDStreamImpl对象。</p><p>MapWithStateDStreamImpl 中创建了一个InternalMapWithStateDStream类型对象internalStream，在MapWithStateDStreamImpl的compute方法中调用了internalStream的getOrCompute方法。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private[streaming] class MapWithStateDStreamImpl[</span><br><span class="line">    KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    dataStream: DStream[(KeyType, ValueType)],</span><br><span class="line">    spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType])</span><br><span class="line">  extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123;</span><br><span class="line"></span><br><span class="line">  private val internalStream =</span><br><span class="line">    new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec)</span><br><span class="line"></span><br><span class="line">  override def slideDuration: Duration = internalStream.slideDuration</span><br><span class="line"></span><br><span class="line">  override def dependencies: List[DStream[_]] = List(internalStream)</span><br><span class="line"></span><br><span class="line">  override def compute(validTime: Time): Option[RDD[MappedType]] = &#123;</span><br><span class="line">    internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>InternalMapWithStateDStream中没有getOrCompute方法，这里调用的是其父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream的Compute方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">/** Method that generates an RDD for the given time */</span><br><span class="line">override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123;</span><br><span class="line">  // Get the previous state or create a new empty state RDD</span><br><span class="line">  val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123;</span><br><span class="line">    case Some(rdd) =&gt;</span><br><span class="line">      if (rdd.partitioner != Some(partitioner)) &#123;</span><br><span class="line">        // If the RDD is not partitioned the right way, let us repartition it using the</span><br><span class="line">        // partition index as the key. This is to ensure that state RDD is always partitioned</span><br><span class="line">        // before creating another state RDD using it</span><br><span class="line">        MapWithStateRDD.createFromRDD[K, V, S, E](</span><br><span class="line">          rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line">    case None =&gt;</span><br><span class="line">      MapWithStateRDD.createFromPairRDD[K, V, S, E](</span><br><span class="line">        spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)),</span><br><span class="line">        partitioner,</span><br><span class="line">        validTime</span><br><span class="line">      )</span><br><span class="line">  &#125;</span><br><span class="line">  // Compute the new state RDD with previous state RDD and partitioned data RDD</span><br><span class="line">  // Even if there is no data RDD, use an empty one to create a new state RDD</span><br><span class="line">  val dataRDD = parent.getOrCompute(validTime).getOrElse &#123;</span><br><span class="line">    context.sparkContext.emptyRDD[(K, V)]</span><br><span class="line">  &#125;</span><br><span class="line">  val partitionedDataRDD = dataRDD.partitionBy(partitioner)</span><br><span class="line">  val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt;</span><br><span class="line">    (validTime - interval).milliseconds</span><br><span class="line">  &#125;</span><br><span class="line">  Some(new MapWithStateRDD(</span><br><span class="line">    prevStateRDD, partitionedDataRDD, mappingFunction, </span><br><span class="line">    validTime, timeoutThresholdTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>根据给定的时间生成一个MapWithStateRDD，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD:dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取partitionedDataRDD。最后将preStateRDD，partitionedDataRDD和用户定义的函数mappingFunction传给新生成的MapWithStateRDD对象返回。</p><p>后续若有兴趣可以继续跟进MapWithStateRDD的compute方法，限于篇幅不再展示。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL 之外部数据源如何成为在企业开发中的一把利器</title>
      <link href="/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/"/>
      <url>/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h4><p>1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。<br>2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。<br>3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。<br>4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。<br><a id="more"></a></p><h4 id="2-读取json文件"><a href="#2-读取json文件" class="headerlink" title="2.读取json文件"></a>2.读取json文件</h4><p>启动shell进行测试<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//标准写法</span><br><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)</span><br><span class="line">//另外一种写法</span><br><span class="line">spark.read.json(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">看看源码这两者之间到底有啥不同呢？</span><br><span class="line">/**</span><br><span class="line">   * Loads a JSON file and returns the results as a `DataFrame`.</span><br><span class="line">   *</span><br><span class="line">   * See the documentation on the overloaded `json()` method with varargs for more details.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def json(path: String): DataFrame = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    json(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line">我们调用josn() 方法其实进行了 overloaded ，我们继续查看</span><br><span class="line"> def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*)</span><br><span class="line"> 这句话是不是很熟悉，其实就是我们的标准写法</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"> df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h4 id="3-读取parquet数据"><a href="#3-读取parquet数据" class="headerlink" title="3 读取parquet数据"></a>3 读取parquet数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h4 id="4-读取hive中的数据"><a href="#4-读取hive中的数据" class="headerlink" title="4 读取hive中的数据"></a>4 读取hive中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">| default|states_raw|      false|</span><br><span class="line">| default|states_seq|      false|</span><br><span class="line">| default|        t1|      false|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line"></span><br><span class="line">spark.table(&quot;states_raw&quot;).show</span><br><span class="line">+-----+------+</span><br><span class="line">| code|  name|</span><br><span class="line">+-----+------+</span><br><span class="line">|hello|  java|</span><br><span class="line">|hello|hadoop|</span><br><span class="line">|hello|  hive|</span><br><span class="line">|hello| sqoop|</span><br><span class="line">|hello|  hdfs|</span><br><span class="line">|hello| spark|</span><br><span class="line">+-----+------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select name from states_raw &quot;).show</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|  java|</span><br><span class="line">|hadoop|</span><br><span class="line">|  hive|</span><br><span class="line">| sqoop|</span><br><span class="line">|  hdfs|</span><br><span class="line">| spark|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure><h4 id="5-保存数据"><a href="#5-保存数据" class="headerlink" title="5 保存数据"></a>5 保存数据</h4><p>注意：</p><ol><li>保存的文件夹不能存在，否则报错(默认情况下，可以选择不同的模式)：org.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;</li><li>保存成文本格式，只能保存一列，否则报错：org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line">//保存</span><br><span class="line">df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[hadoop@hadoop out]$ pwd</span><br><span class="line">/home/hadoop/data/out</span><br><span class="line">[hadoop@hadoop out]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:34 _SUCCESS</span><br><span class="line">[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt </span><br><span class="line">Michael</span><br><span class="line">Andy</span><br><span class="line">Justin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//保存为json格式</span><br><span class="line">df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)</span><br><span class="line"></span><br><span class="line">结果</span><br><span class="line">[hadoop@hadoop data]$ cd out1</span><br><span class="line">[hadoop@hadoop out1]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:35 _SUCCESS</span><br><span class="line">[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;</span><br></pre></td></tr></table></figure></li></ol><p>上面说了在保存数据时如果目录已经存在，在默认模式下会报错，那我们下面讲解保存的几种模式：<br><img src="/assets/blogImg/606_1.png" alt="enter description here"></p><h4 id="6-读取mysql中的数据"><a href="#6-读取mysql中的数据" class="headerlink" title="6 读取mysql中的数据"></a>6 读取mysql中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">val jdbcDF = spark.read</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;123456&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- TBL_ID: long (nullable = false)</span><br><span class="line"> |-- CREATE_TIME: integer (nullable = false)</span><br><span class="line"> |-- DB_ID: long (nullable = true)</span><br><span class="line"> |-- LAST_ACCESS_TIME: integer (nullable = false)</span><br><span class="line"> |-- OWNER: string (nullable = true)</span><br><span class="line"> |-- RETENTION: integer (nullable = false)</span><br><span class="line"> |-- SD_ID: long (nullable = true)</span><br><span class="line"> |-- TBL_NAME: string (nullable = true)</span><br><span class="line"> |-- TBL_TYPE: string (nullable = true)</span><br><span class="line"> |-- VIEW_EXPANDED_TEXT: string (nullable = true)</span><br><span class="line"> |-- VIEW_ORIGINAL_TEXT: string (nullable = true)</span><br><span class="line"></span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="7-spark-SQL操作mysql表数据"><a href="#7-spark-SQL操作mysql表数据" class="headerlink" title="7 spark SQL操作mysql表数据"></a>7 spark SQL操作mysql表数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW jdbcTable</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost:3306&quot;,</span><br><span class="line">  dbtable &quot;basic01.tbls&quot;,</span><br><span class="line">  user &apos;root&apos;,</span><br><span class="line">  password &apos;123456&apos;,</span><br><span class="line">  driver &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">查看：</span><br><span class="line">show tables;</span><br><span class="line">default states_raw      false</span><br><span class="line">default states_seq      false</span><br><span class="line">default t1      false</span><br><span class="line">jdbctable       true</span><br><span class="line"></span><br><span class="line">select * from jdbctable;</span><br><span class="line">1       1519944170      6       0       hadoop  0       1       page_views      MANAGED_TABLE   NULL    NULL</span><br><span class="line">2       1519944313      6       0       hadoop  0       2       page_views_bzip2        MANAGED_TABLE   NULL    NULL</span><br><span class="line">3       1519944819      6       0       hadoop  0       3       page_views_snappy       MANAGED_TABLE   NULL    NULL</span><br><span class="line">21      1520067771      6       0       hadoop  0       21      tt      MANAGED_TABLE   NULL    NULL</span><br><span class="line">22      1520069148      6       0       hadoop  0       22      page_views_seq  MANAGED_TABLE   NULL    NULL</span><br><span class="line">23      1520071381      6       0       hadoop  0       23      page_views_rcfile       MANAGED_TABLE   NULL    NULL</span><br><span class="line">24      1520074675      6       0       hadoop  0       24      page_views_orc_zlib     MANAGED_TABLE   NULL    NULL</span><br><span class="line">27      1520078184      6       0       hadoop  0       27      page_views_lzo_index    MANAGED_TABLE   NULL    NULL</span><br><span class="line">30      1520083461      6       0       hadoop  0       30      page_views_lzo_index1   MANAGED_TABLE   NULL    NULL</span><br><span class="line">31      1524370014      1       0       hadoop  0       31      t1      EXTERNAL_TABLE  NULL    NULL</span><br><span class="line">37      1524468636      1       0       hadoop  0       37      states_raw      MANAGED_TABLE   NULL    NULL</span><br><span class="line">38      1524468678      1       0       hadoop  0       38      states_seq      MANAGED_TABLE   NULL    NULL</span><br><span class="line"></span><br><span class="line">mysql中的tbls的数据已经存在jdbctable表中了。</span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="8-分区推测（Partition-Discovery）"><a href="#8-分区推测（Partition-Discovery）" class="headerlink" title="8 分区推测（Partition Discovery）"></a>8 分区推测（Partition Discovery）</h4><p>表分区是在像Hive这样的系统中使用的常见优化方法。 在分区表中，数据通常存储在不同的目录中，分区列值在每个分区目录的路径中编码。 所有内置的文件源（包括Text / CSV / JSON / ORC / Parquet）都能够自动发现和推断分区信息。 例如，我们创建如下的目录结构;<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN</span><br><span class="line"></span><br><span class="line">添加json文件：</span><br><span class="line">people.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line"></span><br><span class="line"> hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN</span><br></pre></td></tr></table></figure><p></p><p>我们使用spark sql读取外部数据源：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p></p><p>我们改变读取的目录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)</span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- country: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+-------+</span><br><span class="line">| age|   name|country|</span><br><span class="line">+----+-------+-------+</span><br><span class="line">|null|Michael|     CN|</span><br><span class="line">|  30|   Andy|     CN|</span><br><span class="line">|  19| Justin|     CN|</span><br><span class="line">+----+-------+-------+</span><br></pre></td></tr></table></figure><p></p><p>大家有没有发现什么呢？Spark SQL将自动从路径中提取分区信息。<br>注意，分区列的数据类型是自动推断的。目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以通过</p><p><font color="#FF4500">spark.sql.sources.partitionColumnTypeInference.enabled</font>进行配置，默认为true。当禁用类型推断时，字符串类型将用于分区列。<br>从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下找到分区。对于上面的示例，如果用户将路径/table/gender=male传递给</p><p><font color="#FF4500">SparkSession.read.parquet或SparkSession.read.load</font>，则不会将性别视为分区列。如果用户需要指定启动分区发现的基本路径，则可以basePath在数据源选项中进行设置。例如，当path/to/table/gender=male是数据路径并且用户将basePath设置为path/to/table/时，性别将是分区列。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统重要参数调优，你知道吗</title>
      <link href="/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/"/>
      <url>/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="当前会话生效"><a href="#当前会话生效" class="headerlink" title="当前会话生效"></a>当前会话生效</h4><p>ulimit -u -&gt; 查看当前最大进程数<br>ulimit -n -&gt;查看当前最大文件数<br>ulimit -u xxx -&gt; 修改当前最大进程数为xxx<br>ulimit -n xxx -&gt; 修改当前最大文件数为xxx</p><h4 id="永久生效"><a href="#永久生效" class="headerlink" title="永久生效"></a>永久生效</h4><p>1.vi /etc/security/limits.conf，添加如下的行</p><ul><li>soft noproc 11000</li><li>hard noproc 11000</li><li>soft nofile 4100</li><li>hard nofile 4100<a id="more"></a> 说明：</li><li>代表针对所有用户<br>noproc 是代表最大进程数<br>nofile 是代表最大文件打开数</li></ul><h4 id="2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制："><a href="#2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制：" class="headerlink" title="2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制："></a>2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制：</h4><ul><li>1)、vi /etc/ssh/sshd_config<br>把 UserLogin 的值改为 yes，并把 # 注释去掉</li><li>2)、重启 sshd 服务<br>/etc/init.d/sshd restart</li><li>3)、修改所有 linux 用户的环境变量文件：<br>vi /etc/profile<br>ulimit -u 10000<br>ulimit -n 4096<br>ulimit -d unlimited<br>ulimit -m unlimited<br>ulimit -s unlimited<br>ulimit -t unlimited<br>ulimit -v unlimited</li><li>4)、生效<br>source /etc/profile</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark动态内存管理源码解析！</title>
      <link href="/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/"/>
      <url>/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="一、Spark内存管理模式"><a href="#一、Spark内存管理模式" class="headerlink" title="一、Spark内存管理模式"></a>一、Spark内存管理模式</h4><p>Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// SparkEnv.scala</span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="二、Spark动态内存管理空间分配"><a href="#二、Spark动态内存管理空间分配" class="headerlink" title="二、Spark动态内存管理空间分配"></a>二、Spark动态内存管理空间分配</h4><p><img src="/assets/blogImg/603_1.png" alt="enter description here"><br>相比于Static MemoryManager模式，Unified MemoryManager模型打破了存储内存和运行内存的界限，使每一个内存区能够动态伸缩，降低OOM的概率。由上图可知，executor JVM内存主要由以下几个区域组成：</p><ul><li>（1）Reserved Memory（预留内存）：这部分内存预留给系统使用，默认为300MB，可通过spark.testing.reservedMemory进行设置。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024</span><br></pre></td></tr></table></figure></li></ul><p>另外，JVM内存的最小值也与reserved Memory有关，即minSystemMemory = reserved Memory<em>1.5，即默认情况下JVM内存最小值为300MB</em>1.5=450MB。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong</span><br></pre></td></tr></table></figure><p></p><ul><li>（2）Spark Memeoy:分为execution Memory和storage Memory。去除掉reserved Memory，剩下usableMemory的一部分用于execution和storage这两类堆内存，默认是0.6，可通过spark.memory.fraction进行设置。例如：JVM内存是1G，那么用于execution和storage的默认内存为（1024-300）*0.6=434MB。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val usableMemory = systemMemory - reservedMemory</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    (usableMemory * memoryFraction).toLong</span><br></pre></td></tr></table></figure></li></ul><p>他们的边界由spark.memory.storageFraction设定，默认为0.5。即默认状态下storage Memory和execution Memory为1：1.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">     onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</span><br><span class="line">      numCores = numCores)</span><br></pre></td></tr></table></figure><p></p><ul><li>（3）user Memory:剩余内存，用户根据需要使用，默认占usableMemory的（1-0.6）=0.4.</li></ul><h5 id="三、内存控制详解"><a href="#三、内存控制详解" class="headerlink" title="三、内存控制详解"></a>三、内存控制详解</h5><p>首先我们先来了解一下Spark内存管理实现类之前的关系。<br><img src="/assets/blogImg/603_2.png" alt="enter description here"></p><h5 id="1-MemoryManager主要功能是："><a href="#1-MemoryManager主要功能是：" class="headerlink" title="1.MemoryManager主要功能是："></a>1.MemoryManager主要功能是：</h5><ul><li>（1）记录用了多少StorageMemory和ExecutionMemory；</li><li>（2）申请Storage、Execution和Unroll Memory；</li><li>（3）释放Stroage和Execution Memory。</li></ul><p>Execution内存用来执行shuffle、joins、sorts和aggegations操作，Storage内存用于缓存和广播数据，每一个JVM中都存在着一个MemoryManager。构造MemoryManager需要指定onHeapStorageMemory和onHeapExecutionMemory参数。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> // MemoryManager.scala</span><br><span class="line">private[spark] abstract class MemoryManager(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    numCores: Int,</span><br><span class="line">    onHeapStorageMemory: Long,</span><br><span class="line">    onHeapExecutionMemory: Long) extends Logging &#123;</span><br></pre></td></tr></table></figure><p></p><p>创建StorageMemoryPool和ExecutionMemoryPool对象，用来创建堆内或堆外的Storage和Execution内存池，管理Storage和Execution的内存分配。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)</span><br></pre></td></tr></table></figure><p></p><p>默认情况下，不使用堆外内存，可通过saprk.memory.offHeap.enabled设置，默认堆外内存为0，可使用spark.memory.offHeap.size参数设置。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// All the code you will ever need</span><br><span class="line"> final val tungstenMemoryMode: MemoryMode = &#123;</span><br><span class="line">    if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123;</span><br><span class="line">      require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0,</span><br><span class="line">        &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;)</span><br><span class="line">      require(Platform.unaligned(),</span><br><span class="line">        &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;)</span><br><span class="line">      MemoryMode.OFF_HEAP</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MemoryMode.ON_HEAP</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)</span><br></pre></td></tr></table></figure><p>释放numBytes字节的Execution内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放指定task的所有Execution内存并将该task标记为inactive。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123;</span><br><span class="line">    onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) +</span><br><span class="line">      offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放numBytes字节的Stoarge内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放所有Storage内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">final def releaseAllStorageMemory(): Unit = synchronized &#123;</span><br><span class="line">    onHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">    offHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"><a href="#2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？" class="headerlink" title="2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"></a>2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？</h5><p>UnifiedMemoryManage继承了MemoryManager<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">private[spark] class UnifiedMemoryManager private[memory] (</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    val maxHeapMemory: Long,</span><br><span class="line">    onHeapStorageRegionSize: Long,</span><br><span class="line">    numCores: Int)</span><br><span class="line">  extends MemoryManager(</span><br><span class="line">    conf,</span><br><span class="line">    numCores,</span><br><span class="line">    onHeapStorageRegionSize,</span><br><span class="line">    maxHeapMemory - onHeapStorageRegionSize) &#123;</span><br></pre></td></tr></table></figure><p></p><p>重写了maxOnHeapStorageMemory方法，最大Storage内存=最大内存-最大Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line"> override def maxOnHeapStorageMemory: Long = synchronized &#123;</span><br><span class="line">    maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireStorageMemory：申请Storage内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override def acquireStorageMemory(</span><br><span class="line">      blockId: BlockId,</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      memoryMode: MemoryMode): Boolean = synchronized &#123;</span><br><span class="line">    assertInvariants()</span><br><span class="line">    assert(numBytes &gt;= 0)</span><br><span class="line">    val (executionPool, storagePool, maxMemory) = memoryMode match &#123;</span><br><span class="line">      //根据不同的内存模式去创建StorageMemoryPool和ExecutionMemoryPool</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; (</span><br><span class="line">        onHeapExecutionMemoryPool,</span><br><span class="line">        onHeapStorageMemoryPool,</span><br><span class="line">        maxOnHeapStorageMemory)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; (</span><br><span class="line">        offHeapExecutionMemoryPool,</span><br><span class="line">        offHeapStorageMemoryPool,</span><br><span class="line">        maxOffHeapMemory)</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; maxMemory) &#123;</span><br><span class="line">      // 若申请内存大于最大内存，则申请失败</span><br><span class="line">      logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; +</span><br><span class="line">        s&quot;memory limit ($maxMemory bytes)&quot;)</span><br><span class="line">      return false</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">      // 如果Storage内存池没有足够的内存，则向Execution内存池借用</span><br><span class="line">      val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//当Execution内存有空闲时，Storage才能借到内存</span><br><span class="line">      executionPool.decrementPoolSize(memoryBorrowedFromExecution)//缩小Execution内存</span><br><span class="line">      storagePool.incrementPoolSize(memoryBorrowedFromExecution)//增加Storage内存</span><br><span class="line">    &#125;</span><br><span class="line">    storagePool.acquireMemory(blockId, numBytes)</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireExecutionMemory：申请Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override private[memory] def acquireExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Long = synchronized &#123;//使用了synchronized关键字，调用acquireExecutionMemory方法可能会阻塞，直到Execution内存池有足够的内存。</span><br><span class="line">   ...</span><br><span class="line">    executionPool.acquireMemory(</span><br><span class="line">      numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>方法最后调用了ExecutionMemoryPool的acquireMemory方法，该方法的参数需要两个函数：maybeGrowExecutionPool()和computeMaxExecutionPoolSize()。<br>每个Task能够使用的内存被限制在pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks。其中maxPoolSize代表了execution pool的最大内存，poolSize表示当前这个pool的大小。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// ExecutionMemoryPool.scala</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">      val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br></pre></td></tr></table></figure><p></p><p>maybeGrowExecutionPool()方法实现了如何动态增加Execution内存区的大小。<br>在每次申请execution内存的同时，execution内存池会进行多次尝试，每次尝试都可能会回收一些存储内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">     def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">      if (extraMemoryNeeded &gt; 0) &#123;//如果申请的内存大于0</span><br><span class="line">        //计算execution可借到的storage内存，是storage剩余内存和可借出内存的最大值</span><br><span class="line">        val memoryReclaimableFromStorage = math.max(</span><br><span class="line">          storagePool.memoryFree,</span><br><span class="line">          storagePool.poolSize - storageRegionSize)</span><br><span class="line">        if (memoryReclaimableFromStorage &gt; 0) &#123;//如果可以申请到内存</span><br><span class="line">          val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">            math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//实际需要的内存，取实际需要的内存和storage内存区域全部可用内存大小的最小值</span><br><span class="line">          storagePool.decrementPoolSize(spaceToReclaim)//storage内存区域减少</span><br><span class="line">          executionPool.incrementPoolSize(spaceToReclaim)//execution内存区域增加</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽大数据-零基础学员深圳某司高薪面试题</title>
      <link href="/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>啥也不说！直接上题</p><p>面试时间：20180531</p><ul><li>简单说下hdfs读文件和写文件的流程</li><li>每天数据量有多大？生产集群规模有多大？</li><li>说几个spark开发中遇到的问题，和解决的方案</li><li>阐述一下最近开发的项目，以及担任的角色位置</li><li>kafka有做过哪些调优</li><li>我们项目中数据倾斜的场景和解决方案</li></ul><p>零基础➕四个月紧跟若泽大数据学习之后是这样</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从Hive中的stored as file_foramt看hive调优</title>
      <link href="/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/"/>
      <url>/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="一、行式数据库和列式数据库的对比"><a href="#一、行式数据库和列式数据库的对比" class="headerlink" title="一、行式数据库和列式数据库的对比"></a>一、行式数据库和列式数据库的对比</h4><h5 id="1、存储比较"><a href="#1、存储比较" class="headerlink" title="1、存储比较"></a>1、存储比较</h5><p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p><h5 id="2、压缩比较"><a href="#2、压缩比较" class="headerlink" title="2、压缩比较"></a>2、压缩比较</h5><a id="more"></a><p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p><p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p><h5 id="3、查询比较"><a href="#3、查询比较" class="headerlink" title="3、查询比较"></a>3、查询比较</h5><p>假设执行的查询操作是：select id,name from table_emp;</p><p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p><p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p><p>假设执行的查询操作是：select * from table_emp;</p><p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p><p><strong><font color="#FF4500">但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</font></strong></p><h4 id="二、stored-as-file-format-详解"><a href="#二、stored-as-file-format-详解" class="headerlink" title="二、stored as file_format 详解"></a>二、stored as file_format 详解</h4><h5 id="1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"><a href="#1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。" class="headerlink" title="1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"></a>1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tt (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CREATE TABLE tt2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;</span><br><span class="line"></span><br><span class="line">CREATE TABLE tt3 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS </span><br><span class="line">INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;</span><br><span class="line">OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;</span><br><span class="line"></span><br><span class="line">#以上三种方式存储的格式都是TEXTFILE。</span><br></pre></td></tr></table></figure><h5 id="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"><a href="#2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较" class="headerlink" title="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"></a>2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">file_format:</span><br><span class="line">  | SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p><strong>TEXTFILE:</strong> 只是hive中表数据默认的存储格式，它将所有类型的数据都存储为String类型，不便于数据的解析，但它却比较通用。不具备随机读写的能力。支持压缩。</p><p><strong>SEQUENCEFILE:</strong> 这种储存格式比TEXTFILE格式多了头部、标识、信息长度等信息，这些信息使得其具备随机读写的能力。支持压缩，但压缩的是value。（存储相同的数据，SEQUENCEFILE比TEXTFILE略大）</p><p><strong>RCFILE（Record Columnar File）:</strong> 现在水平上划分为很多个Row Group,每个Row Group默认大小4MB，Row Group内部再按列存储信息。由facebook开源，比标准行式存储节约10%的空间。</p><p><strong>ORC:</strong> 优化过后的RCFile,现在水平上划分为多个Stripes,再在Stripe中按列存储。每个Stripe由一个Index Data、一个Row Data、一个Stripe Footer组成。每个Stripes的大小为250MB，每个Index Data记录的是整型数据最大值最小值、字符串数据前后缀信息，每个列的位置等等诸如此类的信息。这就使得查询十分得高效，默认每一万行数据建立一个Index Data。ORC存储大小为TEXTFILE的40%左右，使用压缩则可以进一步将这个数字降到10%~20%。</p><p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure><p></p><p>The parameters are all placed in the TBLPROPERTIES (see Create Table). They are:</p><p>Key|Default|Notes<br>|-|-|-|<br>orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)<br>|orc.compress.size|262,144|number of bytes in each compression chunk<br>|orc.stripe.size|67,108,864|number of bytes in each stripe<br>|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)<br>|orc.create.index|true|whether to create row indexes<br>|orc.bloom.filter.columns |””| comma separated list of column names for which bloom filter should be created<br>|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</p><p>示例：创建带压缩的ORC存储表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table Addresses (</span><br><span class="line">  name string,</span><br><span class="line">  street string,</span><br><span class="line">  city string,</span><br><span class="line">  state string,</span><br><span class="line">  zip int</span><br><span class="line">) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure><p></p><p>PARQUET: 存储大小为TEXTFILE的60%~70%，压缩后在20%~30%之间。</p><hr><p>注意：</p><ol><li><p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p></li><li><p>如果要使用TEXTFILE作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“insert into table table_stored_file_ORC select <em>from table_t0;”创建。或者使用”create table as select </em>from table_t0;”创建。</p></li></ol><hr><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之序列化在生产中的应用</title>
      <link href="/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h4 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h4><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_1.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_2.png" alt="enter description here"></p><h4 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h4><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_3.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_4.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次。<br><img src="/assets/blogImg/529_5.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_6.png" alt="enter description here"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br><img src="/assets/blogImg/529_7.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据带你随时了解业界面试题，随时跳高薪</title>
      <link href="/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/"/>
      <url>/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="链家-一面，二面"><a href="#链家-一面，二面" class="headerlink" title="链家(一面，二面)"></a>链家(一面，二面)</h4><p>0.自我介绍</p><p>1.封装继承多态概念</p><p>2.mvc设计思想</p><p>3.线程池,看过源码吗<br><a id="more"></a><br>4.ssh框架中分别对应mvc中那一层</p><p>5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）</p><p>6.spring ioc aop 原理</p><p>7.单利模式</p><p>8.SQL题，想不起来了。。</p><p>9.jvm 运行时数据区域</p><p>10.spring mvc知道吗。。</p><p>11.工厂模式</p><p>12.mr 计算流程</p><p>13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费 查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）</p><p>14.git的使用</p><p>15.hadoop的理解</p><p>16.hive内部表和外部表的区别</p><p>17.hive存储格式和压缩格式</p><p>18.对spark了解吗？ 当时高级班还没学。。</p><p>19.hive于关系型数据库的区别</p><p>20.各种排序 手写堆排序,说说原理</p><p>21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。</p><p>22中间也穿插了项目。</p><p>无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。</p><p>若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一次跳槽经历（阿里/美团/头条/网易/有赞...)</title>
      <link href="/2018/05/24/%E6%9C%89%E8%B5%9E...)/"/>
      <url>/2018/05/24/%E6%9C%89%E8%B5%9E...)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h6 id="为啥跳槽"><a href="#为啥跳槽" class="headerlink" title="为啥跳槽"></a>为啥跳槽</h6><p>每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。<br><img src="//yoursite.com/2018/05/24/有赞...)/blogImg/tiaocao524.png" alt="enter description here"><br><a id="more"></a></p><h6 id="面试过程"><a href="#面试过程" class="headerlink" title="面试过程"></a>面试过程</h6><p>（先打个广告，有兴趣加入阿里的欢迎发简历至 <a href="mailto:zhangzb2007@gmail.com" target="_blank" rel="noopener">zhangzb2007@gmail.com</a>，或简书上给我发信息）<br>面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。</p><h6 id="上半场"><a href="#上半场" class="headerlink" title="上半场"></a>上半场</h6><ul><li><p>曹操专车<br>这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。</p></li><li><p>美亚柏科<br>估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。</p></li><li><p>有赞<br>绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。<br>这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。</p></li><li><p>字节跳动(今日头条)<br>HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。<br>一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。<br>二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。</p></li></ul><h6 id="下半场"><a href="#下半场" class="headerlink" title="下半场"></a>下半场</h6><p>一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。</p><ul><li><p>美团<br>这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。<br>两点半进去。<br>一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。<br>二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。<br>三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。<br>四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。<br>出来的时候已经是六点半。</p></li><li><p>网易<br>面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。<br>一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。<br>然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。</p></li><li><p>阿里<br>这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。<br>一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。<br>二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。<br>三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。<br>不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。</p></li></ul><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。<strong>整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。</strong> 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中自定义UDAF函数生产小案例</title>
      <link href="/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="一、UDAF-回顾"><a href="#一、UDAF-回顾" class="headerlink" title="一、UDAF 回顾"></a>一、UDAF 回顾</h4><ul><li>1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。</li><li>2.Hive有两种UDAF：简单和通用<br>简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。<br>通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。</li><li>3.一个计算函数必须实现的5个方法的具体含义如下：<br>init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。<br>iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。<br>terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。<br>merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。<br>terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。<h4 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h4>使用UDAF简单方式实现统计区域产品用户访问排名<a id="more"></a><h4 id="三、自定义UDAF函数代码实现"><a href="#三、自定义UDAF函数代码实现" class="headerlink" title="三、自定义UDAF函数代码实现"></a>三、自定义UDAF函数代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">package hive.org.ruozedata;</span><br><span class="line">import java.util.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">public class UserClickUDAF extends UDAF &#123;</span><br><span class="line">    // 日志对象初始化</span><br><span class="line">    public static Logger logger = Logger.getLogger(UserClickUDAF.class);</span><br><span class="line">    // 静态类实现UDAFEvaluator</span><br><span class="line">    public static class Evaluator implements UDAFEvaluator &#123;</span><br><span class="line">        // 设置成员变量，存储每个统计范围内的总记录数</span><br><span class="line">        private static Map&lt;String, String&gt; courseScoreMap;</span><br><span class="line">        private static Map&lt;String, String&gt; city_info;</span><br><span class="line">        private static Map&lt;String, String&gt; product_info;</span><br><span class="line">        private static Map&lt;String, String&gt; user_click;</span><br><span class="line">        //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span><br><span class="line">        public Evaluator() &#123;</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line">        // 初始化函数间传递的中间变量</span><br><span class="line">        public void init() &#123;</span><br><span class="line">            courseScoreMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">            city_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">            product_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">        //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出</span><br><span class="line">        public boolean iterate(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (pcid == null || pcname == null || pccount == null) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (pccount.equals(&quot;-1&quot;)) &#123;</span><br><span class="line">                // 城市表</span><br><span class="line">                city_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (pccount.equals(&quot;-2&quot;)) &#123;</span><br><span class="line">                // 产品表</span><br><span class="line">                product_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // 处理用户点击关联</span><br><span class="line">                unionCity_Prod_UserClic1(pcid, pcname, pccount);</span><br><span class="line">           &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 处理用户点击关联</span><br><span class="line">        private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (product_info.containsKey(pcid)) &#123;</span><br><span class="line">                if (city_info.containsKey(pcname)) &#123;</span><br><span class="line">                    String city_name = city_info.get(pcname);</span><br><span class="line">                    String prod_name = product_info.get(pcid);</span><br><span class="line">                    String cp_name = city_name + prod_name;</span><br><span class="line">                    // 如果之前已经Put过Key值为区域信息，则把记录相加处理</span><br><span class="line">                    if (courseScoreMap.containsKey(cp_name)) &#123;</span><br><span class="line">                        int pcrn = 0;</span><br><span class="line">                        String strTemp = courseScoreMap.get(cp_name);</span><br><span class="line">                        String courseScoreMap_pn </span><br><span class="line">                         = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim();</span><br><span class="line">                        pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn);</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn));</span><br><span class="line">                    &#125;</span><br><span class="line">                    else &#123;</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput</span><br><span class="line">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span><br><span class="line">         */</span><br><span class="line">        public Map&lt;String, String&gt; terminatePartial() &#123;</span><br><span class="line">            return courseScoreMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span><br><span class="line">        public boolean merge(Map&lt;String, String&gt; mapOutput) &#123;</span><br><span class="line">            this.courseScoreMap.putAll(mapOutput);</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span><br><span class="line">        public String terminate() &#123;</span><br><span class="line">            return courseScoreMap.toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="四、创建hive中的临时函数"><a href="#四、创建hive中的临时函数" class="headerlink" title="四、创建hive中的临时函数"></a>四、创建hive中的临时函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION user_click;</span><br><span class="line">add jar /data/hive_udf-1.0.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;</span><br></pre></td></tr></table></figure><h4 id="五、调用自定义UDAF函数处理数据"><a href="#五、调用自定义UDAF函数处理数据" class="headerlink" title="五、调用自定义UDAF函数处理数据"></a>五、调用自定义UDAF函数处理数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from (</span><br><span class="line">  select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from (</span><br><span class="line">    select * from (</span><br><span class="line">      select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info</span><br><span class="line">      union all</span><br><span class="line">      select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info</span><br><span class="line">      union all</span><br><span class="line">      select count(1) as type,</span><br><span class="line">             product_id as pcid,</span><br><span class="line">             city_id as pcname</span><br><span class="line">        from user_click</span><br><span class="line">       where action_time=&apos;2016-05-05&apos;</span><br><span class="line">      group by product_id,city_id</span><br><span class="line">    ) a</span><br><span class="line">  order by type) b</span><br><span class="line">) c ;</span><br></pre></td></tr></table></figure><h4 id="六、创建Hive临时外部表"><a href="#六、创建Hive临时外部表" class="headerlink" title="六、创建Hive临时外部表"></a>六、创建Hive临时外部表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table tmp1(</span><br><span class="line">city_name string,</span><br><span class="line">product_name string,</span><br><span class="line">rn string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">location &apos;/works/tmp1&apos;;</span><br></pre></td></tr></table></figure><h4 id="七、统计最终区域前3产品排名"><a href="#七、统计最终区域前3产品排名" class="headerlink" title="七、统计最终区域前3产品排名"></a>七、统计最终区域前3产品排名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">select city_name,</span><br><span class="line">       product_name,</span><br><span class="line">       floor(sum(rn)) visit_num,</span><br><span class="line">       row_number()over(partition by city_name order by sum(rn) desc) rn,</span><br><span class="line">       &apos;2016-05-05&apos; action_time</span><br><span class="line">  from tmp1 </span><br><span class="line"> group by city_name,product_name</span><br><span class="line">) a where rn &lt;=3 ;</span><br></pre></td></tr></table></figure><h4 id="八、最终结果"><a href="#八、最终结果" class="headerlink" title="八、最终结果"></a>八、最终结果</h4><p><img src="/assets/blogImg/hive523.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark History Server Web UI配置</title>
      <link href="/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h5 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd /opt/app/spark/conf</span><br><span class="line">[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /Found 3 items</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line">drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 items</span><br><span class="line">drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog</span><br></pre></td></tr></table></figure><p>在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息</p><h5 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir             hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br></pre></td></tr></table></figure><p>spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建<br>spark.yarn.historyServer.address : Spark history server的地址(不加http://).<br>这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</p><h5 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 conf]# vi spark-env.sh</span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h5 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01  ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer</span><br><span class="line">[root@hadoop01  ~]# ps -ef|grep sparkroot     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop01  ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java</span><br></pre></td></tr></table></figure><p>以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 基本概念</title>
      <link href="/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><strong>基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）</strong><br><a id="more"></a></p><h5 id="spark名词解释"><a href="#spark名词解释" class="headerlink" title="spark名词解释"></a>spark名词解释</h5><ul><li><p>Application jar：应用程序jar包<br>包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；</p></li><li><p>Driver Program：<br>这个进程运行应用程序的 main 方法并且新建 SparkContext ；</p></li><li><p>Cluster Manager：集群管理者<br>在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）</p></li><li><p>Deploy mode：部署模式<br>告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；</p></li><li><p>Worker Node：工作节点<br>集群中任何可以运行应用代码的节点；（yarn上就是node manager）</p></li><li><p>Executor：<br>在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；</p></li><li><p>Task：任务<br>被送到某个 executor 上执行的工作单元；</p></li><li><p>Job：<br>包含很多并行计算的task。一个 action 就会产生一个job；</p></li><li><p>Stage：<br>一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。</p></li></ul><h5 id="spark工作流程"><a href="#spark工作流程" class="headerlink" title="spark工作流程"></a>spark工作流程</h5><p>1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。</p><p>spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。<br>如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行</p><ul><li>1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。</li><li>2、spark并不关心底层的集群管理。</li><li>3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。</li><li>4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark不得不理解的重要概念——从源码角度看RDD</title>
      <link href="/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
      <url>/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1.RDD是什么"></a>1.RDD是什么</h4><p>Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合</p><h4 id="2-RDD五大特性"><a href="#2-RDD五大特性" class="headerlink" title="2.RDD五大特性"></a>2.RDD五大特性</h4><a id="more"></a><ol><li><p>A list of partitions<br>每个rdd有多个分区<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>计算作用到每个分区<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rdd之间存在依赖（RDD的血缘关系）如：<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选，默认哈希的分区<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>源码来自github。</p><h4 id="3-如何创建RDD"><a href="#3-如何创建RDD" class="headerlink" title="3.如何创建RDD"></a>3.如何创建RDD</h4><p>创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfile（）</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><p><strong>源码总结：<br>1）.取_2是因为数据为（key（偏移量），value（数据））</strong></p><h4 id="4-常见的transformation和action"><a href="#4-常见的transformation和action" class="headerlink" title="4.常见的transformation和action"></a>4.常见的transformation和action</h4><p>由于比较简单，大概说一下常用的用处，不做代码测试</p><p>transformation</p><ul><li>Map：对数据集的每一个元素进行操作</li><li>FlatMap：先对数据集进行扁平化处理，然后再Map</li><li>Filter：对数据进行过滤，为true则通过</li><li>destinct：去重操作</li></ul><p>action</p><ul><li>reduce：对数据进行聚集</li><li>reduceBykey：对key值相同的进行操作</li><li>collect：没有效果的action，但是很有用</li><li>saveAstextFile：数据存入文件系统</li><li>foreach：对每个元素进行func的操作</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>美味不用等大数据面试题(201804月)</title>
      <link href="/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/"/>
      <url>/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h6 id="1-若泽大数据线下班，某某某的小伙伴现场面试题截图"><a href="#1-若泽大数据线下班，某某某的小伙伴现场面试题截图" class="headerlink" title="1.若泽大数据线下班，某某某的小伙伴现场面试题截图:"></a><strong>1.若泽大数据线下班，某某某的小伙伴现场面试题截图:</strong></h6><a id="more"></a><p><img src="/assets/blogImg/520_1.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_2.png" alt="enter description here"></p><h6 id="2-分享另外1家的忘记名字公司的大数据面试题："><a href="#2-分享另外1家的忘记名字公司的大数据面试题：" class="headerlink" title="2.分享另外1家的忘记名字公司的大数据面试题："></a><strong>2.分享另外1家的忘记名字公司的大数据面试题：</strong></h6><p><img src="/assets/blogImg/520_3.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD、DataFrame和DataSet的区别</title>
      <link href="/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br>在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！</font><h5 id="一-、共性"><a href="#一-、共性" class="headerlink" title="一 、共性"></a>一 、共性</h5><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p><p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</p><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>4、三者都有partition的概念。<br><a id="more"></a></p><h5 id="二、RDD优缺点"><a href="#二、RDD优缺点" class="headerlink" title="二、RDD优缺点"></a>二、RDD优缺点</h5><p><strong>优点：</strong></p><ul><li><p>1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</p></li><li><p>2、面向对象的编程风格</p></li><li><p>3、编译时类型安全，编译时就能检查出类型错误</p></li></ul><p><strong>缺点：</strong></p><ul><li><p>1、序列化和反序列化的性能开销</p></li><li><p>2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p></li></ul><h5 id="三、DataFrame"><a href="#三、DataFrame" class="headerlink" title="三、DataFrame"></a>三、DataFrame</h5><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreach&#123;</span><br><span class="line">  x =&gt;</span><br><span class="line">    val v1=x.getAs[String](&quot;v1&quot;)</span><br><span class="line">    val v2=x.getAs[String](&quot;v2&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>2、DataFrame引入了schema和off-heap</p><ul><li><p>schema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.</p></li><li><p>off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.</p></li><li><p>off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.</p></li></ul><p>3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表</p><p>4、兼容Hive，支持Hql、UDF</p><p><strong>有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.</strong></p><h5 id="四、DataSet"><a href="#四、DataSet" class="headerlink" title="四、DataSet"></a>四、DataSet</h5><p>1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。</p><p>2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。</p><p>3、Dataset<row>等同于DataFrame（Spark 2.X）</row></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决</title>
      <link href="/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
      <url>/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h5 id="一-数据源同步中间件："><a href="#一-数据源同步中间件：" class="headerlink" title="一.数据源同步中间件："></a>一.数据源同步中间件：</h5><p>Canal<br><a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">https://github.com/alibaba/canal</a><br><a href="https://github.com/Hackeruncle/syncClient" target="_blank" rel="noopener">https://github.com/Hackeruncle/syncClient</a></p><p>Maxwell<br><a href="https://github.com/zendesk/maxwell" target="_blank" rel="noopener">https://github.com/zendesk/maxwell</a><br><img src="/assets/blogImg/514_1.png" alt="maxwell"><br><a id="more"></a></p><h5 id="二-架构使用"><a href="#二-架构使用" class="headerlink" title="二.架构使用"></a>二.架构使用</h5><p>MySQL —- 中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra 增量的<br>a.全量 bootstrap<br>b.增量</p><h6 id="1-对比"><a href="#1-对比" class="headerlink" title="1.对比"></a>1.对比</h6><table><thead><tr><th></th><th></th><th>Canal(服务端)</th><th>Maxwell(服务端+客户端)</th></tr></thead><tbody><tr><td>语言</td><td>Java</td><td>Java</td><td></td></tr><tr><td>活跃度</td><td>活跃</td><td>活跃</td><td></td></tr><tr><td>HA</td><td>支持</td><td>定制 但是支持断点还原功能</td></tr><tr><td>数据落地</td><td>定制</td><td>落地到kafka</td></tr><tr><td>分区</td><td>支持</td><td>支持</td></tr><tr><td>bootstrap(引导)</td><td>不支持</td><td>支持</td></tr><tr><td>数据格式</td><td>格式自由</td><td>json(格式固定) spark json–&gt;DF</td></tr><tr><td>文档</td><td>较详细</td><td>较详细</td><td></td></tr><tr><td>随机读</td><td>支持</td><td>支持</td><td></td></tr></tbody></table><p><strong>个人选择Maxwell</strong></p><p>a.服务端+客户端一体，轻量级的<br>b.支持断点还原功能+bootstrap+json<br>Can do SELECT * from table (bootstrapping) initial loads of a table.<br>supports automatic position recover on master promotion<br>flexible partitioning schemes for Kakfa - by database, table, primary key, or column<br>Maxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).</p><h6 id="2-官网解读"><a href="#2-官网解读" class="headerlink" title="2.官网解读"></a>2.官网解读</h6><p><a href="https://www.bilibili.com/video/av34778187?from=search&amp;seid=18393822973469412185" target="_blank" rel="noopener">B站视频</a></p><h6 id="3-部署"><a href="#3-部署" class="headerlink" title="3.部署"></a>3.部署</h6><p><strong>3.1 MySQL Install</strong><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a><br><a href="https://ke.qq.com/course/262452?tuin=11cffd50" target="_blank" rel="noopener">https://ke.qq.com/course/262452?tuin=11cffd50</a></p><p><strong>3.2 修改</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">binlog_format=row</span><br><span class="line"></span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line">3.3 创建Maxwell的db和用户</span><br><span class="line">mysql&gt; create database maxwell;</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4解压</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz</span><br></pre></td></tr></table></figure><p></p><p><strong>3.5测试STDOUT:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --user=&apos;maxwell&apos; \</span><br><span class="line">--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \</span><br><span class="line">--producer=stdout</span><br></pre></td></tr></table></figure><p></p><p>测试1：insert sql：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br></pre></td></tr></table></figure><p></p><p>maxwell输出：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;insert&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959044,</span><br><span class="line">    &quot;xid&quot;: 201,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>测试1：update sql:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update ruozedata set age=29 where id=999;</span><br></pre></td></tr></table></figure><p></p><p><strong>问题: ROW，你觉得binlog更新几个字段？</strong></p><p>maxwell输出：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;update&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959208,</span><br><span class="line">    &quot;xid&quot;: 255,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 29,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;old&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="4-其他注意点和新特性"><a href="#4-其他注意点和新特性" class="headerlink" title="4.其他注意点和新特性"></a>4.其他注意点和新特性</h6><p><strong>4.1 kafka_version 版本</strong><br>Using kafka version: 0.11.0.1 0.10<br>jar:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 kafka-clients]# ll</span><br><span class="line">total 4000</span><br><span class="line">-rw-r--r--. 1 ruoze games  746207 May  8 06:34 kafka-clients-0.10.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  951041 May  8 06:35 kafka-clients-0.10.2.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games 1419544 May  8 06:35 kafka-clients-0.11.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  324016 May  8 06:34 kafka-clients-0.8.2.2.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  641408 May  8 06:34 kafka-clients-0.9.0.1.jar</span><br><span class="line">[root@hadoop000 kafka-clients]#</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 其他组件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> maxwell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on YARN-Cluster和YARN-Client的区别</title>
      <link href="/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h5 id="一-YARN-Cluster和YARN-Client的区别"><a href="#一-YARN-Cluster和YARN-Client的区别" class="headerlink" title="一. YARN-Cluster和YARN-Client的区别"></a>一. YARN-Cluster和YARN-Client的区别</h5><p><img src="/assets/blogImg/512_1.png" alt><br>（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；<br>（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；<br>（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。<br><a id="more"></a></p><h5 id="二-yarn-client-模式"><a href="#二-yarn-client-模式" class="headerlink" title="二. yarn client 模式"></a>二. yarn client 模式</h5><p><img src="/assets/blogImg/512_2.png" alt></p><p><font color="#FF4200">yarn-client 模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。<br></font></p><h5 id="三-yarn-cluster-模式"><a href="#三-yarn-cluster-模式" class="headerlink" title="三.yarn  cluster 模式"></a>三.yarn cluster 模式</h5><p><img src="/assets/blogImg/512_3.png" alt></p><p><font color="#FF4200">yarn-cluster 模式的话， client 关闭是可以提交任务的 ，<br></font></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p><strong>1.spark-shell/spark-sql 只支持 yarn-client模式；<br>2.spark-submit对于两种模式都支持。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产改造Spark1.6源代码，create table语法支持Oracle列表分区</title>
      <link href="/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/"/>
      <url>/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h5 id="1-需求"><a href="#1-需求" class="headerlink" title="1.需求"></a>1.需求</h5><p>通过Spark SQL JDBC 方法，抽取Oracle表数据。</p><h5 id="2-问题"><a href="#2-问题" class="headerlink" title="2.问题"></a>2.问题</h5><p>大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。<br>参考 <a href="http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases" target="_blank" rel="noopener">http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases</a><br><a id="more"></a></p><h5 id="3-Oracle的分区"><a href="#3-Oracle的分区" class="headerlink" title="3.Oracle的分区"></a>3.Oracle的分区</h5><h6 id="3-1列表分区"><a href="#3-1列表分区" class="headerlink" title="3.1列表分区:"></a>3.1列表分区:</h6><p>该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。<br>例一:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PROBLEM_TICKETS</span><br><span class="line">(</span><br><span class="line">PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),</span><br><span class="line">CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,</span><br><span class="line">STATUS VARCHAR2(20)</span><br><span class="line">)</span><br><span class="line">PARTITION BY LIST (STATUS)</span><br><span class="line">(</span><br><span class="line">PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,</span><br><span class="line">PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h6 id="3-2散列分区"><a href="#3-2散列分区" class="headerlink" title="3.2散列分区:"></a>3.2散列分区:</h6><p>这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。<br>例一:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE HASH_TABLE</span><br><span class="line">(</span><br><span class="line">COL NUMBER(8),</span><br><span class="line">INF VARCHAR2(100) </span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (COL)</span><br><span class="line">(</span><br><span class="line">PARTITION PART01 TABLESPACE HASH_TS01, </span><br><span class="line">PARTITION PART02 TABLESPACE HASH_TS02, </span><br><span class="line">PARTITION PART03 TABLESPACE HASH_TS03</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h5 id="4-改造"><a href="#4-改造" class="headerlink" title="4.改造"></a>4.改造</h5><p>蓝色代码是改造Spark源代码,加课程顾问领取PDF。</p><h6 id="1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。"><a href="#1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。" class="headerlink" title="1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。"></a>1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE TBLS_IN</span><br><span class="line">USING org.apache.spark.sql.jdbc OPTIONS (</span><br><span class="line">driver &quot;com.mysql.jdbc.Driver&quot;,</span><br><span class="line">url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,</span><br><span class="line">fetchSize &quot;1000&quot;,</span><br><span class="line">partitionColumn &quot;TBL_ID&quot;,</span><br><span class="line">numPartitions &quot;null&quot;,</span><br><span class="line">lowerBound &quot;null&quot;,</span><br><span class="line">upperBound &quot;null&quot;,</span><br><span class="line">user &quot;hive2user&quot;,</span><br><span class="line">password &quot;hive2user&quot;,</span><br><span class="line">partitionInRule &quot;1|15,16,18,19|20,21&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation"><a href="#2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation" class="headerlink" title="2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation"></a>2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">sqlContext: SQLContext,</span><br><span class="line">parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))</span><br><span class="line">val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)</span><br><span class="line">var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)</span><br><span class="line">var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)</span><br><span class="line"></span><br><span class="line">// add partition in rule</span><br><span class="line">val partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)</span><br><span class="line">// validind all the partition in rule </span><br><span class="line">if (partitionColumn != null</span><br><span class="line">&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)</span><br><span class="line">&amp;&amp; partitionInRule == null </span><br><span class="line">)&#123;</span><br><span class="line">   sys.error(&quot;Partitioning incompletely specified&quot;) </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val partitionInfo = </span><br><span class="line">if (partitionColumn == null) &#123; </span><br><span class="line">    null</span><br><span class="line">&#125; else &#123;</span><br><span class="line"></span><br><span class="line">val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123;</span><br><span class="line">val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot;</span><br><span class="line">upperBound = &quot;0&quot;</span><br><span class="line">inGroups &#125;</span><br><span class="line">else&#123;</span><br><span class="line">Array[String]() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JDBCPartitioningInfo( partitionColumn, </span><br><span class="line">lowerBound.toLong, </span><br><span class="line">upperBound.toLong, </span><br><span class="line">numPartitions.toInt, </span><br><span class="line">inPartitions)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val parts = JDBCRelation.columnPartition(partitionInfo)</span><br><span class="line">val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))</span><br><span class="line">// parameters is immutable</span><br><span class="line">if(numPartitions != null)&#123;</span><br><span class="line">properties.put(&quot;numPartitions&quot; , numPartitions) &#125;</span><br><span class="line">JDBCRelation(url, table, parts, properties)(sqlContext)</span><br><span class="line"></span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition"><a href="#3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition" class="headerlink" title="3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition"></a>3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;</span><br><span class="line">if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))</span><br><span class="line">val column = partitioning.column</span><br><span class="line">var i: Int = 0</span><br><span class="line">var ans = new ArrayBuffer[Partition]()</span><br><span class="line"></span><br><span class="line">// partition by long if(partitioning.inPartitions.length == 0)&#123;</span><br><span class="line"></span><br><span class="line">val numPartitions = partitioning.numPartitions</span><br><span class="line">if (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.</span><br><span class="line">// Here we get a little roundoff, but that&apos;s (hopefully) OK.</span><br><span class="line">val stride: Long = (partitioning.upperBound / numPartitions</span><br><span class="line"></span><br><span class="line">- partitioning.lowerBound / numPartitions)</span><br><span class="line">var currentValue: Long = partitioning.lowerBound</span><br><span class="line">while (i &lt; numPartitions) &#123;</span><br><span class="line">val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else null</span><br><span class="line">currentValue += stride</span><br><span class="line">val upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =</span><br><span class="line"></span><br><span class="line">if (upperBound == null) &#123; </span><br><span class="line">  lowerBound</span><br><span class="line"></span><br><span class="line">&#125; else if (lowerBound == null) &#123; </span><br><span class="line">  upperBound</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">  s&quot;$lowerBound AND $upperBound&quot; </span><br><span class="line">&#125;</span><br><span class="line">  ans += JDBCPartition(whereClause, i)</span><br><span class="line">   i= i+ 1 &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// partition by in </span><br><span class="line">else&#123;</span><br><span class="line">    while(i &lt; partitioning.inPartitions.length)&#123;</span><br><span class="line">           val inContent = partitioning.inPartitions(i)</span><br><span class="line">           val whereClause = s&quot;$column in ($inContent)&quot; </span><br><span class="line">           ans += JDBCPartition(whereClause, i)</span><br><span class="line">           i= i+ 1</span><br><span class="line">     &#125; </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ans.toArray </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="4-对外方法org-apache-spark-sql-SQLContext-方法jdbc"><a href="#4-对外方法org-apache-spark-sql-SQLContext-方法jdbc" class="headerlink" title="4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc"></a>4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">url: String,</span><br><span class="line">table: String,</span><br><span class="line">columnName: String,</span><br><span class="line">lowerBound: Long,</span><br><span class="line">upperBound: Long,</span><br><span class="line">numPartitions: Int,</span><br><span class="line">inPartitions: Array[String] = Array[String]()</span><br><span class="line"></span><br><span class="line">): DataFrame = &#123;</span><br><span class="line">read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产中Hive静态和动态分区表，该怎样抉择呢？</title>
      <link href="/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/"/>
      <url>/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h6 id="一-需求"><a href="#一-需求" class="headerlink" title="一.需求"></a>一.需求</h6><p>按照不同部门作为分区，导数据到目标表</p><h6 id="二-使用静态分区表来完成"><a href="#二-使用静态分区表来完成" class="headerlink" title="二.使用静态分区表来完成"></a>二.使用静态分区表来完成</h6><p>71.创建静态分区表：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table emp_static_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><p>2.插入数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_static_partition partition(deptno=10)</span><br><span class="line">     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><p>3.查询数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_static_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_1.png" alt></p><h6 id="三-使用动态分区表来完成"><a href="#三-使用动态分区表来完成" class="headerlink" title="三.使用动态分区表来完成"></a>三.使用动态分区表来完成</h6><p>1.创建动态分区表：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table emp_dynamic_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的</font><p>2.插入数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_dynamic_partition partition(deptno)     </span><br><span class="line">select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where</font><p>需要设置属性的值：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；</span><br></pre></td></tr></table></figure><p></p><p>假如不设置，报错如下:<br><img src="/assets/blogImg/0506_2.png" alt><br>3.查询数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_dynamic_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_3.png" alt></p><p><font color="#FF4500">分区列为deptno，实现了动态分区</font></p><h6 id="四-总结"><a href="#四-总结" class="headerlink" title="四.总结"></a>四.总结</h6><p>在生产上我们更倾向是选择<strong>动态分区</strong>，<br>无需手工指定数据导入的具体分区，<br>而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用</title>
      <link href="/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><img src="/assets/blogImg/504_1.png" alt><br><a id="more"></a></p><h6 id="1-介绍："><a href="#1-介绍：" class="headerlink" title="1. 介绍："></a>1. 介绍：</h6><p>两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，<br>客户端可以在不启动CLI的情况下对Hive中的数据进行操作，<br>两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果<br>（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。</p><p>HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，<br>而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？<br>这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，<br>不能通过修改HiveServer的代码修正。</p><p>因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。<br>HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。</p><h6 id="2-配置参数"><a href="#2-配置参数" class="headerlink" title="2.配置参数"></a>2.配置参数</h6><p>Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：<br>参数 | 含义 |<br>-|-|<br>hive.server2.thrift.min.worker.threads| 最小工作线程数，默认为5。<br>hive.server2.thrift.max.worker.threads| 最小工作线程数，默认为500。<br>hive.server2.thrift.port| TCP 的监听端口，默认为10000。<br>hive.server2.thrift.bind.host| TCP绑定的主机，默认为localhost</p><p>配置监听端口和路径<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;192.168.48.130&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="3-启动hiveserver2"><a href="#3-启动hiveserver2" class="headerlink" title="3. 启动hiveserver2"></a>3. 启动hiveserver2</h6><p>使用hadoop用户启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/</span><br><span class="line">[hadoop@hadoop001 bin]$ hiveserver2 </span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br></pre></td></tr></table></figure><p></p><h6 id="4-重新开个窗口，使用beeline方式连接"><a href="#4-重新开个窗口，使用beeline方式连接" class="headerlink" title="4. 重新开个窗口，使用beeline方式连接"></a>4. 重新开个窗口，使用beeline方式连接</h6><ul><li>-n 指定机器登陆的名字，当前机器的登陆用户名</li><li>-u 指定一个连接串</li><li>每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK</li><li>如果命令错误，hiveserver2那个窗口就会抛出异常</li></ul><p>使用hadoop用户启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoop</span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br><span class="line">scan complete in 4ms</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.7.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.7.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.7.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt;</span><br></pre></td></tr></table></figure><p></p><p>使用SQL<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show databases;</span><br><span class="line">INFO  : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</span><br><span class="line">INFO  : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">1 row selected</span><br></pre></td></tr></table></figure><p></p><h6 id="5-使用编写java代码方式连接"><a href="#5-使用编写java代码方式连接" class="headerlink" title="5.使用编写java代码方式连接"></a>5.使用编写java代码方式连接</h6><p><strong>5.1</strong>使用maven构建项目，pom.xml文件如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-train&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive-train&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;    </span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>5.2</strong>JdbcApp.java文件代码:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class JdbcApp &#123;</span><br><span class="line">     private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">         &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">             // TODO Auto-generated catch block</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">             System.exit(1);</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;);</span><br><span class="line">         Statement stmt = con.createStatement();</span><br><span class="line">         //select table:ename</span><br><span class="line">         String tableName = &quot;emp&quot;;</span><br><span class="line">         String sql = &quot;select ename from &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">          while(res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1));</span><br><span class="line">         &#125;</span><br><span class="line">         // describe table</span><br><span class="line">         sql = &quot;describe &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         res = stmt.executeQuery(sql);</span><br><span class="line">         while (res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2min快速了解，Hive内部表和外部表</title>
      <link href="/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
      <url>/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p><font color="#FF4500"><br></font><br>在了解内部表和外部表区别前，<br>我们需要先了解一下<strong>Hive架构</strong> ：</p><p><img src="/assets/blogImg/501_1.png" alt="Hive架构"><br><a id="more"></a><br>大家可以简单看一下这个架构图，我介绍其中要点：<br>Hive的数据分为两种，<strong>一种为普通数据，一种为元数据。</strong></p><ol><li>元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。</li><li>Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。</li></ol><p>下面我们来介绍表的两种类型：内部表和外部表</p><ol><li><p>内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。</p></li><li><p>外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表<br>而内部表和外部表的主要区别就是</p><ul><li>内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；</li><li>外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；</li></ul></li></ol><h6 id="1-准备数据-按tab键制表符作为字段分割符"><a href="#1-准备数据-按tab键制表符作为字段分割符" class="headerlink" title="1.准备数据:  按tab键制表符作为字段分割符"></a>1.准备数据: 按tab键制表符作为字段分割符</h6><pre><code>cat /tmp/ruozedata.txt1   jepson  32  1102   ruoze   22  1123   www.ruozedata.com   18  120</code></pre><h6 id="2-内部表测试："><a href="#2-内部表测试：" class="headerlink" title="2.内部表测试："></a>2.内部表测试：</h6><ol><li><p>在Hive里面创建一个表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table ruozedata(id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tele string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.759 seconds</span><br></pre></td></tr></table></figure></li><li><p>这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>内部表删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table ruozedata;</span><br></pre></td></tr></table></figure></li></ol><h6 id="3-外部表测试"><a href="#3-外部表测试" class="headerlink" title="3.外部表测试:"></a>3.外部表测试:</h6><ol><li>创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table exter_ruozedata(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tel string)</span><br><span class="line">    &gt; location &apos;/hive/external&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.098 seconds</span><br></pre></td></tr></table></figure></li></ol><p>创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径<br>（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）</p><ol start="2"><li><p>外部表导入数据和内部表一样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table exter_ruozedata;</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谈谈我和大数据的情缘及入门</title>
      <link href="/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>&#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。</p><p>&#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。<br><a id="more"></a><br>&#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。</p><p>&#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。</p><p>&#8195;<strong>后来这样的进度太慢了</strong>，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。</p><p>&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。</p><p>&#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。<strong>然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。</strong></p><p>&#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！</p><p><strong>以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。</strong><br><strong>1. 心态要端正。</strong><br>既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。<br>后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。</p><p><strong>2. 心目中要有计划。</strong><br>先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，<br>然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。</p><p><strong>3. 各种方式学习。</strong><br>QQ群，博客，上下班看技术文章，选择好的老师和课程培训，</p><p><font color="#FF4500"><br>(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)</font><br>可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。</p><p><strong>4. 项目经验。</strong><br>很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，<br>这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。<br>而面试，就看看其他人面试分享，学习他人。</p><p><strong>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 有缘大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义函数(UDF)的部署使用，你会吗？</title>
      <link href="/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Hive自定义函数(UDF)的部署使用，你会吗，三种方式！<br><a id="more"></a></p><font color="#FF4500"><br></font><h6 id="一-临时函数"><a href="#一-临时函数" class="headerlink" title="一.临时函数"></a>一.临时函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>添加jar包<br>hive&gt;add xxx.jar jar_filepath;</li><li>查看jar包<br>hive&gt;list jars;</li><li>创建临时函数<br>hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;</li></ol><h6 id="二-持久函数"><a href="#二-持久函数" class="headerlink" title="二.持久函数"></a>二.持久函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>上传到HDFS<br>$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jar</li><li>创建持久函数<br>hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;</li></ol><p><strong>注意点：</strong></p><ul><li><ol><li>此方法在show functions时是看不到的，但是可以使用</li></ol></li><li><ol start="2"><li>需要上传至hdfs</li></ol></li></ul><h6 id="三-持久函数，并注册"><a href="#三-持久函数，并注册" class="headerlink" title="三.持久函数，并注册"></a>三.持久函数，并注册</h6><p>环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>下载源码<br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a></p></li><li><p>解压源码<br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>将HelloUDF.java文件增加到HIVE源码中<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>修改FunctionRegistry.java 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>重新编译<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>编译结果全部为：BUILD SUCCESS<br>文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>配置hive环境<br>配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：<br>7.1. 全部配置：参照之前文档 <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hive全网最详细的编译及部署</a></p><p>7.2. 将编译后带UDF函数的包复制到旧hive环境<br>到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉<br>命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>最终启动hive</p></li><li><p>测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- 能查看到有 helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudf函数生效</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义函数(UDF)的编程开发，你会吗？</title>
      <link href="/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>本地开发环境：IntelliJ IDEA+Maven3.3.9<br><a id="more"></a></p><h6 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h6><p>打开IntelliJ IDEA<br>File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h6><p>在工程中找到pom.xml文件，添加hadoop、hive依赖<br><img src="/assets/blogImg/425hive1.png" alt="Hive图1"></p><h6 id="3-创建类、并编写一个HelloUDF-java，代码如下："><a href="#3-创建类、并编写一个HelloUDF-java，代码如下：" class="headerlink" title="3. 创建类、并编写一个HelloUDF.java，代码如下："></a>3. 创建类、并编写一个HelloUDF.java，代码如下：</h6><p><img src="/assets/blogImg/425hive2.png" alt="Hive图2"></p><p><strong>首先一个UDF必须满足下面两个条件:</strong></p><ul><li><ol><li>一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）</li></ol></li><li><ol start="2"><li>一个UDF必须至少实现了evaluate()方法</li></ol></li></ul><h6 id="4-测试，右击运行run-‘HelloUDF-main-’"><a href="#4-测试，右击运行run-‘HelloUDF-main-’" class="headerlink" title="4. 测试，右击运行run ‘HelloUDF.main()’"></a>4. 测试，右击运行run ‘HelloUDF.main()’</h6><h6 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h6><p>在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包<br>执行成功后在日志中找：<br><font color="#FF4500">[INFO] Building jar: (路径)/hive-1.0.jar</font></p><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive DDL，你真的了解吗？</title>
      <link href="/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，带你全面剖析Hive DDL！</p><font color="#FF4500"><br></font><p><img src="/assets/blogImg/hive424.png" alt="Hive架构图"><br><a id="more"></a></p><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p><strong>Database</strong><br>Hive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）</p><p><strong>Table</strong><br>Hive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table</p><p><strong>Partition</strong><br>分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：<br>/user/hadoop/hive/warehouse/[databasename.db]/table</p><h5 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h5><p><strong>Create Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><p></p><p>IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。<br>COMMENT：数据库的描述<br>LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下<br>WITH DBPROPERTIES：数据库的属性</p><p><strong>Drop Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name </span><br><span class="line">[RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><p></p><p>RESTRICT：默认是restrict，如果该数据库还有表存在则报错；<br>CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。</p><p><strong>Alter Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>Use Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br></pre></td></tr></table></figure><p></p><p><strong>Show Databases</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;</span><br><span class="line">“ | ”：可以选择其中一种</span><br><span class="line"></span><br><span class="line">“[ ]”：可选项</span><br><span class="line"></span><br><span class="line">LIKE ‘identifier_with_wildcards’：模糊查询数据库</span><br></pre></td></tr></table></figure><p></p><p><strong>Describe Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE DATABASE [EXTENDED] db_name;</span><br><span class="line">DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；</span><br><span class="line">EXTENDED：加上数据库键值对的属性信息。</span><br><span class="line">hive&gt; describe database default;</span><br><span class="line">OK</span><br><span class="line">default    Default Hive database    hdfs://hadoop1:9000/user/hive/warehouse    public    ROLE    </span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line"></span><br><span class="line">hive&gt; describe database extended hive2;</span><br><span class="line">OK</span><br><span class="line">hive2   it is my database       hdfs://hadoop1:9000/user/hive/warehouse/hive2.db        hadoop      USER    &#123;date=2018-08-08, creator=zhangsan&#125;</span><br><span class="line">Time taken: 0.135 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p></p><p><strong>Create Table</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line"> ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line"> [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line"> | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"></span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br></pre></td></tr></table></figure><p></p><p><strong>data_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">: primitive_type</span><br><span class="line">| array_type</span><br><span class="line">| map_type</span><br><span class="line">| struct_type</span><br><span class="line">| union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>primitive_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> : TINYINT</span><br><span class="line"> | SMALLINT</span><br><span class="line"> | INT</span><br><span class="line"> | BIGINT</span><br><span class="line"> | BOOLEAN</span><br><span class="line">| FLOAT</span><br><span class="line"> | DOUBLE</span><br><span class="line"> | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line"> | STRING</span><br><span class="line"> | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line"> | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>array_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: ARRAY &lt; data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>map_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: MAP &lt; primitive_type, data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>struct_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>union_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note:     Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>row_format</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p><strong>file_format:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p></p><p><strong>constraint_specification:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">TEMPORARY（临时表）</span><br><span class="line">Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。</span><br><span class="line">语法：CREATE TEMPORARY TABLE …</span><br></pre></td></tr></table></figure><p></p><h6 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h6><ol><li>如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表</li><li>临时表限制：不支持分区字段和创建索引</li></ol><p>EXTERNAL（外部表）<br>Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table external_table(</span><br><span class="line">  &gt; id int,</span><br><span class="line">&gt;  name string </span><br><span class="line">&gt; );</span><br></pre></td></tr></table></figure><p></p><p>PARTITIONED BY（分区表）<br>产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。</p><p>可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；</p><p>分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。</p><p>分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。</p><p>单分区：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE order_partition (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">    &gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (event_month string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>多分区：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE TABLE order_partition2 (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">&gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt;  PARTITIONED BY (event_month string,every_day string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db</span><br><span class="line">18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2</span><br><span class="line">[hadoop@hadoop000 ~]$</span><br><span class="line">ROW FORMAT</span><br></pre></td></tr></table></figure><p>官网解释：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char [ESCAPED BY char]]       [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   </span><br><span class="line">-- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p>DELIMITED：分隔符（可以自定义分隔符）；</p><p>FIELDS TERMINATED BY char:每个字段之间使用的分割；</p><p>例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;</p><p>COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；</p><p>MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；</p><p>LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）</p><p>一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。</p><p>创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo1(</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>创建demo2表，并指定其他字段：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo2 (</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string,</span><br><span class="line">&gt; hobbies ARRAY &lt;string&gt;,</span><br><span class="line">&gt; address MAP &lt;string, string&gt;</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;</span><br><span class="line">&gt; MAP KEYS TERMINATED BY &apos;:&apos;;</span><br><span class="line">OK</span><br><span class="line">STORED AS（存储格式）</span><br><span class="line">Create Table As Select</span><br></pre></td></tr></table></figure><p></p><p>创建表（拷贝表结构及数据，并且会运行MapReduce作业）<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp (</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">salary double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure><p></p><p>加载数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;</span><br></pre></td></tr></table></figure><p></p><p>复制整张表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp2 as select * from emp;</span><br><span class="line">Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/</span><br><span class="line">Kill Command = /opt/software/hadoop/bin/hadoop job  -kill job_1514116522188_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2018-01-08 05:21:07,707 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2018-01-08 05:21:19,605 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 810 msec</span><br><span class="line">Ended Job = job_1514116522188_0003</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2</span><br><span class="line">Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 3927 HDFS Write: 730 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 810 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 33.322 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp2</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.071 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>复制表中的一些字段<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table emp3 as select empno,ename from emp;</span><br></pre></td></tr></table></figure><p></p><p>LIKE<br>使用like创建表时，只会复制表的结构，不会复制表的数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp4 like emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.149 seconds</span><br><span class="line">hive&gt; select * from emp4;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.151 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>并没有查询到数据</p><p>desc formatted table_name<br>查询表的详细信息<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted emp;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>col_name data_type comment<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">empno                   int                                         </span><br><span class="line">ename                   string                                      </span><br><span class="line">job                     string                                      </span><br><span class="line">mgr                     int                                         </span><br><span class="line">hiredate                string                                      </span><br><span class="line">salary                  double                                      </span><br><span class="line">comm                    double                                      </span><br><span class="line">deptno                  int</span><br></pre></td></tr></table></figure><p></p><p>Detailed Table Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Database:               hive                     </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Mon Jan 08 05:17:54 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Protect Mode:           None                     </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">COLUMN_STATS_ACCURATE    true                </span><br><span class="line">numFiles                1                   </span><br><span class="line">numRows                 0                   </span><br><span class="line">rawDataSize             0                   </span><br><span class="line">totalSize               668                 </span><br><span class="line">transient_lastDdlTime    1515359982</span><br></pre></td></tr></table></figure><p></p><p>Storage Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">field.delim             \t                  </span><br><span class="line">serialization.format    \t                  </span><br><span class="line">Time taken: 0.228 seconds, Fetched: 39 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;</p><p>查询数据库下的所有表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp1</span><br><span class="line">emp2</span><br><span class="line">emp3</span><br><span class="line">emp4</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.047 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>查询创建表的语法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show create table emp;</span><br><span class="line">OK</span><br><span class="line">CREATE TABLE `emp`(</span><br><span class="line">  `empno` int, </span><br><span class="line">  `ename` string, </span><br><span class="line">  `job` string, </span><br><span class="line">  `mgr` int, </span><br><span class="line">  `hiredate` string, </span><br><span class="line">  `salary` double, </span><br><span class="line">  `comm` double, </span><br><span class="line">  `deptno` int)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;\t&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;numRows&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;rawDataSize&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;668&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)</span><br><span class="line">Time taken: 0.192 seconds, Fetched: 24 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">Drop Table</span><br><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];     -- (Note: PURGE available in Hive 0.14.0 and later)</span><br></pre></td></tr></table></figure><p></p><p>指定PURGE后，数据不会放到回收箱，会直接删除</p><p>DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失</p><p>删除EXTERNAL表时，表中的数据不会从文件系统中删除<br>Alter Table</p><p>重命名<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table demo2 rename to new_demo2;</span><br><span class="line">OK</span><br><span class="line">Add Partitions</span><br><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line">  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)</span><br><span class="line">用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。</span><br><span class="line">原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。</span><br><span class="line"></span><br><span class="line">hive&gt;  create table dept(</span><br><span class="line">&gt;  deptno int,</span><br><span class="line">&gt; dname string,</span><br><span class="line">&gt; loc string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (dt string)</span><br><span class="line">&gt;  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.953 seconds </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);</span><br><span class="line">Loading data to table default.dept partition (dt=2018-08-08)</span><br><span class="line">Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.147 seconds</span><br></pre></td></tr></table></figure><p></p><p>查询结果<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 0.481 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);</span><br><span class="line">OK</span><br><span class="line">Drop Partitions</span><br><span class="line">ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);</span><br></pre></td></tr></table></figure><p></p><p>查看分区语句<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept;</span><br><span class="line">OK</span><br><span class="line">dt=2018-08-08</span><br><span class="line">dt=2018-09-09</span><br><span class="line">Time taken: 0.385 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><p></p><p>按分区查询<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 2.323 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive生产上，压缩和存储结合使用案例</title>
      <link href="/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>你们Hive生产上，压缩和存储，结合使用了吗？</p><p>案例：<br>原文件大小：19M<br><img src="/assets/blogImg/423_1.png" alt="enter description here"><br><a id="more"></a></p><h6 id="1-ORC-Zlip结合"><a href="#1-ORC-Zlip结合" class="headerlink" title="1. ORC+Zlip结合"></a>1. ORC+Zlip结合</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc_zlib</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><font color="#FF4500">用ORC+Zlip之后的文件为2.8M<br><br></font><br>用ORC+Zlip之后的文件为2.8M<br><img src="/assets/blogImg/423_2.png" alt="enter description here"><br><br><br>###### 2. Parquet+gzip结合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       set parquet.compression=gzip;</span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><br><br><font color="#FF4500"><br>用Parquet+gzip之后的文件为3.9M<br></font><p><img src="/assets/blogImg/423_3.png" alt="enter description here"></p><h6 id="3-Parquet-Lzo结合"><a href="#3-Parquet-Lzo结合" class="headerlink" title="3. Parquet+Lzo结合"></a>3. Parquet+Lzo结合</h6><p><strong>3.1 安装Lzo</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gz</span><br><span class="line">tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">cd lzo-2.06</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzo/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib/</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib64/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export PATH=/usr/local//hadoop/lzo/:$PATH</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.2 安装Lzop</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.lzop.org/download/lzop-1.03.tar.gz</span><br><span class="line">tar -zxvf lzop-1.03.tar.gz</span><br><span class="line">cd lzop-1.03</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzop</span><br><span class="line">make  &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.3 软连接</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4 测试lzop</strong><br>lzop xxx.log<br>若生成xxx.log.lzo文件，则说明成功<br><strong>3.5 安装Hadoop-LZO</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   git或svn 下载https://github.com/twitter/hadoop-lzo</span><br><span class="line">cd hadoop-lzo</span><br><span class="line">mvn clean package -Dmaven.test.skip=true </span><br><span class="line">tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/</span><br></pre></td></tr></table></figure><p></p><p><strong>3.6 配置</strong><br>在core-site.xml配置<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">     org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">           &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在mapred-site.xml中配置</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">           &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在hadoop-env.sh中配置</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure><p></p><p><strong>3.7 测试</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.compress.output=true;  </span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line">SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line">create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">TBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p></p><p><font color="#FF4500">用Parquet+Lzo(未建立索引)之后的文件为5.9M<br></font><br><img src="/assets/blogImg/423_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 压缩格式 </tag>
            
            <tag> 案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>又又又是源码！RDD 作业的DAG是如何切分的？</title>
      <link href="/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/"/>
      <url>/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>我们都知道，RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。那么这整个过程在源码中是如何体现的呢？<br><a id="more"></a></p><h4 id="1-作业的提交"><a href="#1-作业的提交" class="headerlink" title="1.作业的提交"></a>1.作业的提交</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// SparkContext.scala</span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// DAGScheduler.scala</span><br><span class="line">   def runJob[T, U](</span><br><span class="line">    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure><p>可以看到，SparkContext的runjob方法调用了DAGScheduler的runjob方法正式向集群提交任务，最终调用了submitJob方法。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2   def submitJob[T, U](</span><br><span class="line"> 3      rdd: RDD[T],</span><br><span class="line"> 4      func: (TaskContext, Iterator[T]) =&gt; U,</span><br><span class="line"> 5      partitions: Seq[Int],</span><br><span class="line"> 6      callSite: CallSite,</span><br><span class="line"> 7      resultHandler: (Int, U) =&gt; Unit,</span><br><span class="line"> 8      properties: Properties): JobWaiter[U] = &#123;</span><br><span class="line"> 9    // Check to make sure we are not launching a task on a partition that does not exist.</span><br><span class="line">10    val maxPartitions = rdd.partitions.length</span><br><span class="line">11    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;</span><br><span class="line">12      throw new IllegalArgumentException(</span><br><span class="line">13        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +</span><br><span class="line">14          &quot;Total number of partitions: &quot; + maxPartitions)</span><br><span class="line">15    &#125;</span><br><span class="line">16</span><br><span class="line">17    val jobId = nextJobId.getAndIncrement()</span><br><span class="line">18    if (partitions.size == 0) &#123;</span><br><span class="line">19      // Return immediately if the job is running 0 tasks</span><br><span class="line">20      return new JobWaiter[U](this, jobId, 0, resultHandler)</span><br><span class="line">21    &#125;</span><br><span class="line">22</span><br><span class="line">23    assert(partitions.size &gt; 0)</span><br><span class="line">24    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</span><br><span class="line">25    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</span><br><span class="line">26    //给eventProcessLoop发送JobSubmitted消息</span><br><span class="line">27    eventProcessLoop.post(JobSubmitted(</span><br><span class="line">28      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">29      SerializationUtils.clone(properties)))</span><br><span class="line">30    waiter</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><p>这里向eventProcessLoop对象发送了JobSubmitted消息。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">1// DAGScheduler.scala</span><br><span class="line">2   private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)</span><br><span class="line">    eventProcessLoop是DAGSchedulerEventProcessLoop类的一个对象。</span><br><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123;</span><br><span class="line"> 3    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line"> 4      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"> 5</span><br><span class="line"> 6    case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line"> 7      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"> 8</span><br><span class="line"> 9    case StageCancelled(stageId) =&gt;</span><br><span class="line">10      dagScheduler.handleStageCancellation(stageId)</span><br><span class="line">11</span><br><span class="line">12    case JobCancelled(jobId) =&gt;</span><br><span class="line">13      dagScheduler.handleJobCancellation(jobId)</span><br><span class="line">14</span><br><span class="line">15    case JobGroupCancelled(groupId) =&gt;</span><br><span class="line">16      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line">17</span><br><span class="line">18    case AllJobsCancelled =&gt;</span><br><span class="line">19      dagScheduler.doCancelAllJobs()</span><br><span class="line">20</span><br><span class="line">21    case ExecutorAdded(execId, host) =&gt;</span><br><span class="line">22      dagScheduler.handleExecutorAdded(execId, host)</span><br><span class="line">23</span><br><span class="line">24    case ExecutorLost(execId, reason) =&gt;</span><br><span class="line">25      val filesLost = reason match &#123;</span><br><span class="line">26        case SlaveLost(_, true) =&gt; true</span><br><span class="line">27        case _ =&gt; false</span><br><span class="line">28      &#125;</span><br><span class="line">29      dagScheduler.handleExecutorLost(execId, filesLost)</span><br><span class="line">30</span><br><span class="line">31    case BeginEvent(task, taskInfo) =&gt;</span><br><span class="line">32      dagScheduler.handleBeginEvent(task, taskInfo)</span><br><span class="line">33</span><br><span class="line">34    case GettingResultEvent(taskInfo) =&gt;</span><br><span class="line">35      dagScheduler.handleGetTaskResult(taskInfo)</span><br><span class="line">36</span><br><span class="line">37    case completion: CompletionEvent =&gt;</span><br><span class="line">38      dagScheduler.handleTaskCompletion(completion)</span><br><span class="line">39</span><br><span class="line">40    case TaskSetFailed(taskSet, reason, exception) =&gt;</span><br><span class="line">41      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span><br><span class="line">42</span><br><span class="line">43    case ResubmitFailedStages =&gt;</span><br><span class="line">44      dagScheduler.resubmitFailedStages()</span><br><span class="line">45  &#125;</span><br></pre></td></tr></table></figure><p></p><p>DAGSchedulerEventProcessLoop对接收到的消息进行处理，在doOnReceive方法中形成一个event loop。<br>接下来将调用submitStage()方法进行stage的划分。</p><h4 id="2-stage的划分"><a href="#2-stage的划分" class="headerlink" title="2.stage的划分"></a>2.stage的划分</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2 private def submitStage(stage: Stage) &#123;</span><br><span class="line"> 3    val jobId = activeJobForStage(stage)//查找该Stage的所有激活的job</span><br><span class="line"> 4    if (jobId.isDefined) &#123;</span><br><span class="line"> 5      logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)</span><br><span class="line"> 6      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line"> 7        val missing = getMissingParentStages(stage).sortBy(_.id)//得到Stage的父Stage，并排序</span><br><span class="line"> 8        logDebug(&quot;missing: &quot; + missing)</span><br><span class="line"> 9        if (missing.isEmpty) &#123;</span><br><span class="line">10          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)</span><br><span class="line">11          submitMissingTasks(stage, jobId.get)//如果Stage没有父Stage，则提交任务集</span><br><span class="line">12        &#125; else &#123;</span><br><span class="line">13          for (parent &lt;- missing) &#123;//如果有父Stage，递归调用submiStage</span><br><span class="line">14            submitStage(parent)</span><br><span class="line">15          &#125;</span><br><span class="line">16          waitingStages += stage//将其标记为等待状态，等待下次提交</span><br><span class="line">17        &#125;</span><br><span class="line">18      &#125;</span><br><span class="line">19    &#125; else &#123;</span><br><span class="line">20      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//如果该Stage没有激活的job，则丢弃该Stage</span><br><span class="line">21    &#125;</span><br><span class="line">22  &#125;</span><br></pre></td></tr></table></figure><p>在submitStage方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123;</span><br><span class="line"> 3    val missing = new HashSet[Stage] //用于存放父Stage</span><br><span class="line"> 4    val visited = new HashSet[RDD[_]] //用于存放已访问过的RDD</span><br><span class="line"> 5</span><br><span class="line"> 6    val waitingForVisit = new Stack[RDD[_]]</span><br><span class="line"> 7    def visit(rdd: RDD[_]) &#123;</span><br><span class="line"> 8      if (!visited(rdd)) &#123; //如果RDD没有被访问过，则进行访问</span><br><span class="line"> 9        visited += rdd //添加到已访问RDD的HashSet中</span><br><span class="line">10        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)</span><br><span class="line">11        if (rddHasUncachedPartitions) &#123;</span><br><span class="line">12          for (dep &lt;- rdd.dependencies) &#123; //获取该RDD的依赖</span><br><span class="line">13            dep match &#123;</span><br><span class="line">14              case shufDep: ShuffleDependency[_, _, _] =&gt;//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage</span><br><span class="line">15                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//生成父Stage</span><br><span class="line">16                if (!mapStage.isAvailable) &#123;//若父Stage不存在，则添加到父Stage的HashSET中</span><br><span class="line">17                  missing += mapStage</span><br><span class="line">18                &#125;</span><br><span class="line">19              case narrowDep: NarrowDependency[_] =&gt;//若为窄依赖，则继续访问父RDD</span><br><span class="line">20                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">21            &#125;</span><br><span class="line">22          &#125;</span><br><span class="line">23        &#125;</span><br><span class="line">24      &#125;</span><br><span class="line">25    &#125;</span><br><span class="line">26    waitingForVisit.push(stage.rdd)</span><br><span class="line">27    while (waitingForVisit.nonEmpty) &#123;//循环遍历所有RDD</span><br><span class="line">28      visit(waitingForVisit.pop())</span><br><span class="line">29    &#125;</span><br><span class="line">30    missing.toList</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="getmissingParentStages-方法为核心方法。"><a href="#getmissingParentStages-方法为核心方法。" class="headerlink" title="getmissingParentStages()方法为核心方法。"></a>getmissingParentStages()方法为核心方法。</h4><font color="#FF4500"><br><br>这里我们要懂得这样一个逻辑：我们都知道，Stage是通过shuffle划分的，所以，每一Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。<br></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive存储格式的生产应用</title>
      <link href="/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/"/>
      <url>/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p><strong>相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。</strong></p><p>原始大小: 19M</p><p><img src="/assets/blogImg/420_1.png" alt="enter description here"><br><a id="more"></a></p><h5 id="1-TextFile-默认-文件大小为18-1M"><a href="#1-TextFile-默认-文件大小为18-1M" class="headerlink" title="1. TextFile(默认) 文件大小为18.1M"></a>1. TextFile(默认) 文件大小为18.1M</h5><p><img src="/assets/blogImg/420_2.png" alt="enter description here"></p><h5 id="2-SequenceFile"><a href="#2-SequenceFile" class="headerlink" title="2. SequenceFile"></a>2. SequenceFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_seq( </span><br><span class="line">track_time string, </span><br><span class="line">url string, </span><br><span class="line">session_id string, </span><br><span class="line">referer string, </span><br><span class="line">ip string, </span><br><span class="line">end_user_id string, </span><br><span class="line">city_id string </span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” </span><br><span class="line">STORED AS SEQUENCEFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_seq select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用SequenceFile存储后的文件为19.6M</strong><br><img src="/assets/blogImg/420_3.png" alt="enter description here"></p><h5 id="3-RcFile"><a href="#3-RcFile" class="headerlink" title="3. RcFile"></a>3. RcFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_rcfile(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS RCFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_rcfile select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用RcFile存储后的文件为17.9M</strong><br><img src="/assets/blogImg/420_4.png" alt="enter description here"></p><h5 id="4-ORCFile"><a href="#4-ORCFile" class="headerlink" title="4. ORCFile"></a>4. ORCFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用ORCFile存储后的文件为7.7M</strong><br><img src="/assets/blogImg/420_5.png" alt="enter description here"></p><h5 id="5-Parquet"><a href="#5-Parquet" class="headerlink" title="5. Parquet"></a>5. Parquet</h5><pre><code>create table page_views_parquetROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;</code></pre><p><strong>用ORCFile存储后的文件为13.1M</strong><br><img src="/assets/blogImg/420_6.png" alt="enter description here"></p><p><strong>总结：磁盘空间占用大小比较</strong></p><font color="#FF4500">ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)</font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 压缩格式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据压缩，你们真的了解吗？</title>
      <link href="/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，带你们剖析大数据之压缩！<br><a id="more"></a></p><h6 id="1-压缩的好处和坏处"><a href="#1-压缩的好处和坏处" class="headerlink" title="1. 压缩的好处和坏处"></a>1. 压缩的好处和坏处</h6><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h6 id="2-压缩格式"><a href="#2-压缩格式" class="headerlink" title="2. 压缩格式"></a>2. 压缩格式</h6><p><img src="/assets/blogImg/压缩1.png" alt="enter description here"><br>压缩比<br><img src="/assets/blogImg/压缩2.png" alt="enter description here"><br>压缩时间<br><img src="/assets/blogImg/yasuo3.png" alt="enter description here"></p><font color="#FF0000">可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</font><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>gzip</strong></td><td>压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td>不支持split</td></tr><tr><td><strong>lzo</strong></td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td><strong>snappy</strong></td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td><strong>bzip2</strong></td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h5><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h5 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a><strong>应用场景：</strong></h5><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 压缩格式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark2.2.0 全网最详细的源码编译</title>
      <link href="/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，Spark2.2.0 全网最详细的源码编译<br><a id="more"></a></p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><hr><p>JDK： Spark 2.2.0及以上版本只支持JDK1.8</p><hr><p>Maven：3.3.9<br>设置maven环境变量时，需设置maven内存：<br>export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”</p><hr><p>Scala：2.11.8</p><hr><p>Git</p><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>下载spark的tar包，并解压<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</span><br><span class="line">[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz</span><br></pre></td></tr></table></figure><p></p><p>编辑dev/make-distribution.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh</span><br><span class="line">注释以下内容：</span><br><span class="line">#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use &quot;set -o pipefail&quot;</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure><p></p><p>添加以下内容：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure><p></p><p>编辑pom.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml</span><br><span class="line">添加在repositorys内</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">      &lt;id&gt;clouders&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;clouders Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br></pre></td></tr></table></figure><p></p><p>安装<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn</span><br></pre></td></tr></table></figure><p></p><p>稍微等待几小时，网络较好的话，非常快。<br>也可以参考J哥博客：<br>基于CentOS6.4环境编译Spark-2.1.0源码 <a href="http://blog.itpub.net/30089851/viewspace-2140779/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2140779/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop常用命令大全</title>
      <link href="/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
      <url>/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hadoop常用命令大全<br><a id="more"></a></p><h6 id="1-单独启动和关闭hadoop服务"><a href="#1-单独启动和关闭hadoop服务" class="headerlink" title="1. 单独启动和关闭hadoop服务"></a><strong>1. 单独启动和关闭hadoop服务</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>启动名称节点</strong></td><td style="text-align:center">hadoop-daemon.sh start namenode</td></tr><tr><td><strong>启动数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh start datanode slave</td></tr><tr><td><strong>启动secondarynamenode</strong></td><td style="text-align:center">hadoop-daemon.sh start secondarynamenode</td></tr><tr><td><strong>启动resourcemanager</strong></td><td style="text-align:center">yarn-daemon.sh start resourcemanager</td></tr><tr><td><strong>启动nodemanager</strong></td><td style="text-align:center">bin/yarn-daemons.sh start nodemanager</td></tr><tr><td><strong>停止数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh stop datanode</td></tr></tbody></table><h6 id="2-常用的命令"><a href="#2-常用的命令" class="headerlink" title="2. 常用的命令"></a><strong>2. 常用的命令</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>创建目录</strong></td><td style="text-align:center">hdfs dfs -mkdir /input</td></tr><tr><td><strong>查看</strong></td><td style="text-align:center">hdfs dfs -ls</td></tr><tr><td><strong>递归查看</strong></td><td style="text-align:center">hdfs dfs ls -R</td></tr><tr><td><strong>上传</strong></td><td style="text-align:center">hdfs dfs -put</td></tr><tr><td><strong>下载</strong></td><td style="text-align:center">hdfs dfs -get</td></tr><tr><td><strong>删除</strong></td><td style="text-align:center">hdfs dfs -rm</td></tr><tr><td><strong>从本地剪切粘贴到hdfs</strong></td><td style="text-align:center">hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>从hdfs剪切粘贴到本地</strong></td><td style="text-align:center">hdfs fs -moveToLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>追加一个文件到另一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -appedToFile ./hello.txt /input/hello.txt</td></tr><tr><td><strong>查看文件内容</strong></td><td style="text-align:center">hdfs fs -cat /input/hello.txt</td></tr><tr><td><strong>显示一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -tail /input/hello.txt</td></tr><tr><td><strong>以字符串的形式打印文件的内容</strong></td><td style="text-align:center">hdfs fs -text /input/hello.txt</td></tr><tr><td><strong>修改文件权限</strong></td><td style="text-align:center">hdfs fs -chmod 666 /input/hello.txt</td></tr><tr><td><strong>修改文件所属</strong></td><td style="text-align:center">hdfs fs -chown ruoze.ruoze /input/hello.txt</td></tr><tr><td><strong>从本地文件系统拷贝到hdfs里</strong></td><td style="text-align:center">hdfs fs -copyFromLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs拷贝到本地</strong></td><td style="text-align:center">hdfs fs -copyToLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs到一个路径拷贝到另一个路径</strong></td><td style="text-align:center">hdfs fs -cp /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>从hdfs到一个路径移动到另一个路径</strong></td><td style="text-align:center">hdfs fs -mv /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>统计文件系统的可用空间信息</strong></td><td style="text-align:center">hdfs fs -df -h /</td></tr><tr><td><strong>统计文件夹的大小信息</strong></td><td style="text-align:center">hdfs fs -du -s -h /</td></tr><tr><td><strong>统计一个指定目录下的文件节点数量</strong></td><td style="text-align:center">hadoop fs -count /aaa</td></tr><tr><td><strong>设置hdfs的文件副本数量</strong></td><td style="text-align:center">hadoop fs -setrep 3 /input/xx.txt</td></tr></tbody></table><h5 id="总结：一定要学会查看命令帮助"><a href="#总结：一定要学会查看命令帮助" class="headerlink" title="总结：一定要学会查看命令帮助"></a>总结：一定要学会查看命令帮助</h5><p><strong>1.hadoop命令直接回车查看命令帮助<br>2.hdfs命令、hdfs dfs命令直接回车查看命令帮助<br>3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么我们生产上要选择Spark On Yarn模式？</title>
      <link href="/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/"/>
      <url>/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，为什么我们生产上要选择Spark On Yarn？<br><a id="more"></a><br>开发上我们选择local[2]模式<br>生产上跑任务Job，我们选择Spark On Yarn模式 ，</p><p>将Spark Application部署到yarn中，有如下优点：</p><p>1.部署Application和服务更加方便</p><ul><li>只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。</li></ul><p>2.资源隔离机制</p><ul><li>yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。</li></ul><p>3.资源弹性管理</p><ul><li>Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。</li></ul><p>Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。</p><p><strong>运行client模式：</strong></p><ul><li><p>“./spark-shell –master yarn”</p></li><li><p>“./spark-shell –master yarn-client”</p></li><li><p>“./spark-shell –master yarn –deploy-mode client”</p></li></ul><p><strong>运行的是cluster模式</strong></p><ul><li><p>“./spark-shell –master yarn-cluster”</p></li><li><p>“./spark-shell –master yarn –deploy-mode cluster”</p></li></ul><p><strong>client和cluster模式的主要区别：<br>a. client的driver是运行在客户端进程中<br>b. cluster的driver是运行在Application Master之中</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive全网最详细的编译及部署</title>
      <link href="/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hive全网最详细的编译及部署<br><a id="more"></a></p><h6 id="一、需要安装的软件"><a href="#一、需要安装的软件" class="headerlink" title="一、需要安装的软件"></a>一、需要安装的软件</h6><ul><li><p>相关环境：</p><ul><li><p>jdk-7u80</p><ul><li>hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7</li></ul></li><li><p>apache-maven-3.3.9</p></li><li><p>mysql5.1</p></li><li><p>hadoop伪分布集群已启动</p></li></ul></li></ul><h6 id="二、安装jdk"><a href="#二、安装jdk" class="headerlink" title="二、安装jdk"></a>二、安装jdk</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /usr/java &amp;&amp; cd  /usr/java/    </span><br><span class="line"></span><br><span class="line">tar -zxvf  /tmp/server-jre-7u80-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line">chown -R root:root  /usr/java/jdk1.7.0_80/ </span><br><span class="line"></span><br><span class="line">echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="三、安装maven"><a href="#三、安装maven" class="headerlink" title="三、安装maven"></a>三、安装maven</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">unzip /tmp/apache-maven-3.3.9-bin.zip</span><br><span class="line"></span><br><span class="line">chown root: /usr/local/apache-maven-3.3.9 -R</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="四、安装mysql"><a href="#四、安装mysql" class="headerlink" title="四、安装mysql"></a>四、安装mysql</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server mysql</span><br><span class="line"></span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">chkconfig mysqld on</span><br><span class="line"></span><br><span class="line">mysqladmin -u root password 123456</span><br><span class="line"></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;</span><br><span class="line"></span><br><span class="line">delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line"></span><br><span class="line">delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line"></span><br><span class="line">drop database test;</span><br><span class="line"></span><br><span class="line">DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h6 id="五、下载hive源码包："><a href="#五、下载hive源码包：" class="headerlink" title="五、下载hive源码包："></a>五、下载hive源码包：</h6><p>输入：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>根据cdh版本选择对应hive软件包：<br>hive-1.1.0-cdh5.7.1-src.tar.gz<br>解压后使用maven命令编译成安装包</p><h6 id="六、编译"><a href="#六、编译" class="headerlink" title="六、编译:"></a>六、编译:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line"></span><br><span class="line">tar -xf hive-1.1.0-cdh5.7.1-src.tar.gz</span><br><span class="line"></span><br><span class="line">cd /tmp/hive-1.1.0-cdh5.7.1</span><br><span class="line"></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br><span class="line"></span><br><span class="line"># 编译生成的包在以下位置：</span><br><span class="line"></span><br><span class="line"># packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br></pre></td></tr></table></figure><h6 id="七、安装编译生成的Hive包，然后测试"><a href="#七、安装编译生成的Hive包，然后测试" class="headerlink" title="七、安装编译生成的Hive包，然后测试"></a>七、安装编译生成的Hive包，然后测试</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-1.1.0-cdh5.7.1-bin hive</span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin </span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop hive </span><br><span class="line"></span><br><span class="line">echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h6 id="八、更改环境变量"><a href="#八、更改环境变量" class="headerlink" title="八、更改环境变量"></a>八、更改环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">cd /usr/local/hive</span><br><span class="line"></span><br><span class="line">cd conf</span><br></pre></td></tr></table></figure><p>1、hive-env.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh&amp;&amp;vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure><p></p><p>2、hive-site.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt; </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;vincent&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="九、拷贝mysql驱动包到-HIVE-HOME-lib"><a href="#九、拷贝mysql驱动包到-HIVE-HOME-lib" class="headerlink" title="九、拷贝mysql驱动包到$HIVE_HOME/lib"></a>九、拷贝mysql驱动包到$HIVE_HOME/lib</h6><p>上方的hive-site.xml使用了java的mysql驱动包<br>需要将这个包上传到hive的lib目录之下<br>解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">unzip mysql-connector-java-5.1.45.zip</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure><p></p><p>未拷贝有相关报错：</p><p>The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH.</p><p>Please check your CLASSPATH specification,</p><p>and the name of the driver.</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)</title>
      <link href="/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/"/>
      <url>/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)<br><a id="more"></a></p><ol><li><p>修改mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template  mapred-site.xml</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# cd ../../</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop全网最详细的伪分布式部署(HDFS)</title>
      <link href="/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/"/>
      <url>/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Hadoop全网最详细的伪分布式部署(HDFS)<br><a id="more"></a></p><h6 id="1-添加hadoop用户"><a href="#1-添加hadoop用户" class="headerlink" title="1.添加hadoop用户"></a>1.添加hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 ~]# useradd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# vi /etc/sudoers</span><br><span class="line"># 找到root ALL=(ALL) ALL，添加</span><br><span class="line"></span><br><span class="line">hadoop ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure><h6 id="2-上传并解压"><a href="#2-上传并解压" class="headerlink" title="2.上传并解压"></a>2.上传并解压</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz</span><br></pre></td></tr></table></figure><h6 id="3-软连接"><a href="#3-软连接" class="headerlink" title="3.软连接"></a>3.软连接</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span><br></pre></td></tr></table></figure><h6 id="4-设置环境变量"><a href="#4-设置环境变量" class="headerlink" title="4.设置环境变量"></a>4.设置环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="5-设置用户、用户组"><a href="#5-设置用户、用户组" class="headerlink" title="5.设置用户、用户组"></a>5.设置用户、用户组</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# cd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# rm -f *.txt</span><br></pre></td></tr></table></figure><h6 id="6-切换hadoop用户"><a href="#6-切换hadoop用户" class="headerlink" title="6.切换hadoop用户"></a>6.切换hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# ll</span><br><span class="line"></span><br><span class="line">total 32</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 bin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 etc</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 include</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 lib</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexec</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logs</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 sbin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop 4096 Jun  2 14:24 share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin:可执行文件</span><br><span class="line"></span><br><span class="line"># etc: 配置文件</span><br><span class="line"></span><br><span class="line"># sbin:shell脚本，启动关闭hdfs,yarn等</span><br></pre></td></tr></table></figure><h6 id="7-配置文件"><a href="#7-配置文件" class="headerlink" title="7.配置文件"></a>7.配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt;    # 配置自己机器的IP</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="8-配置hadoop用户的ssh信任关系"><a href="#8-配置hadoop用户的ssh信任关系" class="headerlink" title="8.配置hadoop用户的ssh信任关系"></a>8.配置hadoop用户的ssh信任关系</h6><p>8.1公钥/密钥 配置无密码登录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>8.2 查看日期，看是否配置成功<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br></pre></td></tr></table></figure><p></p><h6 id="9-格式化和启动"><a href="#9-格式化和启动" class="headerlink" title="9.格式化和启动"></a>9.格式化和启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">hadoop-01: Error: JAVA_HOME is not set and could not be found.</span><br><span class="line"></span><br><span class="line">localhost: Error: JAVA_HOME is not set and could not be found.</span><br></pre></td></tr></table></figure><p>9.1解决方法:添加环境变量<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]#  vi etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line"># 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied</span><br></pre></td></tr></table></figure><p>9.2解决方法:添加权限<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# exit</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# cd ../</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# cd /opt/software/hadoop</span><br></pre></td></tr></table></figure><p></p><p>9.3 继续启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><p>9.4检查是否成功<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# jps</span><br><span class="line"></span><br><span class="line">19536 DataNode</span><br><span class="line"></span><br><span class="line">19440 NameNode</span><br><span class="line"></span><br><span class="line">19876 Jps</span><br><span class="line"></span><br><span class="line">19740 SecondaryNameNode</span><br></pre></td></tr></table></figure><p></p><p>9.5访问： <a href="http://192.168.137.130:50070" target="_blank" rel="noopener">http://192.168.137.130:50070</a></p><p>9.6修改dfs启动的进程，以hadoop-01启动</p><p>启动的三个进程：</p><p>namenode: hadoop-01 bin/hdfs getconf -namenodes</p><p>datanode: localhost datanodes (using default slaves file) etc/hadoop/slaves</p><p>secondarynamenode: 0.0.0.0<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# echo  &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">hadoop-01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50090&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50091&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><p>9.7重启<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常用命令（二）</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（二）<br><a id="more"></a></p><ol><li>实时查看文件内容 tail filename</li></ol><ul><li><p>tail -f filename 当文件(名)被修改后，不能监视文件内容</p></li><li><p>tail -F filename 当文件(名)被修改后，依然可以监视文件内容</p></li></ul><ol start="2"><li>复制、移动文件</li></ol><ul><li><p>cp oldfilename newfilename 复制</p></li><li><p>mv oldfilename newfilename 移动/重命名</p></li></ul><ol start="3"><li>echo</li></ol><ul><li><p>echo “xxx” 输出</p></li><li><p>echo “xxx” &gt; filename 覆盖</p></li><li><p>echo “xxx” &gt;&gt; filename 追加</p></li></ul><ol start="4"><li>删除 rm</li></ol><ul><li><p>rm -f 强制删除</p></li><li><p>rm -rf 强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件</p></li></ul><ol start="5"><li>别名 alias</li></ol><ul><li><p>alias x=”xxxxxx” 临时引用别名</p></li><li><p>alias x=”xxxxxx” 配置到环境变量中即为永久生效</p></li></ul><ol start="6"><li>查看历史命令 history</li></ol><ul><li><p>history 显示出所有历史记录</p></li><li><p>history n 显示出n条记录</p></li><li><p>!n 执行第n条记录</p></li></ul><ol start="7"><li>管道命令 （ | ）</li></ol><ul><li>管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入</li></ul><ol start="8"><li>查看进程、查看id、端口</li></ol><ul><li><p>ps -ef ｜grep 进程名 查看进程基本信息</p></li><li><p>netstat -npl｜grep 进程名或进程id 查看服务id和端口</p></li></ul><ol start="9"><li>杀死进程 kill</li></ol><ul><li><p>kill -9 进程名/pid 强制删除</p></li><li><p>kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程</p></li></ul><ol start="10"><li>rpm 搜索、卸载</li></ol><ul><li><p>rpm -qa | grep xxx 搜索xxx</p></li><li><p>rpm –nodeps -e xxx 删除xxx</p></li><li><p>–nodeps 不验证包的依赖性</p></li></ul><ol start="11"><li>查询</li></ol><ul><li><p>find 路径 -name xxx (推荐)</p></li><li><p>which xxx</p></li><li><p>local xxx</p></li></ul><ol start="12"><li>查看磁盘、内存、系统的情况</li></ol><ul><li><p>df -h 查看磁盘大小及其使用情况</p></li><li><p>free -m 查看内存大小及其使用情况</p></li><li><p>top 查看系统情况</p></li></ul><ol start="13"><li>软连接</li></ol><ul><li>ln -s 原始目录 目标目录</li></ul><ol start="14"><li>压缩、解压</li></ol><ul><li><p>tar -czf 压缩 tar -xzvf 解压</p></li><li><p>zip 压缩 unzip 解压</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常用命令（一）</title>
      <link href="/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/"/>
      <url>/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（一）<br><a id="more"></a></p><ol><li><p>查看当前目录 pwd</p></li><li><p>查看IP</p></li></ol><ul><li><p>ifconfig 查看虚拟机ip</p></li><li><p>hostname 主机名字</p><ul><li>i 查看主机名映射的IP</li></ul></li></ul><ol start="3"><li>切换目录 cd</li></ol><ul><li><p>cd ~ 切换家目录（root为/root，普通用户为/home/用户名）</p></li><li><p>cd /filename 以绝对路径切换目录</p></li><li><p>cd - 返回上一次操作路径，并输出路径</p></li><li><p>cd ../ 返回上一层目录</p></li></ul><ol start="4"><li><p>清理桌面 clear</p></li><li><p>显示当前目录文件和文件夹 ls</p></li></ol><ul><li><p>ls -l(ll) 显示详细信息</p></li><li><p>ls -la 显示详细信息+隐藏文件（以 . 开头，例：.ssh）</p></li><li><p>ls -lh 显示详细信息+文件大小</p></li><li><p>ls -lrt 显示详细信息+按时间排序</p></li></ul><ol start="6"><li><p>查看文件夹大小 du -sh</p></li><li><p>命令帮助</p></li></ol><ul><li><p>man 命令</p></li><li><p>命令 –help</p></li></ul><ol start="8"><li>创建文件夹 mkdir</li></ol><ul><li>mkdir -p filename1/filename2 递归创建文件夹</li></ul><ol start="9"><li><p>创建文件 touch/vi/echo xx&gt;filename</p></li><li><p>查看文件内容</p></li></ol><ul><li><p>cat filename 直接打印所有内容</p></li><li><p>more filename 根据窗口大小进行分页显示</p></li></ul><ol start="11"><li>文件编辑 vi</li></ol><ul><li><p>vi分为命令行模式，插入模式，尾行模式</p></li><li><p>命令行模式—&gt;插入模式：按i或a键</p></li><li><p>插入模式—&gt;命令行模式：按Esc键</p></li><li><p>命令行模式—&gt;尾行模式：按Shift和:键</p><p>插入模式</p><ul><li><p>dd 删除光标所在行</p></li><li><p>n+dd 删除光标以下的n行</p></li><li><p>dG 删除光标以下行</p></li><li><p>gg 第一行第一个字母</p></li><li><p>G 最后一行第一个字母</p></li><li><p>shift+$ 该行最后一个字母</p><p>尾行模式</p></li><li><p>q! 强制退出</p></li><li><p>qw 写入并退出</p></li><li><p>qw! 强制写入退出</p></li><li><p>x 退出，如果存在改动，则保存再退出</p></li></ul></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令（三）</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（三）<br><a id="more"></a></p><ol><li><p>用户、用户组</p><p>用户</p><ul><li><p>useradd 用户名 添加用户</p></li><li><p>userdel 用户名 删除用户</p></li><li><p>id 用户名 查看用户信息</p></li><li><p>passwd 用户名 修改用户密码</p></li><li><p>su - 用户名 切换用户</p></li><li><p>ll /home/ 查看已有的用户</p><p>用户组</p></li><li><p>groupadd 用户组 添加用户组</p></li><li><p>cat /etc/group 用户组的文件</p></li><li><p>usermod -a -G 用户组 用户 将用户添加到用户组中</p><p>给一个普通用户添加sudo权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span><br><span class="line">    #在root     ALL=(ALL)    ALL    下面添加一行</span><br><span class="line">    用户    ALL=(ALL)    NOPASSWD:ALL</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改文件权限</p><p>chown 修改文件或文件夹的所属用户和用户组</p><ul><li><p>chown -R 用户:用户组 文件夹名 -R 为递归参数，指针对文件夹</p></li><li><p>chown 用户:用户组 文件名</p><p>chmod: 修改文件夹或者文件的权限</p></li><li><p>chmod -R 700 文件夹名</p></li><li><p>chmod 700 文件夹名</p></li></ul></li></ol><pre><code>r  =&gt;    4w  =&gt;    2x  =&gt;    1</code></pre><ol start="3"><li>后台执行命令</li></ol><ul><li><p>&amp;</p></li><li><p>nohup</p></li><li><p>screen</p></li></ul><ol start="4"><li>多人合作 screen</li></ol><ul><li><p>screen -list 查看会话</p></li><li><p>screen -S 建立一个后台的会话</p></li><li><p>screen -r 进入会话</p></li><li><p>ctrl+a+d 退出会话</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构设计及副本放置策略</title>
      <link href="/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"/>
      <url>/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>HDFS架构设计及副本放置策略<br><a id="more"></a><br>HDFS主要由3个组件构成，分别是<strong>NameNode、SecondaryNameNode和DataNode</strong>，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。</p><h6 id="NameNode和DataNode架构图"><a href="#NameNode和DataNode架构图" class="headerlink" title="NameNode和DataNode架构图"></a>NameNode和DataNode架构图</h6><p><img src="/assets/blogImg/1.png" alt="1"><br>NameNode(名称节点)<br>存储：元信息的种类，包含:</p><ul><li>文件名称</li><li>文件目录结构</li><li>文件的属性[权限,创建时间,副本数]</li><li>文件对应哪些数据块–&gt;数据块对应哪些datanode节点</li><li>作用：</li><li>管理着文件系统命名空间</li><li>维护这文件系统树及树中的所有文件和目录</li><li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li></ul><p>DataNode(数据节点)<br>存储：数据块、数据块校验、与NameNode通信<br>作用：</p><ul><li>读写文件的数据块</li><li>NameNode的指示来进行创建、删除、和复制等操作</li><li>通过心跳定期向NameNode发送所存储文件块列表信息</li><li>Scondary NameNode(第二名称节点)<br>存储: 命名空间镜像文件fsimage+编辑日志editlog<br>作用: 定期合并fsimage+editlog文件为新的fsimage推送给NamenNode<h6 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h6><img src="/assets/blogImg/2.png" alt="2"><br><strong>第一副本</strong>：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上<br><strong>第二副本</strong>：放置在与第一个副本不同的机架的节点上<br><strong>第三副本</strong>：与第二个副本相同机架的不同节点上<br>如果还有更多的副本：随机放在节点中</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hdfs </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置多台虚拟机之间的SSH信任</title>
      <link href="/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/"/>
      <url>/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h4 id="本机环境"><a href="#本机环境" class="headerlink" title="本机环境"></a>本机环境</h4><a id="more"></a><p><img src="/assets/blogImg/640.png" alt="1"></p><p>3台机器执行命令ssh-keygen<br><img src="/assets/blogImg/641.png" alt="2"></p><p>选取第一台,生成authorized_keys文件<br><img src="/assets/blogImg/642.png" alt="3"></p><p>hadoop002 hadoop003传输id_rsa.pub文件到hadoop001<br><img src="/assets/blogImg/643.png" alt="4"><br><img src="/assets/blogImg/644.png" alt="5"></p><p>hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys<br><img src="/assets/blogImg/645.png" alt="6"></p><p>设置每台机器的权限<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 -R ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>将authorized_keys分发到hadoop002、hadoop003机器<br><img src="/assets/blogImg/646.png" alt="7"></p><p><img src="/assets/blogImg/647.png" alt="8"></p><p>验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ssh root@hadoop002 date</span><br><span class="line">[root@hadoop002 ~]# ssh root@hadoop001 date</span><br><span class="line">[root@hadoop003 ~]# ssh root@hadoop001 date</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
