<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>è‹¥æ³½æ•°æ®-CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ)</title>
      <link href="/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/"/>
      <url>/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><h2 id="è‹¥æ³½æ•°æ®"><a href="#è‹¥æ³½æ•°æ®" class="headerlink" title="è‹¥æ³½æ•°æ®"></a><a href="www.ruozedata.com">è‹¥æ³½æ•°æ®</a></h2><h2 id="CDH5-16-1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²-å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ"><a href="#CDH5-16-1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²-å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ" class="headerlink" title="CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ)"></a>CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ)</h2><p>è§†é¢‘:<a href="https://www.bilibili.com/video/av52167219" target="_blank" rel="noopener">https://www.bilibili.com/video/av52167219</a><br>PS:å»ºè®®å…ˆçœ‹è¯¾ç¨‹è§†é¢‘1-2ç¯‡ï¼Œå†æ ¹æ®è§†é¢‘æˆ–æ–‡æ¡£éƒ¨ç½²ï¼Œ<br>å¦‚æœ‰é—®é¢˜ï¼ŒåŠæ—¶ä¸@è‹¥æ³½æ•°æ®Jå“¥è”ç³»ã€‚</p><a id="more"></a><hr><h2 id="ä¸€-å‡†å¤‡å·¥ä½œ"><a href="#ä¸€-å‡†å¤‡å·¥ä½œ" class="headerlink" title="ä¸€.å‡†å¤‡å·¥ä½œ"></a>ä¸€.å‡†å¤‡å·¥ä½œ</h2><h4 id="1-ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—"><a href="#1-ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—" class="headerlink" title="1.ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—:"></a>1.ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—:</h4><p>a.MySQLç¦»çº¿éƒ¨ç½²<br>b.CMç¦»çº¿éƒ¨ç½²<br>c.Parcelæ–‡ä»¶ç¦»çº¿æºéƒ¨ç½²</p><h4 id="2-è§„åˆ’"><a href="#2-è§„åˆ’" class="headerlink" title="2.è§„åˆ’:"></a>2.è§„åˆ’:</h4><table><thead><tr><th>èŠ‚ç‚¹</th><th>MySQLéƒ¨ç½²ç»„ä»¶</th><th>Parcelæ–‡ä»¶ç¦»çº¿æº</th><th>CMæœåŠ¡è¿›ç¨‹</th><th>å¤§æ•°æ®ç»„ä»¶</th></tr></thead><tbody><tr><td>hadoop001</td><td>MySQL</td><td>Parcel</td><td>Activity Monitor<br></td><td>NN RM DN NM</td></tr><tr><td>hadoop002</td><td></td><td></td><td>Alert Publisher<br>Event Server</td><td>DN NM</td></tr><tr><td>hadoop003</td><td></td><td></td><td>Host Monitor<br>Service Monitor</td><td>DN NM</td></tr></tbody></table><h3 id="3-ä¸‹è½½æº"><a href="#3-ä¸‹è½½æº" class="headerlink" title="3.ä¸‹è½½æº:"></a>3.ä¸‹è½½æº:</h3><ul><li>CM<br><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz" target="_blank" rel="noopener">cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</a></li><li>Parcel<br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json" target="_blank" rel="noopener">manifest.json</a></li><li><p>JDK<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a><br>ä¸‹è½½jdk-8u202-linux-x64.tar.gz</p></li><li><p>MySQL<br><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a><br>ä¸‹è½½mysql-5.7.26-el7-x86_64.tar.gz</p></li><li><p>MySQL jdbc jar<br><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.47.jar</a><br>ä¸‹è½½å®Œæˆåè¦é‡å‘½åå»æ‰ç‰ˆæœ¬å·ï¼Œ<br>mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar</p></li></ul><hr><p>###å‡†å¤‡å¥½ç™¾åº¦äº‘,ä¸‹è½½å®‰è£…åŒ…:<br>é“¾æ¥:<a href="https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA" target="_blank" rel="noopener">https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA</a> å¯†ç :viqp</p><h2 id="äºŒ-é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–"><a href="#äºŒ-é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–" class="headerlink" title="äºŒ.é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–"></a>äºŒ.é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–</h2><h3 id="1-é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹Ÿæœº"><a href="#1-é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹Ÿæœº" class="headerlink" title="1.é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹Ÿæœº"></a>1.é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹Ÿæœº</h3><p>CentOS7.2æ“ä½œç³»ç»Ÿï¼Œ2æ ¸8Gæœ€ä½é…ç½®</p><h3 id="2-å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶"><a href="#2-å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶" class="headerlink" title="2.å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶"></a>2.å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶</h3><ul><li>MAC: /etc/hosts</li><li>Window: C:\windows\system32\drivers\etc\hosts</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">å…¬ç½‘åœ°å€: </span><br><span class="line">106.15.234.222 hadoop001  </span><br><span class="line">106.15.235.200 hadoop002  </span><br><span class="line">106.15.234.239 hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶"><a href="#3-è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶" class="headerlink" title="3.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶"></a>3.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ç§æœ‰åœ°é“ã€å†…ç½‘åœ°å€:</span><br><span class="line">echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="4-å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™"><a href="#4-å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™" class="headerlink" title="4.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™"></a>4.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure><h3 id="5-å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux"><a href="#5-å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux" class="headerlink" title="5.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux"></a>5.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">å°†SELINUX=enforcingæ”¹ä¸ºSELINUX=disabled </span><br><span class="line">è®¾ç½®åéœ€è¦é‡å¯æ‰èƒ½ç”Ÿæ•ˆ</span><br></pre></td></tr></table></figure><h3 id="6-è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥"><a href="#6-è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥" class="headerlink" title="6.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥"></a>6.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">6.1.æ—¶åŒº</span><br><span class="line">[root@hadoop001 ~]# date</span><br><span class="line">Sat May 11 10:07:53 CST 2019</span><br><span class="line">[root@hadoop001 ~]# timedatectl</span><br><span class="line">      Local time: Sat 2019-05-11 10:10:31 CST</span><br><span class="line">  Universal time: Sat 2019-05-11 02:10:31 UTC</span><br><span class="line">        RTC time: Sat 2019-05-11 10:10:29</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br><span class="line">     NTP enabled: yes</span><br><span class="line">NTP synchronized: yes</span><br><span class="line"> RTC in local TZ: yes</span><br><span class="line">      DST active: n/a</span><br><span class="line"></span><br><span class="line">#æŸ¥çœ‹å‘½ä»¤å¸®åŠ©ï¼Œå­¦ä¹ è‡³å…³é‡è¦ï¼Œæ— éœ€ç™¾åº¦ï¼Œå¤ªğŸ‘</span><br><span class="line">[root@hadoop001 ~]# timedatectl --help</span><br><span class="line">timedatectl [OPTIONS...] COMMAND ...</span><br><span class="line"></span><br><span class="line">Query or change system time and date settings.</span><br><span class="line"></span><br><span class="line">  -h --help                Show this help message</span><br><span class="line">     --version             Show package version</span><br><span class="line">     --no-pager            Do not pipe output into a pager</span><br><span class="line">     --no-ask-password     Do not prompt for password</span><br><span class="line">  -H --host=[USER@]HOST    Operate on remote host</span><br><span class="line">  -M --machine=CONTAINER   Operate on local container</span><br><span class="line">     --adjust-system-clock Adjust system clock when changing local RTC mode</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  status                   Show current time settings</span><br><span class="line">  set-time TIME            Set system time</span><br><span class="line">  set-timezone ZONE        Set system time zone</span><br><span class="line">  list-timezones           Show known time zones</span><br><span class="line">  set-local-rtc BOOL       Control whether RTC is in local time</span><br><span class="line">  set-ntp BOOL             Control whether NTP is enabled</span><br><span class="line"></span><br><span class="line">#æŸ¥çœ‹å“ªäº›æ—¶åŒº</span><br><span class="line">[root@hadoop001 ~]# timedatectl list-timezones</span><br><span class="line">Africa/Abidjan</span><br><span class="line">Africa/Accra</span><br><span class="line">Africa/Addis_Ababa</span><br><span class="line">Africa/Algiers</span><br><span class="line">Africa/Asmara</span><br><span class="line">Africa/Bamako</span><br><span class="line"></span><br><span class="line">#æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®äºšæ´²ä¸Šæµ·æ—¶åŒº </span><br><span class="line">[root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">6.2.æ—¶é—´</span><br><span class="line">#æ‰€æœ‰èŠ‚ç‚¹å®‰è£…ntp</span><br><span class="line">[root@hadoop001 ~]# yum install -y ntp</span><br><span class="line"></span><br><span class="line">#é€‰å–hadoop001ä¸ºntpçš„ä¸»èŠ‚ç‚¹</span><br><span class="line">[root@hadoop001 ~]# vi /etc/ntp.conf </span><br><span class="line"></span><br><span class="line">#time</span><br><span class="line">server 0.asia.pool.ntp.org</span><br><span class="line">server 1.asia.pool.ntp.org</span><br><span class="line">server 2.asia.pool.ntp.org</span><br><span class="line">server 3.asia.pool.ntp.org</span><br><span class="line">#å½“å¤–éƒ¨æ—¶é—´ä¸å¯ç”¨æ—¶ï¼Œå¯ä½¿ç”¨æœ¬åœ°ç¡¬ä»¶æ—¶é—´</span><br><span class="line">server 127.127.1.0 iburst local clock </span><br><span class="line">#å…è®¸å“ªäº›ç½‘æ®µçš„æœºå™¨æ¥åŒæ­¥æ—¶é—´</span><br><span class="line">restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#å¼€å¯ntpdåŠæŸ¥çœ‹çŠ¶æ€</span><br><span class="line">[root@hadoop001 ~]# systemctl start ntpd</span><br><span class="line">[root@hadoop001 ~]# systemctl status ntpd</span><br><span class="line"> ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago</span><br><span class="line"> Main PID: 18518 (ntpd)</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           â””â”€18518 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line"></span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usec</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabled</span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.</span><br><span class="line"></span><br><span class="line">#éªŒè¯</span><br><span class="line">[root@hadoop001 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> LOCAL(0)        .LOCL.          10 l  726   64    0    0.000    0.000   0.000</span><br><span class="line"></span><br><span class="line">#å…¶ä»–ä»èŠ‚ç‚¹åœæ­¢ç¦ç”¨ntpdæœåŠ¡ </span><br><span class="line">[root@hadoop002 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop002 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´</span><br><span class="line">[root@hadoop002 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001  </span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop004 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´</span><br><span class="line">[root@hadoop003 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001</span><br></pre></td></tr></table></figure><h3 id="7-éƒ¨ç½²é›†ç¾¤çš„JDK"><a href="#7-éƒ¨ç½²é›†ç¾¤çš„JDK" class="headerlink" title="7.éƒ¨ç½²é›†ç¾¤çš„JDK"></a>7.éƒ¨ç½²é›†ç¾¤çš„JDK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/</span><br><span class="line">#åˆ‡è®°å¿…é¡»ä¿®æ­£æ‰€å±ç”¨æˆ·åŠç”¨æˆ·ç»„</span><br><span class="line">chown -R root:root /usr/java/jdk1.8.0_45</span><br><span class="line"></span><br><span class="line">echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure><h3 id="8-hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5-7-å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£"><a href="#8-hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5-7-å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£" class="headerlink" title="8.hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5.7(å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£)"></a>8.hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5.7(å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£)</h3><ul><li>æ–‡æ¡£é“¾æ¥:<a href="https://github.com/Hackeruncle/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL</a></li><li>è§†é¢‘é“¾æ¥:<a href="https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ</a> å¯†ç :whic</li></ul><h3 id="9-åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·"><a href="#9-åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·" class="headerlink" title="9.åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·"></a>9.åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create database cmf DEFAULT CHARACTER SET utf8;</span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="10-hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql-jdbc-jar"><a href="#10-hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql-jdbc-jar" class="headerlink" title="10.hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql jdbc jar"></a>10.hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql jdbc jar</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java.jar /usr/share/java/</span><br></pre></td></tr></table></figure><h2 id="ä¸‰-CDHéƒ¨ç½²"><a href="#ä¸‰-CDHéƒ¨ç½²" class="headerlink" title="ä¸‰.CDHéƒ¨ç½²"></a>ä¸‰.CDHéƒ¨ç½²</h2><h3 id="1-ç¦»çº¿éƒ¨ç½²cm-serveråŠagent"><a href="#1-ç¦»çº¿éƒ¨ç½²cm-serveråŠagent" class="headerlink" title="1.ç¦»çº¿éƒ¨ç½²cm serveråŠagent"></a>1.ç¦»çº¿éƒ¨ç½²cm serveråŠagent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1.1.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç›®å½•åŠè§£å‹</span><br><span class="line">mkdir /opt/cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/</span><br><span class="line"></span><br><span class="line">1.2.æ‰€æœ‰èŠ‚ç‚¹ä¿®æ”¹agentçš„é…ç½®ï¼ŒæŒ‡å‘serverçš„èŠ‚ç‚¹hadoop001</span><br><span class="line">sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">1.3.ä¸»èŠ‚ç‚¹ä¿®æ”¹serverçš„é…ç½®:</span><br><span class="line">vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties </span><br><span class="line">com.cloudera.cmf.db.type=mysql</span><br><span class="line">com.cloudera.cmf.db.host=hadoop001</span><br><span class="line">com.cloudera.cmf.db.name=cmf</span><br><span class="line">com.cloudera.cmf.db.user=cmf</span><br><span class="line">com.cloudera.cmf.db.password=Ruozedata123456!</span><br><span class="line">com.cloudera.cmf.db.setupType=EXTERNAL</span><br><span class="line"></span><br><span class="line">1.4.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç”¨æˆ·</span><br><span class="line">useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line"></span><br><span class="line">1.5.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„</span><br><span class="line">chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager</span><br></pre></td></tr></table></figure><h3 id="2-hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº"><a href="#2-hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº" class="headerlink" title="2.hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº"></a>2.hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2.1.éƒ¨ç½²ç¦»çº¿parcelæº</span><br><span class="line">$ mkdir -p /opt/cloudera/parcel-repo</span><br><span class="line">$ ll</span><br><span class="line">total 3081664</span><br><span class="line">-rw-r--r-- 1 root root 2127506677 May  9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</span><br><span class="line">-rw-r--r-- 1 root root         41 May  9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</span><br><span class="line">-rw-r--r-- 1 root root  841524318 May  9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  185515842 Aug 10  2017 jdk-8u144-linux-x64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root      66538 May  9 18:03 manifest.json</span><br><span class="line">-rw-r--r-- 1 root root     989495 May 25  2017 mysql-connector-java.jar</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">#åˆ‡è®°cpæ—¶ï¼Œé‡å‘½åå»æ‰1ï¼Œä¸ç„¶åœ¨éƒ¨ç½²è¿‡ç¨‹CMè®¤ä¸ºå¦‚ä¸Šæ–‡ä»¶ä¸‹è½½æœªå®Œæ•´ï¼Œä¼šæŒç»­ä¸‹è½½</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha</span><br><span class="line">$ cp manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">2.2.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„</span><br><span class="line">$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/</span><br></pre></td></tr></table></figure><h3 id="3-æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™"><a href="#3-æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™" class="headerlink" title="3.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™"></a>3.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™</h3><p>mkdir -p /opt/cloudera/parcels<br>chown -R cloudera-scm:cloudera-scm /opt/cloudera/</p><h3 id="4-hadoop001èŠ‚ç‚¹å¯åŠ¨Server"><a href="#4-hadoop001èŠ‚ç‚¹å¯åŠ¨Server" class="headerlink" title="4.hadoop001èŠ‚ç‚¹å¯åŠ¨Server"></a>4.hadoop001èŠ‚ç‚¹å¯åŠ¨Server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.1.å¯åŠ¨server</span><br><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start</span><br><span class="line"></span><br><span class="line">4.2.é˜¿é‡Œäº‘webç•Œé¢ï¼Œè®¾ç½®è¯¥hadoop001èŠ‚ç‚¹é˜²ç«å¢™æ”¾å¼€7180ç«¯å£</span><br><span class="line">4.3.ç­‰å¾…1minï¼Œæ‰“å¼€ http://hadoop001:7180 è´¦å·å¯†ç :admin/admin</span><br><span class="line">4.4.å‡å¦‚æ‰“ä¸å¼€ï¼Œå»çœ‹serverçš„logï¼Œæ ¹æ®é”™è¯¯ä»”ç»†æ’æŸ¥é”™è¯¯</span><br></pre></td></tr></table></figure><h3 id="5-æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent"><a href="#5-æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent" class="headerlink" title="5.æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent"></a>5.æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h3 id="6-æ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œ"><a href="#6-æ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œ" class="headerlink" title="6.æ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œ"></a>6.æ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œ</h3><p><a href="http://hadoop001:7180/" target="_blank" rel="noopener">http://hadoop001:7180/</a><br>è´¦å·å¯†ç :admin/admin</p><h3 id="7-æ¬¢è¿ä½¿ç”¨Cloudera-Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸æ¡ä»¶ã€‚å‹¾é€‰"><a href="#7-æ¬¢è¿ä½¿ç”¨Cloudera-Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸æ¡ä»¶ã€‚å‹¾é€‰" class="headerlink" title="7.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸æ¡ä»¶ã€‚å‹¾é€‰"></a>7.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸æ¡ä»¶ã€‚å‹¾é€‰</h3><p><img src="install pictures/1.png" alt="avatar"></p><h3 id="8-æ¬¢è¿ä½¿ç”¨Cloudera-Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera-Expresså…è´¹ç‰ˆæœ¬"><a href="#8-æ¬¢è¿ä½¿ç”¨Cloudera-Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera-Expresså…è´¹ç‰ˆæœ¬" class="headerlink" title="8.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera Expresså…è´¹ç‰ˆæœ¬"></a>8.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera Expresså…è´¹ç‰ˆæœ¬</h3><p><img src="install pictures/2.png" alt="avatar"></p><h3 id="9-æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera-Managerå’ŒCDH"><a href="#9-æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera-Managerå’ŒCDH" class="headerlink" title="9.æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera Managerå’ŒCDH"></a>9.æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera Managerå’ŒCDH</h3><p><img src="install pictures/3.png" alt="avatar"></p><h3 id="10-ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©-å½“å‰ç®¡ç†çš„ä¸»æœº-ï¼Œå…¨éƒ¨å‹¾é€‰"><a href="#10-ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©-å½“å‰ç®¡ç†çš„ä¸»æœº-ï¼Œå…¨éƒ¨å‹¾é€‰" class="headerlink" title="10.ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©[å½“å‰ç®¡ç†çš„ä¸»æœº]ï¼Œå…¨éƒ¨å‹¾é€‰"></a>10.ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©[å½“å‰ç®¡ç†çš„ä¸»æœº]ï¼Œå…¨éƒ¨å‹¾é€‰</h3><p><img src="install pictures/4.png" alt="avatar"></p><h3 id="11-é€‰æ‹©å­˜å‚¨åº“"><a href="#11-é€‰æ‹©å­˜å‚¨åº“" class="headerlink" title="11.é€‰æ‹©å­˜å‚¨åº“"></a>11.é€‰æ‹©å­˜å‚¨åº“</h3><p><img src="install pictures/5.png" alt="avatar"></p><h3 id="12-é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚"><a href="#12-é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚" class="headerlink" title="12.é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚"></a>12.é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚</h3><p>æœ¬åœ°parcelç¦»çº¿æºé…ç½®æ­£ç¡®ï¼Œåˆ™â€ä¸‹è½½â€é˜¶æ®µç¬é—´å®Œæˆï¼Œå…¶ä½™é˜¶æ®µè§†èŠ‚ç‚¹æ•°ä¸å†…éƒ¨ç½‘ç»œæƒ…å†µå†³å®šã€‚<br><img src="install pictures/6.png" alt="avatar"></p><h3 id="13-æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§"><a href="#13-æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§" class="headerlink" title="13.æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§"></a>13.æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§</h3><p><img src="install pictures/7.png" alt="avatar"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">13.1.å»ºè®®å°†/proc/sys/vm/swappinessè®¾ç½®ä¸ºæœ€å¤§å€¼10ã€‚</span><br><span class="line">swappinesså€¼æ§åˆ¶æ“ä½œç³»ç»Ÿå°è¯•äº¤æ¢å†…å­˜çš„ç§¯æï¼›</span><br><span class="line">swappiness=0ï¼šè¡¨ç¤ºæœ€å¤§é™åº¦ä½¿ç”¨ç‰©ç†å†…å­˜ï¼Œä¹‹åæ‰æ˜¯swapç©ºé—´ï¼›</span><br><span class="line">swappiness=100ï¼šè¡¨ç¤ºç§¯æä½¿ç”¨swapåˆ†åŒºï¼Œå¹¶ä¸”æŠŠå†…å­˜ä¸Šçš„æ•°æ®åŠæ—¶æ¬è¿åˆ°swapç©ºé—´ï¼›</span><br><span class="line">å¦‚æœæ˜¯æ··åˆæœåŠ¡å™¨ï¼Œä¸å»ºè®®å®Œå…¨ç¦ç”¨swapï¼Œå¯ä»¥å°è¯•é™ä½swappinessã€‚</span><br><span class="line"></span><br><span class="line">ä¸´æ—¶è°ƒæ•´ï¼š</span><br><span class="line">sysctl vm.swappiness=10</span><br><span class="line"></span><br><span class="line">æ°¸ä¹…è°ƒæ•´ï¼š</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># Adjust swappiness value</span><br><span class="line">vm.swappiness=10</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">13.2.å·²å¯ç”¨é€æ˜å¤§é¡µé¢å‹ç¼©ï¼Œå¯èƒ½ä¼šå¯¼è‡´é‡å¤§æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ç¦ç”¨æ­¤è®¾ç½®ã€‚</span><br><span class="line">ä¸´æ—¶è°ƒæ•´ï¼š</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"></span><br><span class="line">æ°¸ä¹…è°ƒæ•´ï¼š</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local</span><br><span class="line"># Disable transparent_hugepage</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># centos7.xç³»ç»Ÿï¼Œéœ€è¦ä¸º&quot;/etc/rc.d/rc.local&quot;æ–‡ä»¶èµ‹äºˆæ‰§è¡Œæƒé™</span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="14-è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡"><a href="#14-è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡" class="headerlink" title="14.è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡"></a>14.è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡</h3><p><img src="install pictures/8.png" alt="avatar"></p><h3 id="15-è‡ªå®šä¹‰è§’è‰²åˆ†é…"><a href="#15-è‡ªå®šä¹‰è§’è‰²åˆ†é…" class="headerlink" title="15.è‡ªå®šä¹‰è§’è‰²åˆ†é…"></a>15.è‡ªå®šä¹‰è§’è‰²åˆ†é…</h3><p><img src="install pictures/9.png" alt="avatar"></p><h3 id="16-æ•°æ®åº“è®¾ç½®"><a href="#16-æ•°æ®åº“è®¾ç½®" class="headerlink" title="16.æ•°æ®åº“è®¾ç½®"></a>16.æ•°æ®åº“è®¾ç½®</h3><p><img src="install pictures/10.png" alt="avatar"></p><h3 id="17-å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯"><a href="#17-å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯" class="headerlink" title="17.å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯"></a>17.å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯</h3><p><img src="install pictures/11.png" alt="avatar"></p><h3 id="18-é¦–æ¬¡è¿è¡Œ"><a href="#18-é¦–æ¬¡è¿è¡Œ" class="headerlink" title="18.é¦–æ¬¡è¿è¡Œ"></a>18.é¦–æ¬¡è¿è¡Œ</h3><p><img src="install pictures/12.png" alt="avatar"></p><h3 id="19-æ­å–œæ‚¨"><a href="#19-æ­å–œæ‚¨" class="headerlink" title="19.æ­å–œæ‚¨!"></a>19.æ­å–œæ‚¨!</h3><p><img src="install pictures/13.png" alt="avatar"></p><h3 id="20-ä¸»é¡µ"><a href="#20-ä¸»é¡µ" class="headerlink" title="20.ä¸»é¡µ"></a>20.ä¸»é¡µ</h3><p><img src="install pictures/14.png" alt="avatar"></p><hr><h3 id="CDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡-ruoze-star"><a href="#CDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡-ruoze-star" class="headerlink" title="CDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡(ruoze_star)"></a>CDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡(ruoze_star)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">0.é’äº‘ç¯å¢ƒä»‹ç»å’Œä½¿ç”¨ </span><br><span class="line">1.Preparation        </span><br><span class="line">è°ˆè°ˆæ€æ ·å…¥é—¨å¤§æ•°æ® </span><br><span class="line">è°ˆè°ˆæ€æ ·åšå¥½ä¸€ä¸ªå¤§æ•°æ®å¹³å°çš„è¿è¥å·¥ä½œ </span><br><span class="line">Linuxæœºå™¨,å„è½¯ä»¶ç‰ˆæœ¬ä»‹ç»åŠå®‰è£…(å½•æ’­) </span><br><span class="line">2.Introduction      </span><br><span class="line">Clouderaã€CMåŠCDHä»‹ç» </span><br><span class="line">CDHç‰ˆæœ¬é€‰æ‹© </span><br><span class="line">CDHå®‰è£…å‡ ç§æ–¹å¼è§£è¯» </span><br><span class="line">3.Install&amp;UnInstall  </span><br><span class="line">é›†ç¾¤èŠ‚ç‚¹è§„åˆ’,ç¯å¢ƒå‡†å¤‡(NTP,Jdk and etc) </span><br><span class="line">MySQLç¼–è¯‘å®‰è£…åŠå¸¸ç”¨å‘½ä»¤ </span><br><span class="line">æ¨è:CDHç¦»çº¿å®‰è£…(è¸©å‘å¿ƒå¾—,å…¨é¢å‰–æ) </span><br><span class="line">è§£è¯»æš´åŠ›å¸è½½è„šæœ¬ </span><br><span class="line"></span><br><span class="line">4.CDH Management      </span><br><span class="line">CDHä½“ç³»æ¶æ„å‰–æ </span><br><span class="line">CDHé…ç½®æ–‡ä»¶æ·±åº¦è§£æ </span><br><span class="line">CMçš„å¸¸ç”¨å‘½ä»¤ </span><br><span class="line">CDHé›†ç¾¤æ­£ç¡®å¯åŠ¨å’Œåœæ­¢é¡ºåº </span><br><span class="line">CDH Tsquery Language </span><br><span class="line">CDHå¸¸è§„ç®¡ç†(ç›‘æ§/é¢„è­¦/é…ç½®/èµ„æº/æ—¥å¿—/å®‰å…¨) </span><br><span class="line"></span><br><span class="line">5.Maintenance Experiment  </span><br><span class="line">HDFS HA é…ç½® åŠhadoop/hdfså¸¸è§„å‘½ä»¤ </span><br><span class="line">Yarn HA é…ç½® åŠyarnå¸¸è§„å‘½ä»¤ </span><br><span class="line">Other CDH Components HA é…ç½® </span><br><span class="line">CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœåŠ¡(hive/spark/hbase) </span><br><span class="line">CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœºå™¨ </span><br><span class="line">CDHåŠ¨æ€æ·»åŠ åˆ é™¤åŠè¿ç§»DataNodeè¿›ç¨‹ç­‰ </span><br><span class="line">CDHå‡çº§(5.10.0--&gt;5.12.0) </span><br><span class="line"></span><br><span class="line">6.Resource Management    </span><br><span class="line">Linux Cgroups </span><br><span class="line">é™æ€èµ„æºæ±  </span><br><span class="line">åŠ¨æ€èµ„æºæ±  </span><br><span class="line">å¤šç§Ÿæˆ·æ¡ˆä¾‹ </span><br><span class="line"></span><br><span class="line">7.Performance Tunning    </span><br><span class="line">Memory/CPU/Network/DiskåŠé›†ç¾¤è§„åˆ’ </span><br><span class="line">Linuxå‚æ•° </span><br><span class="line">HDFSå‚æ•° </span><br><span class="line">MapReduceåŠYarnå‚æ•° </span><br><span class="line">å…¶ä»–æœåŠ¡å‚æ•° </span><br><span class="line"></span><br><span class="line">8.Cases Share </span><br><span class="line">CDH4&amp;5ä¹‹Alternativeså‘½ä»¤ çš„ç ”ç©¶ </span><br><span class="line">CDH5.8.2å®‰è£…ä¹‹Hash verification failed </span><br><span class="line">è®°å½•ä¸€æ¬¡CDH4.8.6 é…ç½®HDFS HA å‘ </span><br><span class="line">CDH5.0é›†ç¾¤IPæ›´æ”¹ </span><br><span class="line">CDHçš„active namenode exit(GC)å’Œå½©è›‹åˆ†äº« </span><br><span class="line"></span><br><span class="line">9. Kerberos</span><br><span class="line">Kerberosç®€ä»‹</span><br><span class="line">Kerberosä½“ç³»ç»“æ„</span><br><span class="line">Kerberoså·¥ä½œæœºåˆ¶</span><br><span class="line">Kerberoså®‰è£…éƒ¨ç½²</span><br><span class="line">CDHå¯ç”¨kerberos</span><br><span class="line">Kerberoså¼€å‘ä½¿ç”¨(çœŸå®ä»£ç )</span><br><span class="line"></span><br><span class="line">10.Summary         </span><br><span class="line">æ€»ç»“</span><br></pre></td></tr></table></figure><hr><h4 id="Join-us-if-you-have-a-dream"><a href="#Join-us-if-you-have-a-dream" class="headerlink" title="Join us if you have a dream."></a>Join us if you have a dream.</h4><h5 id="è‹¥æ³½æ•°æ®å®˜ç½‘-http-ruozedata-com"><a href="#è‹¥æ³½æ•°æ®å®˜ç½‘-http-ruozedata-com" class="headerlink" title="è‹¥æ³½æ•°æ®å®˜ç½‘: http://ruozedata.com"></a>è‹¥æ³½æ•°æ®å®˜ç½‘: <a href="http://ruozedata.com" target="_blank" rel="noopener">http://ruozedata.com</a></h5><h5 id="è…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®-http-ruoze-ke-qq-com"><a href="#è…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®-http-ruoze-ke-qq-com" class="headerlink" title="è…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®: http://ruoze.ke.qq.com"></a>è…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®: <a href="http://ruoze.ke.qq.com" target="_blank" rel="noopener">http://ruoze.ke.qq.com</a></h5><h5 id="Bilibiliç½‘ç«™-æœè‹¥æ³½æ•°æ®-https-space-bilibili-com-356836323"><a href="#Bilibiliç½‘ç«™-æœè‹¥æ³½æ•°æ®-https-space-bilibili-com-356836323" class="headerlink" title="Bilibiliç½‘ç«™,æœè‹¥æ³½æ•°æ®: https://space.bilibili.com/356836323"></a>Bilibiliç½‘ç«™,æœè‹¥æ³½æ•°æ®: <a href="https://space.bilibili.com/356836323" target="_blank" rel="noopener">https://space.bilibili.com/356836323</a></h5><h5 id="è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢"><a href="#è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢" class="headerlink" title="è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢"></a><a href="https://ruozedata.github.io" target="_blank" rel="noopener">è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢</a></h5><h5 id="è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆ"><a href="#è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆ" class="headerlink" title="è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆ"></a><a href="https://github.com/ruozedata/BigData/blob/master/blog/BigDataBlogOverview.md" target="_blank" rel="noopener">è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆ</a></h5><h5 id="è‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜"><a href="#è‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜" class="headerlink" title="è‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜"></a><a href="https://github.com/ruozedata/BigData/blob/master/interview/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.md" target="_blank" rel="noopener">è‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜</a></h5><h5 id="æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦"><a href="#æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦" class="headerlink" title="æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦:"></a>æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦:</h5><p><img src="install pictures/è‹¥æ³½æ•°æ®--æ‰«æå…¥å£.png" alt="avatar"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cdh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è‹¥æ³½æ•°æ®è¯¾ç¨‹ä¸€è§ˆ</title>
      <link href="/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/"/>
      <url>/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h1 id="è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—"><a href="#è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—" class="headerlink" title="è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—"></a>è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—</h1><h2 id="åŸºç¡€ç­"><a href="#åŸºç¡€ç­" class="headerlink" title="åŸºç¡€ç­"></a>åŸºç¡€ç­</h2><h3 id="Liunx"><a href="#Liunx" class="headerlink" title="Liunx"></a>Liunx</h3><ul><li>VMè™šæ‹Ÿæœºå®‰è£…</li><li>Liunxå¸¸ç”¨å‘½ä»¤ï¼ˆé‡ç‚¹ï¼‰</li><li>å¼€å‘ç¯å¢ƒæ­</li></ul><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ul><li>æºç å®‰è£…&amp;yumå®‰è£…</li><li>CRUDç¼–å†™</li><li>æƒé™æ§åˆ¶</li></ul><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>æ¶æ„ä»‹ç»&amp;&amp;æºç ç¼–è¯‘</li><li>ä¼ªåˆ†å¸ƒå¼å®‰è£…&amp;&amp;ä¼ä¸šåº”ç”¨</li><li><p>HDFSï¼ˆé‡ç‚¹ï¼‰</p><ul><li>æ¶æ„è®¾è®¡</li><li>å‰¯æœ¬æ”¾ç½®ç­–ç•¥</li><li>è¯»å†™æµç¨‹</li></ul></li><li><p>YARNï¼ˆé‡ç‚¹ï¼‰</p><ul><li>æ¶æ„è®¾è®¡</li><li>å·¥ä½œæµç¨‹</li><li>è°ƒåº¦ç®¡ç†&amp;&amp;å¸¸è§å‚æ•°é…ç½®ï¼ˆè°ƒä¼˜ï¼‰</li></ul></li><li><p>MapReduce</p><ul><li>æ¶æ„è®¾è®¡</li><li>wordcountåŸç†&amp;&amp;joinåŸç†å’Œæ¡ˆä¾‹<a id="more"></a><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3></li></ul></li><li><p>æ¶æ„è®¾è®¡</p></li><li>Hive DDL&amp;DML</li><li>joinåœ¨å¤§æ•°æ®ä¸­çš„ä½¿ç”¨</li><li>ä½¿ç”¨è‡ªå¸¦UDFå’Œå¼€å‘è‡ªå®šä¹‰UDF</li></ul><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><ul><li>æ¶æ„è®¾è®¡</li><li>RDBMSå¯¼å…¥å¯¼å‡º</li></ul><h3 id="æ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚"><a href="#æ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚" class="headerlink" title="æ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚"></a>æ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚</h3><h3 id="äººå·¥æ™ºèƒ½åŸºç¡€"><a href="#äººå·¥æ™ºèƒ½åŸºç¡€" class="headerlink" title="äººå·¥æ™ºèƒ½åŸºç¡€"></a>äººå·¥æ™ºèƒ½åŸºç¡€</h3><ul><li>pythonåŸºç¡€</li><li>å¸¸ç”¨åº“â€”â€”pandasã€numpyã€sklearnã€keras</li></ul><h2 id="é«˜çº§ç­"><a href="#é«˜çº§ç­" class="headerlink" title="é«˜çº§ç­"></a>é«˜çº§ç­</h2><h3 id="scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰"><a href="#scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰" class="headerlink" title="scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰"></a>scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰</h3><h3 id="Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰"><a href="#Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰" class="headerlink" title="Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰"></a>Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰</h3><h3 id="Hadoopé«˜çº§"><a href="#Hadoopé«˜çº§" class="headerlink" title="Hadoopé«˜çº§"></a>Hadoopé«˜çº§</h3><h3 id="Hiveé«˜çº§"><a href="#Hiveé«˜çº§" class="headerlink" title="Hiveé«˜çº§"></a>Hiveé«˜çº§</h3><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><h3 id="å®¹å™¨"><a href="#å®¹å™¨" class="headerlink" title="å®¹å™¨"></a>å®¹å™¨</h3><h3 id="è°ƒåº¦å¹³å°"><a href="#è°ƒåº¦å¹³å°" class="headerlink" title="è°ƒåº¦å¹³å°"></a>è°ƒåº¦å¹³å°</h3><h2 id="çº¿ä¸‹ç­"><a href="#çº¿ä¸‹ç­" class="headerlink" title="çº¿ä¸‹ç­"></a>çº¿ä¸‹ç­</h2><p><img src="/assets/blogImg/è‹¥æ³½æ•°æ®.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> è¯¾ç¨‹ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dockerå¸¸ç”¨å‘½ä»¤ä»¥åŠå®‰è£…mysql</title>
      <link href="/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/"/>
      <url>/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h3 id="1-ç®€ä»‹"><a href="#1-ç®€ä»‹" class="headerlink" title="1.ç®€ä»‹"></a>1.ç®€ä»‹</h3><p>Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„åº”ç”¨å®¹å™¨å¼•æ“ï¼›æ˜¯ä¸€ä¸ªè½»é‡çº§å®¹å™¨æŠ€æœ¯ï¼›</p><p>Dockeræ”¯æŒå°†è½¯ä»¶ç¼–è¯‘æˆä¸€ä¸ªé•œåƒï¼›ç„¶ååœ¨é•œåƒä¸­å„ç§è½¯ä»¶åšå¥½é…ç½®ï¼Œå°†é•œåƒå‘å¸ƒå‡ºå»ï¼Œå…¶ä»–ä½¿ç”¨è€…å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªé•œåƒï¼›</p><p>è¿è¡Œä¸­çš„è¿™ä¸ªé•œåƒç§°ä¸ºå®¹å™¨ï¼Œå®¹å™¨å¯åŠ¨æ˜¯éå¸¸å¿«é€Ÿçš„ã€‚<br><a id="more"></a></p><h3 id="2-æ ¸å¿ƒæ¦‚å¿µ"><a href="#2-æ ¸å¿ƒæ¦‚å¿µ" class="headerlink" title="2.æ ¸å¿ƒæ¦‚å¿µ"></a>2.æ ¸å¿ƒæ¦‚å¿µ</h3><p>dockerä¸»æœº(Host)ï¼šå®‰è£…äº†Dockerç¨‹åºçš„æœºå™¨ï¼ˆDockerç›´æ¥å®‰è£…åœ¨æ“ä½œç³»ç»Ÿä¹‹ä¸Šï¼‰ï¼›</p><p>dockerå®¢æˆ·ç«¯(Client)ï¼šè¿æ¥dockerä¸»æœºè¿›è¡Œæ“ä½œï¼›</p><p>dockerä»“åº“(Registry)ï¼šç”¨æ¥ä¿å­˜å„ç§æ‰“åŒ…å¥½çš„è½¯ä»¶é•œåƒï¼›</p><p>dockeré•œåƒ(Images)ï¼šè½¯ä»¶æ‰“åŒ…å¥½çš„é•œåƒï¼›æ”¾åœ¨dockerä»“åº“ä¸­ï¼›</p><p>dockerå®¹å™¨(Container)ï¼šé•œåƒå¯åŠ¨åçš„å®ä¾‹ç§°ä¸ºä¸€ä¸ªå®¹å™¨ï¼›å®¹å™¨æ˜¯ç‹¬ç«‹è¿è¡Œçš„ä¸€ä¸ªæˆ–ä¸€ç»„åº”ç”¨</p><h3 id="3-å®‰è£…ç¯å¢ƒ"><a href="#3-å®‰è£…ç¯å¢ƒ" class="headerlink" title="3.å®‰è£…ç¯å¢ƒ"></a>3.å®‰è£…ç¯å¢ƒ</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VM ware Workstation10</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">uname -r</span><br><span class="line">3.10.0-862.el7.x86_64</span><br></pre></td></tr></table></figure><p><strong>æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Š</strong> æŸ¥çœ‹å‘½ä»¤ï¼šuname -r</p><h3 id="4-åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…docker"><a href="#4-åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…docker" class="headerlink" title="4.åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…docker"></a>4.åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…docker</h3><p>æ­¥éª¤ï¼š</p><p>1ã€æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Š<br>uname -r</p><p>2ã€å®‰è£…docker<br>yum install docker</p><p>3ã€è¾“å…¥yç¡®è®¤å®‰è£…<br>Dependency Updated:<br>audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1</p><p>Complete!<br>(æˆåŠŸæ ‡å¿—)</p><p>4ã€å¯åŠ¨docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl start docker</span><br><span class="line">[root@hadoop000 ~]# docker -v</span><br><span class="line">Docker version 1.13.1, build 8633870/1.13.1</span><br></pre></td></tr></table></figure><p></p><p>5ã€å¼€æœºå¯åŠ¨docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><p></p><p>6ã€åœæ­¢docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl stop docker</span><br><span class="line">``` </span><br><span class="line">### 5.å¸¸ç”¨å‘½ä»¤</span><br><span class="line"></span><br><span class="line">é•œåƒæ“ä½œ</span><br><span class="line">|æ“ä½œ|å‘½ä»¤|è¯´æ˜|</span><br><span class="line">|---|---|---|</span><br><span class="line">æ£€ç´¢|docker search å…³é”®å­— egï¼šdocker search redis|æˆ‘ä»¬ç»å¸¸å»docker hubä¸Šæ£€ç´¢é•œåƒçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚é•œåƒçš„TAGã€‚|</span><br><span class="line">æ‹‰å–|docker pull é•œåƒå:tag|:tagæ˜¯å¯é€‰çš„ï¼Œtagè¡¨ç¤ºæ ‡ç­¾ï¼Œå¤šä¸ºè½¯ä»¶çš„ç‰ˆæœ¬ï¼Œé»˜è®¤æ˜¯latest</span><br><span class="line">åˆ—è¡¨|docker images|æŸ¥çœ‹æ‰€æœ‰æœ¬åœ°é•œåƒ</span><br><span class="line">åˆ é™¤|docker rmi image-id|åˆ é™¤æŒ‡å®šçš„æœ¬åœ°é•œåƒ</span><br><span class="line"></span><br><span class="line">å½“ç„¶å¤§å®¶ä¹Ÿå¯ä»¥åœ¨å®˜ç½‘æŸ¥æ‰¾ï¼šhttps://hub.docker.com/</span><br><span class="line"></span><br><span class="line">å®¹å™¨æ“ä½œ</span><br><span class="line">è½¯ä»¶é•œåƒï¼ˆQQå®‰è£…ç¨‹åºï¼‰----è¿è¡Œé•œåƒ----äº§ç”Ÿä¸€ä¸ªå®¹å™¨ï¼ˆæ­£åœ¨è¿è¡Œçš„è½¯ä»¶ï¼Œè¿è¡Œçš„QQï¼‰ï¼›</span><br><span class="line"></span><br><span class="line">æ­¥éª¤ï¼š</span><br><span class="line"></span><br><span class="line">- 1ã€æœç´¢é•œåƒ</span><br><span class="line">[root@localhost ~]# docker search tomcat</span><br><span class="line">- 2ã€æ‹‰å–é•œåƒ</span><br><span class="line">[root@localhost ~]# docker pull tomcat</span><br><span class="line">- 3ã€æ ¹æ®é•œåƒå¯åŠ¨å®¹å™¨</span><br><span class="line">docker run --name mytomcat -d tomcat:latest</span><br><span class="line">- 4ã€docker ps  </span><br><span class="line">æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨</span><br><span class="line">- 5ã€ åœæ­¢è¿è¡Œä¸­çš„å®¹å™¨</span><br><span class="line">docker stop  å®¹å™¨çš„id</span><br><span class="line">- 6ã€æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨</span><br><span class="line">docker ps -a</span><br><span class="line">- 7ã€å¯åŠ¨å®¹å™¨</span><br><span class="line">docker start å®¹å™¨id</span><br><span class="line">- 8ã€åˆ é™¤ä¸€ä¸ªå®¹å™¨</span><br><span class="line"> docker rm å®¹å™¨id</span><br><span class="line">- 9ã€å¯åŠ¨ä¸€ä¸ªåšäº†ç«¯å£æ˜ å°„çš„tomcat</span><br><span class="line">[root@localhost ~]# docker run -d -p 8888:8080 tomcat</span><br><span class="line">-dï¼šåå°è¿è¡Œ</span><br><span class="line">-p: å°†ä¸»æœºçš„ç«¯å£æ˜ å°„åˆ°å®¹å™¨çš„ä¸€ä¸ªç«¯å£    ä¸»æœºç«¯å£:å®¹å™¨å†…éƒ¨çš„ç«¯å£</span><br><span class="line"></span><br><span class="line">- 10ã€ä¸ºäº†æ¼”ç¤ºç®€å•å…³é—­äº†linuxçš„é˜²ç«å¢™</span><br><span class="line">service firewalld status ï¼›æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€</span><br><span class="line">service firewalld stopï¼šå…³é—­é˜²ç«å¢™</span><br><span class="line">systemctl disable firewalld.service #ç¦æ­¢firewallå¼€æœºå¯åŠ¨</span><br><span class="line">- 11ã€æŸ¥çœ‹å®¹å™¨çš„æ—¥å¿—</span><br><span class="line">docker logs container-name/container-id</span><br><span class="line"></span><br><span class="line">æ›´å¤šå‘½ä»¤å‚çœ‹</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/docker/</span><br><span class="line">å¯ä»¥å‚è€ƒé•œåƒæ–‡æ¡£</span><br><span class="line"></span><br><span class="line">### 6.ä½¿ç”¨dockerå®‰è£…mysql</span><br><span class="line"></span><br><span class="line">- docker pull mysql</span><br></pre></td></tr></table></figure><p></p><p>docker pull mysql<br>Using default tag: latest<br>Trying to pull repository docker.io/library/mysql â€¦<br>latest: Pulling from docker.io/library/mysql<br>a5a6f2f73cd8: Pull complete<br>936836019e67: Pull complete<br>283fa4c95fb4: Pull complete<br>1f212fb371f9: Pull complete<br>e2ae0d063e89: Pull complete<br>5ed0ae805b65: Pull complete<br>0283dc49ef4e: Pull complete<br>a7e1170b4fdb: Pull complete<br>88918a9e4742: Pull complete<br>241282fa67c2: Pull complete<br>b0fecf619210: Pull complete<br>bebf9f901dcc: Pull complete<br>Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8<br>Status: Downloaded newer image for docker.io/mysql:latest<br>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- å¯åŠ¨</span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br>[root@hadoop000 ~]# docker run â€“name mysql01 -d mysql<br>756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a<br>[root@hadoop000 ~]# docker ps -a<br>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES<br>756620c8e583 mysql â€œdocker-entrypointâ€¦â€ 9 seconds ago Exited (1) 7 seconds ago mysql01<br>[root@hadoop000 ~]# docker logs 756620c8e583<br>error: database is uninitialized and password option is not specified<br>You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¯ä»¥çœ‹åˆ°ä¸Šé¢å¯åŠ¨çš„æ–¹å¼æ˜¯é”™è¯¯çš„ï¼Œæç¤ºæˆ‘ä»¬è¦å¸¦ä¸Šå…·ä½“çš„å¯†ç </span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker run -p 3306:3306 â€“name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysql<br>eae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc06<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">â€“nameï¼šç»™æ–°åˆ›å»ºçš„å®¹å™¨å‘½åï¼Œæ­¤å¤„å‘½åä¸ºpwc-mysql</span><br><span class="line">-eï¼šé…ç½®ä¿¡æ¯ï¼Œæ­¤å¤„é…ç½®mysqlçš„rootç”¨æˆ·çš„ç™»é™†å¯†ç </span><br><span class="line">-pï¼šç«¯å£æ˜ å°„ï¼Œæ­¤å¤„æ˜ å°„ä¸»æœº3306ç«¯å£åˆ°å®¹å™¨pwc-mysqlçš„3306ç«¯å£</span><br><span class="line">-dï¼šæˆåŠŸå¯åŠ¨å®¹å™¨åè¾“å‡ºå®¹å™¨çš„å®Œæ•´IDï¼Œä¾‹å¦‚ä¸Šå›¾ 73f8811f669ee...</span><br></pre></td></tr></table></figure><p></p><ul><li><p>æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES</span><br><span class="line">eae86796e132        mysql               &quot;docker-entrypoint...&quot;   8 minutes ago       Up 8 minutes        0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql02</span><br></pre></td></tr></table></figure></li><li><p>ç™»é™†MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql04 /bin/bash</span><br><span class="line">root@e34aba02c0c3:/# mysql -uroot -p123456 </span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 80</span><br><span class="line">Server version: 8.0.13 MySQL Community Server - GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></li><li><p>å…¶ä»–çš„é«˜çº§æ“ä½œ</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br><span class="line">æŠŠä¸»æœºçš„/conf/mysqlæ–‡ä»¶å¤¹æŒ‚è½½åˆ° mysqldockerå®¹å™¨çš„/etc/mysql/conf.dæ–‡ä»¶å¤¹é‡Œé¢</span><br><span class="line">æ”¹mysqlçš„é…ç½®æ–‡ä»¶å°±åªéœ€è¦æŠŠmysqlé…ç½®æ–‡ä»¶æ”¾åœ¨è‡ªå®šä¹‰çš„æ–‡ä»¶å¤¹ä¸‹ï¼ˆ/conf/mysqlï¼‰</span><br><span class="line"></span><br><span class="line">docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br><span class="line">æŒ‡å®šmysqlçš„ä¸€äº›é…ç½®å‚æ•°</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark2.4.2è¯¦ç»†ä»‹ç»</title>
      <link href="/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Sparkå‘å¸ƒäº†æœ€æ–°çš„ç‰ˆæœ¬spark-2.4.2<br>æ ¹æ®å®˜ç½‘ä»‹ç»ï¼Œæ­¤ç‰ˆæœ¬å¯¹äºä½¿ç”¨spark2.4çš„ç”¨æˆ·æ¥è¯´å¸®åŠ©æ˜¯å·¨å¤§çš„</p><h4 id="ç‰ˆæœ¬ä»‹ç»"><a href="#ç‰ˆæœ¬ä»‹ç»" class="headerlink" title="ç‰ˆæœ¬ä»‹ç»"></a>ç‰ˆæœ¬ä»‹ç»</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2æ˜¯ä¸€ä¸ªåŒ…å«ç¨³å®šæ€§ä¿®å¤çš„ç»´æŠ¤ç‰ˆæœ¬ã€‚ æ­¤ç‰ˆæœ¬åŸºäºSpark2.4ç»´æŠ¤åˆ†æ”¯ã€‚<font color="#FF4500"> <strong>æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‰€æœ‰2.4ç”¨æˆ·å‡çº§åˆ°æ­¤ç¨³å®šç‰ˆæœ¬ã€‚</strong></font><br><a id="more"></a></p><h4 id="æ˜¾è‘—çš„å˜åŒ–"><a href="#æ˜¾è‘—çš„å˜åŒ–" class="headerlink" title="æ˜¾è‘—çš„å˜åŒ–"></a>æ˜¾è‘—çš„å˜åŒ–</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419ï¼šåœ¨spark2.4ä¸­å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äº1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥ã€‚ å› ä¸ºè¯¥å€¼å°†è½¬æ¢ä¸º0ï¼Œå¿ƒè·³å°†å§‹ç»ˆè¶…æ—¶ï¼Œå¹¶æœ€ç»ˆç»ˆæ­¢æ‰§è¡Œç¨‹åºã€‚</li><li>è¿˜åŸSPARK-25250ï¼šå¯èƒ½å¯¼è‡´ä½œä¸šæ°¸ä¹…æŒ‚èµ·ï¼Œåœ¨2.4.2ä¸­è¿˜åŸã€‚</li></ul><h4 id="è¯¦ç»†æ›´æ”¹"><a href="#è¯¦ç»†æ›´æ”¹" class="headerlink" title="è¯¦ç»†æ›´æ”¹"></a>è¯¦ç»†æ›´æ”¹</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>å†…å®¹æ‘˜è¦</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>åœ¨Spark Driverä¸­å‘ç°Javaæ­»é”</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>åœ¨Standaloneæ¨¡å¼ä¸‹æ‰§è¡Œâ€™ps -efâ€™ç¨‹åºè¿›ç¨‹,è¾“å‡ºspark.ssl.keyStorePasswordçš„æ˜æ–‡</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>å°†RoaringBitmapå‡çº§åˆ°0.7.45ä»¥ä¿®å¤Kryoä¸å®‰å…¨çš„ser / dseré—®é¢˜</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>ä½¿ç”¨é€‰é¡¹logConf = trueæ—¶å¯†ç å°†ä»¥confçš„æ˜æ–‡å½¢å¼è®°å½•</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>ç”¨Snappy 1.1.7.1è§£å‹ã€å‹ç¼©ç©ºåºåˆ—åŒ–æ•°æ®æ—¶å¤±è´¥</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferToä¸­çš„æ½œåœ¨æŸå</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointDataå› æ–‡ä»¶ç³»ç»Ÿå·²ç¼“å­˜è€Œæ— æ³•æ¸…ç†</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManagerå’ŒUnsafeExternalSorter $ SpillableIteratorä¹‹é—´çš„æ­»é”</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>åœ¨ä»…ä½¿ç”¨ç©ºå€¼åˆ—çš„AggregateEstimationä¹‹åçš„é”™è¯¯outputRowsä¼°è®¡</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>ä¿®å¤åŒ…åç§°ä¸åŒ¹é…</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>å½“æ²¡æœ‰ä»»åŠ¡å¼€å§‹æˆ–ç»“æŸæ—¶ï¼ŒUI çš„é™ˆæ—§æ€§å¯èƒ½æŒç»­æ•°åˆ†é’Ÿæˆ–æ•°å°æ—¶</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>ä¿®å¤updateTableStatsä»¥ä½¿ç”¨æ–°ç»Ÿè®¡ä¿¡æ¯æˆ–æ— æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>å½“ä¸¤å°æœºå™¨å…·æœ‰ä¸åŒçš„Oopså¤§å°æ—¶ï¼ŒUnsafeArrayDataåºåˆ—åŒ–ä¼šä¸­æ–­</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äº1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1é™é»˜åˆ é™¤DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="æ”¹è¿›"><a href="#æ”¹è¿›" class="headerlink" title="æ”¹è¿›"></a>æ”¹è¿›</h6><table><thead><tr><th>issues</th><th>å†…å®¹æ‘˜è¦</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>æ¾å¼€åœ¨ExpressionInfoçš„â€™examplesâ€™å­—æ®µä¸­æ¢è¡Œæ–­è¨€æ¡ä»¶</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>å°†jqueryæ›´æ–°ä¸º1.12.xä»¥è·å–å®‰å…¨ä¿®å¤ç¨‹åº</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>éšè—â€œorg.apache.spark.util.kvstoreâ€çš„APIæ–‡æ¡£</td></tr></tbody></table><h6 id="å·¥ä½œ"><a href="#å·¥ä½œ" class="headerlink" title="å·¥ä½œ"></a>å·¥ä½œ</h6><table><thead><tr><th>issues</th><th>å†…å®¹æ‘˜è¦</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>åœ¨HiveExternalCatalogVersionsSuiteä¸­æ›´æ–°Spark 2.4.xæµ‹è¯•</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æˆ‘å¸Kafka+Flink+MySQLç”Ÿäº§å®Œæ•´æ¡ˆä¾‹ä»£ç </title>
      <link href="/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
      <url>/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h6 id="1-ç‰ˆæœ¬ä¿¡æ¯ï¼š"><a href="#1-ç‰ˆæœ¬ä¿¡æ¯ï¼š" class="headerlink" title="1.ç‰ˆæœ¬ä¿¡æ¯ï¼š"></a>1.ç‰ˆæœ¬ä¿¡æ¯ï¼š</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flinkå®æˆ˜</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="3-å·¥ç¨‹pom-xml"><a href="#3-å·¥ç¨‹pom-xml" class="headerlink" title="3.å·¥ç¨‹pom.xml"></a>3.å·¥ç¨‹pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConfç±» å®šä¹‰ä¸MySQLè¿æ¥çš„JDBCçš„å‚æ•°<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author è‹¥æ³½æ•°æ®soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="5-MySQLSlinkç±»"><a href="#5-MySQLSlinkç±»" class="headerlink" title="5.MySQLSlinkç±»"></a>5.MySQLSlinkç±»</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author è‹¥æ³½æ•°æ®soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»"><a href="#6-æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»" class="headerlink" title="6.æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»"></a>6.æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //æµ‹è¯•ä¸€ä¸‹</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQLå»ºè¡¨"><a href="#7-MySQLå»ºè¡¨" class="headerlink" title="7.MySQLå»ºè¡¨"></a>7.MySQLå»ºè¡¨</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-ä¸»ç¨‹åºï¼š"><a href="#8-ä¸»ç¨‹åºï¼š" class="headerlink" title="8.ä¸»ç¨‹åºï¼š"></a>8.ä¸»ç¨‹åºï¼š</h6><p>ä¸»è¦æ˜¯å°†timeçš„æ ¼å¼è½¬æˆyyyyMMddHHmmss,</p><p>è¿˜æœ‰å–URLä¸­çš„è¯¾ç¨‹IDï¼Œå°†ä¸æ˜¯/classå¼€å¤´çš„è¿‡æ»¤æ‰ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafkaçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·åˆ†éš”</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeperçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·è¿›è¡Œåˆ†éš”</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flinkçš„æ¶ˆè´¹è€…çš„group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="9-å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢"><a href="#9-å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢" class="headerlink" title="9.å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢"></a>9.å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafkaè¿‡æ¥çš„æ¶ˆæ¯æ˜¯æˆ‘æ¨¡æ‹Ÿçš„ï¼Œä¸€åˆ†é’Ÿäº§ç”Ÿ100æ¡ã€‚</p><p>ä»¥ä¸Šæ˜¯æˆ‘å¸ç”Ÿäº§é¡¹ç›®ä»£ç çš„æŠ½å–å‡ºæ¥çš„æ¡ˆä¾‹ä»£ç V1ã€‚ç¨åè¿˜æœ‰WaterMarkä¹‹ç±»ä¼šåšåˆ†äº«ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkåœ¨æºç¨‹çš„å®è·µï¼ˆäºŒï¼‰</title>
      <link href="/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed May 22 2019 20:21:18 GMT+0800 (GMT+08:00) --><p>ä»¥ä¸‹å†…å®¹æ¥è‡ªç¬¬ä¸‰å±Šæºç¨‹å¤§æ•°æ®æ²™é¾™</p><h3 id="ä¸ƒã€é‡åˆ°çš„é—®é¢˜"><a href="#ä¸ƒã€é‡åˆ°çš„é—®é¢˜" class="headerlink" title="ä¸ƒã€é‡åˆ°çš„é—®é¢˜"></a>ä¸ƒã€é‡åˆ°çš„é—®é¢˜</h3><h5 id="orc-split"><a href="#orc-split" class="headerlink" title="orc split"></a>orc split</h5><p>Sparkè¯»å–Hiveè¡¨ç”¨çš„å„ä¸ªæ–‡ä»¶æ ¼å¼çš„InuptFormatï¼Œè®¡ç®—è¯»å–è¡¨éœ€è¦çš„taskæ•°é‡ä¾èµ–äºInputFormat#getSplits<br>ç”±äºå¤§éƒ¨åˆ†è¡¨çš„å­˜å‚¨æ ¼å¼ä¸»è¦ä½¿ç”¨çš„æ˜¯orcï¼Œå½“ä¸€ä¸ªorcæ–‡ä»¶è¶…è¿‡256MBï¼Œsplitç®—æ³•å¹¶è¡Œå»è¯»å–orcå…ƒæ•°æ®ï¼Œæœ‰æ—¶å€™Driverå†…å­˜é£™å‡ï¼ŒOOM crashï¼ŒFull GCå¯¼è‡´network timeoutï¼Œspark context stop<br>Hiveè¯»è¿™äº›å¤§è¡¨ä¸ºä½•æ²¡æœ‰é—®é¢˜ï¼Ÿå› ä¸ºHiveé»˜è®¤ä½¿ç”¨çš„æ˜¯CombineHiveInputFormatï¼Œsplitæ˜¯åŸºäºæ–‡ä»¶å¤§å°çš„ã€‚<br>Sparkä¹Ÿéœ€è¦å®ç°ç±»ä¼¼äºHiveçš„CombineInputFormatï¼Œè¿˜èƒ½è§£å†³å°æ–‡ä»¶è¿‡å¤šå¯¼è‡´æäº¤taskæ•°é‡è¿‡å¤šçš„é—®é¢˜ã€‚<br>Executor Container killed<br>Executor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead<br><a id="more"></a></p><h5 id="åŸå› ï¼š"><a href="#åŸå› ï¼š" class="headerlink" title="åŸå› ï¼š"></a>åŸå› ï¼š</h5><p>1.Shuffle Readæ—¶nettyå †å¤–å†…å­˜çš„ä½¿ç”¨<br>2.Window function spill thresholdè¿‡å°ï¼Œå¯¼è‡´æ¯4096æ¡æˆ–è€…64MBä¸ºä¸€ä¸ªæ–‡ä»¶å†™åˆ°ç£ç›˜<br>å¤–éƒ¨æ’åºåŒæ—¶æ‰“å¼€æ¯ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶å ç”¨1MBçš„å †å¤–å†…å­˜ï¼Œå¯¼è‡´containerä½¿ç”¨çš„å†…å­˜è¿œè¶…è¿‡ç”³è¯·çš„å†…å­˜ï¼Œé‚è¢«yarn killã€‚<br>è§£å†³ï¼š<br>Patchï¼š<br>[SPARK-19659] Fetch big blocks to disk when shuffle-read<br>[SPARK-21369][CORE] Donâ€™t use Scala Tuple2 in common/network-<em><br>å‚æ•°ï¼šspark.reducer.maxReqSizeShuffleToMem=209715200<br>Patchï¼š<br>[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArray<br>å‚æ•°ï¼š<br>spark.sql.windowExec.buffer.in.memory.threshold=4096<br>spark.sql.windowExec.buffer.spill.threshold= 1024 </em>1024 * 1024 / 2</p><h5 id="å°æ–‡ä»¶é—®é¢˜"><a href="#å°æ–‡ä»¶é—®é¢˜" class="headerlink" title="å°æ–‡ä»¶é—®é¢˜"></a>å°æ–‡ä»¶é—®é¢˜</h5><p>Sparkå†™æ•°æ®æ—¶ç”Ÿæˆå¾ˆå¤šå°æ–‡ä»¶ï¼Œå¯¹NameNodeäº§ç”Ÿå·¨å¤§çš„å‹åŠ›ï¼Œåœ¨ä¸€å¼€å§‹Sparkç°åº¦ä¸Šçº¿çš„æ—¶å€™ï¼Œæ–‡ä»¶æ•°å’ŒBlockæ•°é£™å‡ï¼Œæ–‡ä»¶å˜å°å¯¼è‡´å‹ç¼©ç‡é™ä½ï¼Œå®¹é‡ä¹Ÿè·Ÿç€ä¸Šå»ã€‚</p><h5 id="ç§»æ¤Hive-MergeFileTaskçš„å®ç°"><a href="#ç§»æ¤Hive-MergeFileTaskçš„å®ç°" class="headerlink" title="ç§»æ¤Hive MergeFileTaskçš„å®ç°"></a>ç§»æ¤Hive MergeFileTaskçš„å®ç°</h5><p>åœ¨Sparkæœ€åå†™ç›®æ ‡è¡¨çš„é˜¶æ®µè¿½åŠ å…¥äº†ä¸€ä¸ªMergeFileTaskï¼Œå‚è€ƒäº†Hiveçš„å®ç°<br>org.apache.hadoop.hive.ql.io.merge.MergeFileTask<br>org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator</p><h5 id="æ— æ•°æ®çš„æƒ…å†µä¸‹ä¸åˆ›å»ºç©ºæ–‡ä»¶"><a href="#æ— æ•°æ®çš„æƒ…å†µä¸‹ä¸åˆ›å»ºç©ºæ–‡ä»¶" class="headerlink" title="æ— æ•°æ®çš„æƒ…å†µä¸‹ä¸åˆ›å»ºç©ºæ–‡ä»¶"></a>æ— æ•°æ®çš„æƒ…å†µä¸‹ä¸åˆ›å»ºç©ºæ–‡ä»¶</h5><p>[SPARK-21435][SQL]<br>Empty files should be skipped while write to file</p><h3 id="å…«ã€ä¼˜åŒ–"><a href="#å…«ã€ä¼˜åŒ–" class="headerlink" title="å…«ã€ä¼˜åŒ–"></a>å…«ã€ä¼˜åŒ–</h3><p>1.æŸ¥è¯¢åˆ†åŒºè¡¨æ—¶æ”¯æŒbroadcast joinï¼ŒåŠ é€ŸæŸ¥è¯¢<br>2.å‡å°‘Broadcast joinçš„å†…å­˜å‹åŠ› SPARK-22170<br>3.Fetchå¤±è´¥åèƒ½å¿«é€Ÿå¤±è´¥ï¼Œä»¥å…ä½œä¸šå¡å‡ ä¸ªå°æ—¶ SPARK-19753<br>4.Spark Thrift Serverç¨³å®šæ€§<br>ç»å¸¸æŒ‚æ‰ï¼Œæ—¥å¿—é‡Œå¼‚å¸¸ï¼Œmore than one active taskSet for stage<br>Apply SPARK-23433ä»æœ‰å°‘æ•°æŒ‚æ‰çš„æƒ…å†µï¼Œ<br>æäº¤SPARK-24677åˆ°ç¤¾åŒºï¼Œä¿®å¤ä¹‹<br>5.ä½œä¸šhangä½ SPARK-21834 SPARK-19326 SPARK-11334</p><h3 id="ä¹ã€æœªæ¥è®¡åˆ’"><a href="#ä¹ã€æœªæ¥è®¡åˆ’" class="headerlink" title="ä¹ã€æœªæ¥è®¡åˆ’"></a>ä¹ã€æœªæ¥è®¡åˆ’</h3><h5 id="è‡ªåŠ¨è°ƒä¼˜å†…å­˜"><a href="#è‡ªåŠ¨è°ƒä¼˜å†…å­˜" class="headerlink" title="è‡ªåŠ¨è°ƒä¼˜å†…å­˜"></a>è‡ªåŠ¨è°ƒä¼˜å†…å­˜</h5><p>æ‰‹æœºspark driverå’Œexecutorå†…å­˜ä½¿ç”¨æƒ…å†µ<br>æ ¹æ®ä½œä¸šå†å²çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œåœ¨è°ƒåº¦ç³»ç»Ÿç«¯è‡ªåŠ¨è®¾ç½®åˆé€‚çš„å†…å­˜<br><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener">https://github.com/uber-common/jvm-profiler</a></p><h5 id="spark-adaptive"><a href="#spark-adaptive" class="headerlink" title="spark adaptive"></a>spark adaptive</h5><p>åŠ¨æ€è°ƒæ•´æ‰§è¡Œè®¡åˆ’ SortMergeJoinè½¬åŒ–ä¸ºBroadcastHashJoin<br>åŠ¨æ€å¤„ç†æ•°æ®å€¾æ–œ<br><a href="https://issues.apache.org/jira/browse/SPARK-23128" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-23128</a><br><a href="https://github.com/Intel-bigdata/spark-adaptive" target="_blank" rel="noopener">https://github.com/Intel-bigdata/spark-adaptive</a></p><!-- rebuild by neat -->]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkåœ¨æºç¨‹çš„å®è·µï¼ˆä¸€ï¼‰</title>
      <link href="/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Tue May 21 2019 20:46:50 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="ä¸€ã€Sparkåœ¨æºç¨‹åº”ç”¨çš„ç°çŠ¶"><a href="#ä¸€ã€Sparkåœ¨æºç¨‹åº”ç”¨çš„ç°çŠ¶" class="headerlink" title="ä¸€ã€Sparkåœ¨æºç¨‹åº”ç”¨çš„ç°çŠ¶"></a>ä¸€ã€Sparkåœ¨æºç¨‹åº”ç”¨çš„ç°çŠ¶</h3><h6 id="é›†ç¾¤è§„æ¨¡ï¼š"><a href="#é›†ç¾¤è§„æ¨¡ï¼š" class="headerlink" title="é›†ç¾¤è§„æ¨¡ï¼š"></a>é›†ç¾¤è§„æ¨¡ï¼š</h6><p>å¹³å‡æ¯å¤©MRä»»åŠ¡æ•°ï¼š30W+</p><h6 id="å¼€å‘å¹³å°ï¼š"><a href="#å¼€å‘å¹³å°ï¼š" class="headerlink" title="å¼€å‘å¹³å°ï¼š"></a>å¼€å‘å¹³å°ï¼š</h6><p>è°ƒåº¦ç³»ç»Ÿè¿è¡Œçš„ä»»åŠ¡æ•°ï¼š10W+<br>æ¯å¤©è¿è¡Œä»»åŠ¡å®ä¾‹æ•°ï¼š23W+<br>ETL/è®¡ç®—ä»»åŠ¡ï¼š~58%</p><h6 id="æŸ¥è¯¢å¹³å°"><a href="#æŸ¥è¯¢å¹³å°" class="headerlink" title="æŸ¥è¯¢å¹³å°:"></a>æŸ¥è¯¢å¹³å°:</h6><p>adhocæŸ¥è¯¢ï¼š2W+<br>æ”¯æŒSpark/Hive/Presto<br><img src="/assets/blogImg/1209_1.png" alt="enter description here"></p><h3 id="äºŒã€Hiveä¸Sparkçš„åŒºåˆ«"><a href="#äºŒã€Hiveä¸Sparkçš„åŒºåˆ«" class="headerlink" title="äºŒã€Hiveä¸Sparkçš„åŒºåˆ«"></a>äºŒã€Hiveä¸Sparkçš„åŒºåˆ«</h3><h6 id="Hiveï¼š"><a href="#Hiveï¼š" class="headerlink" title="Hiveï¼š"></a>Hiveï¼š</h6><p>ä¼˜ç‚¹ï¼šè¿è¡Œç¨³å®šï¼Œå®¢æˆ·ç«¯å†…å­˜æ¶ˆè€—å°ã€‚<br>å­˜åœ¨é—®é¢˜ï¼šç”Ÿæˆå¤šä¸ªMapReduceä½œä¸šï¼›ä¸­é—´ç»“æœè½åœ°ï¼ŒIOå¼€é”€å¤§ï¼›é¢‘ç¹ç”³è¯·å’Œé‡Šæ”¾containerï¼Œèµ„æºæ²¡æœ‰åˆç†å……åˆ†åˆ©ç”¨</p><h6 id="Sparkï¼š"><a href="#Sparkï¼š" class="headerlink" title="Sparkï¼š"></a>Sparkï¼š</h6><p>å¿«ï¼šé«˜æ•ˆçš„DAGæ‰§è¡Œå¼•æ“ï¼Œå¯ä»¥åŸºäºå†…å­˜æ¥é«˜æ•ˆçš„å¤„ç†æ•°æ®æµï¼ŒèŠ‚çœå¤§é‡IOå¼€é”€<br>é€šç”¨æ€§ï¼šSparkSQLèƒ½ç›´æ¥ä½¿ç”¨HiveQLè¯­æ³•ï¼ŒHive Metastoreï¼ŒSerdesï¼ŒUDFs<br><img src="/assets/blogImg/1209_2.png" alt="enter description here"></p><h3 id="ä¸‰ã€è¿ç§»SparkSQLçš„æŒ‘æˆ˜"><a href="#ä¸‰ã€è¿ç§»SparkSQLçš„æŒ‘æˆ˜" class="headerlink" title="ä¸‰ã€è¿ç§»SparkSQLçš„æŒ‘æˆ˜"></a>ä¸‰ã€è¿ç§»SparkSQLçš„æŒ‘æˆ˜</h3><h6 id="å…¼å®¹æ€§ï¼š"><a href="#å…¼å®¹æ€§ï¼š" class="headerlink" title="å…¼å®¹æ€§ï¼š"></a>å…¼å®¹æ€§ï¼š</h6><p>HiveåŸå…ˆçš„æƒé™æ§åˆ¶<br>SQLè¯­æ³•ï¼ŒUDFå’ŒHiveçš„å…¼å®¹æ€§</p><h6 id="ç¨³å®šæ€§ï¼š"><a href="#ç¨³å®šæ€§ï¼š" class="headerlink" title="ç¨³å®šæ€§ï¼š"></a>ç¨³å®šæ€§ï¼š</h6><p>è¿ç§»é€æ˜ï¼Œä½ä¼˜å…ˆçº§ç”¨æˆ·æ— æ„ŸçŸ¥<br>ç›‘æ§ä½œä¸šè¿ç§»åæˆåŠŸç‡åŠè¿è¡Œæ—¶é•¿å¯¹æ¯”</p><h6 id="å‡†ç¡®æ€§ï¼š"><a href="#å‡†ç¡®æ€§ï¼š" class="headerlink" title="å‡†ç¡®æ€§ï¼š"></a>å‡†ç¡®æ€§ï¼š</h6><p>æ•°æ®ä¸€è‡´<br>åŠŸèƒ½å¢å¼ºï¼š<br>ç”¨æˆ·ä½“éªŒï¼Œæ˜¯å¦æ˜“ç”¨ï¼ŒæŠ¥é”™ä¿¡æ¯æ˜¯å¦å¯è¯»<br>æ½œåœ¨Bug<br>å‘¨è¾¹ç³»ç»Ÿé…åˆæ”¹é€ <br>è¡€ç¼˜æ”¶é›†</p><h3 id="å››ã€å…¼å®¹æ€§æ”¹é€ "><a href="#å››ã€å…¼å®¹æ€§æ”¹é€ " class="headerlink" title="å››ã€å…¼å®¹æ€§æ”¹é€ "></a>å››ã€å…¼å®¹æ€§æ”¹é€ </h3><h6 id="ç§»æ¤hiveæƒé™"><a href="#ç§»æ¤hiveæƒé™" class="headerlink" title="ç§»æ¤hiveæƒé™"></a>ç§»æ¤hiveæƒé™</h6><p>Sparkæ²¡æœ‰æƒé™è®¤è¯æ¨¡å—ï¼Œå¯å¯¹ä»»æ„è¡¨è¿›è¡ŒæŸ¥è¯¢ï¼Œæœ‰å®‰å…¨éšæ‚£<br>éœ€è¦ä¸Hiveå…±äº«åŒä¸€å¥—æƒé™</p><h6 id="æ–¹æ¡ˆï¼š"><a href="#æ–¹æ¡ˆï¼š" class="headerlink" title="æ–¹æ¡ˆï¼š"></a>æ–¹æ¡ˆï¼š</h6><p>æ‰§è¡ŒSQLæ—¶ï¼Œå¯¹SQLè§£æå¾—åˆ°LogicalPlanï¼Œå¯¹LogicalPlanè¿›è¡Œéå†ï¼Œæå–è¯»å–çš„è¡¨åŠå†™å…¥çš„è¡¨ï¼Œè°ƒç”¨Hvieçš„è®¤è¯æ–¹æ³•è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœæœ‰æƒé™åˆ™ç»§ç»­æ‰§è¡Œï¼Œå¦åˆ™æ‹’ç»è¯¥ç”¨æˆ·çš„æ“ä½œã€‚<br>SQLè¯­æ³•å’Œhiveå…¼å®¹<br>Sparkåˆ›å»ºçš„æŸäº›è§†å›¾ï¼Œåœ¨HiveæŸ¥è¯¢æ—¶æŠ¥é”™ï¼ŒSparkåˆ›å»ºçš„è§†å›¾ä¸ä¼šå¯¹SQLè¿›è¡Œå±•å¼€ï¼Œè§†å›¾å®šä¹‰æ²¡æœ‰å½“å‰çš„DBä¿¡æ¯ï¼ŒHiveä¸å…¼å®¹è¯»å–è¿™æ ·çš„è§†å›¾</p><h6 id="æ–¹æ¡ˆï¼šã€"><a href="#æ–¹æ¡ˆï¼šã€" class="headerlink" title="æ–¹æ¡ˆï¼šã€"></a>æ–¹æ¡ˆï¼šã€</h6><p>ä¿æŒä¸Hiveä¸€è‡´ï¼Œåœ¨Sparkåˆ›å»ºå’Œä¿®æ”¹è§†å›¾æ—¶ï¼Œä½¿ç”¨hive cli driverå»æ‰§è¡Œcreate/alter view sql<br>UDFä¸hiveå…¼å®¹<br>UDFè®¡ç®—ç»“æœä¸ä¸€æ ·ï¼Œå³ä½¿æ˜¯æ­£å¸¸æ•°æ®ï¼ŒSparkè¿”å›nullï¼ŒHiveç»“æœæ­£ç¡®ï¼›å¼‚å¸¸æ•°æ®ï¼ŒSparkæŠ›exceptionå¯¼è‡´ä½œä¸šå¤±è´¥ï¼ŒHiveè¿”å›çš„nullã€‚</p><h6 id="æ–¹æ¡ˆï¼š-1"><a href="#æ–¹æ¡ˆï¼š-1" class="headerlink" title="æ–¹æ¡ˆï¼š"></a>æ–¹æ¡ˆï¼š</h6><p>Sparkå‡½æ•°ä¿®å¤ï¼Œæ¯”å¦‚roundå‡½æ•°<br>å°†hiveä¸€äº›å‡½æ•°ç§»æ¤ï¼Œå¹¶æ³¨å†Œæˆæ°¸ä¹…å‡½æ•°<br>æ•´ç†Sparkå’ŒHiveè¯­æ³•å’ŒUDFå·®å¼‚<br>äº”ã€ç¨³å®šæ€§å’Œå‡†ç¡®æ€§</p><h6 id="ç¨³å®šæ€§ï¼š-1"><a href="#ç¨³å®šæ€§ï¼š-1" class="headerlink" title="ç¨³å®šæ€§ï¼š"></a>ç¨³å®šæ€§ï¼š</h6><p>è¿ç§»é€æ˜ï¼šè°ƒåº¦ç³»ç»Ÿå¯¹ä½ä¼˜å…ˆçº§ä½œä¸šï¼ŒæŒ‰ä½œä¸šç²’åº¦åˆ‡æ¢æˆSparkæ‰§è¡Œï¼Œå¤±è´¥åå†åˆ‡æ¢æˆhive<br>ç°åº¦å˜æ›´ï¼Œå¤šç§å˜æ›´è§„åˆ™ï¼šæ”¯æŒå¤šç‰ˆæœ¬Sparkï¼Œè‡ªåŠ¨åˆ‡æ¢å¼•æ“ï¼ŒSpark v2 -&gt; Spark v1 -&gt; Hiveï¼›ç°åº¦æ¨é€å‚æ•°ï¼Œè°ƒä¼˜å‚æ•°ï¼ŒæŸäº›åŠŸèƒ½<br>ç›‘æ§ï¼šæ¯æ—¥ç»Ÿè®¡sparkå’Œhiveè¿è¡Œå¯¹æ¯”ï¼Œæ¯æ—¶æ”¶é›†ä½œä¸šç²’åº¦å¤±è´¥çš„Sparkä½œä¸šï¼Œåˆ†æå¤±è´¥åŸå› <br>å‡†ç¡®æ€§ï¼š<br>æ•°æ®è´¨é‡ç³»ç»Ÿï¼šæ ¡éªŒä»»åŠ¡ï¼Œæ£€æŸ¥æ•°æ®å‡†ç¡®æ€§</p><h3 id="å…­ã€åŠŸèƒ½å¢å¼º"><a href="#å…­ã€åŠŸèƒ½å¢å¼º" class="headerlink" title="å…­ã€åŠŸèƒ½å¢å¼º"></a>å…­ã€åŠŸèƒ½å¢å¼º</h3><h6 id="Spark-Thrift-Serverï¼š"><a href="#Spark-Thrift-Serverï¼š" class="headerlink" title="Spark Thrift Serverï¼š"></a>Spark Thrift Serverï¼š</h6><ul><li>1.åŸºäºdelegation tokençš„impersontion<br>Driverï¼š<br>ä¸ºä¸åŒçš„ç”¨æˆ·æ‹¿delegation tokenï¼Œå†™åˆ°stagingç›®å½•ï¼Œè®°å½•User-&gt;SQL-&gt;Jobæ˜ å°„å…³ç³»ï¼Œåˆ†å‘taskå¸¦ä¸Šå¯¹åº”çš„username<br>Executorï¼š<br>æ ¹æ®taskä¿¡æ¯å¸¦çš„usernameæ‰¾åˆ°stagingç›®å½•ä¸‹çš„tokenï¼ŒåŠ åˆ°å½“å‰proxy userçš„ugiï¼Œå®ç°impersonate</li><li>2.åŸºäºzookeeperçš„æœåŠ¡å‘ç°ï¼Œæ”¯æŒå¤šå°server<br>è¿™ä¸€å—ä¸»è¦ç§»æ¤äº†Hive zookeeperçš„å®ç°</li><li>3.é™åˆ¶å¤§æŸ¥è¯¢ä½œä¸šï¼Œé˜²æ­¢driver OOM<br>é™åˆ¶æ¯ä¸ªjobäº§ç”Ÿçš„taskæœ€å¤§æ•°é‡<br>é™åˆ¶æŸ¥è¯¢SQLçš„æœ€å¤§è¡Œæ•°ï¼Œå®¢æˆ·ç«¯æŸ¥è¯¢å¤§æ‰¹é‡æ•°æ®ï¼Œæ•°æ®æŒ¤å‹åœ¨Thrift Serverï¼Œå †å†…å†…å­˜é£™å‡ï¼Œå¼ºåˆ¶åœ¨åªæœ‰æŸ¥çš„SQLåŠ ä¸Šlimit<br>é™åˆ¶æŸ¥è¯¢SQLçš„ç»“æœé›†æ•°æ®å¤§å°</li><li>4.ç›‘æ§<br>å¯¹æ¯ä¸ªserverå®šæ—¶æŸ¥è¯¢ï¼Œæ£€æµ‹æ˜¯å¦å¯ç”¨<br>å¤šè¿è¡Œæ—¶é•¿è¾ƒä¹…çš„ä½œä¸šï¼Œä¸»åŠ¨kill<h6 id="ç”¨æˆ·ä½“éªŒ"><a href="#ç”¨æˆ·ä½“éªŒ" class="headerlink" title="ç”¨æˆ·ä½“éªŒ"></a>ç”¨æˆ·ä½“éªŒ</h6>ç”¨æˆ·çœ‹åˆ°çš„æ˜¯ç±»ä¼¼Hive MRè¿›åº¦çš„æ—¥å¿—ï¼ŒINFOçº§åˆ«æ—¥å¿—æ”¶é›†åˆ°ESï¼Œå¯ä¾›æ—¥å¿—çš„åˆ†æå’Œæ’æŸ¥é—®é¢˜<br>æ”¶é›†ç”Ÿæˆçš„è¡¨æˆ–è€…åˆ†åŒºçš„numRows numFile totalSizeï¼Œè¾“å‡ºåˆ°æ—¥å¿—<br>å¯¹ç®€å•çš„è¯­å¥ï¼Œå¦‚DDLè¯­å¥ï¼Œè‡ªåŠ¨ä½¿ç”¨â€“master=localæ–¹å¼å¯åŠ¨<h6 id="Combine-input-Format"><a href="#Combine-input-Format" class="headerlink" title="Combine input Format"></a>Combine input Format</h6>åœ¨HadoopTableReader#makeRDDForTableï¼Œæ‹¿åˆ°å¯¹åº”tableçš„InputFormatClassï¼Œè½¬æ¢æˆå¯¹åº”æ ¼å¼çš„CombineInputFormat<br>é€šè¿‡å¼€å…³æ¥å†³å®šæ˜¯å¦å¯ç”¨è¿™ä¸ªç‰¹æ€§<br>set spark.sql.combine.input.splits.enable=true<br>é€šè¿‡å‚æ•°æ¥è°ƒæ•´æ¯ä¸ªsplitçš„total input size<br>mapreduce.input.fileinputformat.split.maxsize=256MB <em>1024</em>1024<br>ä¹‹å‰driverè¯»å¤§è¡¨é«˜å³°æ—¶æ®µsplitéœ€è¦30åˆ†é’Ÿä¸æ­¢ï¼Œæ‰æŠŠä»»åŠ¡æäº¤ä¸Šï¼Œç°åœ¨åªè¦å‡ åˆ†é’Ÿå°±ç®—å¥½splitçš„æ•°é‡å¹¶æäº¤ä»»åŠ¡ï¼Œä¹Ÿè§£å†³äº†ä¸€äº›è¡¨ä¸å¤§ï¼Œå°æ–‡ä»¶å¤šï¼Œèƒ½åˆå¹¶åˆ°åŒä¸€ä¸ªtaskè¿›è¡Œè¯»å–</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä»£ç  | Sparkè¯»å–mongoDBæ•°æ®å†™å…¥Hiveæ™®é€šè¡¨å’Œåˆ†åŒºè¡¨</title>
      <link href="/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/"/>
      <url>/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 20 2019 22:37:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="ç‰ˆæœ¬ï¼š"><a href="#ç‰ˆæœ¬ï¼š" class="headerlink" title="ç‰ˆæœ¬ï¼š"></a>ç‰ˆæœ¬ï¼š</h3><p>spark 2.2.0<br>hive 1.1.0<br>scala 2.11.8<br>hadoop-2.6.0-cdh5.7.0<br>jdk 1.8<br>MongoDB 3.6.4</p><h3 id="ä¸€-åŸå§‹æ•°æ®åŠHiveè¡¨"><a href="#ä¸€-åŸå§‹æ•°æ®åŠHiveè¡¨" class="headerlink" title="ä¸€ åŸå§‹æ•°æ®åŠHiveè¡¨"></a>ä¸€ åŸå§‹æ•°æ®åŠHiveè¡¨</h3><h5 id="MongoDBæ•°æ®æ ¼å¼"><a href="#MongoDBæ•°æ®æ ¼å¼" class="headerlink" title="MongoDBæ•°æ®æ ¼å¼"></a>MongoDBæ•°æ®æ ¼å¼</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;),</span><br><span class="line">    &quot;id&quot; : &quot;1&quot;,</span><br><span class="line">    &quot;name&quot; : &quot;lisi&quot;,</span><br><span class="line">    &quot;age&quot; : &quot;18&quot;,</span><br><span class="line">    &quot;deptno&quot; : &quot;01&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="Hiveæ™®é€šè¡¨"><a href="#Hiveæ™®é€šè¡¨" class="headerlink" title="Hiveæ™®é€šè¡¨"></a>Hiveæ™®é€šè¡¨</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table mg_hive_test(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string,</span><br><span class="line">deptno string</span><br><span class="line">)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h5 id="Hiveåˆ†åŒºè¡¨"><a href="#Hiveåˆ†åŒºè¡¨" class="headerlink" title="Hiveåˆ†åŒºè¡¨"></a>Hiveåˆ†åŒºè¡¨</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table  mg_hive_external(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string</span><br><span class="line">)</span><br><span class="line">partitioned by (deptno string)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h3 id="äºŒ-IDEA-Maven-Java"><a href="#äºŒ-IDEA-Maven-Java" class="headerlink" title="äºŒ IDEA+Maven+Java"></a>äºŒ IDEA+Maven+Java</h3><h5 id="ä¾èµ–"><a href="#ä¾èµ–" class="headerlink" title="ä¾èµ–"></a>ä¾èµ–</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.6.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.2.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a>ä»£ç </h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/2 21:00</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import com.mongodb.spark.MongoSpark;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import org.bson.Document;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class sparkreadmgtohive &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        //spark 2.x</span><br><span class="line">        String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath();</span><br><span class="line">        SparkSession spark = SparkSession.builder()</span><br><span class="line">                .master(&quot;local[2]&quot;)</span><br><span class="line">                .appName(&quot;SparkReadMgToHive&quot;)</span><br><span class="line">                .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">                .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line"></span><br><span class="line">        //spark 1.x</span><br><span class="line">//        JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);</span><br><span class="line">//        SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);</span><br><span class="line">//        conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);</span><br><span class="line">//        conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);</span><br><span class="line">//        HiveContext sqlContext = new HiveContext(sc);</span><br><span class="line">//        //create df from mongo</span><br><span class="line">//        Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();</span><br><span class="line">//        df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">        String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;;</span><br><span class="line">        String opType =&quot;P&quot;;</span><br><span class="line"></span><br><span class="line">        SQLUtils sqlUtils = new SQLUtils();</span><br><span class="line">        List&lt;String&gt; column = sqlUtils.getColumns(querysql);</span><br><span class="line"></span><br><span class="line">        //create rdd from mongo</span><br><span class="line">        JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc);</span><br><span class="line">        //å°†Documentè½¬æˆObject</span><br><span class="line">        JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123;</span><br><span class="line">            public Object call(Document document)&#123;</span><br><span class="line">                List list = new ArrayList();</span><br><span class="line">                for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">                    list.add(String.valueOf(document.get(column.get(i))));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line"></span><br><span class="line">//                return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(Ordd.first());</span><br><span class="line">        //é€šè¿‡ç¼–ç¨‹æ–¹å¼å°†RDDè½¬æˆDF</span><br><span class="line">        List ls= new ArrayList();</span><br><span class="line">        for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">            ls.add(column.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">        String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;);</span><br><span class="line">        System.out.println(schemaString);</span><br><span class="line"></span><br><span class="line">        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();</span><br><span class="line">        for (String fieldName : schemaString.split(&quot;,&quot;)) &#123;</span><br><span class="line">            StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);</span><br><span class="line">            fields.add(field);</span><br><span class="line">        &#125;</span><br><span class="line">        StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123;</span><br><span class="line">            List fileds = (List) record;</span><br><span class="line">//            String[] attributes = record.toString().split(&quot;,&quot;);</span><br><span class="line">            return RowFactory.create(fileds.toArray());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema);</span><br><span class="line"></span><br><span class="line">        //å°†DFå†™å…¥åˆ°Hiveä¸­</span><br><span class="line">        //é€‰æ‹©Hiveæ•°æ®åº“</span><br><span class="line">        spark.sql(&quot;use datalake&quot;);</span><br><span class="line">        //æ³¨å†Œä¸´æ—¶è¡¨</span><br><span class="line">        df.registerTempTable(&quot;mgtable&quot;);</span><br><span class="line"></span><br><span class="line">        if (&quot;O&quot;.equals(opType.trim())) &#123;</span><br><span class="line">            System.out.println(&quot;æ•°æ®æ’å…¥åˆ°Hive ordinary table&quot;);</span><br><span class="line">            Long t1 = System.currentTimeMillis();</span><br><span class="line">            spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;);</span><br><span class="line">            Long t2 = System.currentTimeMillis();</span><br><span class="line">            System.out.println(&quot;å…±è€—æ—¶ï¼š&quot; + (t2 - t1) / 60000 + &quot;åˆ†é’Ÿ&quot;);</span><br><span class="line">        &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123;</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;æ•°æ®æ’å…¥åˆ°Hive  dynamic partition table&quot;);</span><br><span class="line">        Long t3 = System.currentTimeMillis();</span><br><span class="line">        //å¿…é¡»è®¾ç½®ä»¥ä¸‹å‚æ•° å¦åˆ™æŠ¥é”™</span><br><span class="line">        spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;);</span><br><span class="line">        //deptonä¸ºåˆ†åŒºå­—æ®µ   selectè¯­å¥æœ€åä¸€ä¸ªå­—æ®µå¿…é¡»æ˜¯deptno</span><br><span class="line">        spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;);</span><br><span class="line">        Long t4 = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;å…±è€—æ—¶ï¼š&quot;+(t4 -t3)/60000+ &quot;åˆ†é’Ÿ&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="å·¥å…·ç±»"><a href="#å·¥å…·ç±»" class="headerlink" title="å·¥å…·ç±»"></a>å·¥å…·ç±»</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/3 23:20</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class SQLUtils &#123;</span><br><span class="line"></span><br><span class="line">    public List&lt;String&gt; getColumns(String querysql)&#123;</span><br><span class="line">        List&lt;String&gt; column = new ArrayList&lt;String&gt;();</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6,</span><br><span class="line">                querysql.indexOf(&quot;from&quot;)).trim();</span><br><span class="line">        if (tmp.indexOf(&quot;*&quot;) == -1)&#123;</span><br><span class="line">            String cols[] = tmp.split(&quot;,&quot;);</span><br><span class="line">            for (String c:cols)&#123;</span><br><span class="line">                column.add(c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return column;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTBname(String querysql)&#123;</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim();</span><br><span class="line">        int sx = tmp.indexOf(&quot; &quot;);</span><br><span class="line">        if(sx == -1)&#123;</span><br><span class="line">            return tmp;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return tmp.substring(0,sx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ä¸‰-é”™è¯¯è§£å†³åŠæ³•"><a href="#ä¸‰-é”™è¯¯è§£å†³åŠæ³•" class="headerlink" title="ä¸‰ é”™è¯¯è§£å†³åŠæ³•"></a>ä¸‰ é”™è¯¯è§£å†³åŠæ³•</h3><p>1 IDEAä¼šè·å–ä¸åˆ°Hiveçš„æ•°æ®åº“å’Œè¡¨ï¼Œå°†hive-site.xmlæ”¾å…¥resourcesæ–‡ä»¶ä¸­ã€‚å¹¶ä¸”å°†resourcesè®¾ç½®æˆé…ç½®æ–‡ä»¶(è®¾ç½®æˆåŠŸæ–‡ä»¶å¤¹æ˜¯è“è‰²å¦åˆ™æ˜¯ç°è‰²)<br>fileâ€“&gt;Project Structureâ€“&gt;Modulesâ€“&gt;Source<br><img src="/assets/blogImg/1120_1.png" alt="enter description here"><br>2 ä¸Šé¢é”™è¯¯å¤„ç†å®Œåå¦‚æœæŠ¥JDOç±»å‹çš„é”™è¯¯ï¼Œé‚£ä¹ˆæ£€æŸ¥HIVE_HOME/libä¸‹æ—¶å€™å¦mysqlé©±åŠ¨ï¼Œå¦‚æœç¡®å®šæœ‰ï¼Œé‚£ä¹ˆå°±æ˜¯IDEAè·å–ä¸åˆ°ã€‚è§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š</p><p>å°†mysqlé©±åŠ¨æ‹·è´åˆ°jdk1.8.0_171.jdk/Contents/Home/jre/lib/extè·¯å¾„ä¸‹(jdk/jre/lib/ext)<br>åœ¨IDEAé¡¹ç›®External Librariesä¸‹çš„&lt;1.8&gt;é‡Œé¢æ·»åŠ mysqlé©±åŠ¨<br><img src="/assets/blogImg/1120_2.png" alt="enter description here"></p><h3 id="å››-æ³¨æ„ç‚¹"><a href="#å››-æ³¨æ„ç‚¹" class="headerlink" title="å›› æ³¨æ„ç‚¹"></a>å›› æ³¨æ„ç‚¹</h3><p>ç”±äºå°†MongoDBæ•°æ®è¡¨æ³¨å†Œæˆäº†ä¸´æ—¶è¡¨å’ŒHiveè¡¨è¿›è¡Œäº†å…³è”ï¼Œæ‰€ä»¥è¦å°†MongoDBä¸­çš„idå­—æ®µè®¾ç½®æˆç´¢å¼•å­—æ®µï¼Œå¦åˆ™æ€§èƒ½ä¼šå¾ˆæ…¢ã€‚<br>MongoDBè®¾ç½®ç´¢å¼•æ–¹æ³•ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;</span><br></pre></td></tr></table></figure><p></p><p>æŸ¥çœ‹ç´¢å¼•ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).getIndexes()</span><br><span class="line">MongoSparkç½‘å€ï¼šhttps://docs.mongodb.com/spark-connector/current/java-api/</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æœ€å…¨çš„Flinkéƒ¨ç½²åŠå¼€å‘æ¡ˆä¾‹(KafkaSource+SinkToMySQL)</title>
      <link href="/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
      <url>/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="1-ä¸‹è½½Flinkå®‰è£…åŒ…"><a href="#1-ä¸‹è½½Flinkå®‰è£…åŒ…" class="headerlink" title="1.ä¸‹è½½Flinkå®‰è£…åŒ…"></a>1.ä¸‹è½½Flinkå®‰è£…åŒ…</h5><p>flinkä¸‹è½½åœ°å€</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>å› ä¸ºä¾‹å­ä¸éœ€è¦hadoopï¼Œä¸‹è½½flink-1.5.0-bin-scala_2.11.tgzå³å¯</p><p>ä¸Šä¼ è‡³æœºå™¨çš„/optç›®å½•ä¸‹<br><a id="more"></a></p><h5 id="2-è§£å‹"><a href="#2-è§£å‹" class="headerlink" title="2.è§£å‹"></a>2.è§£å‹</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-é…ç½®masterèŠ‚ç‚¹"><a href="#3-é…ç½®masterèŠ‚ç‚¹" class="headerlink" title="3.é…ç½®masterèŠ‚ç‚¹"></a>3.é…ç½®masterèŠ‚ç‚¹</h5><p>é€‰æ‹©ä¸€ä¸ª masterèŠ‚ç‚¹(JobManager)ç„¶ååœ¨conf/flink-conf.yamlä¸­è®¾ç½®jobmanager.rpc.address é…ç½®é¡¹ä¸ºè¯¥èŠ‚ç‚¹çš„IP æˆ–è€…ä¸»æœºåã€‚ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹æœ‰æœ‰ä¸€æ ·çš„jobmanager.rpc.address é…ç½®ã€‚</p><p>jobmanager.rpc.address: node1</p><p>(é…ç½®ç«¯å£å¦‚æœè¢«å ç”¨ä¹Ÿè¦æ”¹ å¦‚é»˜è®¤8080å·²ç»è¢«sparkå ç”¨ï¼Œæ”¹æˆäº†8088)</p><p>rest.port: 8088</p><p>æœ¬æ¬¡å®‰è£… masterèŠ‚ç‚¹ä¸ºnode1ï¼Œå› ä¸ºå•æœºï¼ŒslaveèŠ‚ç‚¹ä¹Ÿä¸ºnode1</p><h5 id="4-é…ç½®slaves"><a href="#4-é…ç½®slaves" class="headerlink" title="4.é…ç½®slaves"></a>4.é…ç½®slaves</h5><p>å°†æ‰€æœ‰çš„ worker èŠ‚ç‚¹ ï¼ˆTaskManagerï¼‰çš„IP æˆ–è€…ä¸»æœºåï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰å¡«å…¥conf/slaves æ–‡ä»¶ä¸­ã€‚</p><h5 id="5-å¯åŠ¨flinké›†ç¾¤"><a href="#5-å¯åŠ¨flinké›†ç¾¤" class="headerlink" title="5.å¯åŠ¨flinké›†ç¾¤"></a>5.å¯åŠ¨flinké›†ç¾¤</h5><p>bin/start-cluster.sh</p><p>æ‰“å¼€ <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> æŸ¥çœ‹webé¡µé¢<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managersä»£è¡¨å½“å‰çš„flinkåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªtaskè¿˜æœ‰ä¸¤ä¸ªslots</p><h5 id="6-æµ‹è¯•"><a href="#6-æµ‹è¯•" class="headerlink" title="6.æµ‹è¯•"></a>6.æµ‹è¯•</h5><p><strong>ä¾èµ–</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p></p><h5 id="7-Socketæµ‹è¯•ä»£ç "><a href="#7-Socketæµ‹è¯•ä»£ç " class="headerlink" title="7.Socketæµ‹è¯•ä»£ç "></a>7.Socketæµ‹è¯•ä»£ç </h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>æ‰“åŒ…mvn clean install (å¦‚æœæ‰“åŒ…è¿‡ç¨‹ä¸­æŠ¥é”™java.lang.OutOfMemoryError)</p><p>åœ¨å‘½ä»¤è¡Œset MAVEN_OPTS= -Xms128m -Xmx512m</p><p>ç»§ç»­æ‰§è¡Œmvn clean install</p><p>ç”ŸæˆFlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>æ‰¾åˆ°æ‰“æˆçš„jarï¼Œå¹¶uploadï¼Œå¼€å§‹ä¸Šä¼ <br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>è¿è¡Œå‚æ•°ä»‹ç»<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>æäº¤ç»“æŸä¹‹åå»overviewç•Œé¢çœ‹ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå¯ç”¨çš„slotså˜æˆäº†ä¸€ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬çš„socketç¨‹åºå ç”¨äº†ä¸€ä¸ªï¼Œæ­£åœ¨runningçš„jobå˜æˆäº†ä¸€ä¸ª</p><p>å‘é€æ•°æ®<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>ç‚¹å¼€runningçš„jobï¼Œä½ å¯ä»¥çœ‹è§æ¥æ”¶çš„å­—èŠ‚æ•°ç­‰ä¿¡æ¯</p><p>åˆ°logç›®å½•ä¸‹å¯ä»¥æ¸…æ¥šçš„çœ‹è§è¾“å‡º<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure><p></p><p>é™¤äº†å¯ä»¥åœ¨ç•Œé¢æäº¤ï¼Œè¿˜å¯ä»¥å°†jarä¸Šä¼ çš„linuxä¸­è¿›è¡Œæäº¤ä»»åŠ¡</p><p>è¿è¡Œflinkä¸Šä¼ çš„jar<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure><p></p><p>å…¶ä»–æ­¥éª¤ä¸€è‡´ã€‚</p><h5 id="8-ä½¿ç”¨kafkaä½œä¸ºsource"><a href="#8-ä½¿ç”¨kafkaä½œä¸ºsource" class="headerlink" title="8.ä½¿ç”¨kafkaä½œä¸ºsource"></a>8.ä½¿ç”¨kafkaä½œä¸ºsource</h5><p>åŠ ä¸Šä¾èµ–<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //å¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…åˆé€‚çš„topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //é…ç½®ä»æœ€æ–°çš„åœ°æ–¹å¼€å§‹æ¶ˆè´¹</span><br><span class="line">        kafkaSource.setStartFromLatest();        //ä½¿ç”¨addsourceï¼Œå°†kafkaçš„è¾“å…¥è½¬å˜ä¸ºdatastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-ä½¿ç”¨mysqlä½œä¸ºsink"><a href="#9-ä½¿ç”¨mysqlä½œä¸ºsink" class="headerlink" title="9.ä½¿ç”¨mysqlä½œä¸ºsink"></a>9.ä½¿ç”¨mysqlä½œä¸ºsink</h5><p>flinkæœ¬èº«å¹¶æ²¡æœ‰æä¾›datastreamè¾“å‡ºåˆ°mysqlï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±å»å®ç°</p><p>é¦–å…ˆï¼Œå¯¼å…¥ä¾èµ–<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>è‡ªå®šä¹‰sinkï¼Œé¦–å…ˆæƒ³åˆ°çš„æ˜¯extends SinkFunctionï¼Œé›†æˆflinkè‡ªå¸¦çš„sinkfunctionï¼Œå†å½“ä¸­å®ç°æ–¹æ³•ï¼Œå®ç°å¦‚ä¸‹<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>è¿™æ ·å®ç°æœ‰ä¸ªé—®é¢˜ï¼Œæ¯ä¸€æ¡æ•°æ®ï¼Œéƒ½è¦æ‰“å¼€mysqlè¿æ¥ï¼Œå†å…³é—­ï¼Œæ¯”è¾ƒè€—æ—¶ï¼Œè¿™ä¸ªå¯ä»¥ä½¿ç”¨flinkä¸­æ¯”è¾ƒå¥½çš„Richæ–¹å¼æ¥å®ç°ï¼Œä»£ç å¦‚ä¸‹<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//è¿”å›æˆåŠŸçš„è¯å°±æ˜¯ä¸€ä¸ªï¼Œå¦åˆ™å°±æ˜¯0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>Richæ–¹å¼çš„ä¼˜ç‚¹åœ¨äºï¼Œæœ‰ä¸ªopenå’Œcloseæ–¹æ³•ï¼Œåœ¨åˆå§‹åŒ–çš„æ—¶å€™å»ºç«‹ä¸€æ¬¡è¿æ¥ï¼Œä¹‹åä¸€ç›´ä½¿ç”¨è¿™ä¸ªè¿æ¥å³å¯ï¼Œç¼©çŸ­å»ºç«‹å’Œå…³é—­è¿æ¥çš„æ—¶é—´ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿æ¥æ± å®ç°ï¼Œè¿™é‡Œåªæ˜¯æä¾›è¿™æ ·ä¸€ç§æ€è·¯ã€‚</p><p>ä½¿ç”¨è¿™ä¸ªmysqlsinkä¹Ÿéå¸¸ç®€å•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//ç›´æ¥addsinkï¼Œå³å¯è¾“å‡ºåˆ°è‡ªå®šä¹‰çš„mysqlä¸­ï¼Œä¹Ÿå¯ä»¥å°†mysqlçš„å­—æ®µç­‰å†™æˆå¯é…ç½®çš„ï¼Œæ›´åŠ æ–¹ä¾¿å’Œé€šç”¨proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure><p></p><h5 id="10-æ€»ç»“"><a href="#10-æ€»ç»“" class="headerlink" title="10.æ€»ç»“"></a>10.æ€»ç»“</h5><p>æœ¬æ¬¡çš„ç¬”è®°åšäº†ç®€å•çš„éƒ¨ç½²ã€æµ‹è¯•ã€kafkademoï¼Œä»¥åŠè‡ªå®šä¹‰å®ç°mysqlsinkçš„ä¸€äº›å†…å®¹ï¼Œå…¶ä¸­æ¯”è¾ƒé‡è¦çš„æ˜¯Richçš„ä½¿ç”¨ï¼Œå¸Œæœ›å¤§å®¶èƒ½æœ‰æ‰€æ”¶è·ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹hadoop-2.7.3ç¼–è¯‘å’Œæ­å»ºé›†ç¾¤ç¯å¢ƒ(HDFS HA,Yarn HA)</title>
      <link href="/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/"/>
      <url>/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun May 26 2019 21:16:06 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="1-ä¸‹è½½hadoop2-7-3æœ€æ–°æºç "><a href="#1-ä¸‹è½½hadoop2-7-3æœ€æ–°æºç " class="headerlink" title="1.ä¸‹è½½hadoop2.7.3æœ€æ–°æºç "></a>1.ä¸‹è½½hadoop2.7.3æœ€æ–°æºç </h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt </span><br><span class="line">Build instructions for Hadoop</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Requirements:</span><br><span class="line">* Unix System</span><br><span class="line">* JDK 1.7+</span><br><span class="line">* Maven 3.0 or later</span><br><span class="line">* Findbugs 1.3.9 (if running findbugs)</span><br><span class="line">* ProtocolBuffer 2.5.0</span><br><span class="line">* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac</span><br><span class="line">* Zlib devel (if compiling native code)</span><br><span class="line">* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )</span><br><span class="line">* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )</span><br><span class="line">* Internet connection for first build (to fetch all Maven and Hadoop dependencies)</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:</span><br><span class="line">* Oracle JDK 1.7 (preferred)</span><br><span class="line">  $ sudo apt-get purge openjdk*</span><br><span class="line">  $ sudo apt-get install software-properties-common</span><br><span class="line">  $ sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">  $ sudo apt-get update</span><br><span class="line">  $ sudo apt-get install oracle-java7-installer</span><br><span class="line">* Maven</span><br><span class="line">  $ sudo apt-get -y install maven</span><br><span class="line">* Native libraries</span><br><span class="line">  $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev</span><br><span class="line">* ProtocolBuffer 2.5.0 (required)</span><br><span class="line">  $ sudo apt-get -y install libprotobuf-dev protobuf-compiler</span><br><span class="line">Optional packages:</span><br><span class="line">* Snappy compression</span><br><span class="line">  $ sudo apt-get install snappy libsnappy-dev</span><br><span class="line">* Bzip2</span><br><span class="line">  $ sudo apt-get install bzip2 libbz2-dev</span><br><span class="line">* Jansson (C Library for JSON)</span><br><span class="line">  $ sudo apt-get install libjansson-dev</span><br><span class="line">* Linux FUSE</span><br><span class="line">  $ sudo apt-get install fuse libfuse-dev</span><br></pre></td></tr></table></figure><h3 id="2-å®‰è£…ä¾èµ–åŒ…"><a href="#2-å®‰è£…ä¾èµ–åŒ…" class="headerlink" title="2.å®‰è£…ä¾èµ–åŒ…"></a>2.å®‰è£…ä¾èµ–åŒ…</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*</span><br></pre></td></tr></table></figure><h3 id="3-å®‰è£…jdk"><a href="#3-å®‰è£…jdk" class="headerlink" title="3.å®‰è£…jdk"></a>3.å®‰è£…jdk</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# java -version</span><br><span class="line">java version &quot;1.7.0_67&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_67-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#</span><br></pre></td></tr></table></figure><h3 id="4-å®‰è£…maven"><a href="#4-å®‰è£…maven" class="headerlink" title="4.å®‰è£…maven"></a>4.å®‰è£…maven</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">#åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ä¸ºäº†é˜²æ­¢Javaå†…å­˜æº¢å‡ºï¼Œéœ€è¦åŠ å…¥ä»¥ä¸‹ç¯å¢ƒå˜é‡</span><br><span class="line">export MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;</span><br><span class="line">export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# mvn -version</span><br><span class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</span><br><span class="line">Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">Java version: 1.7.0_67, vendor: Oracle Corporation</span><br><span class="line">Java home: /usr/java/jdk1.7.0_67-cloudera/jre</span><br><span class="line">Default locale: en_US, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#</span><br></pre></td></tr></table></figure><h3 id="5-ç¼–è¯‘å®‰è£…protobuf"><a href="#5-ç¼–è¯‘å®‰è£…protobuf" class="headerlink" title="5.ç¼–è¯‘å®‰è£…protobuf"></a>5.ç¼–è¯‘å®‰è£…protobuf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make install</span><br><span class="line">#æŸ¥çœ‹protobufç‰ˆæœ¬ä»¥æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸ</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">libprotoc 2.5.0</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]#</span><br></pre></td></tr></table></figure><h3 id="6-å®‰è£…snappy"><a href="#6-å®‰è£…snappy" class="headerlink" title="6.å®‰è£…snappy"></a>6.å®‰è£…snappy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz</span><br><span class="line">#ç”¨rootç”¨æˆ·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install</span><br><span class="line">#æŸ¥çœ‹snappyåº“æ–‡ä»¶</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy</span><br><span class="line">-rw-r--r--  1 root root 229K Jun 21 15:46 libsnappy.a</span><br><span class="line">-rwxr-xr-x  1 root root  953 Jun 21 15:46 libsnappy.la</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0</span><br><span class="line">-rwxr-xr-x  1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]#</span><br></pre></td></tr></table></figure><h3 id="7-ç¼–è¯‘"><a href="#7-ç¼–è¯‘" class="headerlink" title="7.ç¼–è¯‘"></a>7.ç¼–è¯‘</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">mvn clean package -Pdist,native -DskipTests -Dtar</span><br><span class="line">æˆ–</span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package â€“Pdist,native â€“DskipTests â€“Dtar</span><br><span class="line">[INFO] Executing tasks</span><br><span class="line">main:</span><br><span class="line">     [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3</span><br><span class="line">     [exec] $ gzip -f hadoop-2.7.3.tar</span><br><span class="line">     [exec] </span><br><span class="line">     [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz</span><br><span class="line">     [exec] </span><br><span class="line">[INFO] Executed tasks</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---</span><br><span class="line">[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s]</span><br><span class="line">[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  6.832 s]</span><br><span class="line">[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s]</span><br><span class="line">[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s]</span><br><span class="line">[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.411 s]</span><br><span class="line">[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  4.814 s]</span><br><span class="line">[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s]</span><br><span class="line">[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min]</span><br><span class="line">[INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s]</span><br><span class="line">[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s]</span><br><span class="line">[INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min]</span><br><span class="line">[INFO] Apache Hadoop NFS .................................. SUCCESS [  9.285 s]</span><br><span class="line">[INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s]</span><br><span class="line">[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.049 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min]</span><br><span class="line">[INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  4.599 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.044 s]</span><br><span class="line">[INFO] hadoop-yarn ........................................ SUCCESS [  0.043 s]</span><br><span class="line">[INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min]</span><br><span class="line">[INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s]</span><br><span class="line">[INFO] hadoop-yarn-server ................................. SUCCESS [  0.041 s]</span><br><span class="line">[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s]</span><br><span class="line">[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s]</span><br><span class="line">[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  6.415 s]</span><br><span class="line">[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s]</span><br><span class="line">[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s]</span><br><span class="line">[INFO] hadoop-yarn-server-tests ........................... SUCCESS [  7.751 s]</span><br><span class="line">[INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s]</span><br><span class="line">[INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [  5.612 s]</span><br><span class="line">[INFO] hadoop-yarn-applications ........................... SUCCESS [  0.038 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  4.029 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  2.611 s]</span><br><span class="line">[INFO] hadoop-yarn-site ................................... SUCCESS [  0.077 s]</span><br><span class="line">[INFO] hadoop-yarn-registry ............................... SUCCESS [  8.045 s]</span><br><span class="line">[INFO] hadoop-yarn-project ................................ SUCCESS [  5.456 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.226 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  6.697 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  9.328 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  2.412 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  8.876 s]</span><br><span class="line">[INFO] hadoop-mapreduce ................................... SUCCESS [  4.237 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s]</span><br><span class="line">[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s]</span><br><span class="line">[INFO] Apache Hadoop Archives ............................. SUCCESS [  3.069 s]</span><br><span class="line">[INFO] Apache Hadoop Rumen ................................ SUCCESS [  7.446 s]</span><br><span class="line">[INFO] Apache Hadoop Gridmix .............................. SUCCESS [  5.765 s]</span><br><span class="line">[INFO] Apache Hadoop Data Join ............................ SUCCESS [  3.752 s]</span><br><span class="line">[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  2.771 s]</span><br><span class="line">[INFO] Apache Hadoop Extras ............................... SUCCESS [  5.612 s]</span><br><span class="line">[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s]</span><br><span class="line">[INFO] Apache Hadoop OpenStack support .................... SUCCESS [  7.131 s]</span><br><span class="line">[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min]</span><br><span class="line">[INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s]</span><br><span class="line">[INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s]</span><br><span class="line">[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.142 s]</span><br><span class="line">[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  7.354 s]</span><br><span class="line">[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s]</span><br><span class="line">[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.035 s]</span><br><span class="line">[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 26:29 min</span><br><span class="line">[INFO] Finished at: 2016-12-24T21:07:09+08:00</span><br><span class="line">[INFO] Final Memory: 214M/740M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]# ll</span><br><span class="line">total 193152</span><br><span class="line">drwxr-xr-x 5 root root      4096 Dec 24 20:24 compilesoft</span><br><span class="line">-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]#</span><br></pre></td></tr></table></figure><h3 id="8-æ­å»ºHDFS-HA-YARN-HAé›†ç¾¤ï¼ˆ5ä¸ªèŠ‚ç‚¹ï¼‰"><a href="#8-æ­å»ºHDFS-HA-YARN-HAé›†ç¾¤ï¼ˆ5ä¸ªèŠ‚ç‚¹ï¼‰" class="headerlink" title="8.æ­å»ºHDFS HA,YARN HAé›†ç¾¤ï¼ˆ5ä¸ªèŠ‚ç‚¹ï¼‰"></a>8.æ­å»ºHDFS HA,YARN HAé›†ç¾¤ï¼ˆ5ä¸ªèŠ‚ç‚¹ï¼‰</h3><p>å‚è€ƒ:<br><a href="http://blog.itpub.net/30089851/viewspace-1994585/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-1994585/</a><br><a href="https://github.com/Hackeruncle/Hadoop" target="_blank" rel="noopener">https://github.com/Hackeruncle/Hadoop</a></p><h3 id="9-æ­å»ºé›†ç¾¤-éªŒè¯ç‰ˆæœ¬å’Œæ”¯æŒçš„å‹ç¼©ä¿¡æ¯"><a href="#9-æ­å»ºé›†ç¾¤-éªŒè¯ç‰ˆæœ¬å’Œæ”¯æŒçš„å‹ç¼©ä¿¡æ¯" class="headerlink" title="9.æ­å»ºé›†ç¾¤,éªŒè¯ç‰ˆæœ¬å’Œæ”¯æŒçš„å‹ç¼©ä¿¡æ¯"></a>9.æ­å»ºé›†ç¾¤,éªŒè¯ç‰ˆæœ¬å’Œæ”¯æŒçš„å‹ç¼©ä¿¡æ¯</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop version</span><br><span class="line">Hadoop 2.7.3</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2016-12-24T12:45Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4</span><br><span class="line">This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop checknative</span><br><span class="line">16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">zlib:    true /lib64/libz.so.1</span><br><span class="line">snappy:  true /usr/local/lib/libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   true /lib64/libbz2.so.1</span><br><span class="line">openssl: true /usr/lib64/libcrypto.so</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]#</span><br></pre></td></tr></table></figure><p>[å‚è€ƒ]</p><ul><li><a href="http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html" target="_blank" rel="noopener">http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html</a></li><li><a href="http://blog.csdn.net/haohaixingyun/article/details/52800048" target="_blank" rel="noopener">http://blog.csdn.net/haohaixingyun/article/details/52800048</a></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹Flume-1.7.0æºç ç¼–è¯‘å¯¼å…¥eclipse</title>
      <link href="/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/"/>
      <url>/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sat May 25 2019 21:07:00 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="ã€å‰æã€‘"><a href="#ã€å‰æã€‘" class="headerlink" title="ã€å‰æã€‘:"></a>ã€å‰æã€‘:</h3><p>1.windows 7 å®‰è£…maven-3.3.9<br>å…¶ä¸­åœ¨conf/setting.xmlæ–‡ä»¶æ·»åŠ <br>D:\software\apache-maven-3.3.9\repository<br><a href="http://blog.csdn.net/defonds/article/details/41957287" target="_blank" rel="noopener">http://blog.csdn.net/defonds/article/details/41957287</a><br>2.windows 7 å®‰è£…eclipse 64ä½(ç™¾åº¦ä¸‹è½½ï¼Œè§£å‹å³å¯)<br>3.eclipseå®‰è£…mavenæ’ä»¶ï¼Œé€‰æ‹©ç¬¬äºŒç§æ–¹å¼link<br><a href="http://blog.csdn.net/lfsfxy9/article/details/9397937" target="_blank" rel="noopener">http://blog.csdn.net/lfsfxy9/article/details/9397937</a><br>å…¶ä¸­ eclipse-maven3-plugin.7z è¿™ä¸ªåŒ…å¯ä»¥åŠ ç¾¤258669058æ‰¾æˆ‘ï¼Œåˆ†äº«ç»™ä½ </p><h3 id="ã€flume-ng-1-7-0æºç çš„ç¼–è¯‘å¯¼å…¥eclipseã€‘"><a href="#ã€flume-ng-1-7-0æºç çš„ç¼–è¯‘å¯¼å…¥eclipseã€‘" class="headerlink" title="ã€flume-ng 1.7.0æºç çš„ç¼–è¯‘å¯¼å…¥eclipseã€‘:"></a>ã€flume-ng 1.7.0æºç çš„ç¼–è¯‘å¯¼å…¥eclipseã€‘:</h3><h4 id="1-ä¸‹è½½å®˜ç½‘çš„æºç -ä¸è¦ä¸‹è½½GitHubä¸Šæºç ï¼Œå› ä¸ºè¿™æ—¶pomæ–‡ä»¶ä¸­ç‰ˆæœ¬ä¸º1-8-0ï¼Œç¼–è¯‘ä¼šæœ‰é—®é¢˜"><a href="#1-ä¸‹è½½å®˜ç½‘çš„æºç -ä¸è¦ä¸‹è½½GitHubä¸Šæºç ï¼Œå› ä¸ºè¿™æ—¶pomæ–‡ä»¶ä¸­ç‰ˆæœ¬ä¸º1-8-0ï¼Œç¼–è¯‘ä¼šæœ‰é—®é¢˜" class="headerlink" title="1.ä¸‹è½½å®˜ç½‘çš„æºç (ä¸è¦ä¸‹è½½GitHubä¸Šæºç ï¼Œå› ä¸ºè¿™æ—¶pomæ–‡ä»¶ä¸­ç‰ˆæœ¬ä¸º1.8.0ï¼Œç¼–è¯‘ä¼šæœ‰é—®é¢˜)"></a>1.ä¸‹è½½å®˜ç½‘çš„æºç (ä¸è¦ä¸‹è½½GitHubä¸Šæºç ï¼Œå› ä¸ºè¿™æ—¶pomæ–‡ä»¶ä¸­ç‰ˆæœ¬ä¸º1.8.0ï¼Œç¼–è¯‘ä¼šæœ‰é—®é¢˜)</h4><p><a href="http://archive.apache.org/dist/flume/1.7.0/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/1.7.0/</a><br>a.ä¸‹è½½apache-flume-1.7.0-src.tar.gz<br>b.è§£å‹é‡å‘½åä¸ºflume-1.7.0</p><h4 id="2-ä¿®æ”¹pom-xml-å¤§æ¦‚åœ¨621è¡Œï¼Œå°†è‡ªå¸¦çš„repositoryæ³¨é‡Šæ‰ï¼Œæ·»åŠ ä»¥ä¸‹çš„"><a href="#2-ä¿®æ”¹pom-xml-å¤§æ¦‚åœ¨621è¡Œï¼Œå°†è‡ªå¸¦çš„repositoryæ³¨é‡Šæ‰ï¼Œæ·»åŠ ä»¥ä¸‹çš„" class="headerlink" title="2.ä¿®æ”¹pom.xml (å¤§æ¦‚åœ¨621è¡Œï¼Œå°†è‡ªå¸¦çš„repositoryæ³¨é‡Šæ‰ï¼Œæ·»åŠ ä»¥ä¸‹çš„)"></a>2.ä¿®æ”¹pom.xml (å¤§æ¦‚åœ¨621è¡Œï¼Œå°†è‡ªå¸¦çš„repositoryæ³¨é‡Šæ‰ï¼Œæ·»åŠ ä»¥ä¸‹çš„)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;repository&gt;</span><br><span class="line">       &lt;id&gt;maven.tempo-db.com&lt;/id&gt;</span><br><span class="line">       &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt;</span><br><span class="line"> &lt;/repository&gt;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/0828_1.png" alt="enter description here"></p><h4 id="3-æ‰“å¼€cmd-ç¼–è¯‘"><a href="#3-æ‰“å¼€cmd-ç¼–è¯‘" class="headerlink" title="3.æ‰“å¼€cmd,ç¼–è¯‘"></a>3.æ‰“å¼€cmd,ç¼–è¯‘</h4><p>cd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0<br>mvn compile<br><img src="/assets/blogImg/0828_2.png" alt="enter description here"></p><h4 id="4-æ‰“å¼€eclipse-å•å‡»Windowâ€“-gt-Perferencesâ€“-gt-å·¦ä¾§çš„Mavenâ€“-gt-User-Settings"><a href="#4-æ‰“å¼€eclipse-å•å‡»Windowâ€“-gt-Perferencesâ€“-gt-å·¦ä¾§çš„Mavenâ€“-gt-User-Settings" class="headerlink" title="4.æ‰“å¼€eclipse,å•å‡»Windowâ€“&gt;Perferencesâ€“&gt;å·¦ä¾§çš„Mavenâ€“&gt;User Settings"></a>4.æ‰“å¼€eclipse,å•å‡»Windowâ€“&gt;Perferencesâ€“&gt;å·¦ä¾§çš„Mavenâ€“&gt;User Settings</h4><p>ç„¶åè®¾ç½®è‡ªå·±çš„mvnçš„setting.xmlè·¯å¾„å’ŒLocal Repository<br>(æœ€å¥½ä½¿ç”¨Maven3.3.xç‰ˆæœ¬ä»¥ä¸Šï¼Œæˆ‘æ˜¯3.3.9)<br><img src="/assets/blogImg/0828_3.png" alt="enter description here"></p><h4 id="5-å…³é—­eclipseçš„-Projectâ€“-gt-Buid-Automatically"><a href="#5-å…³é—­eclipseçš„-Projectâ€“-gt-Buid-Automatically" class="headerlink" title="5.å…³é—­eclipseçš„ Projectâ€“&gt;Buid Automatically"></a>5.å…³é—­eclipseçš„ Projectâ€“&gt;Buid Automatically</h4><p><img src="/assets/blogImg/0828_4.png" alt="enter description here"></p><h4 id="6-å…³é—­eclipseçš„Download-repository-index-updates-on-startup"><a href="#6-å…³é—­eclipseçš„Download-repository-index-updates-on-startup" class="headerlink" title="6.å…³é—­eclipseçš„Download repository index updates on startup"></a>6.å…³é—­eclipseçš„Download repository index updates on startup</h4><p><img src="/assets/blogImg/0828_5.png" alt="enter description here"></p><h4 id="7-å¯¼å…¥flume1-7-0æºç "><a href="#7-å¯¼å…¥flume1-7-0æºç " class="headerlink" title="7.å¯¼å…¥flume1.7.0æºç "></a>7.å¯¼å…¥flume1.7.0æºç </h4><p>a.Fileâ€“&gt;Importâ€“&gt;Mavenâ€“&gt;Existing Maven Projectsâ€“&gt;Next<br>b.é€‰æ‹©ç›®å½•â€“&gt; Finish</p><h4 id="8-æ£€æŸ¥æºç ï¼Œæ²¡æœ‰æŠ›ä»»ä½•é”™è¯¯"><a href="#8-æ£€æŸ¥æºç ï¼Œæ²¡æœ‰æŠ›ä»»ä½•é”™è¯¯" class="headerlink" title="8.æ£€æŸ¥æºç ï¼Œæ²¡æœ‰æŠ›ä»»ä½•é”™è¯¯"></a>8.æ£€æŸ¥æºç ï¼Œæ²¡æœ‰æŠ›ä»»ä½•é”™è¯¯</h4><p><img src="/assets/blogImg/0828_6.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è¿™æ˜¯ä¸€ç¯‡çƒ­è…¾è…¾çš„é¢ç»</title>
      <link href="/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/"/>
      <url>/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:24:17 GMT+0800 (GMT+08:00) --><p>ä¼Ÿæ¢¦ï¼š<br>1.ä¸»è¦è¿˜æ˜¯é¡¹ç›®ï¼Ÿ<br>åŸºæœ¬ä¸Šæ²¡é—®ä»€ä¹ˆæŠ€æœ¯ï¼Œæˆ‘å°±è¯´äº†ä¸€éé¡¹ç›®æµç¨‹ï¼Œ<br>ç„¶åè¯´å‡ ä¸ªä¼˜åŒ–ç‚¹ï¼Œæ¯”å¦‚ä¸Šæ¬¡è®²çš„è¡€æ¡ˆï¼Œæˆ‘ä¹Ÿé¡ºå¸¦æäº†ä¸€ä¸‹ã€‚<br>2.åœ¨å¤§æ•°æ®ä¸­ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆæ˜¯ä¸è¶³çš„ï¼Œé‡åˆ°è¿‡ä»€ä¹ˆé—®é¢˜ï¼Ÿ<br><a id="more"></a></p><p>å¾®ç›Ÿï¼š<br>1.SparkStreamingå¤„ç†å®Œä¸€æ‰¹æ¬¡çš„æ•°æ®ï¼Œå†™åç§»é‡ä¹‹å‰æŒ‚äº†ï¼Œæ•°æ®æ€ä¹ˆä¿è¯ä¸é‡ï¼Ÿ<br>2.Maxwellçš„åº•å±‚åŸç†ï¼Ÿ<br>3.æ‰‹å†™Springï¼Ÿ<br>4.éå†äºŒå‰æ ‘ï¼Ÿ<br>5.ç”¨è¿‡ä»€ä¹ˆç®—æ³•ï¼Ÿ<br>6.å¤šçº¿ç¨‹æ–¹é¢ï¼Œæ€ä¹ˆå®ç°ä¸€ä¸ªä¸»çº¿ç¨‹ï¼Œç­‰å¾…å…¶ä»–å­çº¿ç¨‹å®Œæˆåå†è¿è¡Œï¼Ÿ<br>7.Maxwellå’ŒCannalçš„æ¯”è¾ƒï¼Ÿ<br>8.directæ¯”è¾ƒreceiverçš„ä¼˜åŠ¿ï¼Ÿ<br>9.åŸæ¥æ˜¯æŠŠæ•°æ®ä¼ å…¥åˆ°Hiveï¼Œä¹‹åæ”¹äº†æ¶æ„ï¼Œæ€ä¹ˆæŠŠHiveçš„æ•°æ®å¯¼å…¥åˆ°Hbaseï¼Ÿ<br>10.ä¸ºä»€ä¹ˆç”¨Kafkaè‡ªå·±å­˜å‚¨offsetæ¥æ›¿ä»£checkpointï¼Œæ€ä¹ˆé˜²æ­¢äº†æ•°æ®åŒä»½è½åœ°ï¼Œæ•°æ®åŒä»½æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ<br>11.å•ä¾‹ç”¨è¿‡å—ï¼Ÿ</p><p>å¹³å®‰ï¼š<br>1.é—®é¡¹ç›®ï¼Œæµç¨‹ï¼Œä¸šåŠ¡ï¼Ÿ<br>2.æ•°æ®é‡ï¼Œå¢é‡ï¼Ÿ<br>3.å‡ ä¸ªäººå¼€å‘çš„ï¼Œä»£ç é‡å¤šå°‘ï¼Ÿ<br>4.ä½ ä¸»è¦åšä»€ä¹ˆçš„ï¼Ÿ<br>5.ä»€ä¹ˆåœºæ™¯ï¼Œç”¨SparkSqlåˆ†æä»€ä¹ˆä¸œè¥¿ï¼Ÿ</p><p>æ€»ç»“ï¼š<br>åŸºæœ¬ä¸Šéƒ½æ˜¯å›´ç»•é¡¹ç›®æ¥é¢ï¼Œç¬¬ä¸€å®¶é—®çš„æ¯”è¾ƒå°‘ï¼Œè€Œä¸”éƒ½æ˜¯å…³äºé¡¹ç›®ï¼›å¾®ç›Ÿçš„é¢è¯•å®˜åšçš„é¡¹ç›®ï¼Œ<br>è·Ÿç®€å†ä¸Šçš„é¡¹ç›®ï¼Œæ¶æ„ä¸ŠåŸºæœ¬ä¸€æ ·ï¼Œæ‰€ä»¥é—®çš„æ¯”è¾ƒæ·±ï¼Œé—®æˆ‘Maxwellçš„åº•å±‚åŸç†ï¼Œå¯¹æ¯”Cannalæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Œ<br>ä¸ºä»€ä¹ˆé€‰æ‹©å®ƒï¼Œè¿™ä¸ªæˆ‘æ²¡å›ç­”ä¸Šæ¥ï¼Œåæ¥è®©æ‰‹å†™Springï¼Œç®—æ³•ï¼Œåæ¥å°±è®©æˆ‘èµ°äº†ï¼›<br>å¹³å®‰ä¹Ÿæ˜¯åŸºæœ¬å›´ç»•é¡¹ç›®ï¼Œä¸šåŠ¡ï¼Œæ•°æ®é‡ï¼Œæ²¡é—®ä»€ä¹ˆæŠ€æœ¯ï¼Œè€Œä¸”æˆ‘è¯´äº†å…³äºä¼˜åŒ–çš„ç‚¹(é¢è¯•å®˜è¯´ä¸è¦è¯´ç½‘ä¸Šéƒ½æœ‰çš„ä¸œè¥¿)ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> é¢è¯•é¢˜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¤§æ•°æ®é¢è¯•é¢˜ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹é¡¹ç›®æ¦‚è¿°</title>
      <link href="/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/"/>
      <url>/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sat May 25 2019 21:10:04 GMT+0800 (GMT+08:00) --><h3 id="1-å‰æœŸåŸºæœ¬æ¶æ„å›¾"><a href="#1-å‰æœŸåŸºæœ¬æ¶æ„å›¾" class="headerlink" title="1.å‰æœŸåŸºæœ¬æ¶æ„å›¾"></a>1.å‰æœŸåŸºæœ¬æ¶æ„å›¾</h3><p><img src="/assets/blogImg/0827_1.png" alt="enter description here"></p><h3 id="2-æœ€ç»ˆåŸºæœ¬æ¶æ„å›¾"><a href="#2-æœ€ç»ˆåŸºæœ¬æ¶æ„å›¾" class="headerlink" title="2.æœ€ç»ˆåŸºæœ¬æ¶æ„å›¾"></a>2.æœ€ç»ˆåŸºæœ¬æ¶æ„å›¾</h3><p><img src="/assets/blogImg/0827_2.png" alt="enter description here"><br><a id="more"></a></p><h3 id="3-ç‰ˆæœ¬"><a href="#3-ç‰ˆæœ¬" class="headerlink" title="3.ç‰ˆæœ¬"></a>3.ç‰ˆæœ¬</h3><table><thead><tr><th>ç»„ä»¶</th><th>ç‰ˆæœ¬</th></tr></thead><tbody><tr><td>Flume:</td><td>1.7</td></tr><tr><td>Hadoop:</td><td>2.7.3</td></tr><tr><td>Scala:</td><td>2.11</td></tr><tr><td>Kafka:</td><td>0.10.1.0</td></tr><tr><td>Spark:</td><td>2.0.2</td></tr><tr><td>InfluxDB:</td><td>1.2.0</td></tr><tr><td>Grafana:</td><td>4.1.1</td></tr><tr><td>maven:</td><td>3.3.9</td></tr></tbody></table><h3 id="4-ä¸»è¦ç›®çš„"><a href="#4-ä¸»è¦ç›®çš„" class="headerlink" title="4.ä¸»è¦ç›®çš„"></a>4.ä¸»è¦ç›®çš„</h3><p>ä¸»è¦æ˜¯æƒ³åŸºäºExec Sourceå¼€å‘è‡ªå®šä¹‰æ’ä»¶AdvancedExecSourceï¼Œå°†æœºå™¨åç§° å’Œ æœåŠ¡åç§° æ·»åŠ åˆ°cdh æœåŠ¡çš„è§’è‰²logæ•°æ®çš„æ¯ä¸€è¡Œå‰é¢ï¼Œåˆ™æ ¼å¼ä¸ºï¼šæœºå™¨åç§° æœåŠ¡åç§° å¹´æœˆæ—¥ æ—¶åˆ†ç§’.æ¯«ç§’ æ—¥å¿—çº§åˆ« æ—¥å¿—ä¿¡æ¯ ï¼›<br>ç„¶ååœ¨åé¢çš„spark streaming å®æ—¶è®¡ç®—æˆ‘ä»¬æ‰€éœ€æ±‚ï¼šæ¯”å¦‚ç»Ÿè®¡æ¯å°æœºå™¨çš„æœåŠ¡çš„æ¯ç§’å‡ºç°çš„erroræ¬¡æ•° ã€ç»Ÿè®¡æ¯5ç§’çš„warnï¼Œerroræ¬¡æ•°ç­‰ç­‰ï¼›<br>æ¥å®æ—¶å¯è§†åŒ–å±•ç¤ºå’Œé‚®ä»¶çŸ­ä¿¡ã€å¾®ä¿¡ä¼ä¸šå·é€šçŸ¥ã€‚</p><p>å…¶å®ä¸»è¦æˆ‘ä»¬ç°åœ¨çš„å¾ˆå¤šç›‘æ§æœåŠ¡åŸºæœ¬è¾¾ä¸åˆ°ç§’çº§çš„é€šçŸ¥ï¼Œéƒ½ä¸º5åˆ†é’Ÿç­‰ç­‰ï¼Œä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬è‡ªå·±çš„ç»´æŠ¤ï¼›<br>å…¶å®å¯¹ä¸€äº›å³å°†å‡ºç°çš„é—®é¢˜å¯ä»¥æå‰é¢„çŸ¥ï¼›<br>å…¶å®æœ€ä¸»è¦å¯ä»¥æœ‰æ•ˆæ‰©å±•åˆ°å®æ—¶è®¡ç®—æ•°æ®åº“çº§åˆ«æ—¥å¿—ï¼Œæ¯”å¦‚MySQLæ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œnginxï¼Œtomcatï¼Œlinuxçš„ç³»ç»Ÿçº§åˆ«æ—¥å¿—ç­‰ç­‰ã€‚</p><h3 id="5-å¤§æ¦‚æµç¨‹"><a href="#5-å¤§æ¦‚æµç¨‹" class="headerlink" title="5.å¤§æ¦‚æµç¨‹"></a>5.å¤§æ¦‚æµç¨‹</h3><p>1.æ­å»ºhadoop cluster<br>2.eclipse å¯¼å…¥flumeæºä»£ç ï¼ˆwindow7 å®‰è£…mavenï¼Œeclipseï¼Œeclipseä¸mavené›†æˆï¼‰<br>3.å¼€å‘flume-ng è‡ªå®šä¹‰æ’ä»¶<br>4.flume æ”¶é›†ï¼Œæ±‡èšåˆ°hdfs(ä¸»è¦æµ‹è¯•æ˜¯å¦æ±‡èšæˆåŠŸï¼ŒåæœŸä¹Ÿå¯ä»¥åšç¦»çº¿å¤„ç†)<br>5.flume æ”¶é›†ï¼Œæ±‡èšåˆ°kafka<br>6.æ­å»ºkafka monitor<br>7.æ­å»º spark client<br>8.window7è£…iedaå¼€å‘å·¥å…·<br>9.ideaå¼€å‘ spark streaming çš„wc<br>10.è¯»å–kafkaæ—¥å¿—ï¼Œå¼€å‘spark streamingçš„è¿™å—æ—¥å¿—åˆ†æ<br>11.å†™å…¥influxdb<br>12.grafanaå¯è§†åŒ–å±•ç¤º<br>13.é›†æˆé‚®ä»¶</p><p>###è¯´æ˜ï¼š<br>é’ˆå¯¹è‡ªèº«æƒ…å†µï¼Œè‡ªè¡Œé€‰æ‹©ï¼Œæ­¥éª¤å¦‚ä¸Šï¼Œä½†ä¸æ˜¯å›ºå®šçš„ï¼Œæœ‰äº›é¡ºåºæ˜¯å¯ä»¥æ‰“ä¹±çš„ï¼Œä¾‹å¦‚å¼€å‘å·¥å…·çš„å®‰è£…ï¼Œå¯ä»¥ä¸€èµ·æ“ä½œçš„ï¼Œå†å¦‚è¿™å‡ ä¸ªç»„ä»¶çš„ä¸‹è½½ç¼–è¯‘ï¼Œå¦‚æœä¸<br>æƒ³ç¼–è¯‘å¯ä»¥ç›´æ¥ä¸‹taråŒ…çš„ï¼Œè‡ªè¡Œé€‰æ‹©å°±å¥½ï¼Œä½†æ˜¯å»ºè®®è¿˜æ˜¯è‡ªå·±ç¼–è¯‘ï¼Œé‡åˆ°å‘æ‰èƒ½æ›´å¥½çš„è®°ä½è¿™ä¸ªä¸œè¥¿ï¼Œæœ¬èº«è¿™ä¸ªé¡¹ç›®å°±æ˜¯å­¦ä¹ æå‡çš„è¿‡ç¨‹ï¼Œè¦æ˜¯ä»€ä¹ˆéƒ½æ˜¯ç°æˆçš„ï¼Œ<br>é‚£å°±æ²¡ä»€ä¹ˆæ„ä¹‰äº†</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›® </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparkä¸­é…ç½®å¯ç”¨LZOå‹ç¼©</title>
      <link href="/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/"/>
      <url>/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --><p>Sparkä¸­é…ç½®å¯ç”¨LZOå‹ç¼©ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š</p><h3 id="ä¸€ã€spark-env-shé…ç½®"><a href="#ä¸€ã€spark-env-shé…ç½®" class="headerlink" title="ä¸€ã€spark-env.shé…ç½®"></a>ä¸€ã€spark-env.shé…ç½®</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h3 id="äºŒã€spark-defaults-confé…ç½®"><a href="#äºŒã€spark-defaults-confé…ç½®" class="headerlink" title="äºŒã€spark-defaults.confé…ç½®"></a>äºŒã€spark-defaults.confé…ç½®</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br></pre></td></tr></table></figure><p><font color="#FF4500">æ³¨ï¼šæŒ‡å‘ç¼–è¯‘ç”Ÿæˆlzoçš„jaråŒ…</font><br><a id="more"></a></p><h3 id="ä¸‰ã€æµ‹è¯•"><a href="#ä¸‰ã€æµ‹è¯•" class="headerlink" title="ä¸‰ã€æµ‹è¯•"></a>ä¸‰ã€æµ‹è¯•</h3><h4 id="1ã€è¯»å–Lzoæ–‡ä»¶"><a href="#1ã€è¯»å–Lzoæ–‡ä»¶" class="headerlink" title="1ã€è¯»å–Lzoæ–‡ä»¶"></a>1ã€è¯»å–Lzoæ–‡ä»¶</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)</span><br></pre></td></tr></table></figure><h4 id="2ã€å†™å‡ºlzoæ–‡ä»¶"><a href="#2ã€å†™å‡ºlzoæ–‡ä»¶" class="headerlink" title="2ã€å†™å‡ºlzoæ–‡ä»¶"></a>2ã€å†™å‡ºlzoæ–‡ä»¶</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val lzoTest = sc.parallelize(1 to 10)</span><br><span class="line">scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])</span><br></pre></td></tr></table></figure><p>ç»“æœï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2018-03-16 23:24 /input/test_lzo/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         61 2018-03-16 23:24 /input/test_lzo/part-00001.lzo</span><br></pre></td></tr></table></figure><p></p><p>è‡³æ­¤é…ç½®ä¸æµ‹è¯•å®Œæˆã€‚</p><h3 id="å››ã€é…ç½®ä¸æµ‹è¯•ä¸­å­˜é—®é¢˜"><a href="#å››ã€é…ç½®ä¸æµ‹è¯•ä¸­å­˜é—®é¢˜" class="headerlink" title="å››ã€é…ç½®ä¸æµ‹è¯•ä¸­å­˜é—®é¢˜"></a>å››ã€é…ç½®ä¸æµ‹è¯•ä¸­å­˜é—®é¢˜</h3><h4 id="1ã€å¼•ç”¨nativeï¼Œç¼ºå°‘LD-LIBRARY-PATH"><a href="#1ã€å¼•ç”¨nativeï¼Œç¼ºå°‘LD-LIBRARY-PATH" class="headerlink" title="1ã€å¼•ç”¨nativeï¼Œç¼ºå°‘LD_LIBRARY_PATH"></a>1ã€å¼•ç”¨nativeï¼Œç¼ºå°‘LD_LIBRARY_PATH</h4><h5 id="1-1ã€é”™è¯¯æç¤ºï¼š"><a href="#1-1ã€é”™è¯¯æç¤ºï¼š" class="headerlink" title="1.1ã€é”™è¯¯æç¤ºï¼š"></a>1.1ã€é”™è¯¯æç¤ºï¼š</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: native-lzo library not available</span><br><span class="line">  at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120)</span><br><span class="line">  at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178)</span><br><span class="line">  at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111)</span><br><span class="line">  at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)</span><br><span class="line">  at org.apache.spark.scheduler.Task.run(Task.scala:108)</span><br><span class="line">  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><h5 id="1-2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-evn-shï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š"><a href="#1-2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-evn-shï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š" class="headerlink" title="1.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-evn.shï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š"></a>1.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-evn.shï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h4 id="2ã€æ— æ³•æ‰¾åˆ°LzopCodecç±»"><a href="#2ã€æ— æ³•æ‰¾åˆ°LzopCodecç±»" class="headerlink" title="2ã€æ— æ³•æ‰¾åˆ°LzopCodecç±»"></a>2ã€æ— æ³•æ‰¾åˆ°LzopCodecç±»</h4><h5 id="2-1ã€é”™è¯¯æç¤ºï¼š"><a href="#2-1ã€é”™è¯¯æç¤ºï¼š" class="headerlink" title="2.1ã€é”™è¯¯æç¤ºï¼š"></a>2.1ã€é”™è¯¯æç¤ºï¼š</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found.</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)</span><br><span class="line">    at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found</span><br><span class="line">    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)</span><br></pre></td></tr></table></figure><h5 id="2-2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-defaults-confï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š"><a href="#2-2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-defaults-confï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š" class="headerlink" title="2.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-defaults.confï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š"></a>2.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-defaults.confï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFSä¹‹åƒåœ¾å›æ”¶ç®±é…ç½®åŠä½¿ç”¨</title>
      <link href="/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --><p>HDFSä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºä¸€ä¸ªå›æ”¶ç«™:<br>ç›®å½•:/user/ç”¨æˆ·/.Trash/Current, ç³»ç»Ÿå›æ”¶ç«™éƒ½æœ‰ä¸€ä¸ªå‘¨æœŸ,å‘¨æœŸè¿‡åhdfsä¼šå½»åº•åˆ é™¤æ¸…ç©º,å‘¨æœŸå†…å¯ä»¥æ¢å¤ã€‚<br><a id="more"></a></p><h4 id="ä¸€ã€HDFSåˆ é™¤æ–‡ä»¶-æ— æ³•æ¢å¤"><a href="#ä¸€ã€HDFSåˆ é™¤æ–‡ä»¶-æ— æ³•æ¢å¤" class="headerlink" title="ä¸€ã€HDFSåˆ é™¤æ–‡ä»¶,æ— æ³•æ¢å¤"></a>ä¸€ã€HDFSåˆ é™¤æ–‡ä»¶,æ— æ³•æ¢å¤</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">Deleted /123.log</span><br></pre></td></tr></table></figure><h4 id="äºŒã€-å¯ç”¨å›æ”¶ç«™åŠŸèƒ½"><a href="#äºŒã€-å¯ç”¨å›æ”¶ç«™åŠŸèƒ½" class="headerlink" title="äºŒã€ å¯ç”¨å›æ”¶ç«™åŠŸèƒ½"></a>äºŒã€ å¯ç”¨å›æ”¶ç«™åŠŸèƒ½</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vim core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--å¤šé•¿æ—¶é—´åˆ›å»ºCheckPoint NameNodeèŠ‚ç‚¹ä¸Šè¿è¡Œçš„CheckPointer </span><br><span class="line">ä»Currentæ–‡ä»¶å¤¹åˆ›å»ºCheckPoint; é»˜è®¤: 0 ç”±fs.trash.intervalé¡¹æŒ‡å®š --&gt;</span><br><span class="line">&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--å¤šå°‘åˆ†é’Ÿ.Trashä¸‹çš„CheckPointç›®å½•ä¼šè¢«åˆ é™¤,</span><br><span class="line">è¯¥é…ç½®æœåŠ¡å™¨è®¾ç½®ä¼˜å…ˆçº§å¤§äºå®¢æˆ·ç«¯ï¼Œé»˜è®¤:ä¸å¯ç”¨ --&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;  -- æ¸…é™¤å‘¨æœŸåˆ†é’Ÿ(24å°æ—¶)</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="1ã€é‡å¯hdfsæœåŠ¡"><a href="#1ã€é‡å¯hdfsæœåŠ¡" class="headerlink" title="1ã€é‡å¯hdfsæœåŠ¡"></a>1ã€é‡å¯hdfsæœåŠ¡</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sbin]$ ./stop-dfs.sh</span><br><span class="line">[hadoop@hadoop001 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure><h5 id="2ã€æµ‹è¯•å›æ”¶ç«™åŠŸèƒ½"><a href="#2ã€æµ‹è¯•å›æ”¶ç«™åŠŸèƒ½" class="headerlink" title="2ã€æµ‹è¯•å›æ”¶ç«™åŠŸèƒ½"></a>2ã€æµ‹è¯•å›æ”¶ç«™åŠŸèƒ½</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /123.log</span><br></pre></td></tr></table></figure><h5 id="æ–‡ä»¶åˆ é™¤æˆåŠŸå­˜æ”¾å›æ”¶ç«™è·¯å¾„ä¸‹"><a href="#æ–‡ä»¶åˆ é™¤æˆåŠŸå­˜æ”¾å›æ”¶ç«™è·¯å¾„ä¸‹" class="headerlink" title="æ–‡ä»¶åˆ é™¤æˆåŠŸå­˜æ”¾å›æ”¶ç«™è·¯å¾„ä¸‹"></a>æ–‡ä»¶åˆ é™¤æˆåŠŸå­˜æ”¾å›æ”¶ç«™è·¯å¾„ä¸‹</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="æ¢å¤æ–‡ä»¶"><a href="#æ¢å¤æ–‡ä»¶" class="headerlink" title="æ¢å¤æ–‡ä»¶"></a>æ¢å¤æ–‡ä»¶</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /456.log</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="åˆ é™¤æ–‡ä»¶è·³è¿‡å›æ”¶ç«™"><a href="#åˆ é™¤æ–‡ä»¶è·³è¿‡å›æ”¶ç«™" class="headerlink" title="åˆ é™¤æ–‡ä»¶è·³è¿‡å›æ”¶ç«™"></a>åˆ é™¤æ–‡ä»¶è·³è¿‡å›æ”¶ç«™</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.log</span><br><span class="line">Deleted /456.log</span><br></pre></td></tr></table></figure><p>æºç å‚è€ƒï¼š<br><a href="https://blog.csdn.net/tracymkgld/article/details/17557655" target="_blank" rel="noopener">https://blog.csdn.net/tracymkgld/article/details/17557655</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkåºåˆ—åŒ–ï¼Œä½ äº†è§£å—</title>
      <link href="/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri May 17 2019 22:28:01 GMT+0800 (GMT+08:00) --><p>åºåˆ—åŒ–åœ¨åˆ†å¸ƒå¼åº”ç”¨çš„æ€§èƒ½ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ ¼å¼åŒ–å¯¹è±¡ç¼“æ…¢ï¼Œæˆ–è€…æ¶ˆè€—å¤§é‡çš„å­—èŠ‚æ ¼å¼åŒ–ï¼Œä¼šå¤§å¤§é™ä½è®¡ç®—æ€§èƒ½ã€‚é€šå¸¸è¿™æ˜¯åœ¨sparkåº”ç”¨ä¸­ç¬¬ä¸€ä»¶éœ€è¦ä¼˜åŒ–çš„äº‹æƒ…ã€‚Sparkçš„ç›®æ ‡æ˜¯åœ¨ä¾¿åˆ©ä¸æ€§èƒ½ä¸­å–å¾—å¹³è¡¡ï¼Œæ‰€ä»¥æä¾›2ç§åºåˆ—åŒ–çš„é€‰æ‹©ã€‚<br><a id="more"></a></p><h3 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h3><p>åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä¼šä½¿ç”¨Javaçš„ObjectOutputStreamæ¡†æ¶å¯¹å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä»»ä½•å®ç°java.io.Serializableçš„ç±»ä¸€èµ·å·¥ä½œã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æ‰©å±•java.io.Externalizableæ¥æ›´ç´§å¯†åœ°æ§åˆ¶åºåˆ—åŒ–çš„æ€§èƒ½ã€‚Javaåºåˆ—åŒ–æ˜¯çµæ´»çš„ï¼Œä½†é€šå¸¸ç›¸å½“æ…¢ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´è®¸å¤šç±»çš„å¤§å‹åºåˆ—åŒ–æ ¼å¼ã€‚</p><h4 id="æµ‹è¯•ä»£ç ï¼š"><a href="#æµ‹è¯•ä»£ç ï¼š" class="headerlink" title="æµ‹è¯•ä»£ç ï¼š"></a>æµ‹è¯•ä»£ç ï¼š</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="æµ‹è¯•ç»“æœï¼š"><a href="#æµ‹è¯•ç»“æœï¼š" class="headerlink" title="æµ‹è¯•ç»“æœï¼š"></a>æµ‹è¯•ç»“æœï¼š</h4><p><img src="/assets/blogImg/716_1.png" alt="enter description here"></p><h3 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h3><p>Sparkè¿˜å¯ä»¥ä½¿ç”¨Kryoåº“ï¼ˆç‰ˆæœ¬2ï¼‰æ¥æ›´å¿«åœ°åºåˆ—åŒ–å¯¹è±¡ã€‚Kryoæ¯”Javaä¸²è¡ŒåŒ–ï¼ˆé€šå¸¸å¤šè¾¾10å€ï¼‰è¦å¿«å¾—å¤šï¼Œä¹Ÿæ›´ç´§å‡‘ï¼Œä½†æ˜¯ä¸æ”¯æŒæ‰€æœ‰å¯ä¸²è¡ŒåŒ–ç±»å‹ï¼Œå¹¶ä¸”è¦æ±‚æ‚¨æå‰æ³¨å†Œæ‚¨å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„ç±»ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚</p><h4 id="æµ‹è¯•ä»£ç ï¼š-1"><a href="#æµ‹è¯•ä»£ç ï¼š-1" class="headerlink" title="æµ‹è¯•ä»£ç ï¼š"></a>æµ‹è¯•ä»£ç ï¼š</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/716_2.png" alt="enter description here"><br>æµ‹è¯•ç»“æœä¸­å‘ç°ï¼Œä½¿ç”¨ Kryo serialization çš„åºåˆ—åŒ–å¯¹è±¡ æ¯”ä½¿ç”¨ Java serializationçš„åºåˆ—åŒ–å¯¹è±¡è¦å¤§ï¼Œä¸æè¿°çš„ä¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ<br>æŸ¥æ‰¾å®˜ç½‘ï¼Œå‘ç°è¿™ä¹ˆä¸€å¥è¯ Finally, if you donâ€™t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.ã€‚<br>ä¿®æ”¹ä»£ç ååœ¨æµ‹è¯•ä¸€æ¬¡<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">      .registerKryoClasses(Array(classOf[Student])) // å°†è‡ªå®šä¹‰çš„ç±»æ³¨å†Œåˆ°Kryo</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="æµ‹è¯•ç»“æœï¼š-1"><a href="#æµ‹è¯•ç»“æœï¼š-1" class="headerlink" title="æµ‹è¯•ç»“æœï¼š"></a>æµ‹è¯•ç»“æœï¼š</h4><p><img src="/assets/blogImg/716_3.png" alt="enter description here"></p><h3 id="æ€»ç»“ï¼š"><a href="#æ€»ç»“ï¼š" class="headerlink" title="æ€»ç»“ï¼š"></a>æ€»ç»“ï¼š</h3><p>Kryo serialization æ€§èƒ½å’Œåºåˆ—åŒ–å¤§å°éƒ½æ¯”é»˜è®¤æä¾›çš„ Java serialization è¦å¥½ï¼Œä½†æ˜¯ä½¿ç”¨Kryoéœ€è¦å°†è‡ªå®šä¹‰çš„ç±»å…ˆæ³¨å†Œè¿›å»ï¼Œä½¿ç”¨èµ·æ¥æ¯”Java serializationéº»çƒ¦ã€‚è‡ªä»Spark 2.0.0ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ç®€å•ç±»å‹ã€ç®€å•ç±»å‹æ•°ç»„æˆ–å­—ç¬¦ä¸²ç±»å‹çš„ç®€å•ç±»å‹æ¥è°ƒæ•´RDDsæ—¶ï¼Œåœ¨å†…éƒ¨ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨ã€‚<br>é€šè¿‡æŸ¥æ‰¾sparkcontextåˆå§‹åŒ–çš„æºç ï¼Œå¯ä»¥å‘ç°æŸäº›ç±»å‹å·²ç»åœ¨sparkcontextåˆå§‹åŒ–çš„æ—¶å€™è¢«æ³¨å†Œè¿›å»ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line"> * Component which configures serialization, compression and encryption for various Spark</span><br><span class="line"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span><br><span class="line"> */</span><br><span class="line">private[spark] class SerializerManager(</span><br><span class="line">    defaultSerializer: Serializer,</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    encryptionKey: Option[Array[Byte]]) &#123;</span><br><span class="line"></span><br><span class="line">  def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None)</span><br><span class="line"></span><br><span class="line">  private[this] val kryoSerializer = new KryoSerializer(conf)</span><br><span class="line"></span><br><span class="line">  private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]]</span><br><span class="line">  private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123;</span><br><span class="line">    val primitiveClassTags = Set[ClassTag[_]](</span><br><span class="line">      ClassTag.Boolean,</span><br><span class="line">      ClassTag.Byte,</span><br><span class="line">      ClassTag.Char,</span><br><span class="line">      ClassTag.Double,</span><br><span class="line">      ClassTag.Float,</span><br><span class="line">      ClassTag.Int,</span><br><span class="line">      ClassTag.Long,</span><br><span class="line">      ClassTag.Null,</span><br><span class="line">      ClassTag.Short</span><br><span class="line">    )</span><br><span class="line">    val arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">    primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming çŠ¶æ€ç®¡ç†å‡½æ•°ï¼Œä½ äº†è§£å—</title>
      <link href="/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Thu May 16 2019 19:43:16 GMT+0800 (GMT+08:00) --><a id="more"></a><p>Spark Streaming çŠ¶æ€ç®¡ç†å‡½æ•°åŒ…æ‹¬updateStateByKeyå’ŒmapWithState</p><h4 id="ä¸€ã€updateStateByKey"><a href="#ä¸€ã€updateStateByKey" class="headerlink" title="ä¸€ã€updateStateByKey"></a>ä¸€ã€updateStateByKey</h4><p>å®˜ç½‘åŸè¯ï¼šIn every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p><p>ç»Ÿè®¡å…¨å±€çš„keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å°±ç®—æ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä»–ä¹Ÿä¼šåœ¨æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶å€™è¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ã€‚</p><p>è¿™æ ·çš„ç¼ºç‚¹ï¼šå¦‚æœæ•°æ®é‡å¤ªå¤§çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦checkpointæ•°æ®ä¼šå ç”¨è¾ƒå¤§çš„å­˜å‚¨ã€‚è€Œä¸”æ•ˆç‡ä¹Ÿä¸é«˜<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//[root@bda3 ~]# nc -lk 9999  </span><br><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //æ³¨æ„ï¼šupdateStateByKeyå¿…é¡»è®¾ç½®checkpointç›®å½•  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start()  // ä¸€å®šè¦å†™  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">  /*çŠ¶æ€æ›´æ–°å‡½æ•°  </span><br><span class="line">  * @param currentValues  keyç›¸åŒvalueå½¢æˆçš„åˆ—è¡¨  </span><br><span class="line">  * @param preValues      keyå¯¹åº”çš„valueï¼Œå‰ä¸€çŠ¶æ€  </span><br><span class="line">  * */  </span><br><span class="line">  def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seqåˆ—è¡¨ä¸­æ‰€æœ‰valueæ±‚å’Œ  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //è·å–ä¸Šä¸€çŠ¶æ€å€¼  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="äºŒã€mapWithState-æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨"><a href="#äºŒã€mapWithState-æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨" class="headerlink" title="äºŒã€mapWithState  (æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨)"></a>äºŒã€mapWithState (æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨)</h4><p>mapWithStateï¼šä¹Ÿæ˜¯ç”¨äºå…¨å±€ç»Ÿè®¡keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å®ƒå¦‚æœæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä¾¿ä¸ä¼šè¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ï¼Œæœ‰ä¸€ç‚¹å¢é‡çš„æ„Ÿè§‰ã€‚</p><p>è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åªæ˜¯å…³å¿ƒé‚£äº›å·²ç»å‘ç”Ÿçš„å˜åŒ–çš„keyï¼Œå¯¹äºæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œåˆ™ä¸ä¼šè¿”å›é‚£äº›æ²¡æœ‰å˜åŒ–çš„keyçš„æ•°æ®ã€‚è¿™æ ·çš„è¯ï¼Œå³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œcheckpointä¹Ÿä¸ä¼šåƒupdateStateByKeyé‚£æ ·ï¼Œå ç”¨å¤ªå¤šçš„å­˜å‚¨ã€‚</p><p>å®˜æ–¹ä»£ç å¦‚ä¸‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/**  </span><br><span class="line"> * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every  </span><br><span class="line"> * second starting with initial value of word count.  </span><br><span class="line"> * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;  </span><br><span class="line"> *   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive  </span><br><span class="line"> *   data.  </span><br><span class="line"> *  </span><br><span class="line"> * To run this on your local machine, you need to first run a Netcat server  </span><br><span class="line"> *    `$ nc -lk 9999`  </span><br><span class="line"> * and then run the example  </span><br><span class="line"> *    `$ bin/run-example  </span><br><span class="line"> *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`  </span><br><span class="line"> */  </span><br><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;)  </span><br><span class="line">    // Create the context with a 1 second batch size  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)  </span><br><span class="line"></span><br><span class="line">    // Initial state RDD for mapWithState operation  </span><br><span class="line">    val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1)))  </span><br><span class="line"></span><br><span class="line">    // Create a ReceiverInputDStream on target ip:port and count the  </span><br><span class="line">    // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;)  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    // Update the cumulative count using mapWithState  </span><br><span class="line">    // This will give a DStream made of state (which is the cumulative count of the words)  </span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">      StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Sparkå’ŒDL/AIç»“åˆï¼Œè°ä¸äº‰é”‹? æœŸå¾…Spark3.0çš„åˆ°æ¥ï¼</title>
      <link href="/2018/06/22/AI%E7%BB%93%E5%90%88%EF%BC%8C%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/"/>
      <url>/2018/06/22/AI%E7%BB%93%E5%90%88%EF%BC%8C%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:22 GMT+0800 (GMT+08:00) --><p><img src="/assets/blogImg/0622_1.png" alt="enter description here"><br><a id="more"></a></p><p>ä¸çŸ¥å„ä½ï¼Œæ˜¯å¦å…³æ³¨ç¤¾åŒºçš„å‘å±•ï¼Ÿå…³æ³¨Sparkå‘¢ï¼Ÿ</p><p>å®˜ç½‘çš„Sparkå›¾æ ‡å’Œè§£é‡Šè¯­å·²ç»å‘ç”Ÿå˜åŒ–äº†ã€‚</p><p>ç„¶è€Œåœ¨6-18å·ï¼Œç¤¾åŒºæå‡ºSpark and DL/AIç›¸ç»“åˆï¼Œè¿™æ— æ¯”å†ä¸€æ¬¡è¯´æ˜ï¼ŒSparkåœ¨å¤§æ•°æ®çš„åœ°ä½æ˜¯æ— æ³•æ’¼åŠ¨çš„ï¼æœŸå¾…Spark3.0çš„åˆ°æ¥ï¼</p><p>æ¥ä¸‹æ¥å¯¹SPARK-24579çš„ç¿»è¯‘:</p><p>åœ¨å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½çš„åå­—è·¯å£ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†Apache Sparkä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åˆ†æå¼•æ“ä»¥åŠAIæ¡†æ¶å¦‚TensorFlowå’ŒApache MXNet (æ­£åœ¨å­µåŒ–ä¸­)çš„å…´èµ·åŠè¿™ä¸¤å¤§å—çš„å·¨å¤§æˆåŠŸ ã€‚</p><p>å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½éƒ½æ˜¯æ¨åŠ¨ä¼ä¸šåˆ›æ–°çš„ä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ï¼Œ ä¸¤ä¸ªç¤¾åŒºçš„å¤šæ¬¡å°è¯•ï¼Œä½¿ä»–ä»¬ç»“åˆåœ¨ä¸€èµ·ã€‚</p><p>æˆ‘ä»¬çœ‹åˆ°AIç¤¾åŒºçš„åŠªåŠ›ï¼Œä¸ºAIæ¡†æ¶å®ç°æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œå¦‚TF.DATAå’ŒTF.Trorã€‚ç„¶è€Œï¼Œ50+ä¸ªæ•°æ®æºå’Œå†…ç½®SQLã€æ•°æ®æµå’Œæµç‰¹å¾ï¼ŒSparkä»ç„¶æ˜¯å¯¹äºå¤§æ•°æ®ç¤¾åŒºé€‰æ‹©ã€‚</p><p>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çœ‹åˆ°è®¸å¤šåŠªåŠ›,å°†DL/AIæ¡†æ¶ä¸Sparkç»“åˆèµ·æ¥ï¼Œä»¥åˆ©ç”¨å®ƒçš„åŠ›é‡ï¼Œä¾‹å¦‚ï¼ŒSparkæ•°æ®æºTFRecordsã€TensorFlowOnSpark, TensorFramesç­‰ã€‚ä½œä¸ºé¡¹ç›®Hydrogençš„ä¸€éƒ¨åˆ†ï¼Œè¿™ä¸ªSPIPå°†Spark+AIä»ä¸åŒçš„è§’åº¦ç»Ÿä¸€èµ·æ¥ã€‚</p><p>æ²¡æœ‰åœ¨Sparkå’Œå¤–éƒ¨DL/AIæ¡†æ¶ä¹‹é—´äº¤æ¢æ•°æ®ï¼Œè¿™äº›é›†æˆéƒ½æ˜¯ä¸å¯èƒ½çš„,ä¹Ÿæœ‰æ€§èƒ½é—®é¢˜ã€‚ç„¶è€Œï¼Œç›®å‰è¿˜æ²¡æœ‰ä¸€ç§æ ‡å‡†çš„æ–¹å¼æ¥äº¤æ¢æ•°æ®ï¼Œå› æ­¤å®ç°å’Œæ€§èƒ½ä¼˜åŒ–å°±é™·å…¥äº†å›°å¢ƒã€‚ä¾‹å¦‚ï¼Œåœ¨Pythonä¸­ï¼ŒTensorFlowOnSparkä½¿ç”¨Hadoop InputFormat/OutputFormatä½œä¸ºTensorFlowçš„TFRecordsï¼Œæ¥åŠ è½½å’Œä¿å­˜æ•°æ®ï¼Œå¹¶å°†RDDæ•°æ®ä¼ é€’ç»™TensorFlowã€‚TensorFramesä½¿ç”¨TensorFlowçš„Java APIï¼Œè½¬æ¢ä¸º Spark DataFrames Rows to/from TensorFlow Tensors ã€‚æˆ‘ä»¬æ€æ ·æ‰èƒ½é™ä½å¤æ‚æ€§å‘¢?</p><p>è¿™é‡Œçš„å»ºè®®æ˜¯æ ‡å‡†åŒ–Sparkå’ŒDL/AIæ¡†æ¶ä¹‹é—´çš„æ•°æ®äº¤æ¢æ¥å£(æˆ–æ ¼å¼)ï¼Œå¹¶ä¼˜åŒ–ä»/åˆ°è¿™ä¸ªæ¥å£çš„æ•°æ®è½¬æ¢ã€‚å› æ­¤ï¼ŒDL/AIæ¡†æ¶å¯ä»¥åˆ©ç”¨Sparkä»ä»»ä½•åœ°æ–¹åŠ è½½æ•°æ®ï¼Œè€Œæ— éœ€èŠ±è´¹é¢å¤–çš„ç²¾åŠ›æ„å»ºå¤æ‚çš„æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œæ¯”å¦‚ä»ç”Ÿäº§æ•°æ®ä»“åº“è¯»å–ç‰¹æ€§æˆ–æµæ¨¡å‹æ¨æ–­ã€‚Sparkç”¨æˆ·å¯ä»¥ä½¿ç”¨DL/AIæ¡†æ¶ï¼Œè€Œæ— éœ€å­¦ä¹ é‚£é‡Œå®ç°çš„ç‰¹å®šæ•°æ®apiã€‚è€Œä¸”åŒæ–¹çš„å¼€å‘äººå‘˜éƒ½å¯ä»¥ç‹¬ç«‹åœ°è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼Œå› ä¸ºæ¥å£æœ¬èº«ä¸ä¼šå¸¦æ¥å¾ˆå¤§çš„å¼€é”€ã€‚</p><p>ISSUE: <a href="https://issues.apache.org/jira/browse/SPARK-24579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24579</a><br>è‹¥æ³½æ•°æ®ï¼Œæ˜Ÿæ˜Ÿæœ¬äººæ°´å¹³æœ‰é™ï¼Œç¿»è¯‘å¤šå¤šåŒ…æ¶µã€‚</p><p>å¯¹äº†å¿˜è®°è¯´äº†ï¼Œæœ¬ISSUEæœ‰ä¸ªPDFæ–‡æ¡£ï¼Œèµ¶å¿«å»ä¸‹è½½å§ã€‚<br><a href="https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf" target="_blank" rel="noopener">https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç”Ÿäº§å¼€å‘å¿…ç”¨-Spark RDDè½¬DataFrameçš„ä¸¤ç§æ–¹æ³•</title>
      <link href="/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
      <url>/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><p>æœ¬ç¯‡æ–‡ç« å°†ä»‹ç»Spark SQLä¸­çš„DataFrameï¼Œå…³äºDataFrameçš„ä»‹ç»å¯ä»¥å‚è€ƒ:<br><a href="https://blog.csdn.net/lemonzhaotao/article/details/80211231" target="_blank" rel="noopener">https://blog.csdn.net/lemonzhaotao/article/details/80211231</a></p><p>åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œå°†ä»‹ç»RDDè½¬æ¢ä¸ºDataFrameçš„2ç§æ–¹å¼</p><p>å®˜ç½‘ä¹‹RDDè½¬DF:<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds</a><br><a id="more"></a><br>DataFrame ä¸ RDD çš„äº¤äº’</p><p>Spark SQLå®ƒæ”¯æŒä¸¤ç§ä¸åŒçš„æ–¹å¼è½¬æ¢å·²ç»å­˜åœ¨çš„RDDåˆ°DataFrame</p><h4 id="æ–¹æ³•ä¸€"><a href="#æ–¹æ³•ä¸€" class="headerlink" title="æ–¹æ³•ä¸€"></a>æ–¹æ³•ä¸€</h4><p>ç¬¬ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨åå°„çš„æ–¹å¼ï¼Œç”¨åå°„å»æ¨å€’å‡ºæ¥RDDé‡Œé¢çš„schemaã€‚è¿™ä¸ªæ–¹å¼ç®€å•ï¼Œä½†æ˜¯ä¸å»ºè®®ä½¿ç”¨ï¼Œå› ä¸ºåœ¨å·¥ä½œå½“ä¸­ï¼Œä½¿ç”¨è¿™ç§æ–¹å¼æ˜¯æœ‰é™åˆ¶çš„ã€‚<br>å¯¹äºä»¥å‰çš„ç‰ˆæœ¬æ¥è¯´ï¼Œcase classæœ€å¤šæ”¯æŒ22ä¸ªå­—æ®µå¦‚æœè¶…è¿‡äº†22ä¸ªå­—æ®µï¼Œæˆ‘ä»¬å°±å¿…é¡»è¦è‡ªå·±å¼€å‘ä¸€ä¸ªç±»ï¼Œå®ç°productæ¥å£æ‰è¡Œã€‚å› æ­¤è¿™ç§æ–¹å¼è™½ç„¶ç®€å•ï¼Œä½†æ˜¯ä¸é€šç”¨ï¼›å› ä¸ºç”Ÿäº§ä¸­çš„å­—æ®µæ˜¯éå¸¸éå¸¸å¤šçš„ï¼Œæ˜¯ä¸å¯èƒ½åªæœ‰20æ¥ä¸ªå­—æ®µçš„ã€‚<br>ç¤ºä¾‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 1</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runInferSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;)</span><br><span class="line">  val df = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">              .map(x =&gt; People(x(0), x(1).trim.toInt))  //å°†rddçš„æ¯ä¸€è¡Œéƒ½è½¬æ¢æˆäº†ä¸€ä¸ªpeople</span><br><span class="line">              .toDF         //å¿…é¡»å…ˆå¯¼å…¥import spark.implicits._  ä¸ç„¶è¿™ä¸ªæ–¹æ³•ä¼šæŠ¥é”™</span><br><span class="line">  df.show()</span><br><span class="line">  df.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">  // è¿™ä¸ªDFåŒ…å«äº†ä¸¤ä¸ªå­—æ®µnameå’Œage</span><br><span class="line">  val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">  // teenager(0)ä»£è¡¨ç¬¬ä¸€ä¸ªå­—æ®µ</span><br><span class="line">  // å–å€¼çš„ç¬¬ä¸€ç§æ–¹å¼ï¼šindex from zero</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line">  // å–å€¼çš„ç¬¬äºŒç§æ–¹å¼ï¼šbyName</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()</span><br><span class="line">&#125;</span><br><span class="line">// æ³¨æ„ï¼šcase classå¿…é¡»å®šä¹‰åœ¨mainæ–¹æ³•ä¹‹å¤–ï¼›å¦åˆ™ä¼šæŠ¥é”™</span><br><span class="line">case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p></p><h4 id="æ–¹æ³•äºŒ"><a href="#æ–¹æ³•äºŒ" class="headerlink" title="æ–¹æ³•äºŒ"></a>æ–¹æ³•äºŒ</h4><p>åˆ›å»ºä¸€ä¸ªDataFrameï¼Œä½¿ç”¨ç¼–ç¨‹çš„æ–¹å¼ è¿™ä¸ªæ–¹å¼ç”¨çš„éå¸¸å¤šã€‚é€šè¿‡ç¼–ç¨‹æ–¹å¼æŒ‡å®šschema ï¼Œå¯¹äºç¬¬ä¸€ç§æ–¹å¼çš„schemaå…¶å®å®šä¹‰åœ¨äº†case classé‡Œé¢äº†ã€‚<br>å®˜ç½‘è§£è¯»ï¼š<br>å½“æˆ‘ä»¬çš„case classä¸èƒ½æå‰å®šä¹‰(å› ä¸ºä¸šåŠ¡å¤„ç†çš„è¿‡ç¨‹å½“ä¸­ï¼Œä½ çš„å­—æ®µå¯èƒ½æ˜¯åœ¨å˜åŒ–çš„),å› æ­¤ä½¿ç”¨case classå¾ˆéš¾å»æå‰å®šä¹‰ã€‚<br>ä½¿ç”¨è¯¥æ–¹å¼åˆ›å»ºDFçš„ä¸‰å¤§æ­¥éª¤ï¼š</p><ul><li>Create an RDD of Rows from the original RDD;</li><li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li><li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>ç¤ºä¾‹ï¼š<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 2</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  // 1.è½¬æˆRDD</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;)</span><br><span class="line">  // 2.å®šä¹‰schemaï¼Œå¸¦æœ‰StructTypeçš„</span><br><span class="line">  // å®šä¹‰schemaä¿¡æ¯</span><br><span class="line">  val schemaString = &quot;name age&quot;</span><br><span class="line">  // å¯¹schemaä¿¡æ¯æŒ‰ç©ºæ ¼è¿›è¡Œåˆ†å‰²</span><br><span class="line">  // æœ€ç»ˆfiledsé‡ŒåŒ…å«äº†2ä¸ªStructField</span><br><span class="line">  val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">                            // å­—æ®µç±»å‹ï¼Œå­—æ®µåç§°åˆ¤æ–­æ˜¯ä¸æ˜¯ä¸ºç©º</span><br><span class="line">                           .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">  val schema = StructType(fields)</span><br><span class="line">  // 3.æŠŠæˆ‘ä»¬çš„schemaä¿¡æ¯ä½œç”¨åˆ°RDDä¸Š</span><br><span class="line">  //   è¿™ä¸ªRDDé‡Œé¢åŒ…å«äº†ä¸€äº›è¡Œ</span><br><span class="line">  // å½¢æˆRowç±»å‹çš„RDD</span><br><span class="line">  val rowRDD = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">                  .map(x =&gt; Row(x(0), x(1).trim))</span><br><span class="line">  // é€šè¿‡SparkSessionåˆ›å»ºä¸€ä¸ªDataFrame</span><br><span class="line">  // ä¼ è¿›æ¥ä¸€ä¸ªrowRDDå’Œschemaï¼Œå°†schemaä½œç”¨åˆ°rowRDDä¸Š</span><br><span class="line">  val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="æ‰©å±•-ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹"><a href="#æ‰©å±•-ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹" class="headerlink" title="[æ‰©å±•]ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹"></a>[æ‰©å±•]ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹</h4><p>åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å…¶å®é€‰æ‹©çš„æ˜¯æ–¹å¼äºŒè¿™ç§è¿›è¡Œåˆ›å»ºDataFrameçš„ï¼Œè¿™é‡Œå°†å±•ç¤ºéƒ¨åˆ†ä»£ç ï¼š</p><h4 id="Schemaçš„å®šä¹‰"><a href="#Schemaçš„å®šä¹‰" class="headerlink" title="Schemaçš„å®šä¹‰"></a>Schemaçš„å®šä¹‰</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">object AccessConvertUtil &#123;</span><br><span class="line">  val struct = StructType(</span><br><span class="line">    Array(</span><br><span class="line">      StructField(&quot;url&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsType&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsId&quot;,LongType),</span><br><span class="line">      StructField(&quot;traffic&quot;,LongType),</span><br><span class="line">      StructField(&quot;ip&quot;,StringType),</span><br><span class="line">      StructField(&quot;city&quot;,StringType),</span><br><span class="line">      StructField(&quot;time&quot;,StringType),</span><br><span class="line">      StructField(&quot;day&quot;,StringType)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  /**</span><br><span class="line">    * æ ¹æ®è¾“å…¥çš„æ¯ä¸€è¡Œä¿¡æ¯è½¬æ¢æˆè¾“å‡ºçš„æ ·å¼</span><br><span class="line">    */</span><br><span class="line">  def parseLog(log:String) = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val splits = log.split(&quot;\t&quot;)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      val traffic = splits(2).toLong</span><br><span class="line">      val ip = splits(3)</span><br><span class="line">      val domain = &quot;http://www.imooc.com/&quot;</span><br><span class="line">      val cms = url.substring(url.indexOf(domain) + domain.length)</span><br><span class="line">      val cmsTypeId = cms.split(&quot;/&quot;)</span><br><span class="line">      var cmsType = &quot;&quot;</span><br><span class="line">      var cmsId = 0l</span><br><span class="line">      if (cmsTypeId.length &gt; 1) &#123;</span><br><span class="line">        cmsType = cmsTypeId(0)</span><br><span class="line">        cmsId = cmsTypeId(1).toLong</span><br><span class="line">      &#125;</span><br><span class="line">      val city = IpUtils.getCity(ip)</span><br><span class="line">      val time = splits(0)</span><br><span class="line">      val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;)</span><br><span class="line">      //è¿™ä¸ªRowé‡Œé¢çš„å­—æ®µè¦å’Œstructä¸­çš„å­—æ®µå¯¹åº”ä¸Š</span><br><span class="line">      Row(url, cmsType, cmsId, traffic, ip, city, time, day)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt; Row(0)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="åˆ›å»ºDataFrame"><a href="#åˆ›å»ºDataFrame" class="headerlink" title="åˆ›å»ºDataFrame"></a>åˆ›å»ºDataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object SparkStatCleanJob &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;)</span><br><span class="line">    //accessRDD.take(10).foreach(println)</span><br><span class="line">    //RDD ==&gt; DFï¼Œåˆ›å»ºç”ŸæˆDataFrame</span><br><span class="line">    val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)),</span><br><span class="line">      AccessConvertUtil.struct)</span><br><span class="line">    accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite)</span><br><span class="line">            .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æœ€å‰æ²¿ï¼å¸¦ä½ è¯»Structured Streamingé‡é‡çº§è®ºæ–‡ï¼</title>
      <link href="/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/"/>
      <url>/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-è®ºæ–‡ä¸‹è½½åœ°å€"><a href="#1-è®ºæ–‡ä¸‹è½½åœ°å€" class="headerlink" title="1.è®ºæ–‡ä¸‹è½½åœ°å€"></a>1.è®ºæ–‡ä¸‹è½½åœ°å€</h4><p><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf</a></p><h4 id="2-å‰è¨€"><a href="#2-å‰è¨€" class="headerlink" title="2.å‰è¨€"></a>2.å‰è¨€</h4><p>å»ºè®®é¦–å…ˆé˜…è¯»Structured Streamingå®˜ç½‘ï¼š<a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a><br>ä»¥åŠè¿™ä¸¤ç¯‡Databricksåœ¨2016å¹´å…³äºStructured Streamingçš„æ–‡ç« ï¼š</p><p><a href="https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html</a></p><p><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</a></p><p>è¨€å½’æ­£ä¼ <br>è¯¥è®ºæ–‡æ”¶å½•è‡ª2018å¹´ACM SIGMODä¼šè®®ï¼Œæ˜¯ç”±ç¾å›½è®¡ç®—æœºåä¼šï¼ˆACMï¼‰å‘èµ·çš„ã€åœ¨æ•°æ®åº“é¢†åŸŸå…·æœ‰æœ€é«˜å­¦æœ¯åœ°ä½çš„å›½é™…æ€§å­¦æœ¯ä¼šè®®ã€‚è®ºæ–‡çš„ä½œè€…ä¸ºDatabricksçš„å·¥ç¨‹å¸ˆåŠSparkçš„å¼€å‘è€…ï¼Œå…¶æƒå¨æ€§ã€é‡è¦ç¨‹åº¦ä¸è¨€è€Œå–»ã€‚æ–‡ç« å¼€å¤´ä¸ºè¯¥è®ºæ–‡çš„ä¸‹è½½åœ°å€ï¼Œä¾›è¯»è€…é˜…è¯»äº¤æµã€‚æœ¬æ–‡å¯¹è¯¥è®ºæ–‡è¿›è¡Œç®€è¦çš„æ€»ç»“ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿä¸‹è½½åŸæ–‡ç»†ç»†å“è¯»ï¼Œäº†è§£æœ€å‰æ²¿çš„å¤§æ•°æ®æŠ€æœ¯ã€‚</p><h4 id="3-è®ºæ–‡ç®€è¦æ€»ç»“"><a href="#3-è®ºæ–‡ç®€è¦æ€»ç»“" class="headerlink" title="3.è®ºæ–‡ç®€è¦æ€»ç»“"></a>3.è®ºæ–‡ç®€è¦æ€»ç»“</h4><p>é¢˜ç›®ï¼šStructured Streaming: A Declarative API for Real-Time Applications in Apache Spark</p><h5 id="3-1-æ‘˜è¦"><a href="#3-1-æ‘˜è¦" class="headerlink" title="3.1 æ‘˜è¦"></a>3.1 æ‘˜è¦</h5><p>æ‘˜è¦æ˜¯ä¸€ç¯‡è®ºæ–‡çš„ç²¾é«“ï¼Œè¿™é‡Œç»™å‡ºæ‘˜è¦å®Œæ•´çš„ç¿»è¯‘ã€‚<br>éšç€å®æ—¶æ•°æ®çš„æ™®éå­˜åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å¯æ‰©å±•çš„ã€æ˜“ç”¨çš„ã€æ˜“äºé›†æˆçš„æµå¼å¤„ç†ç³»ç»Ÿã€‚ç»“æ„åŒ–æµæ˜¯åŸºäºæˆ‘ä»¬å¯¹Spark Streamingçš„ç»éªŒå¼€å‘å‡ºæ¥çš„é«˜çº§åˆ«çš„Sparkæµå¼APIã€‚ç»“æ„åŒ–æµä¸å…¶ä»–ç°æœ‰çš„æµå¼APIï¼Œå¦‚è°·æ­Œçš„Dataflowï¼Œä¸»è¦æœ‰ä¸¤ç‚¹ä¸åŒã€‚ç¬¬ä¸€ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºè‡ªåŠ¨å¢é‡åŒ–çš„å…³ç³»å‹æŸ¥è¯¢APIï¼Œæ— éœ€ç”¨æˆ·è‡ªå·±æ„å»ºDAGï¼›ç¬¬äºŒï¼Œç»“æ„åŒ–æµæ—¨åœ¨äºæ”¯æŒç«¯åˆ°ç«¯çš„å®æ—¶åº”ç”¨å¹¶æ•´åˆæµä¸æ‰¹å¤„ç†çš„äº¤äº’åˆ†æã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ•´åˆæ˜¯ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ã€‚ç»“æ„åŒ–æµé€šè¿‡Spark SQLçš„ä»£ç ç”Ÿæˆå¼•æ“å®ç°äº†å¾ˆé«˜çš„æ€§èƒ½ï¼Œæ˜¯Apache Flinkçš„ä¸¤å€ä»¥åŠApache Kafkaçš„90å€ã€‚å®ƒè¿˜æä¾›äº†ä¸°å¯Œçš„è¿è¡Œç‰¹æ€§ï¼Œå¦‚å›æ»šã€ä»£ç æ›´æ–°ä»¥åŠæµ/æ‰¹æ··åˆæ‰§è¡Œã€‚æœ€åæˆ‘ä»¬æè¿°äº†ç³»ç»Ÿçš„è®¾è®¡ä»¥åŠéƒ¨ç½²åœ¨Databrickså‡ ç™¾ä¸ªç”Ÿäº§èŠ‚ç‚¹çš„ä¸€ä¸ªç”¨ä¾‹ã€‚</p><h5 id="3-2-æµå¼å¤„ç†é¢ä¸´çš„æŒ‘æˆ˜"><a href="#3-2-æµå¼å¤„ç†é¢ä¸´çš„æŒ‘æˆ˜" class="headerlink" title="3.2 æµå¼å¤„ç†é¢ä¸´çš„æŒ‘æˆ˜"></a>3.2 æµå¼å¤„ç†é¢ä¸´çš„æŒ‘æˆ˜</h5><p>(1) å¤æ‚ã€ä½çº§åˆ«çš„API<br>(2) ç«¯åˆ°ç«¯åº”ç”¨çš„é›†æˆ<br>(3) è¿è¡Œæ—¶æŒ‘æˆ˜ï¼šå®¹ç¾ï¼Œä»£ç æ›´æ–°ï¼Œç›‘æ§ç­‰<br>(4) æˆæœ¬å’Œæ€§èƒ½æŒ‘æˆ˜</p><h5 id="3-3-ç»“æ„åŒ–æµåŸºæœ¬æ¦‚å¿µ"><a href="#3-3-ç»“æ„åŒ–æµåŸºæœ¬æ¦‚å¿µ" class="headerlink" title="3.3 ç»“æ„åŒ–æµåŸºæœ¬æ¦‚å¿µ"></a>3.3 ç»“æ„åŒ–æµåŸºæœ¬æ¦‚å¿µ</h5><p><img src="/assets/blogImg/614_1.png" alt="enter description here"><br>å›¾1 ç»“æ„åŒ–æµçš„ç»„æˆéƒ¨åˆ†</p><p>(1) Input and Output<br>Input sources å¿…é¡»æ˜¯ replayable çš„ï¼Œæ”¯æŒèŠ‚ç‚¹å®•æœºåä»å½“å‰è¾“å…¥ç»§ç»­è¯»å–ã€‚ä¾‹å¦‚ï¼šApache Kinesiså’ŒApache Kafkaã€‚<br>Output sinks å¿…é¡»æ”¯æŒ idempotent ï¼ˆå¹‚ç­‰ï¼‰ï¼Œç¡®ä¿åœ¨èŠ‚ç‚¹å®•æœºæ—¶å¯é çš„æ¢å¤ã€‚<br>(2) APIs<br>ç¼–å†™ç»“æ„åŒ–æµç¨‹åºæ—¶ï¼Œå¯ä»¥ä½¿ç”¨Spark SQLçš„APIsï¼šDataFrameå’ŒSQLæ¥æŸ¥è¯¢streamså’Œtablesï¼Œè¯¥æŸ¥è¯¢å®šä¹‰äº†ä¸€ä¸ªoutput tableï¼ˆè¾“å‡ºè¡¨ï¼‰ï¼Œç”¨æ¥æ¥æ”¶æ¥è‡ªsteamçš„æ•°æ®ã€‚engineå†³å®šå¦‚ä½•è®¡ç®—å¹¶å°†è¾“å‡ºè¡¨ incrementallyï¼ˆå¢é‡åœ°ï¼‰å†™å…¥sinkã€‚ä¸åŒçš„sinksæ”¯æŒä¸åŒçš„output modesï¼ˆè¾“å‡ºæ¨¡å¼ï¼Œåé¢ä¼šæåˆ°ï¼‰ã€‚<br>ä¸ºäº†å¤„ç†æµå¼æ•°æ®ï¼Œç»“æ„åŒ–æµè¿˜å¢åŠ äº†ä¸€äº›APIsä¸å·²æœ‰çš„Spark SQL APIç›¸é…åˆï¼š</p><ul><li>a. Triggers æ§åˆ¶engineå¤šä¹…æ‰§è¡Œä¸€æ¬¡è®¡ç®—</li><li>b. event time æ˜¯æ•°æ®æºçš„æ—¶é—´æˆ³ï¼›watermark ç­–ç•¥ï¼Œä¸event time ç›¸å·®ä¸€æ®µæ—¶é—´åä¸å†æ¥æ”¶æ•°æ®ã€‚</li><li>c.Stateful operatorï¼ˆçŠ¶æ€ç®—å­ï¼‰ï¼Œç±»ä¼¼äºSpark Streaming çš„updateStateByKeyã€‚</li></ul><p>(3) æ‰§è¡Œ<br>ä¸€æ—¦æ¥æ”¶åˆ°äº†æŸ¥è¯¢ï¼Œç»“æ„åŒ–æµå°±ä¼šè¿›è¡Œä¼˜åŒ–é€’å¢ï¼Œå¹¶å¼€å§‹æ‰§è¡Œã€‚ç»“æ„åŒ–æµä½¿ç”¨ä¸¤ç§æŒä¹…åŒ–å­˜å‚¨çš„æ–¹å¼å®ç°å®¹é”™ï¼š</p><ul><li>a.write-ahead log ï¼ˆWALï¼šé¢„å†™æ—¥å¿—ï¼‰æŒç»­è¿½è¸ªå“ªäº›æ•°æ®å·²è¢«æ‰§è¡Œï¼Œç¡®ä¿æ•°æ®çš„å¯é å†™å…¥ã€‚</li><li>b.ç³»ç»Ÿé‡‡ç”¨å¤§è§„æ¨¡çš„ state storeï¼ˆçŠ¶æ€å­˜å‚¨ï¼‰æ¥ä¿å­˜é•¿æ—¶é—´è¿è¡Œçš„èšåˆç®—å­çš„ç®—å­çŠ¶æ€å¿«ç…§ã€‚</li></ul><h5 id="3-4-ç¼–ç¨‹æ¨¡å‹"><a href="#3-4-ç¼–ç¨‹æ¨¡å‹" class="headerlink" title="3.4 ç¼–ç¨‹æ¨¡å‹"></a>3.4 ç¼–ç¨‹æ¨¡å‹</h5><p>ç»“æ„åŒ–æµå°†è°·æ­Œçš„Dataflowã€å¢é‡æŸ¥è¯¢å’ŒSpark Streaming ç»“åˆèµ·æ¥ï¼Œä»¥ä¾¿åœ¨Spark SQLä¸‹å®ç°æµå¼å¤„ç†ã€‚</p><ul><li>a. A Short Example<pre><code>é¦–å…ˆä»ä¸€ä¸ªæ‰¹å¤„ç†ä½œä¸šå¼€å§‹ï¼Œç»Ÿè®¡ä¸€ä¸ªwebåº”ç”¨åœ¨ä¸åŒå›½å®¶çš„ç‚¹å‡»æ•°ã€‚å‡è®¾è¾“å…¥æ•°æ®æ˜¯ä¸€ä¸ªJSONæ–‡ä»¶ï¼Œè¾“å‡ºä¸€ä¸ªParquetæ–‡ä»¶ï¼Œè¯¥ä½œä¸šå¯ä»¥é€šè¿‡DataFrameæ¥å®Œæˆï¼š</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read from static data</span><br><span class="line">2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a static data sink</span><br><span class="line">6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure></li></ul><p>æŠŠè¯¥ä½œä¸šå˜æˆä½¿ç”¨ç»“æ„åŒ–æµä»…ä»…éœ€è¦æ”¹å˜è¾“å…¥å’Œè¾“å‡ºæºï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ–°çš„JSONæ–‡ä»¶continuallyï¼ˆæŒç»­åœ°ï¼‰ä¸Šä¼ ï¼Œæˆ‘ä»¬åªéœ€è¦æ”¹å˜ç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read streaming data</span><br><span class="line">2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a streaming data sink</span><br><span class="line">6counts . writeStream . format (&quot; parquet &quot;)</span><br><span class="line">7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure><p></p><p>ç»“æ„åŒ–æµä¹Ÿæ”¯æŒ windowingï¼ˆçª—å£ï¼‰å’Œé€šè¿‡Spark SQLå·²å­˜åœ¨çš„èšåˆç®—å­å¤„ç†event timeã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹ä¸­é—´çš„ä»£ç ï¼Œè®¡ç®—1å°æ—¶çš„æ»‘åŠ¨çª—å£ï¼Œæ¯äº”åˆ†é’Ÿå‰è¿›ä¸€æ¬¡ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1// Count events by windows on the &quot; time &quot; field</span><br><span class="line">2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()</span><br></pre></td></tr></table></figure><p></p><ul><li>b. ç¼–ç¨‹æ¨¡å‹è¯­ä¹‰<br><img src="/assets/blogImg/614_2.png" alt="enter description here"></li></ul><p>å›¾ 2 ä¸¤ç§è¾“å‡ºæ¨¡å¼</p><ul><li>i. æ¯ä¸€ä¸ªè¾“å…¥æºæä¾›äº†ä¸€ä¸ªåŸºäºæ—¶é—´çš„éƒ¨åˆ†æœ‰åºçš„è®°å½•é›†ï¼ˆset of recordsï¼‰ï¼Œä¾‹å¦‚ï¼ŒKafkaå°†æµå¼æ•°æ®åˆ†ä¸ºå„è‡ªæœ‰åºçš„partitionsã€‚</li><li>ii. ç”¨æˆ·æä¾›è·¨è¾“å…¥æ•°æ®æ‰§è¡Œçš„æŸ¥è¯¢ï¼Œè¯¥è¾“å…¥æ•°æ®å¯ä»¥åœ¨ä»»æ„ç»™å®šçš„å¤„ç†æ—¶é—´ç‚¹è¾“å‡ºä¸€ä¸ª result tableï¼ˆç»“æœè¡¨ï¼‰ã€‚ç»“æ„åŒ–æµæ€»ä¼šäº§ç”Ÿä¸æ‰€æœ‰è¾“å…¥æºçš„æ•°æ®çš„å‰ç¼€ä¸Šï¼ˆprefix of the data in all input sourcesï¼‰æŸ¥è¯¢ç›¸ä¸€è‡´çš„ç»“æœã€‚</li><li>iii. Triggers å‘Šè¯‰ç³»ç»Ÿä½•æ—¶å»è¿è¡Œä¸€ä¸ªæ–°çš„å¢é‡è®¡ç®—ï¼Œä½•æ—¶æ›´æ–°result tableã€‚ä¾‹å¦‚ï¼Œåœ¨å¾®æ‰¹å¤„ç†æ¨¡å¼ï¼Œç”¨æˆ·å¸Œæœ›ä¼šæ¯åˆ†é’Ÿè§¦å‘ä¸€æ¬¡å¢é‡è®¡ç®—ã€‚</li><li>iiii. engineæ”¯æŒä¸‰ç§output modeï¼š<pre><code>  Completeï¼šengineä¸€æ¬¡å†™æ‰€æœ‰result tableã€‚  Appendï¼šengineä»…ä»…å‘sinkå¢åŠ è®°å½•ã€‚  Updateï¼šengineåŸºäºkeyæ›´æ–°æ¯ä¸€ä¸ªrecordï¼Œæ›´æ–°å€¼æ”¹å˜çš„keysã€‚è¯¥æ¨¡å‹æœ‰ä¸¤ä¸ªç‰¹æ€§ï¼šç¬¬ä¸€ï¼Œç»“æœè¡¨çš„å†…å®¹ç‹¬ç«‹äºè¾“å‡ºæ¨¡å¼ã€‚ç¬¬äºŒï¼Œè¯¥æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè¢«ç§°ä¸ºprefix consistencyã€‚</code></pre>c.æµå¼ç®—å­<pre><code>åŠ å…¥äº†ä¸¤ç§ç±»å‹çš„ç®—å­ï¼šwatermarkingç®—å­å‘Šè¯‰ç³»ç»Ÿä½•æ—¶å…³é—­event time windowå’Œè¾“å‡ºç»“æœï¼›ç»“æ„åŒ–æµå…è®¸ç”¨æˆ·é€šè¿‡withWatermarkç®—å­æ¥è®¾ç½®ä¸€ä¸ªwatermarkï¼Œè¯¥ç®—å­ç»™ç³»ç»Ÿè®¾ç½®ä¸€ä¸ªç»™å®šæ—¶é—´æˆ³Cçš„å»¶è¿Ÿé˜ˆå€¼tCï¼Œåœ¨ä»»æ„æ—¶é—´ç‚¹ï¼ŒCçš„watermarkæ˜¯maxï¼ˆCï¼‰-tCã€‚stateful operatorså…è®¸ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰é€»è¾‘æ¥å®ç°å¤æ‚çš„åŠŸèƒ½ã€‚</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1// Define an update function that simply tracks the</span><br><span class="line"> 2// number of events for each key as its state , returns</span><br><span class="line"> 3// that as its result , and times out keys after 30 min.</span><br><span class="line"> 4def updateFunc (key: UserId , newValues : Iterator [ Event ],</span><br><span class="line"> 5state : GroupState [Int ]): Int = &#123;</span><br><span class="line"> 6val totalEvents = state .get () + newValues . size ()</span><br><span class="line"> 7state . update ( totalEvents )</span><br><span class="line"> 8state . setTimeoutDuration (&quot;30 min&quot;)</span><br><span class="line"> 9return totalEvents</span><br><span class="line">10&#125;</span><br><span class="line">11// Use this update function on a stream , returning a</span><br><span class="line">12// new table lens that contains the session lengths .</span><br><span class="line">13lens = events . groupByKey ( event =&gt; event . userId )</span><br><span class="line">14. mapGroupsWithState ( updateFunc )</span><br></pre></td></tr></table></figure></li></ul><p>ç”¨mapGroupWithStateç®—å­æ¥è¿½è¸ªæ¯ä¸ªä¼šè¯çš„äº‹ä»¶æ•°é‡ï¼Œ30åˆ†é’Ÿåå…³é—­ä¼šè¯ã€‚</p><h5 id="3-5-è¿è¡Œç‰¹æ€§"><a href="#3-5-è¿è¡Œç‰¹æ€§" class="headerlink" title="3.5 è¿è¡Œç‰¹æ€§"></a>3.5 è¿è¡Œç‰¹æ€§</h5><p>(1) ä»£ç æ›´æ–°ï¼ˆcode updateï¼‰<br>å¼€å‘è€…èƒ½å¤Ÿåœ¨ç¼–ç¨‹è¿‡ç¨‹ä¸­æ›´æ–°UDFï¼Œå¹¶ä¸”å¯ä»¥ç®€å•çš„é‡å¯ä»¥ä½¿ç”¨æ–°ç‰ˆæœ¬çš„ä»£ç ã€‚<br>(2) æ‰‹åŠ¨å›æ»šï¼ˆmanual rollbackï¼‰<br>æœ‰æ—¶åœ¨ç”¨æˆ·å‘ç°ä¹‹å‰ï¼Œç¨‹åºä¼šè¾“å‡ºé”™è¯¯çš„ç»“æœï¼Œå› æ­¤å›æ»šè‡³å…³é‡è¦ã€‚ç»“æ„åŒ–æµå¾ˆå®¹æ˜“å®šä½é—®é¢˜æ‰€åœ¨ã€‚åŒæ—¶æ‰‹åŠ¨å›æ»šä¸å‰é¢æåˆ°çš„prefix consistencyæœ‰å¾ˆå¥½çš„äº¤äº’ã€‚<br>(3) æµå¼å’Œæ‰¹æ¬¡æ··åˆå¤„ç†<br>è¿™æ˜¯ç»“æ„åŒ–æµæœ€æ˜¾è€Œæ˜“è§çš„å¥½å¤„ï¼Œç”¨æˆ·èƒ½å¤Ÿå…±ç”¨æµå¼å¤„ç†å’Œæ‰¹å¤„ç†ä½œä¸šçš„ä»£ç ã€‚<br>(4) ç›‘æ§<br>ç»“æ„åŒ–æµä½¿ç”¨Sparkå·²æœ‰çš„APIå’Œç»“æ„åŒ–æ—¥å¿—æ¥æŠ¥å‘Šä¿¡æ¯ï¼Œä¾‹å¦‚å¤„ç†è¿‡çš„è®°å½•æ•°é‡ï¼Œè·¨ç½‘ç»œçš„å­—èŠ‚æ•°ç­‰ã€‚è¿™äº›æ¥å£è¢«Sparkå¼€å‘è€…æ‰€ç†ŸçŸ¥ï¼Œå¹¶æ˜“äºè¿æ¥åˆ°ä¸åŒçš„UIå·¥å…·ã€‚</p><h5 id="3-6-ç”Ÿäº§ç”¨ä¾‹ä¸æ€»ç»“"><a href="#3-6-ç”Ÿäº§ç”¨ä¾‹ä¸æ€»ç»“" class="headerlink" title="3.6 ç”Ÿäº§ç”¨ä¾‹ä¸æ€»ç»“"></a>3.6 ç”Ÿäº§ç”¨ä¾‹ä¸æ€»ç»“</h5><p>ç»™å‡ºç®€è¦æ¶æ„å›¾ï¼Œç¯‡å¹…åŸå› ä¸å†èµ˜è¿°ï¼Œå¸Œæœ›è¯¦ç»†äº†è§£çš„ä¸‹è½½è®ºæ–‡è‡ªè¡Œé˜…è¯»ã€‚æœ¬æ–‡åªæŒ‘é€‰äº†éƒ¨åˆ†å…³é”®ç‚¹è¿›è¡Œäº†æµ…å±‚æ¬¡çš„å™è¿°ï¼Œå¸Œæœ›è¯»è€…èƒ½å¤Ÿå°†è®ºæ–‡ä¸‹è½½ä¸‹æ¥è®¤çœŸå“è¯»ï¼Œææ‡‚å¼€å‘è€…çš„å¼€å‘æ€è·¯ï¼Œè·Ÿä¸Šå¤§æ•°æ®çš„å‰æ²¿æ­¥ä¼ã€‚<br><img src="/assets/blogImg/614_3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä½ å¤§çˆ·æ°¸è¿œæ˜¯ä½ å¤§çˆ·ï¼ŒRDDè¡€ç¼˜å…³ç³»æºç è¯¦è§£ï¼</title>
      <link href="/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/"/>
      <url>/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><h4 id="ä¸€ã€RDDçš„ä¾èµ–å…³ç³»"><a href="#ä¸€ã€RDDçš„ä¾èµ–å…³ç³»" class="headerlink" title="ä¸€ã€RDDçš„ä¾èµ–å…³ç³»"></a>ä¸€ã€RDDçš„ä¾èµ–å…³ç³»</h4><p>RDDçš„ä¾èµ–å…³ç³»åˆ†ä¸ºä¸¤ç±»ï¼šå®½ä¾èµ–å’Œçª„ä¾èµ–ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¤ä¸ºï¼š</p><ul><li><p>ï¼ˆ1ï¼‰çª„ä¾èµ–ï¼šæ¯ä¸ªparent RDD çš„ partition æœ€å¤šè¢« child RDD çš„ä¸€ä¸ªpartition ä½¿ç”¨ã€‚</p></li><li><p>ï¼ˆ2ï¼‰å®½ä¾èµ–ï¼šæ¯ä¸ªparent RDD partition è¢«å¤šä¸ª child RDD çš„partition ä½¿ç”¨ã€‚</p></li></ul><p>çª„ä¾èµ–æ¯ä¸ª child RDD çš„ partition çš„ç”Ÿæˆæ“ä½œéƒ½æ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œè€Œå®½ä¾èµ–åˆ™éœ€è¦æ‰€æœ‰çš„ parent RDD partition shuffle ç»“æœå¾—åˆ°åå†è¿›è¡Œã€‚<br><a id="more"></a></p><h4 id="äºŒã€org-apache-spark-Dependency-scala-æºç è§£æ"><a href="#äºŒã€org-apache-spark-Dependency-scala-æºç è§£æ" class="headerlink" title="äºŒã€org.apache.spark.Dependency.scala æºç è§£æ"></a>äºŒã€org.apache.spark.Dependency.scala æºç è§£æ</h4><p>Dependencyæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">  def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>å®ƒæœ‰ä¸¤ä¸ªå­ç±»ï¼šNarrowDependency å’Œ ShuffleDenpendencyï¼Œåˆ†åˆ«å¯¹åº”çª„ä¾èµ–å’Œå®½ä¾èµ–ã€‚</p><h5 id="ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»"><a href="#ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»" class="headerlink" title="ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»"></a>ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»</h5><p>å®šä¹‰äº†æŠ½è±¡æ–¹æ³•getParentsï¼Œè¾“å…¥partitionIdï¼Œç”¨äºè·å¾—child RDD çš„æŸä¸ªpartitionä¾èµ–çš„parent RDDçš„æ‰€æœ‰ partitionsã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;  </span><br><span class="line">/**</span><br><span class="line">   * Get the parent partitions for a child partition.</span><br><span class="line">   * @param partitionId a partition of the child RDD</span><br><span class="line">   * @return the partitions of the parent RDD that the child partition depends upon</span><br><span class="line">   */</span><br><span class="line">  def getParents(partitionId: Int): Seq[Int]</span><br><span class="line"></span><br><span class="line">  override def rdd: RDD[T] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>çª„ä¾èµ–åˆæœ‰ä¸¤ä¸ªå…·ä½“çš„å®ç°ï¼šOneToOneDependencyå’ŒRangeDependencyã€‚<br>ï¼ˆaï¼‰OneToOneDependencyæŒ‡child RDDçš„partitionåªä¾èµ–äºparent RDD çš„ä¸€ä¸ªpartitionï¼Œäº§ç”ŸOneToOneDependencyçš„ç®—å­æœ‰mapï¼Œfilterï¼ŒflatMapç­‰ã€‚å¯ä»¥çœ‹åˆ°getParentså®ç°å¾ˆç®€å•ï¼Œå°±æ˜¯ä¼ è¿›å»ä¸€ä¸ªpartitionIdï¼Œå†æŠŠpartitionIdæ”¾åœ¨Listé‡Œé¢ä¼ å‡ºå»ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = List(partitionId)</span><br><span class="line">&#125;</span><br><span class="line">        ï¼ˆbï¼‰RangeDependencyæŒ‡child RDD partitionåœ¨ä¸€å®šçš„èŒƒå›´å†…ä¸€å¯¹ä¸€çš„ä¾èµ–äºparent RDD partitionï¼Œä¸»è¦ç”¨äºunionã€‚</span><br><span class="line"></span><br><span class="line">// Denpendency.scala</span><br><span class="line">class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)  </span><br><span class="line">  extends NarrowDependency[T](rdd) &#123;//inStartè¡¨ç¤ºparent RDDçš„å¼€å§‹ç´¢å¼•ï¼ŒoutStartè¡¨ç¤ºchild RDD çš„å¼€å§‹ç´¢å¼•</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = &#123;    </span><br><span class="line">    if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      List(partitionId - outStart + inStart)//è¡¨ç¤ºäºå½“å‰ç´¢å¼•çš„ç›¸å¯¹ä½ç½®</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      Nil</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–"><a href="#ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–" class="headerlink" title="ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–"></a>ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–</h5><p>è¡¨ç¤ºä¸€ä¸ªparent RDDçš„partitionä¼šè¢«child RDDçš„partitionä½¿ç”¨å¤šæ¬¡ã€‚éœ€è¦ç»è¿‡shuffleæ‰èƒ½å½¢æˆã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</span><br><span class="line">    @transient private val _rdd: RDD[_ &lt;: Product2[K, V]],    </span><br><span class="line">    val partitioner: Partitioner,    </span><br><span class="line">    val serializer: Serializer = SparkEnv.get.serializer,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = false)</span><br><span class="line">  extends Dependency[Product2[K, V]] &#123;  //shuffleéƒ½æ˜¯åŸºäºPairRDDè¿›è¡Œçš„ï¼Œæ‰€ä»¥ä¼ å…¥çš„RDDè¦æ˜¯key-valueç±»å‹çš„</span><br><span class="line">  override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"></span><br><span class="line">  private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName</span><br><span class="line">  private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName</span><br><span class="line">  private[spark] val combinerClassName: Option[String] =</span><br><span class="line">    Option(reflect.classTag[C]).map(_.runtimeClass.getName)  //è·å–shuffleId</span><br><span class="line">  val shuffleId: Int = _rdd.context.newShuffleId()  //å‘shuffleManageræ³¨å†Œshuffleä¿¡æ¯</span><br><span class="line">  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, this)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ç”±äºshuffleæ¶‰åŠåˆ°ç½‘ç»œä¼ è¾“ï¼Œæ‰€ä»¥è¦æœ‰åºåˆ—åŒ–serializerï¼Œä¸ºäº†å‡å°‘ç½‘ç»œä¼ è¾“ï¼Œå¯ä»¥mapç«¯èšåˆï¼Œé€šè¿‡mapSideCombineå’Œaggregatoræ§åˆ¶ï¼Œè¿˜æœ‰keyæ’åºç›¸å…³çš„keyOrderingï¼Œä»¥åŠé‡è¾“å‡ºçš„æ•°æ®å¦‚ä½•åˆ†åŒºçš„partitionerï¼Œè¿˜æœ‰ä¸€äº›classä¿¡æ¯ã€‚Partitionä¹‹é—´çš„å…³ç³»åœ¨shuffleå¤„æˆ›ç„¶è€Œæ­¢ï¼Œå› æ­¤shuffleæ˜¯åˆ’åˆ†stageçš„ä¾æ®ã€‚</p><h4 id="ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†"><a href="#ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†" class="headerlink" title="ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†"></a>ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†</h4><p>é¦–å…ˆï¼Œçª„ä¾èµ–å…è®¸åœ¨ä¸€ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Šä»¥æµæ°´çº¿çš„æ–¹å¼ï¼ˆpipelineï¼‰è®¡ç®—æ‰€æœ‰çˆ¶åˆ†åŒºã€‚ä¾‹å¦‚ï¼Œé€ä¸ªå…ƒç´ åœ°æ‰§è¡Œmapã€ç„¶åfilteræ“ä½œï¼›è€Œå®½ä¾èµ–åˆ™éœ€è¦é¦–å…ˆè®¡ç®—å¥½æ‰€æœ‰çˆ¶åˆ†åŒºæ•°æ®ï¼Œç„¶ååœ¨èŠ‚ç‚¹ä¹‹é—´è¿›è¡ŒShuffleï¼Œè¿™ä¸MapReduceç±»ä¼¼ã€‚ç¬¬äºŒï¼Œçª„ä¾èµ–èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤±æ•ˆèŠ‚ç‚¹çš„æ¢å¤ï¼Œå³åªéœ€é‡æ–°è®¡ç®—ä¸¢å¤±RDDåˆ†åŒºçš„çˆ¶åˆ†åŒºï¼Œè€Œä¸”ä¸åŒèŠ‚ç‚¹ä¹‹é—´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼›è€Œå¯¹äºä¸€ä¸ªå®½ä¾èµ–å…³ç³»çš„Lineageå›¾ï¼Œå•ä¸ªèŠ‚ç‚¹å¤±æ•ˆå¯èƒ½å¯¼è‡´è¿™ä¸ªRDDçš„æ‰€æœ‰ç¥–å…ˆä¸¢å¤±éƒ¨åˆ†åˆ†åŒºï¼Œå› è€Œéœ€è¦æ•´ä½“é‡æ–°è®¡ç®—ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Javaå¯æ‰©å±•çº¿ç¨‹æ± ä¹‹ThreadPoolExecutor</title>
      <link href="/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/"/>
      <url>/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --><h4 id="1ã€ThreadPoolExecutor"><a href="#1ã€ThreadPoolExecutor" class="headerlink" title="1ã€ThreadPoolExecutor"></a>1ã€ThreadPoolExecutor</h4><p>æˆ‘ä»¬çŸ¥é“ThreadPoolExecutoræ˜¯å¯æ‰©å±•çš„,å®ƒæä¾›äº†å‡ ä¸ªå¯ä»¥åœ¨å­ç±»ä¸­æ”¹å†™çš„ç©ºæ–¹æ³•å¦‚ä¸‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;</span><br><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;  </span><br><span class="line">protected void terminated() &#123; &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="2ã€ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰©å±•ï¼Ÿ"><a href="#2ã€ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰©å±•ï¼Ÿ" class="headerlink" title="2ã€ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰©å±•ï¼Ÿ"></a>2ã€ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰©å±•ï¼Ÿ</h4><p>å› ä¸ºåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å¯¹çº¿ç¨‹æ± è¿è¡ŒçŠ¶æ€è¿›è¡Œè·Ÿè¸ªï¼Œè¾“å‡ºä¸€äº›æœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯ï¼Œä»¥å¸®åŠ©æ•…éšœè¯Šæ–­ã€‚</p><h4 id="3ã€ThreadPoolExecutor-Workerçš„runæ–¹æ³•å®ç°"><a href="#3ã€ThreadPoolExecutor-Workerçš„runæ–¹æ³•å®ç°" class="headerlink" title="3ã€ThreadPoolExecutor.Workerçš„runæ–¹æ³•å®ç°"></a>3ã€ThreadPoolExecutor.Workerçš„runæ–¹æ³•å®ç°</h4><p>é€šè¿‡çœ‹æºç æˆ‘ä»¬å‘ç° ThreadPoolExecutorçš„å·¥ä½œçº¿ç¨‹å…¶å®å°±æ˜¯Workerå®ä¾‹ï¼ŒWorker.runTask()ä¼šè¢«çº¿ç¨‹æ± ä»¥å¤šçº¿ç¨‹æ¨¡å¼å¼‚æ­¥è°ƒç”¨ï¼Œåˆ™ä»¥ä¸Šä¸‰ä¸ªæ–¹æ³•ä¹Ÿå°†è¢«å¤šçº¿ç¨‹åŒæ—¶è®¿é—®ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">1// åŸºäºjdk1.8.0_161final void runWorker(Worker w) &#123;</span><br><span class="line"> 2         Thread wt = Thread.currentThread();</span><br><span class="line"> 3         Runnable task = w.firstTask;</span><br><span class="line"> 4         w.firstTask = null;</span><br><span class="line"> 5         w.unlock(); // allow interrupts</span><br><span class="line"> 6         boolean completedAbruptly = true;        </span><br><span class="line"> 7             try &#123;            </span><br><span class="line"> 8             while (task != null || (task = getTask()) != null) &#123;</span><br><span class="line"> 9                  w.lock();                </span><br><span class="line">10              if ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class="line">11                     (Thread.interrupted() &amp;&amp;</span><br><span class="line">12                      runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class="line">13                    !wt.isInterrupted())</span><br><span class="line">14                    wt.interrupt();               </span><br><span class="line">15              try &#123;</span><br><span class="line">16                    beforeExecute(wt, task);</span><br><span class="line">17                    Throwable thrown = null;                   </span><br><span class="line">18              try &#123;</span><br><span class="line">19                        task.run();</span><br><span class="line">20                    &#125; catch (RuntimeException x) &#123;</span><br><span class="line">21                        thrown = x; throw x;</span><br><span class="line">22                    &#125; catch (Error x) &#123;</span><br><span class="line">23                        thrown = x; throw x;</span><br><span class="line">24                    &#125; catch (Throwable x) &#123;</span><br><span class="line">25                        thrown = x; throw new Error(x);</span><br><span class="line">26                    &#125; finally &#123;</span><br><span class="line">27                        afterExecute(task, thrown);</span><br><span class="line">28                    &#125;</span><br><span class="line">29                &#125; finally &#123;</span><br><span class="line">30                    task = null;</span><br><span class="line">31                    w.completedTasks++;</span><br><span class="line">32                    w.unlock();</span><br><span class="line">33                &#125;</span><br><span class="line">34            &#125;</span><br><span class="line">35            completedAbruptly = false;</span><br><span class="line">36        &#125; finally &#123;</span><br><span class="line">37            processWorkerExit(w, completedAbruptly);</span><br><span class="line">38        &#125;</span><br><span class="line">39    &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="4ã€æ‰©å±•çº¿ç¨‹æ± å®ç°"><a href="#4ã€æ‰©å±•çº¿ç¨‹æ± å®ç°" class="headerlink" title="4ã€æ‰©å±•çº¿ç¨‹æ± å®ç°"></a>4ã€æ‰©å±•çº¿ç¨‹æ± å®ç°</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1public class ExtThreadPool &#123;</span><br><span class="line"> 2    public static class MyTask implements Runnable &#123;</span><br><span class="line"> 3        public String name;        </span><br><span class="line"> 4        public MyTask(String name) &#123;            </span><br><span class="line"> 5          this.name = name;</span><br><span class="line"> 6        &#125;       </span><br><span class="line"> 7        public void run() &#123;</span><br><span class="line"> 8            System.out.println(&quot;æ­£åœ¨æ‰§è¡Œ:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name);            try &#123;</span><br><span class="line"> 9                Thread.sleep(100);</span><br><span class="line">10            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">11                e.printStackTrace();</span><br><span class="line">12            &#125;</span><br><span class="line">13        &#125;</span><br><span class="line">14    &#125;    </span><br><span class="line">15public static void main(String args[]) throws InterruptedException &#123;</span><br><span class="line">16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,</span><br><span class="line">17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123;            </span><br><span class="line">18protected void beforeExecute(Thread t, Runnable r) &#123;</span><br><span class="line">19 System.out.println(&quot;å‡†å¤‡æ‰§è¡Œï¼š&quot; + ((MyTask) r).name);</span><br><span class="line">20&#125;            </span><br><span class="line">21protected void afterExecute(Thread t, Runnable r) &#123;</span><br><span class="line">22  System.out.println(&quot;æ‰§è¡Œå®Œæˆ&quot; + ((MyTask) r).name);</span><br><span class="line">23&#125;            </span><br><span class="line">24protected void terminated() &#123;</span><br><span class="line">25  System.out.println(&quot;çº¿ç¨‹æ± é€€å‡ºï¼&quot;);</span><br><span class="line">26&#125;</span><br><span class="line">27&#125;;        </span><br><span class="line">28for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">29 MyTask task = new MyTask(&quot;TASK--&quot; + i);</span><br><span class="line">30            executorService.execute(task);</span><br><span class="line">31            Thread.sleep(10);</span><br><span class="line">32        &#125;</span><br><span class="line">33 executorService.shutdown();</span><br><span class="line">34    &#125;</span><br><span class="line">35&#125;</span><br></pre></td></tr></table></figure><p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“0 </span><br><span class="line">æ­£åœ¨æ‰§è¡Œ:Thread ID:10,Task Name:TASKâ€“0 </span><br><span class="line">å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“1 </span><br><span class="line">æ­£åœ¨æ‰§è¡Œ:Thread ID:11,Task Name:TASKâ€“1 </span><br><span class="line">å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“2 </span><br><span class="line">æ­£åœ¨æ‰§è¡Œ:Thread ID:12,Task Name:TASKâ€“2 </span><br><span class="line">å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“3 </span><br><span class="line">æ­£åœ¨æ‰§è¡Œ:Thread ID:13,Task Name:TASKâ€“3 </span><br><span class="line">å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“4 </span><br><span class="line">æ­£åœ¨æ‰§è¡Œ:Thread ID:14,Task Name:TASKâ€“4 </span><br><span class="line">çº¿ç¨‹æ± é€€å‡ºï¼</span><br></pre></td></tr></table></figure><p></p><p>è¿™æ ·å°±å®ç°äº†åœ¨æ‰§è¡Œå‰åè¿›è¡Œçš„ä¸€äº›æ§åˆ¶ï¼Œé™¤æ­¤ä¹‹å¤–æˆ‘ä»¬è¿˜å¯ä»¥è¾“å‡ºæ¯ä¸ªçº¿ç¨‹çš„æ‰§è¡Œæ—¶é—´ï¼Œæˆ–è€…ä¸€äº›å…¶ä»–å¢å¼ºæ“ä½œã€‚</p><h4 id="5ã€æ€è€ƒï¼Ÿ"><a href="#5ã€æ€è€ƒï¼Ÿ" class="headerlink" title="5ã€æ€è€ƒï¼Ÿ"></a>5ã€æ€è€ƒï¼Ÿ</h4><p>è¯·è¯»è€…æ€è€ƒshutdownNowå’Œshutdownæ–¹æ³•çš„åŒºåˆ«ï¼Ÿ<br>å¦‚ä½•ä¼˜é›…çš„å…³é—­çº¿ç¨‹æ± ï¼Ÿ</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark æŠ€æœ¯å›¢é˜Ÿå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow</title>
      <link href="/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/"/>
      <url>/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è¿‘æ—¥ï¼Œæ¥è‡ª Databricks çš„ Matei Zaharia å®£å¸ƒæ¨å‡ºå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow ã€‚Matei Zaharia æ˜¯ Apache Spark å’Œ Apache Mesos çš„æ ¸å¿ƒä½œè€…ï¼Œä¹Ÿæ˜¯ Databrick çš„é¦–å¸­æŠ€æœ¯ä¸“å®¶ã€‚Databrick æ˜¯ç”± Apache Spark æŠ€æœ¯å›¢é˜Ÿæ‰€åˆ›ç«‹çš„å•†ä¸šåŒ–å…¬å¸ã€‚MLflow ç›®å‰å·²å¤„äºæ—©æœŸæµ‹è¯•é˜¶æ®µï¼Œå¼€å‘è€…å¯ä¸‹è½½æºç ä½“éªŒã€‚<br><a id="more"></a><br><img src="/assets/blogImg/612_1.png" alt="enter description here"><br>Matei Zaharia è¡¨ç¤ºå½“å‰åœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ çš„å…¬å¸æ™®éå­˜åœ¨å·¥å…·è¿‡å¤šã€éš¾ä»¥è·Ÿè¸ªå®éªŒã€éš¾ä»¥é‡ç°ç»“æœã€éš¾ä»¥éƒ¨ç½²ç­‰é—®é¢˜ã€‚ä¸ºè®©æœºå™¨å­¦ä¹ å¼€å‘å˜å¾—ä¸ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸€æ ·å¼ºå¤§ã€å¯é¢„æµ‹å’Œæ™®åŠï¼Œè®¸å¤šä¼ä¸šå·²å¼€å§‹æ„å»ºå†…éƒ¨æœºå™¨å­¦ä¹ å¹³å°æ¥ç®¡ç† MLç”Ÿå‘½å‘¨æœŸã€‚åƒæ˜¯ Facebookã€Google å’Œ Uber å°±å·²åˆ†åˆ«æ„å»ºäº† FBLearner Flowã€TFX å’Œ Michelangelo æ¥ç®¡ç†æ•°æ®ã€æ¨¡å‹åŸ¹è®­å’Œéƒ¨ç½²ã€‚ä¸è¿‡ç”±äºè¿™äº›å†…éƒ¨å¹³å°å­˜åœ¨å±€é™æ€§å’Œç»‘å®šæ€§ï¼Œæ— æ³•å¾ˆå¥½åœ°ä¸ç¤¾åŒºå…±äº«æˆæœï¼Œå…¶ä»–ç”¨æˆ·ä¹Ÿæ— æ³•è½»æ˜“ä½¿ç”¨ã€‚<br>MLflow æ­£æ˜¯å—ç°æœ‰çš„ ML å¹³å°å¯å‘ï¼Œä¸»æ‰“å¼€æ”¾æ€§ï¼š</p><ul><li>å¼€æ”¾æ¥å£ï¼šå¯ä¸ä»»æ„ ML åº“ã€ç®—æ³•ã€éƒ¨ç½²å·¥å…·æˆ–ç¼–ç¨‹è¯­è¨€ä¸€èµ·ä½¿ç”¨ã€‚</li><li>å¼€æºï¼šå¼€å‘è€…å¯è½»æ¾åœ°å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œå¹¶è·¨ç»„ç»‡å…±äº«å·¥ä½œæµæ­¥éª¤å’Œæ¨¡å‹ã€‚<br>MLflow ç›®å‰çš„ alpha ç‰ˆæœ¬åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼š<br><img src="/assets/blogImg/612_2.png" alt="enter description here"><br>å…¶ä¸­ï¼ŒMLflow Trackingï¼ˆè·Ÿè¸ªç»„ä»¶ï¼‰æä¾›äº†ä¸€ç»„ API å’Œç”¨æˆ·ç•Œé¢ï¼Œç”¨äºåœ¨è¿è¡Œæœºå™¨å­¦ä¹ ä»£ç æ—¶è®°å½•å’ŒæŸ¥è¯¢å‚æ•°ã€ä»£ç ç‰ˆæœ¬ã€æŒ‡æ ‡å’Œè¾“å‡ºæ–‡ä»¶ï¼Œä»¥ä¾¿ä»¥åå¯è§†åŒ–å®ƒä»¬ã€‚<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow</span><br><span class="line"></span><br><span class="line"># Log parameters (key-value pairs)</span><br><span class="line">mlflow.log_param(&quot;num_dimensions&quot;, 8)</span><br><span class="line">mlflow.log_param(&quot;regularization&quot;, 0.1)</span><br><span class="line"></span><br><span class="line"># Log a metric; metrics can be updated throughout the run</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.1)</span><br><span class="line">...</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.45)</span><br><span class="line"></span><br><span class="line"># Log artifacts (output files)</span><br><span class="line">mlflow.log_artifact(&quot;roc.png&quot;)</span><br><span class="line">mlflow.log_artifact(&quot;model.pkl&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>MLflow Projectsï¼ˆé¡¹ç›®ç»„ä»¶ï¼‰æä¾›äº†æ‰“åŒ…å¯é‡ç”¨æ•°æ®ç§‘å­¦ä»£ç çš„æ ‡å‡†æ ¼å¼ã€‚æ¯ä¸ªé¡¹ç›®éƒ½åªæ˜¯ä¸€ä¸ªåŒ…å«ä»£ç æˆ– Git å­˜å‚¨åº“çš„ç›®å½•ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªæè¿°ç¬¦æ–‡ä»¶æ¥æŒ‡å®šå®ƒçš„ä¾èµ–å…³ç³»ä»¥åŠå¦‚ä½•è¿è¡Œä»£ç ã€‚æ¯ä¸ª MLflow é¡¹ç›®éƒ½æ˜¯ç”±ä¸€ä¸ªç®€å•çš„åä¸º MLproject çš„ YAML æ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name: My Project</span><br><span class="line">conda_env: conda.yaml</span><br><span class="line">entry_points:</span><br><span class="line">  main:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">      regularization: &#123;type: float, default: 0.1&#125;</span><br><span class="line">    command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot;</span><br><span class="line">  validate:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">    command: &quot;python validate.py &#123;data_file&#125;&quot;</span><br></pre></td></tr></table></figure><p></p><p>MLflow Modelsï¼ˆæ¨¡å‹ç»„ä»¶ï¼‰æä¾›äº†ä¸€ç§ç”¨å¤šç§æ ¼å¼æ‰“åŒ…æœºå™¨å­¦ä¹ æ¨¡å‹çš„è§„èŒƒï¼Œè¿™äº›æ ¼å¼è¢«ç§°ä¸º â€œflavorâ€ ã€‚MLflow æä¾›äº†å¤šç§å·¥å…·æ¥éƒ¨ç½²ä¸åŒ flavor çš„æ¨¡å‹ã€‚æ¯ä¸ª MLflow æ¨¡å‹è¢«ä¿å­˜æˆä¸€ä¸ªç›®å½•ï¼Œç›®å½•ä¸­åŒ…å«äº†ä»»æ„æ¨¡å‹æ–‡ä»¶å’Œä¸€ä¸ª MLmodel æè¿°ç¬¦æ–‡ä»¶ï¼Œæ–‡ä»¶ä¸­åˆ—å‡ºäº†ç›¸åº”çš„ flavor ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_created: 2018-02-21T13:21:34.12</span><br><span class="line">flavors:</span><br><span class="line">  sklearn:</span><br><span class="line">    sklearn_version: 0.19.1</span><br><span class="line">    pickled_model: model.pkl</span><br><span class="line">  python_function:</span><br><span class="line">    loader_module: mlflow.sklearn</span><br><span class="line">    pickled_model: model.pkl</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL ä¹‹å¤–éƒ¨æ•°æ®æºå¦‚ä½•æˆä¸ºåœ¨ä¼ä¸šå¼€å‘ä¸­çš„ä¸€æŠŠåˆ©å™¨</title>
      <link href="/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/"/>
      <url>/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="1-æ¦‚è¿°"><a href="#1-æ¦‚è¿°" class="headerlink" title="1 æ¦‚è¿°"></a>1 æ¦‚è¿°</h4><p>1.Spark1.2ä¸­ï¼ŒSpark SQLå¼€å§‹æ­£å¼æ”¯æŒå¤–éƒ¨æ•°æ®æºã€‚Spark SQLå¼€æ”¾äº†ä¸€ç³»åˆ—æ¥å…¥å¤–éƒ¨æ•°æ®æºçš„æ¥å£ï¼Œæ¥è®©å¼€å‘è€…å¯ä»¥å®ç°ã€‚ä½¿å¾—Spark SQLå¯ä»¥åŠ è½½ä»»ä½•åœ°æ–¹çš„æ•°æ®ï¼Œä¾‹å¦‚mysqlï¼Œhiveï¼Œhdfsï¼Œhbaseç­‰ï¼Œè€Œä¸”æ”¯æŒå¾ˆå¤šç§æ ¼å¼å¦‚json, parquet, avro, csvæ ¼å¼ã€‚æˆ‘ä»¬å¯ä»¥å¼€å‘å‡ºä»»æ„çš„å¤–éƒ¨æ•°æ®æºæ¥è¿æ¥åˆ°Spark SQLï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIæ¥è¿›è¡Œæ“ä½œã€‚<br>2.æˆ‘ä»¬é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIè¯»å–å„ç§æ ¼å¼çš„æ•°æ®ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªDataFrameï¼Œè¿™æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„æ–¹å¼å•Šï¼Œå°±å¯ä»¥ä½¿ç”¨DataFrameçš„APIæˆ–è€…SQLçš„APIè¿›è¡Œæ“ä½œå“ˆã€‚<br>3.å¤–éƒ¨æ•°æ®æºçš„APIå¯ä»¥è‡ªåŠ¨åšä¸€äº›åˆ—çš„è£å‰ªï¼Œä»€ä¹ˆå«åˆ—çš„è£å‰ªï¼Œå‡å¦‚ä¸€ä¸ªuserè¡¨æœ‰id,name,age,gender4ä¸ªåˆ—ï¼Œåœ¨åšselectçš„æ—¶å€™ä½ åªéœ€è¦id,nameè¿™ä¸¤åˆ—ï¼Œé‚£ä¹ˆå…¶ä»–åˆ—ä¼šé€šè¿‡åº•å±‚çš„ä¼˜åŒ–å»ç»™æˆ‘ä»¬è£å‰ªæ‰ã€‚<br>4.ä¿å­˜æ“ä½œå¯ä»¥é€‰æ‹©ä½¿ç”¨SaveModeï¼ŒæŒ‡å®šå¦‚ä½•ä¿å­˜ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚<br><a id="more"></a></p><h4 id="2-è¯»å–jsonæ–‡ä»¶"><a href="#2-è¯»å–jsonæ–‡ä»¶" class="headerlink" title="2.è¯»å–jsonæ–‡ä»¶"></a>2.è¯»å–jsonæ–‡ä»¶</h4><p>å¯åŠ¨shellè¿›è¡Œæµ‹è¯•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//æ ‡å‡†å†™æ³•</span><br><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)</span><br><span class="line">//å¦å¤–ä¸€ç§å†™æ³•</span><br><span class="line">spark.read.json(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">çœ‹çœ‹æºç è¿™ä¸¤è€…ä¹‹é—´åˆ°åº•æœ‰å•¥ä¸åŒå‘¢ï¼Ÿ</span><br><span class="line">/**</span><br><span class="line">   * Loads a JSON file and returns the results as a `DataFrame`.</span><br><span class="line">   *</span><br><span class="line">   * See the documentation on the overloaded `json()` method with varargs for more details.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def json(path: String): DataFrame = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    json(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line">æˆ‘ä»¬è°ƒç”¨josn() æ–¹æ³•å…¶å®è¿›è¡Œäº† overloaded ï¼Œæˆ‘ä»¬ç»§ç»­æŸ¥çœ‹</span><br><span class="line"> def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*)</span><br><span class="line"> è¿™å¥è¯æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œå…¶å®å°±æ˜¯æˆ‘ä»¬çš„æ ‡å‡†å†™æ³•</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"> df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h4 id="3-è¯»å–parquetæ•°æ®"><a href="#3-è¯»å–parquetæ•°æ®" class="headerlink" title="3 è¯»å–parquetæ•°æ®"></a>3 è¯»å–parquetæ•°æ®</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h4 id="4-è¯»å–hiveä¸­çš„æ•°æ®"><a href="#4-è¯»å–hiveä¸­çš„æ•°æ®" class="headerlink" title="4 è¯»å–hiveä¸­çš„æ•°æ®"></a>4 è¯»å–hiveä¸­çš„æ•°æ®</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">| default|states_raw|      false|</span><br><span class="line">| default|states_seq|      false|</span><br><span class="line">| default|        t1|      false|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line"></span><br><span class="line">spark.table(&quot;states_raw&quot;).show</span><br><span class="line">+-----+------+</span><br><span class="line">| code|  name|</span><br><span class="line">+-----+------+</span><br><span class="line">|hello|  java|</span><br><span class="line">|hello|hadoop|</span><br><span class="line">|hello|  hive|</span><br><span class="line">|hello| sqoop|</span><br><span class="line">|hello|  hdfs|</span><br><span class="line">|hello| spark|</span><br><span class="line">+-----+------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select name from states_raw &quot;).show</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|  java|</span><br><span class="line">|hadoop|</span><br><span class="line">|  hive|</span><br><span class="line">| sqoop|</span><br><span class="line">|  hdfs|</span><br><span class="line">| spark|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure><h4 id="5-ä¿å­˜æ•°æ®"><a href="#5-ä¿å­˜æ•°æ®" class="headerlink" title="5 ä¿å­˜æ•°æ®"></a>5 ä¿å­˜æ•°æ®</h4><p>æ³¨æ„ï¼š</p><ol><li>ä¿å­˜çš„æ–‡ä»¶å¤¹ä¸èƒ½å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™(é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å¼)ï¼šorg.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;</li><li>ä¿å­˜æˆæ–‡æœ¬æ ¼å¼ï¼Œåªèƒ½ä¿å­˜ä¸€åˆ—ï¼Œå¦åˆ™æŠ¥é”™ï¼šorg.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line">//ä¿å­˜</span><br><span class="line">df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)</span><br><span class="line"></span><br><span class="line">ç»“æœï¼š</span><br><span class="line">[hadoop@hadoop out]$ pwd</span><br><span class="line">/home/hadoop/data/out</span><br><span class="line">[hadoop@hadoop out]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:34 _SUCCESS</span><br><span class="line">[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt </span><br><span class="line">Michael</span><br><span class="line">Andy</span><br><span class="line">Justin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//ä¿å­˜ä¸ºjsonæ ¼å¼</span><br><span class="line">df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)</span><br><span class="line"></span><br><span class="line">ç»“æœ</span><br><span class="line">[hadoop@hadoop data]$ cd out1</span><br><span class="line">[hadoop@hadoop out1]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:35 _SUCCESS</span><br><span class="line">[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;</span><br></pre></td></tr></table></figure></li></ol><p>ä¸Šé¢è¯´äº†åœ¨ä¿å­˜æ•°æ®æ—¶å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåœ¨é»˜è®¤æ¨¡å¼ä¸‹ä¼šæŠ¥é”™ï¼Œé‚£æˆ‘ä»¬ä¸‹é¢è®²è§£ä¿å­˜çš„å‡ ç§æ¨¡å¼ï¼š<br><img src="/assets/blogImg/606_1.png" alt="enter description here"></p><h4 id="6-è¯»å–mysqlä¸­çš„æ•°æ®"><a href="#6-è¯»å–mysqlä¸­çš„æ•°æ®" class="headerlink" title="6 è¯»å–mysqlä¸­çš„æ•°æ®"></a>6 è¯»å–mysqlä¸­çš„æ•°æ®</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">val jdbcDF = spark.read</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;123456&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- TBL_ID: long (nullable = false)</span><br><span class="line"> |-- CREATE_TIME: integer (nullable = false)</span><br><span class="line"> |-- DB_ID: long (nullable = true)</span><br><span class="line"> |-- LAST_ACCESS_TIME: integer (nullable = false)</span><br><span class="line"> |-- OWNER: string (nullable = true)</span><br><span class="line"> |-- RETENTION: integer (nullable = false)</span><br><span class="line"> |-- SD_ID: long (nullable = true)</span><br><span class="line"> |-- TBL_NAME: string (nullable = true)</span><br><span class="line"> |-- TBL_TYPE: string (nullable = true)</span><br><span class="line"> |-- VIEW_EXPANDED_TEXT: string (nullable = true)</span><br><span class="line"> |-- VIEW_ORIGINAL_TEXT: string (nullable = true)</span><br><span class="line"></span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="7-spark-SQLæ“ä½œmysqlè¡¨æ•°æ®"><a href="#7-spark-SQLæ“ä½œmysqlè¡¨æ•°æ®" class="headerlink" title="7 spark SQLæ“ä½œmysqlè¡¨æ•°æ®"></a>7 spark SQLæ“ä½œmysqlè¡¨æ•°æ®</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW jdbcTable</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost:3306&quot;,</span><br><span class="line">  dbtable &quot;basic01.tbls&quot;,</span><br><span class="line">  user &apos;root&apos;,</span><br><span class="line">  password &apos;123456&apos;,</span><br><span class="line">  driver &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">æŸ¥çœ‹ï¼š</span><br><span class="line">show tables;</span><br><span class="line">default states_raw      false</span><br><span class="line">default states_seq      false</span><br><span class="line">default t1      false</span><br><span class="line">jdbctable       true</span><br><span class="line"></span><br><span class="line">select * from jdbctable;</span><br><span class="line">1       1519944170      6       0       hadoop  0       1       page_views      MANAGED_TABLE   NULL    NULL</span><br><span class="line">2       1519944313      6       0       hadoop  0       2       page_views_bzip2        MANAGED_TABLE   NULL    NULL</span><br><span class="line">3       1519944819      6       0       hadoop  0       3       page_views_snappy       MANAGED_TABLE   NULL    NULL</span><br><span class="line">21      1520067771      6       0       hadoop  0       21      tt      MANAGED_TABLE   NULL    NULL</span><br><span class="line">22      1520069148      6       0       hadoop  0       22      page_views_seq  MANAGED_TABLE   NULL    NULL</span><br><span class="line">23      1520071381      6       0       hadoop  0       23      page_views_rcfile       MANAGED_TABLE   NULL    NULL</span><br><span class="line">24      1520074675      6       0       hadoop  0       24      page_views_orc_zlib     MANAGED_TABLE   NULL    NULL</span><br><span class="line">27      1520078184      6       0       hadoop  0       27      page_views_lzo_index    MANAGED_TABLE   NULL    NULL</span><br><span class="line">30      1520083461      6       0       hadoop  0       30      page_views_lzo_index1   MANAGED_TABLE   NULL    NULL</span><br><span class="line">31      1524370014      1       0       hadoop  0       31      t1      EXTERNAL_TABLE  NULL    NULL</span><br><span class="line">37      1524468636      1       0       hadoop  0       37      states_raw      MANAGED_TABLE   NULL    NULL</span><br><span class="line">38      1524468678      1       0       hadoop  0       38      states_seq      MANAGED_TABLE   NULL    NULL</span><br><span class="line"></span><br><span class="line">mysqlä¸­çš„tblsçš„æ•°æ®å·²ç»å­˜åœ¨jdbctableè¡¨ä¸­äº†ã€‚</span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="8-åˆ†åŒºæ¨æµ‹ï¼ˆPartition-Discoveryï¼‰"><a href="#8-åˆ†åŒºæ¨æµ‹ï¼ˆPartition-Discoveryï¼‰" class="headerlink" title="8 åˆ†åŒºæ¨æµ‹ï¼ˆPartition Discoveryï¼‰"></a>8 åˆ†åŒºæ¨æµ‹ï¼ˆPartition Discoveryï¼‰</h4><p>è¡¨åˆ†åŒºæ˜¯åœ¨åƒHiveè¿™æ ·çš„ç³»ç»Ÿä¸­ä½¿ç”¨çš„å¸¸è§ä¼˜åŒ–æ–¹æ³•ã€‚ åœ¨åˆ†åŒºè¡¨ä¸­ï¼Œæ•°æ®é€šå¸¸å­˜å‚¨åœ¨ä¸åŒçš„ç›®å½•ä¸­ï¼Œåˆ†åŒºåˆ—å€¼åœ¨æ¯ä¸ªåˆ†åŒºç›®å½•çš„è·¯å¾„ä¸­ç¼–ç ã€‚ æ‰€æœ‰å†…ç½®çš„æ–‡ä»¶æºï¼ˆåŒ…æ‹¬Text / CSV / JSON / ORC / Parquetï¼‰éƒ½èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å’Œæ¨æ–­åˆ†åŒºä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ›å»ºå¦‚ä¸‹çš„ç›®å½•ç»“æ„;<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN</span><br><span class="line"></span><br><span class="line">æ·»åŠ jsonæ–‡ä»¶ï¼š</span><br><span class="line">people.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line"></span><br><span class="line"> hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN</span><br></pre></td></tr></table></figure><p></p><p>æˆ‘ä»¬ä½¿ç”¨spark sqlè¯»å–å¤–éƒ¨æ•°æ®æºï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p></p><p>æˆ‘ä»¬æ”¹å˜è¯»å–çš„ç›®å½•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)</span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- country: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+-------+</span><br><span class="line">| age|   name|country|</span><br><span class="line">+----+-------+-------+</span><br><span class="line">|null|Michael|     CN|</span><br><span class="line">|  30|   Andy|     CN|</span><br><span class="line">|  19| Justin|     CN|</span><br><span class="line">+----+-------+-------+</span><br></pre></td></tr></table></figure><p></p><p>å¤§å®¶æœ‰æ²¡æœ‰å‘ç°ä»€ä¹ˆå‘¢ï¼ŸSpark SQLå°†è‡ªåŠ¨ä»è·¯å¾„ä¸­æå–åˆ†åŒºä¿¡æ¯ã€‚<br>æ³¨æ„ï¼Œåˆ†åŒºåˆ—çš„æ•°æ®ç±»å‹æ˜¯è‡ªåŠ¨æ¨æ–­çš„ã€‚ç›®å‰æ”¯æŒæ•°å­—æ•°æ®ç±»å‹ï¼Œæ—¥æœŸï¼Œæ—¶é—´æˆ³å’Œå­—ç¬¦ä¸²ç±»å‹ã€‚æœ‰æ—¶ç”¨æˆ·å¯èƒ½ä¸æƒ³è‡ªåŠ¨æ¨æ–­åˆ†åŒºåˆ—çš„æ•°æ®ç±»å‹ã€‚å¯¹äºè¿™äº›ç”¨ä¾‹ï¼Œè‡ªåŠ¨ç±»å‹æ¨æ–­å¯ä»¥é€šè¿‡</p><p><font color="#FF4500">spark.sql.sources.partitionColumnTypeInference.enabled</font>è¿›è¡Œé…ç½®ï¼Œé»˜è®¤ä¸ºtrueã€‚å½“ç¦ç”¨ç±»å‹æ¨æ–­æ—¶ï¼Œå­—ç¬¦ä¸²ç±»å‹å°†ç”¨äºåˆ†åŒºåˆ—ã€‚<br>ä»Spark 1.6.0å¼€å§‹ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ†åŒºå‘ç°ä»…åœ¨ç»™å®šè·¯å¾„ä¸‹æ‰¾åˆ°åˆ†åŒºã€‚å¯¹äºä¸Šé¢çš„ç¤ºä¾‹ï¼Œå¦‚æœç”¨æˆ·å°†è·¯å¾„/table/gender=maleä¼ é€’ç»™</p><p><font color="#FF4500">SparkSession.read.parquetæˆ–SparkSession.read.load</font>ï¼Œåˆ™ä¸ä¼šå°†æ€§åˆ«è§†ä¸ºåˆ†åŒºåˆ—ã€‚å¦‚æœç”¨æˆ·éœ€è¦æŒ‡å®šå¯åŠ¨åˆ†åŒºå‘ç°çš„åŸºæœ¬è·¯å¾„ï¼Œåˆ™å¯ä»¥basePathåœ¨æ•°æ®æºé€‰é¡¹ä¸­è¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼Œå½“path/to/table/gender=maleæ˜¯æ•°æ®è·¯å¾„å¹¶ä¸”ç”¨æˆ·å°†basePathè®¾ç½®ä¸ºpath/to/table/æ—¶ï¼Œæ€§åˆ«å°†æ˜¯åˆ†åŒºåˆ—ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStreaming çŠ¶æ€ç®¡ç†å‡½æ•°çš„é€‰æ‹©æ¯”è¾ƒ</title>
      <link href="/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/"/>
      <url>/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="ä¸€ã€updateStateByKey"><a href="#ä¸€ã€updateStateByKey" class="headerlink" title="ä¸€ã€updateStateByKey"></a>ä¸€ã€updateStateByKey</h4><p>å®˜ç½‘åŸè¯ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</span><br></pre></td></tr></table></figure><p></p><p>ä¹Ÿå³æ˜¯è¯´å®ƒä¼šç»Ÿè®¡å…¨å±€çš„keyçš„çŠ¶æ€ï¼Œå°±ç®—æ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œå®ƒä¹Ÿä¼šåœ¨æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶å€™è¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ã€‚</p><p>ç¼ºç‚¹ï¼šè‹¥æ•°æ®é‡å¤ªå¤§çš„è¯ï¼Œéœ€è¦checkpointçš„æ•°æ®ä¼šå ç”¨è¾ƒå¤§çš„å­˜å‚¨ï¼Œæ•ˆç‡ä½ä¸‹ã€‚</p><p>ç¨‹åºç¤ºä¾‹å¦‚ä¸‹ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //æ³¨æ„ï¼šè¦ä½¿ç”¨updateStateByKeyå¿…é¡»è®¾ç½®checkpointç›®å½•  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start() </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;   </span><br><span class="line"> /*çŠ¶æ€æ›´æ–°å‡½æ•°  </span><br><span class="line">  * @param currentValues  keyç›¸åŒvalueå½¢æˆçš„åˆ—è¡¨  </span><br><span class="line">  * @param preValues      keyå¯¹åº”çš„valueï¼Œå‰ä¸€çŠ¶æ€  </span><br><span class="line">  * */  </span><br><span class="line">def updateFunction(currentValues: Seq[Int], preValues: Option[Int]):                                Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seqåˆ—è¡¨ä¸­æ‰€æœ‰valueæ±‚å’Œ  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //è·å–ä¸Šä¸€çŠ¶æ€å€¼  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="äºŒã€mapWithState"><a href="#äºŒã€mapWithState" class="headerlink" title="äºŒã€mapWithState"></a>äºŒã€mapWithState</h4><p>mapWithStateï¼šä¹Ÿæ˜¯ç”¨äºå…¨å±€ç»Ÿè®¡keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å®ƒå¦‚æœæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä¾¿ä¸ä¼šè¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ï¼Œæœ‰ä¸€ç‚¹å¢é‡çš„æ„Ÿè§‰ã€‚æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨</p><p>å®˜æ–¹ä»£ç å¦‚ä¸‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount </span><br><span class="line">      &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;StatefulNetworkWordCount&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)   </span><br><span class="line">    val initialRDD = ssc.sparkContext</span><br><span class="line">      .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1)))  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], </span><br><span class="line">     state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">    StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="ä¸‰ã€æºç åˆ†æ"><a href="#ä¸‰ã€æºç åˆ†æ" class="headerlink" title="ä¸‰ã€æºç åˆ†æ"></a>ä¸‰ã€æºç åˆ†æ</h4><h5 id="upateStateByKeyï¼š"><a href="#upateStateByKeyï¼š" class="headerlink" title="upateStateByKeyï¼š"></a>upateStateByKeyï¼š</h5><ul><li>mapè¿”å›çš„æ˜¯MappedDStreamï¼Œè€ŒMappedDStreamå¹¶æ²¡æœ‰updateStateByKeyæ–¹æ³•ï¼Œå¹¶ä¸”å®ƒçš„çˆ¶ç±»DStreamä¸­ä¹Ÿæ²¡æœ‰è¯¥æ–¹æ³•ã€‚ä½†æ˜¯DStreamçš„ä¼´ç”Ÿå¯¹è±¡ä¸­æœ‰ä¸€ä¸ªéšå¼è½¬æ¢å‡½æ•°ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">object DStream &#123;</span><br><span class="line">  implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)])</span><br><span class="line">      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null):</span><br><span class="line">    PairDStreamFunctions[K, V] = &#123;</span><br><span class="line">    new PairDStreamFunctions[K, V](stream)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>è·Ÿè¿›å» PairDStreamFunctions ï¼Œå‘ç°æœ€ç»ˆè°ƒç”¨çš„æ˜¯è‡ªå·±çš„updateStateByKeyã€‚<br>å…¶ä¸­updateFuncå°±è¦ä¼ å…¥çš„å‚æ•°ï¼Œä»–æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒSeq[V]è¡¨ç¤ºå½“å‰keyå¯¹åº”çš„æ‰€æœ‰å€¼ï¼Œ<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Option[S] æ˜¯å½“å‰keyçš„å†å²çŠ¶æ€ï¼Œè¿”å›çš„æ˜¯æ–°çš„çŠ¶æ€ã€‚</span><br><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Seq[V], Option[S]) =&gt; Option[S]</span><br><span class="line">  ): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>æœ€ç»ˆè°ƒç”¨ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],</span><br><span class="line">    partitioner: Partitioner,</span><br><span class="line">    rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  val cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">  val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123;</span><br><span class="line">    cleanedFunc(it)</span><br><span class="line">  &#125;</span><br><span class="line">  new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>å†è·Ÿè¿›å» new StateDStream:<br>åœ¨è¿™é‡Œé¢newå‡ºäº†ä¸€ä¸ªStateDStreamå¯¹è±¡ã€‚åœ¨å…¶computeæ–¹æ³•ä¸­ï¼Œä¼šå…ˆè·å–ä¸Šä¸€ä¸ªbatchè®¡ç®—å‡ºçš„RDDï¼ˆåŒ…å«äº†è‡³ç¨‹åºå¼€å§‹åˆ°ä¸Šä¸€ä¸ªbatchå•è¯çš„ç´¯è®¡è®¡æ•°ï¼‰ï¼Œç„¶ååœ¨è·å–æœ¬æ¬¡batchä¸­StateDStreamçš„çˆ¶ç±»è®¡ç®—å‡ºçš„RDDï¼ˆæœ¬æ¬¡batchçš„å•è¯è®¡æ•°ï¼‰åˆ†åˆ«æ˜¯prevStateRDDå’ŒparentRDDï¼Œç„¶ååœ¨è°ƒç”¨ computeUsingPreviousRDD æ–¹æ³•ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private [this] def computeUsingPreviousRDD(</span><br><span class="line">    batchTime: Time,</span><br><span class="line">    parentRDD: RDD[(K, V)],</span><br><span class="line">    prevStateRDD: RDD[(K, S)]) = &#123;</span><br><span class="line">  // Define the function for the mapPartition operation on cogrouped RDD;</span><br><span class="line">  // first map the cogrouped tuple to tuples of required type,</span><br><span class="line">  // and then apply the update function</span><br><span class="line">  val updateFuncLocal = updateFunc</span><br><span class="line">  val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123;</span><br><span class="line">    val i = iterator.map &#123; t =&gt;</span><br><span class="line">      val itr = t._2._2.iterator</span><br><span class="line">      val headOption = if (itr.hasNext) Some(itr.next()) else None</span><br><span class="line">      (t._1, t._2._1.toSeq, headOption)</span><br><span class="line">    &#125;</span><br><span class="line">    updateFuncLocal(batchTime, i)</span><br><span class="line">  &#125;</span><br><span class="line">  val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</span><br><span class="line">  val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</span><br><span class="line">  Some(stateRDD)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>åœ¨è¿™é‡Œä¸¤ä¸ªRDDè¿›è¡Œcogroupç„¶ååº”ç”¨updateStateByKeyä¼ å…¥çš„å‡½æ•°ã€‚æˆ‘ä»¬çŸ¥é“cogroupçš„æ€§èƒ½æ˜¯æ¯”è¾ƒä½ä¸‹ï¼Œå‚è€ƒ<a href="http://lxw1234.com/archives/2015/07/384.htmã€‚" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/07/384.htmã€‚</a></p><h5 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState:"></a>mapWithState:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Experimental</span><br><span class="line">def mapWithState[StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    spec: StateSpec[K, V, StateType, MappedType]</span><br><span class="line">  ): MapWithStateDStream[K, V, StateType, MappedType] = &#123;</span><br><span class="line">  new MapWithStateDStreamImpl[K, V, StateType, MappedType](</span><br><span class="line">    self,</span><br><span class="line">    spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>è¯´æ˜ï¼šStateSpec å°è£…äº†çŠ¶æ€ç®¡ç†å‡½æ•°ï¼Œå¹¶åœ¨è¯¥æ–¹æ³•ä¸­åˆ›å»ºäº†MapWithStateDStreamImplå¯¹è±¡ã€‚</p><p>MapWithStateDStreamImpl ä¸­åˆ›å»ºäº†ä¸€ä¸ªInternalMapWithStateDStreamç±»å‹å¯¹è±¡internalStreamï¼Œåœ¨MapWithStateDStreamImplçš„computeæ–¹æ³•ä¸­è°ƒç”¨äº†internalStreamçš„getOrComputeæ–¹æ³•ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private[streaming] class MapWithStateDStreamImpl[</span><br><span class="line">    KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    dataStream: DStream[(KeyType, ValueType)],</span><br><span class="line">    spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType])</span><br><span class="line">  extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123;</span><br><span class="line"></span><br><span class="line">  private val internalStream =</span><br><span class="line">    new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec)</span><br><span class="line"></span><br><span class="line">  override def slideDuration: Duration = internalStream.slideDuration</span><br><span class="line"></span><br><span class="line">  override def dependencies: List[DStream[_]] = List(internalStream)</span><br><span class="line"></span><br><span class="line">  override def compute(validTime: Time): Option[RDD[MappedType]] = &#123;</span><br><span class="line">    internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>InternalMapWithStateDStreamä¸­æ²¡æœ‰getOrComputeæ–¹æ³•ï¼Œè¿™é‡Œè°ƒç”¨çš„æ˜¯å…¶çˆ¶ç±» DStream çš„getOrCpmputeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸­æœ€ç»ˆä¼šè°ƒç”¨InternalMapWithStateDStreamçš„Computeæ–¹æ³•ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">/** Method that generates an RDD for the given time */</span><br><span class="line">override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123;</span><br><span class="line">  // Get the previous state or create a new empty state RDD</span><br><span class="line">  val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123;</span><br><span class="line">    case Some(rdd) =&gt;</span><br><span class="line">      if (rdd.partitioner != Some(partitioner)) &#123;</span><br><span class="line">        // If the RDD is not partitioned the right way, let us repartition it using the</span><br><span class="line">        // partition index as the key. This is to ensure that state RDD is always partitioned</span><br><span class="line">        // before creating another state RDD using it</span><br><span class="line">        MapWithStateRDD.createFromRDD[K, V, S, E](</span><br><span class="line">          rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line">    case None =&gt;</span><br><span class="line">      MapWithStateRDD.createFromPairRDD[K, V, S, E](</span><br><span class="line">        spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)),</span><br><span class="line">        partitioner,</span><br><span class="line">        validTime</span><br><span class="line">      )</span><br><span class="line">  &#125;</span><br><span class="line">  // Compute the new state RDD with previous state RDD and partitioned data RDD</span><br><span class="line">  // Even if there is no data RDD, use an empty one to create a new state RDD</span><br><span class="line">  val dataRDD = parent.getOrCompute(validTime).getOrElse &#123;</span><br><span class="line">    context.sparkContext.emptyRDD[(K, V)]</span><br><span class="line">  &#125;</span><br><span class="line">  val partitionedDataRDD = dataRDD.partitionBy(partitioner)</span><br><span class="line">  val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt;</span><br><span class="line">    (validTime - interval).milliseconds</span><br><span class="line">  &#125;</span><br><span class="line">  Some(new MapWithStateRDD(</span><br><span class="line">    prevStateRDD, partitionedDataRDD, mappingFunction, </span><br><span class="line">    validTime, timeoutThresholdTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>æ ¹æ®ç»™å®šçš„æ—¶é—´ç”Ÿæˆä¸€ä¸ªMapWithStateRDDï¼Œé¦–å…ˆè·å–äº†å…ˆå‰çŠ¶æ€çš„RDDï¼špreStateRDDå’Œå½“å‰æ—¶é—´çš„RDD:dataRDDï¼Œç„¶åå¯¹dataRDDåŸºäºå…ˆå‰çŠ¶æ€RDDçš„åˆ†åŒºå™¨è¿›è¡Œé‡æ–°åˆ†åŒºè·å–partitionedDataRDDã€‚æœ€åå°†preStateRDDï¼ŒpartitionedDataRDDå’Œç”¨æˆ·å®šä¹‰çš„å‡½æ•°mappingFunctionä¼ ç»™æ–°ç”Ÿæˆçš„MapWithStateRDDå¯¹è±¡è¿”å›ã€‚</p><p>åç»­è‹¥æœ‰å…´è¶£å¯ä»¥ç»§ç»­è·Ÿè¿›MapWithStateRDDçš„computeæ–¹æ³•ï¼Œé™äºç¯‡å¹…ä¸å†å±•ç¤ºã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linuxç³»ç»Ÿé‡è¦å‚æ•°è°ƒä¼˜ï¼Œä½ çŸ¥é“å—</title>
      <link href="/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/"/>
      <url>/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="å½“å‰ä¼šè¯ç”Ÿæ•ˆ"><a href="#å½“å‰ä¼šè¯ç”Ÿæ•ˆ" class="headerlink" title="å½“å‰ä¼šè¯ç”Ÿæ•ˆ"></a>å½“å‰ä¼šè¯ç”Ÿæ•ˆ</h4><p>ulimit -u -&gt; æŸ¥çœ‹å½“å‰æœ€å¤§è¿›ç¨‹æ•°<br>ulimit -n -&gt;æŸ¥çœ‹å½“å‰æœ€å¤§æ–‡ä»¶æ•°<br>ulimit -u xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§è¿›ç¨‹æ•°ä¸ºxxx<br>ulimit -n xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§æ–‡ä»¶æ•°ä¸ºxxx</p><h4 id="æ°¸ä¹…ç”Ÿæ•ˆ"><a href="#æ°¸ä¹…ç”Ÿæ•ˆ" class="headerlink" title="æ°¸ä¹…ç”Ÿæ•ˆ"></a>æ°¸ä¹…ç”Ÿæ•ˆ</h4><p>1.vi /etc/security/limits.confï¼Œæ·»åŠ å¦‚ä¸‹çš„è¡Œ</p><ul><li>soft noproc 11000</li><li>hard noproc 11000</li><li>soft nofile 4100</li><li>hard nofile 4100<a id="more"></a> è¯´æ˜ï¼š</li><li>ä»£è¡¨é’ˆå¯¹æ‰€æœ‰ç”¨æˆ·<br>noproc æ˜¯ä»£è¡¨æœ€å¤§è¿›ç¨‹æ•°<br>nofile æ˜¯ä»£è¡¨æœ€å¤§æ–‡ä»¶æ‰“å¼€æ•°</li></ul><h4 id="2-è®©-SSH-æ¥å—-Login-ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨-ssh-å®¢æˆ·ç«¯æŸ¥çœ‹-ulimit-a-èµ„æºé™åˆ¶ï¼š"><a href="#2-è®©-SSH-æ¥å—-Login-ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨-ssh-å®¢æˆ·ç«¯æŸ¥çœ‹-ulimit-a-èµ„æºé™åˆ¶ï¼š" class="headerlink" title="2.è®© SSH æ¥å— Login ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨ ssh å®¢æˆ·ç«¯æŸ¥çœ‹ ulimit -a èµ„æºé™åˆ¶ï¼š"></a>2.è®© SSH æ¥å— Login ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨ ssh å®¢æˆ·ç«¯æŸ¥çœ‹ ulimit -a èµ„æºé™åˆ¶ï¼š</h4><ul><li>1)ã€vi /etc/ssh/sshd_config<br>æŠŠ UserLogin çš„å€¼æ”¹ä¸º yesï¼Œå¹¶æŠŠ # æ³¨é‡Šå»æ‰</li><li>2)ã€é‡å¯ sshd æœåŠ¡<br>/etc/init.d/sshd restart</li><li>3)ã€ä¿®æ”¹æ‰€æœ‰ linux ç”¨æˆ·çš„ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼š<br>vi /etc/profile<br>ulimit -u 10000<br>ulimit -n 4096<br>ulimit -d unlimited<br>ulimit -m unlimited<br>ulimit -s unlimited<br>ulimit -t unlimited<br>ulimit -v unlimited</li><li>4)ã€ç”Ÿæ•ˆ<br>source /etc/profile</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkåŠ¨æ€å†…å­˜ç®¡ç†æºç è§£æï¼</title>
      <link href="/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/"/>
      <url>/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼"><a href="#ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼" class="headerlink" title="ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼"></a>ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼</h4><p>Sparkæœ‰ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œé™æ€å†…å­˜ç®¡ç†(Static MemoryManager)å’ŒåŠ¨æ€ï¼ˆç»Ÿä¸€ï¼‰å†…å­˜ç®¡ç†ï¼ˆUnified MemoryManagerï¼‰ã€‚åŠ¨æ€å†…å­˜ç®¡ç†ä»Spark1.6å¼€å§‹å¼•å…¥ï¼Œåœ¨SparkEnv.scalaä¸­çš„æºç å¯ä»¥çœ‹åˆ°ï¼ŒSparkç›®å‰é»˜è®¤é‡‡ç”¨åŠ¨æ€å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œè‹¥å°†spark.memory.useLegacyModeè®¾ç½®ä¸ºtrueï¼Œåˆ™ä¼šæ”¹ä¸ºé‡‡ç”¨é™æ€å†…å­˜ç®¡ç†ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// SparkEnv.scala</span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…"><a href="#äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…" class="headerlink" title="äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…"></a>äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…</h4><p><img src="/assets/blogImg/603_1.png" alt="enter description here"><br>ç›¸æ¯”äºStatic MemoryManageræ¨¡å¼ï¼ŒUnified MemoryManageræ¨¡å‹æ‰“ç ´äº†å­˜å‚¨å†…å­˜å’Œè¿è¡Œå†…å­˜çš„ç•Œé™ï¼Œä½¿æ¯ä¸€ä¸ªå†…å­˜åŒºèƒ½å¤ŸåŠ¨æ€ä¼¸ç¼©ï¼Œé™ä½OOMçš„æ¦‚ç‡ã€‚ç”±ä¸Šå›¾å¯çŸ¥ï¼Œexecutor JVMå†…å­˜ä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªåŒºåŸŸç»„æˆï¼š</p><ul><li>ï¼ˆ1ï¼‰Reserved Memoryï¼ˆé¢„ç•™å†…å­˜ï¼‰ï¼šè¿™éƒ¨åˆ†å†…å­˜é¢„ç•™ç»™ç³»ç»Ÿä½¿ç”¨ï¼Œé»˜è®¤ä¸º300MBï¼Œå¯é€šè¿‡spark.testing.reservedMemoryè¿›è¡Œè®¾ç½®ã€‚<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024</span><br></pre></td></tr></table></figure></li></ul><p>å¦å¤–ï¼ŒJVMå†…å­˜çš„æœ€å°å€¼ä¹Ÿä¸reserved Memoryæœ‰å…³ï¼Œå³minSystemMemory = reserved Memory<em>1.5ï¼Œå³é»˜è®¤æƒ…å†µä¸‹JVMå†…å­˜æœ€å°å€¼ä¸º300MB</em>1.5=450MBã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong</span><br></pre></td></tr></table></figure><p></p><ul><li>ï¼ˆ2ï¼‰Spark Memeoy:åˆ†ä¸ºexecution Memoryå’Œstorage Memoryã€‚å»é™¤æ‰reserved Memoryï¼Œå‰©ä¸‹usableMemoryçš„ä¸€éƒ¨åˆ†ç”¨äºexecutionå’Œstorageè¿™ä¸¤ç±»å †å†…å­˜ï¼Œé»˜è®¤æ˜¯0.6ï¼Œå¯é€šè¿‡spark.memory.fractionè¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼šJVMå†…å­˜æ˜¯1Gï¼Œé‚£ä¹ˆç”¨äºexecutionå’Œstorageçš„é»˜è®¤å†…å­˜ä¸ºï¼ˆ1024-300ï¼‰*0.6=434MBã€‚<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val usableMemory = systemMemory - reservedMemory</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    (usableMemory * memoryFraction).toLong</span><br></pre></td></tr></table></figure></li></ul><p>ä»–ä»¬çš„è¾¹ç•Œç”±spark.memory.storageFractionè®¾å®šï¼Œé»˜è®¤ä¸º0.5ã€‚å³é»˜è®¤çŠ¶æ€ä¸‹storage Memoryå’Œexecution Memoryä¸º1ï¼š1.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">     onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</span><br><span class="line">      numCores = numCores)</span><br></pre></td></tr></table></figure><p></p><ul><li>ï¼ˆ3ï¼‰user Memory:å‰©ä½™å†…å­˜ï¼Œç”¨æˆ·æ ¹æ®éœ€è¦ä½¿ç”¨ï¼Œé»˜è®¤å usableMemoryçš„ï¼ˆ1-0.6ï¼‰=0.4.</li></ul><h5 id="ä¸‰ã€å†…å­˜æ§åˆ¶è¯¦è§£"><a href="#ä¸‰ã€å†…å­˜æ§åˆ¶è¯¦è§£" class="headerlink" title="ä¸‰ã€å†…å­˜æ§åˆ¶è¯¦è§£"></a>ä¸‰ã€å†…å­˜æ§åˆ¶è¯¦è§£</h5><p>é¦–å…ˆæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹Sparkå†…å­˜ç®¡ç†å®ç°ç±»ä¹‹å‰çš„å…³ç³»ã€‚<br><img src="/assets/blogImg/603_2.png" alt="enter description here"></p><h5 id="1-MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼š"><a href="#1-MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼š" class="headerlink" title="1.MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼š"></a>1.MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼š</h5><ul><li>ï¼ˆ1ï¼‰è®°å½•ç”¨äº†å¤šå°‘StorageMemoryå’ŒExecutionMemoryï¼›</li><li>ï¼ˆ2ï¼‰ç”³è¯·Storageã€Executionå’ŒUnroll Memoryï¼›</li><li>ï¼ˆ3ï¼‰é‡Šæ”¾Stroageå’ŒExecution Memoryã€‚</li></ul><p>Executionå†…å­˜ç”¨æ¥æ‰§è¡Œshuffleã€joinsã€sortså’Œaggegationsæ“ä½œï¼ŒStorageå†…å­˜ç”¨äºç¼“å­˜å’Œå¹¿æ’­æ•°æ®ï¼Œæ¯ä¸€ä¸ªJVMä¸­éƒ½å­˜åœ¨ç€ä¸€ä¸ªMemoryManagerã€‚æ„é€ MemoryManageréœ€è¦æŒ‡å®šonHeapStorageMemoryå’ŒonHeapExecutionMemoryå‚æ•°ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> // MemoryManager.scala</span><br><span class="line">private[spark] abstract class MemoryManager(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    numCores: Int,</span><br><span class="line">    onHeapStorageMemory: Long,</span><br><span class="line">    onHeapExecutionMemory: Long) extends Logging &#123;</span><br></pre></td></tr></table></figure><p></p><p>åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPoolå¯¹è±¡ï¼Œç”¨æ¥åˆ›å»ºå †å†…æˆ–å †å¤–çš„Storageå’ŒExecutionå†…å­˜æ± ï¼Œç®¡ç†Storageå’ŒExecutionçš„å†…å­˜åˆ†é…ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)</span><br></pre></td></tr></table></figure><p></p><p>é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ä½¿ç”¨å †å¤–å†…å­˜ï¼Œå¯é€šè¿‡saprk.memory.offHeap.enabledè®¾ç½®ï¼Œé»˜è®¤å †å¤–å†…å­˜ä¸º0ï¼Œå¯ä½¿ç”¨spark.memory.offHeap.sizeå‚æ•°è®¾ç½®ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// All the code you will ever need</span><br><span class="line"> final val tungstenMemoryMode: MemoryMode = &#123;</span><br><span class="line">    if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123;</span><br><span class="line">      require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0,</span><br><span class="line">        &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;)</span><br><span class="line">      require(Platform.unaligned(),</span><br><span class="line">        &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;)</span><br><span class="line">      MemoryMode.OFF_HEAP</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MemoryMode.ON_HEAP</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)</span><br></pre></td></tr></table></figure><p>é‡Šæ”¾numByteså­—èŠ‚çš„Executionå†…å­˜æ–¹æ³•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>é‡Šæ”¾æŒ‡å®štaskçš„æ‰€æœ‰Executionå†…å­˜å¹¶å°†è¯¥taskæ ‡è®°ä¸ºinactiveã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123;</span><br><span class="line">    onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) +</span><br><span class="line">      offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>é‡Šæ”¾numByteså­—èŠ‚çš„Stoargeå†…å­˜æ–¹æ³•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>é‡Šæ”¾æ‰€æœ‰Storageå†…å­˜æ–¹æ³•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">final def releaseAllStorageMemory(): Unit = synchronized &#123;</span><br><span class="line">    onHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">    offHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="2-æ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡Œæ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ"><a href="#2-æ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡Œæ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ" class="headerlink" title="2.æ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡Œæ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ"></a>2.æ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡Œæ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ</h5><p>UnifiedMemoryManageç»§æ‰¿äº†MemoryManager<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">private[spark] class UnifiedMemoryManager private[memory] (</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    val maxHeapMemory: Long,</span><br><span class="line">    onHeapStorageRegionSize: Long,</span><br><span class="line">    numCores: Int)</span><br><span class="line">  extends MemoryManager(</span><br><span class="line">    conf,</span><br><span class="line">    numCores,</span><br><span class="line">    onHeapStorageRegionSize,</span><br><span class="line">    maxHeapMemory - onHeapStorageRegionSize) &#123;</span><br></pre></td></tr></table></figure><p></p><p>é‡å†™äº†maxOnHeapStorageMemoryæ–¹æ³•ï¼Œæœ€å¤§Storageå†…å­˜=æœ€å¤§å†…å­˜-æœ€å¤§Executionå†…å­˜ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line"> override def maxOnHeapStorageMemory: Long = synchronized &#123;</span><br><span class="line">    maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>æ ¸å¿ƒæ–¹æ³•acquireStorageMemoryï¼šç”³è¯·Storageå†…å­˜ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override def acquireStorageMemory(</span><br><span class="line">      blockId: BlockId,</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      memoryMode: MemoryMode): Boolean = synchronized &#123;</span><br><span class="line">    assertInvariants()</span><br><span class="line">    assert(numBytes &gt;= 0)</span><br><span class="line">    val (executionPool, storagePool, maxMemory) = memoryMode match &#123;</span><br><span class="line">      //æ ¹æ®ä¸åŒçš„å†…å­˜æ¨¡å¼å»åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPool</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; (</span><br><span class="line">        onHeapExecutionMemoryPool,</span><br><span class="line">        onHeapStorageMemoryPool,</span><br><span class="line">        maxOnHeapStorageMemory)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; (</span><br><span class="line">        offHeapExecutionMemoryPool,</span><br><span class="line">        offHeapStorageMemoryPool,</span><br><span class="line">        maxOffHeapMemory)</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; maxMemory) &#123;</span><br><span class="line">      // è‹¥ç”³è¯·å†…å­˜å¤§äºæœ€å¤§å†…å­˜ï¼Œåˆ™ç”³è¯·å¤±è´¥</span><br><span class="line">      logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; +</span><br><span class="line">        s&quot;memory limit ($maxMemory bytes)&quot;)</span><br><span class="line">      return false</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">      // å¦‚æœStorageå†…å­˜æ± æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œåˆ™å‘Executionå†…å­˜æ± å€Ÿç”¨</span><br><span class="line">      val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//å½“Executionå†…å­˜æœ‰ç©ºé—²æ—¶ï¼ŒStorageæ‰èƒ½å€Ÿåˆ°å†…å­˜</span><br><span class="line">      executionPool.decrementPoolSize(memoryBorrowedFromExecution)//ç¼©å°Executionå†…å­˜</span><br><span class="line">      storagePool.incrementPoolSize(memoryBorrowedFromExecution)//å¢åŠ Storageå†…å­˜</span><br><span class="line">    &#125;</span><br><span class="line">    storagePool.acquireMemory(blockId, numBytes)</span><br></pre></td></tr></table></figure><p></p><p>æ ¸å¿ƒæ–¹æ³•acquireExecutionMemoryï¼šç”³è¯·Executionå†…å­˜ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override private[memory] def acquireExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Long = synchronized &#123;//ä½¿ç”¨äº†synchronizedå…³é”®å­—ï¼Œè°ƒç”¨acquireExecutionMemoryæ–¹æ³•å¯èƒ½ä¼šé˜»å¡ï¼Œç›´åˆ°Executionå†…å­˜æ± æœ‰è¶³å¤Ÿçš„å†…å­˜ã€‚</span><br><span class="line">   ...</span><br><span class="line">    executionPool.acquireMemory(</span><br><span class="line">      numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>æ–¹æ³•æœ€åè°ƒç”¨äº†ExecutionMemoryPoolçš„acquireMemoryæ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„å‚æ•°éœ€è¦ä¸¤ä¸ªå‡½æ•°ï¼šmaybeGrowExecutionPool()å’ŒcomputeMaxExecutionPoolSize()ã€‚<br>æ¯ä¸ªTaskèƒ½å¤Ÿä½¿ç”¨çš„å†…å­˜è¢«é™åˆ¶åœ¨pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasksã€‚å…¶ä¸­maxPoolSizeä»£è¡¨äº†execution poolçš„æœ€å¤§å†…å­˜ï¼ŒpoolSizeè¡¨ç¤ºå½“å‰è¿™ä¸ªpoolçš„å¤§å°ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// ExecutionMemoryPool.scala</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">      val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br></pre></td></tr></table></figure><p></p><p>maybeGrowExecutionPool()æ–¹æ³•å®ç°äº†å¦‚ä½•åŠ¨æ€å¢åŠ Executionå†…å­˜åŒºçš„å¤§å°ã€‚<br>åœ¨æ¯æ¬¡ç”³è¯·executionå†…å­˜çš„åŒæ—¶ï¼Œexecutionå†…å­˜æ± ä¼šè¿›è¡Œå¤šæ¬¡å°è¯•ï¼Œæ¯æ¬¡å°è¯•éƒ½å¯èƒ½ä¼šå›æ”¶ä¸€äº›å­˜å‚¨å†…å­˜ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">     def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">      if (extraMemoryNeeded &gt; 0) &#123;//å¦‚æœç”³è¯·çš„å†…å­˜å¤§äº0</span><br><span class="line">        //è®¡ç®—executionå¯å€Ÿåˆ°çš„storageå†…å­˜ï¼Œæ˜¯storageå‰©ä½™å†…å­˜å’Œå¯å€Ÿå‡ºå†…å­˜çš„æœ€å¤§å€¼</span><br><span class="line">        val memoryReclaimableFromStorage = math.max(</span><br><span class="line">          storagePool.memoryFree,</span><br><span class="line">          storagePool.poolSize - storageRegionSize)</span><br><span class="line">        if (memoryReclaimableFromStorage &gt; 0) &#123;//å¦‚æœå¯ä»¥ç”³è¯·åˆ°å†…å­˜</span><br><span class="line">          val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">            math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//å®é™…éœ€è¦çš„å†…å­˜ï¼Œå–å®é™…éœ€è¦çš„å†…å­˜å’Œstorageå†…å­˜åŒºåŸŸå…¨éƒ¨å¯ç”¨å†…å­˜å¤§å°çš„æœ€å°å€¼</span><br><span class="line">          storagePool.decrementPoolSize(spaceToReclaim)//storageå†…å­˜åŒºåŸŸå‡å°‘</span><br><span class="line">          executionPool.incrementPoolSize(spaceToReclaim)//executionå†…å­˜åŒºåŸŸå¢åŠ </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> æºç é˜…è¯» </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è‹¥æ³½å¤§æ•°æ®-é›¶åŸºç¡€å­¦å‘˜æ·±åœ³æŸå¸é«˜è–ªé¢è¯•é¢˜</title>
      <link href="/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>å•¥ä¹Ÿä¸è¯´ï¼ç›´æ¥ä¸Šé¢˜</p><p>é¢è¯•æ—¶é—´ï¼š20180531</p><ul><li>ç®€å•è¯´ä¸‹hdfsè¯»æ–‡ä»¶å’Œå†™æ–‡ä»¶çš„æµç¨‹</li><li>æ¯å¤©æ•°æ®é‡æœ‰å¤šå¤§ï¼Ÿç”Ÿäº§é›†ç¾¤è§„æ¨¡æœ‰å¤šå¤§ï¼Ÿ</li><li>è¯´å‡ ä¸ªsparkå¼€å‘ä¸­é‡åˆ°çš„é—®é¢˜ï¼Œå’Œè§£å†³çš„æ–¹æ¡ˆ</li><li>é˜è¿°ä¸€ä¸‹æœ€è¿‘å¼€å‘çš„é¡¹ç›®ï¼Œä»¥åŠæ‹…ä»»çš„è§’è‰²ä½ç½®</li><li>kafkaæœ‰åšè¿‡å“ªäº›è°ƒä¼˜</li><li>æˆ‘ä»¬é¡¹ç›®ä¸­æ•°æ®å€¾æ–œçš„åœºæ™¯å’Œè§£å†³æ–¹æ¡ˆ</li></ul><p>é›¶åŸºç¡€â•å››ä¸ªæœˆç´§è·Ÿè‹¥æ³½å¤§æ•°æ®å­¦ä¹ ä¹‹åæ˜¯è¿™æ ·</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> é¢è¯•é¢˜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¤§æ•°æ®é¢è¯•é¢˜ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä»Hiveä¸­çš„stored as file_foramtçœ‹hiveè°ƒä¼˜</title>
      <link href="/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/"/>
      <url>/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”"><a href="#ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”" class="headerlink" title="ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”"></a>ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”</h4><h5 id="1ã€å­˜å‚¨æ¯”è¾ƒ"><a href="#1ã€å­˜å‚¨æ¯”è¾ƒ" class="headerlink" title="1ã€å­˜å‚¨æ¯”è¾ƒ"></a>1ã€å­˜å‚¨æ¯”è¾ƒ</h5><p>è¡Œå¼æ•°æ®åº“å­˜å‚¨åœ¨hdfsä¸Šå¼æŒ‰è¡Œè¿›è¡Œå­˜å‚¨çš„ï¼Œä¸€ä¸ªblockå­˜å‚¨ä¸€æˆ–å¤šè¡Œæ•°æ®ã€‚è€Œåˆ—å¼æ•°æ®åº“åœ¨hdfsä¸Šåˆ™æ˜¯æŒ‰ç…§åˆ—è¿›è¡Œå­˜å‚¨ï¼Œä¸€ä¸ªblockå¯èƒ½æœ‰ä¸€åˆ—æˆ–å¤šåˆ—æ•°æ®ã€‚</p><h5 id="2ã€å‹ç¼©æ¯”è¾ƒ"><a href="#2ã€å‹ç¼©æ¯”è¾ƒ" class="headerlink" title="2ã€å‹ç¼©æ¯”è¾ƒ"></a>2ã€å‹ç¼©æ¯”è¾ƒ</h5><a id="more"></a><p>å¯¹äºè¡Œå¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰è¡Œå‹ç¼©ï¼Œå½“ä¸€è¡Œä¸­æœ‰å¤šä¸ªå­—æ®µï¼Œå„ä¸ªå­—æ®µå¯¹åº”çš„æ•°æ®ç±»å‹å¯èƒ½ä¸ä¸€è‡´ï¼Œå‹ç¼©æ€§èƒ½å‹ç¼©æ¯”å°±æ¯”è¾ƒå·®ã€‚</p><p>å¯¹äºåˆ—å¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰åˆ—å‹ç¼©ï¼Œæ¯ä¸€åˆ—å¯¹åº”çš„æ˜¯ç›¸åŒæ•°æ®ç±»å‹çš„æ•°æ®ï¼Œæ•…åˆ—å¼æ•°æ®åº“çš„å‹ç¼©æ€§èƒ½è¦å¼ºäºè¡Œå¼æ•°æ®åº“ã€‚</p><h5 id="3ã€æŸ¥è¯¢æ¯”è¾ƒ"><a href="#3ã€æŸ¥è¯¢æ¯”è¾ƒ" class="headerlink" title="3ã€æŸ¥è¯¢æ¯”è¾ƒ"></a>3ã€æŸ¥è¯¢æ¯”è¾ƒ</h5><p>å‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect id,name from table_emp;</p><p>å¯¹äºè¡Œå¼æ•°æ®åº“ï¼Œå®ƒè¦éå†ä¸€æ•´å¼ è¡¨å°†æ¯ä¸€è¡Œä¸­çš„id,nameå­—æ®µæ‹¼æ¥å†å±•ç°å‡ºæ¥ï¼Œè¿™æ ·éœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°±æ¯”è¾ƒå¤§ï¼Œæ•ˆç‡ä½ã€‚</p><p>å¯¹äºåˆ—å¼æ•°æ®åº“ï¼Œå®ƒåªéœ€æ‰¾åˆ°å¯¹åº”çš„id,nameå­—æ®µçš„åˆ—å±•ç°å‡ºæ¥å³å¯ï¼Œéœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°ï¼Œæ•ˆç‡é«˜ã€‚</p><p>å‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect * from table_emp;</p><p>å¯¹äºè¿™ç§æŸ¥è¯¢æ•´ä¸ªè¡¨å…¨éƒ¨ä¿¡æ¯çš„æ“ä½œï¼Œç”±äºåˆ—å¼æ•°æ®åº“éœ€è¦å°†åˆ†æ•£çš„è¡Œè¿›è¡Œé‡æ–°ç»„åˆï¼Œè¡Œå¼æ•°æ®åº“æ•ˆç‡å°±é«˜äºåˆ—å¼æ•°æ®åº“ã€‚</p><p><strong><font color="#FF4500">ä½†æ˜¯ï¼Œåœ¨å¤§æ•°æ®é¢†åŸŸï¼Œè¿›è¡Œå…¨è¡¨æŸ¥è¯¢çš„åœºæ™¯å°‘ä¹‹åˆå°‘ï¼Œè¿›è€Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå¤šçš„è¿˜æ˜¯åˆ—å¼æ•°æ®åº“åŠåˆ—å¼å‚¨å­˜ã€‚</font></strong></p><h4 id="äºŒã€stored-as-file-format-è¯¦è§£"><a href="#äºŒã€stored-as-file-format-è¯¦è§£" class="headerlink" title="äºŒã€stored as file_format è¯¦è§£"></a>äºŒã€stored as file_format è¯¦è§£</h4><h5 id="1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored-as-file-formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚"><a href="#1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored-as-file-formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚" class="headerlink" title="1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored as file_formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚"></a>1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored as file_formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tt (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CREATE TABLE tt2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;</span><br><span class="line"></span><br><span class="line">CREATE TABLE tt3 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS </span><br><span class="line">INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;</span><br><span class="line">OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;</span><br><span class="line"></span><br><span class="line">#ä»¥ä¸Šä¸‰ç§æ–¹å¼å­˜å‚¨çš„æ ¼å¼éƒ½æ˜¯TEXTFILEã€‚</span><br></pre></td></tr></table></figure><h5 id="2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ"><a href="#2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ" class="headerlink" title="2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ"></a>2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">file_format:</span><br><span class="line">  | SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p><strong>TEXTFILE:</strong> åªæ˜¯hiveä¸­è¡¨æ•°æ®é»˜è®¤çš„å­˜å‚¨æ ¼å¼ï¼Œå®ƒå°†æ‰€æœ‰ç±»å‹çš„æ•°æ®éƒ½å­˜å‚¨ä¸ºStringç±»å‹ï¼Œä¸ä¾¿äºæ•°æ®çš„è§£æï¼Œä½†å®ƒå´æ¯”è¾ƒé€šç”¨ã€‚ä¸å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒå‹ç¼©ã€‚</p><p><strong>SEQUENCEFILE:</strong> è¿™ç§å‚¨å­˜æ ¼å¼æ¯”TEXTFILEæ ¼å¼å¤šäº†å¤´éƒ¨ã€æ ‡è¯†ã€ä¿¡æ¯é•¿åº¦ç­‰ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä½¿å¾—å…¶å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒå‹ç¼©ï¼Œä½†å‹ç¼©çš„æ˜¯valueã€‚ï¼ˆå­˜å‚¨ç›¸åŒçš„æ•°æ®ï¼ŒSEQUENCEFILEæ¯”TEXTFILEç•¥å¤§ï¼‰</p><p><strong>RCFILEï¼ˆRecord Columnar Fileï¼‰:</strong> ç°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¾ˆå¤šä¸ªRow Group,æ¯ä¸ªRow Groupé»˜è®¤å¤§å°4MBï¼ŒRow Groupå†…éƒ¨å†æŒ‰åˆ—å­˜å‚¨ä¿¡æ¯ã€‚ç”±facebookå¼€æºï¼Œæ¯”æ ‡å‡†è¡Œå¼å­˜å‚¨èŠ‚çº¦10%çš„ç©ºé—´ã€‚</p><p><strong>ORC:</strong> ä¼˜åŒ–è¿‡åçš„RCFile,ç°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¤šä¸ªStripes,å†åœ¨Stripeä¸­æŒ‰åˆ—å­˜å‚¨ã€‚æ¯ä¸ªStripeç”±ä¸€ä¸ªIndex Dataã€ä¸€ä¸ªRow Dataã€ä¸€ä¸ªStripe Footerç»„æˆã€‚æ¯ä¸ªStripesçš„å¤§å°ä¸º250MBï¼Œæ¯ä¸ªIndex Dataè®°å½•çš„æ˜¯æ•´å‹æ•°æ®æœ€å¤§å€¼æœ€å°å€¼ã€å­—ç¬¦ä¸²æ•°æ®å‰åç¼€ä¿¡æ¯ï¼Œæ¯ä¸ªåˆ—çš„ä½ç½®ç­‰ç­‰è¯¸å¦‚æ­¤ç±»çš„ä¿¡æ¯ã€‚è¿™å°±ä½¿å¾—æŸ¥è¯¢ååˆ†å¾—é«˜æ•ˆï¼Œé»˜è®¤æ¯ä¸€ä¸‡è¡Œæ•°æ®å»ºç«‹ä¸€ä¸ªIndex Dataã€‚ORCå­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„40%å·¦å³ï¼Œä½¿ç”¨å‹ç¼©åˆ™å¯ä»¥è¿›ä¸€æ­¥å°†è¿™ä¸ªæ•°å­—é™åˆ°10%~20%ã€‚</p><p><strong>ORCè¿™ç§æ–‡ä»¶æ ¼å¼å¯ä»¥ä½œç”¨äºè¡¨æˆ–è€…è¡¨çš„åˆ†åŒºï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç§æ–¹å¼è¿›è¡ŒæŒ‡å®šï¼š</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure><p></p><p>The parameters are all placed in the TBLPROPERTIES (see Create Table). They are:</p><p>Key|Default|Notes<br>|-|-|-|<br>orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)<br>|orc.compress.size|262,144|number of bytes in each compression chunk<br>|orc.stripe.size|67,108,864|number of bytes in each stripe<br>|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)<br>|orc.create.index|true|whether to create row indexes<br>|orc.bloom.filter.columns |â€â€| comma separated list of column names for which bloom filter should be created<br>|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</p><p>ç¤ºä¾‹ï¼šåˆ›å»ºå¸¦å‹ç¼©çš„ORCå­˜å‚¨è¡¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table Addresses (</span><br><span class="line">  name string,</span><br><span class="line">  street string,</span><br><span class="line">  city string,</span><br><span class="line">  state string,</span><br><span class="line">  zip int</span><br><span class="line">) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure><p></p><p>PARQUET: å­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„60%~70%ï¼Œå‹ç¼©ååœ¨20%~30%ä¹‹é—´ã€‚</p><hr><p>æ³¨æ„ï¼š</p><ol><li><p>ä¸åŒçš„å­˜å‚¨æ ¼å¼ä¸ä»…è¡¨ç°åœ¨å­˜å‚¨ç©ºé—´ä¸Šçš„ä¸åŒï¼Œå¯¹äºæ•°æ®çš„æŸ¥è¯¢ï¼Œæ•ˆç‡ä¹Ÿä¸ä¸€æ ·ã€‚å› ä¸ºå¯¹äºä¸åŒçš„å­˜å‚¨æ ¼å¼ï¼Œæ‰§è¡Œç›¸åŒçš„æŸ¥è¯¢æ“ä½œï¼Œä»–ä»¬è®¿é—®çš„æ•°æ®é‡å¤§å°æ˜¯ä¸ä¸€æ ·çš„ã€‚</p></li><li><p>å¦‚æœè¦ä½¿ç”¨TEXTFILEä½œä¸ºhiveè¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œåˆ™å¿…é¡»å…ˆå­˜åœ¨ä¸€å¼ ç›¸åŒæ•°æ®çš„å­˜å‚¨æ ¼å¼ä¸ºTEXTFILEçš„è¡¨table_t0,ç„¶ååœ¨å»ºè¡¨æ—¶ä½¿ç”¨â€œinsert into table table_stored_file_ORC select <em>from table_t0;â€åˆ›å»ºã€‚æˆ–è€…ä½¿ç”¨â€create table as select </em>from table_t0;â€åˆ›å»ºã€‚</p></li></ol><hr><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkä¹‹åºåˆ—åŒ–åœ¨ç”Ÿäº§ä¸­çš„åº”ç”¨</title>
      <link href="/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>åºåˆ—åŒ–åœ¨åˆ†å¸ƒå¼åº”ç”¨çš„æ€§èƒ½ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ ¼å¼åŒ–å¯¹è±¡ç¼“æ…¢ï¼Œæˆ–è€…æ¶ˆè€—å¤§é‡çš„å­—èŠ‚æ ¼å¼åŒ–ï¼Œä¼šå¤§å¤§é™ä½è®¡ç®—æ€§èƒ½ã€‚åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåˆ›å»ºå¤§é‡çš„è‡ªå®šä¹‰å®ä½“å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡åœ¨ç½‘ç»œä¼ è¾“æ—¶éœ€è¦åºåˆ—åŒ–ï¼Œè€Œä¸€ç§å¥½çš„åºåˆ—åŒ–æ–¹å¼å¯ä»¥è®©æ•°æ®æœ‰æ›´å¥½çš„å‹ç¼©æ¯”ï¼Œä»è€Œæå‡ç½‘ç»œä¼ è¾“é€Ÿç‡ï¼Œæé«˜sparkä½œä¸šçš„è¿è¡Œé€Ÿåº¦ã€‚é€šå¸¸è¿™æ˜¯åœ¨sparkåº”ç”¨ä¸­ç¬¬ä¸€ä»¶éœ€è¦ä¼˜åŒ–çš„äº‹æƒ…ã€‚Sparkçš„ç›®æ ‡æ˜¯åœ¨ä¾¿åˆ©ä¸æ€§èƒ½ä¸­å–å¾—å¹³è¡¡ï¼Œæ‰€ä»¥æä¾›2ç§åºåˆ—åŒ–çš„é€‰æ‹©ã€‚<br><a id="more"></a></p><h4 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h4><p>åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä¼šä½¿ç”¨Javaçš„ObjectOutputStreamæ¡†æ¶å¯¹å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä»»ä½•å®ç°java.io.Serializableçš„ç±»ä¸€èµ·å·¥ä½œã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æ‰©å±•java.io.Externalizableæ¥æ›´ç´§å¯†åœ°æ§åˆ¶åºåˆ—åŒ–çš„æ€§èƒ½ã€‚Javaåºåˆ—åŒ–æ˜¯çµæ´»çš„ï¼Œä½†é€šå¸¸ç›¸å½“æ…¢ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´è®¸å¤šç±»çš„å¤§å‹åºåˆ—åŒ–æ ¼å¼ã€‚<br><strong>æµ‹è¯•ä»£ç ï¼š</strong><br><img src="/assets/blogImg/529_1.png" alt="enter description here"><br><strong>æµ‹è¯•ç»“æœï¼š</strong><br><img src="/assets/blogImg/529_2.png" alt="enter description here"></p><h4 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h4><p>Sparkè¿˜å¯ä»¥ä½¿ç”¨Kryoåº“ï¼ˆç‰ˆæœ¬2ï¼‰æ¥æ›´å¿«åœ°åºåˆ—åŒ–å¯¹è±¡ã€‚Kryoæ¯”Javaä¸²è¡ŒåŒ–ï¼ˆé€šå¸¸å¤šè¾¾10å€ï¼‰è¦å¿«å¾—å¤šï¼Œä¹Ÿæ›´ç´§å‡‘ï¼Œä½†æ˜¯ä¸æ”¯æŒæ‰€æœ‰å¯ä¸²è¡ŒåŒ–ç±»å‹ï¼Œå¹¶ä¸”è¦æ±‚æ‚¨æå‰æ³¨å†Œæ‚¨å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„ç±»ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚<br><strong>æµ‹è¯•ä»£ç ï¼š</strong><br><img src="/assets/blogImg/529_3.png" alt="enter description here"><br><strong>æµ‹è¯•ç»“æœï¼š</strong><br><img src="/assets/blogImg/529_4.png" alt="enter description here"><br>æµ‹è¯•ç»“æœä¸­å‘ç°ï¼Œä½¿ç”¨ Kryo serialization çš„åºåˆ—åŒ–å¯¹è±¡ æ¯”ä½¿ç”¨ Java serializationçš„åºåˆ—åŒ–å¯¹è±¡è¦å¤§ï¼Œä¸æè¿°çš„ä¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ<br>æŸ¥æ‰¾å®˜ç½‘ï¼Œå‘ç°è¿™ä¹ˆä¸€å¥è¯ Finally, if you donâ€™t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.ã€‚<br>ä¿®æ”¹ä»£ç ååœ¨æµ‹è¯•ä¸€æ¬¡ã€‚<br><img src="/assets/blogImg/529_5.png" alt="enter description here"><br><strong>æµ‹è¯•ç»“æœï¼š</strong><br><img src="/assets/blogImg/529_6.png" alt="enter description here"></p><h4 id="æ€»ç»“ï¼š"><a href="#æ€»ç»“ï¼š" class="headerlink" title="æ€»ç»“ï¼š"></a>æ€»ç»“ï¼š</h4><p>Kryo serialization æ€§èƒ½å’Œåºåˆ—åŒ–å¤§å°éƒ½æ¯”é»˜è®¤æä¾›çš„ Java serialization è¦å¥½ï¼Œä½†æ˜¯ä½¿ç”¨Kryoéœ€è¦å°†è‡ªå®šä¹‰çš„ç±»å…ˆæ³¨å†Œè¿›å»ï¼Œä½¿ç”¨èµ·æ¥æ¯”Java serializationéº»çƒ¦ã€‚è‡ªä»Spark 2.0.0ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ç®€å•ç±»å‹ã€ç®€å•ç±»å‹æ•°ç»„æˆ–å­—ç¬¦ä¸²ç±»å‹çš„ç®€å•ç±»å‹æ¥è°ƒæ•´RDDsæ—¶ï¼Œåœ¨å†…éƒ¨ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨ã€‚<br>é€šè¿‡æŸ¥æ‰¾sparkcontextåˆå§‹åŒ–çš„æºç ï¼Œå¯ä»¥å‘ç°æŸäº›ç±»å‹å·²ç»åœ¨sparkcontextåˆå§‹åŒ–çš„æ—¶å€™è¢«æ³¨å†Œè¿›å»ã€‚<br><img src="/assets/blogImg/529_7.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è‹¥æ³½æ•°æ®å¸¦ä½ éšæ—¶äº†è§£ä¸šç•Œé¢è¯•é¢˜ï¼Œéšæ—¶è·³é«˜è–ª</title>
      <link href="/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/"/>
      <url>/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="é“¾å®¶-ä¸€é¢ï¼ŒäºŒé¢"><a href="#é“¾å®¶-ä¸€é¢ï¼ŒäºŒé¢" class="headerlink" title="é“¾å®¶(ä¸€é¢ï¼ŒäºŒé¢)"></a>é“¾å®¶(ä¸€é¢ï¼ŒäºŒé¢)</h4><p>0.è‡ªæˆ‘ä»‹ç»</p><p>1.å°è£…ç»§æ‰¿å¤šæ€æ¦‚å¿µ</p><p>2.mvcè®¾è®¡æ€æƒ³</p><p>3.çº¿ç¨‹æ± ,çœ‹è¿‡æºç å—<br><a id="more"></a><br>4.sshæ¡†æ¶ä¸­åˆ†åˆ«å¯¹åº”mvcä¸­é‚£ä¸€å±‚</p><p>5.shellå‘½ä»¤ï¼ˆæŸ¥è¯¢ä¸€ä¸ªæ–‡ä»¶æœ‰å¤šå°‘è¡Œã€‚ chown ä¿®æ”¹æ–‡ä»¶æƒé™ï¼Œ åªè®°å¾—é‚£ä¹ˆå¤šäº† ï¼‰</p><p>6.spring ioc aop åŸç†</p><p>7.å•åˆ©æ¨¡å¼</p><p>8.SQLé¢˜ï¼Œæƒ³ä¸èµ·æ¥äº†ã€‚ã€‚</p><p>9.jvm è¿è¡Œæ—¶æ•°æ®åŒºåŸŸ</p><p>10.spring mvcçŸ¥é“å—ã€‚ã€‚</p><p>11.å·¥å‚æ¨¡å¼</p><p>12.mr è®¡ç®—æµç¨‹</p><p>13.hiveæŸ¥è¯¢è¯­å¥ï¼ˆè¡¨1ï¼šæ—¶é—´ é£Ÿå ‚æ¶ˆè´¹ è¡¨äºŒï¼šå„ä¸ªæ—¶é—´æ®µ ç”¨æˆ· æ¯ä¸ªé£Ÿå ‚æ¶ˆè´¹ æŸ¥è¯¢ç”¨æˆ·åœ¨æ¯ä¸ªæ—¶é—´å‡ºç°åœ¨é‚£ä¸ªé£Ÿå ‚ç»Ÿè®¡æ¶ˆè´¹è®°å½• ï¼Œå¤§æ¦‚æ˜¯è¿™æ ·çš„ã€‚ã€‚ï¼‰</p><p>14.gitçš„ä½¿ç”¨</p><p>15.hadoopçš„ç†è§£</p><p>16.hiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„åŒºåˆ«</p><p>17.hiveå­˜å‚¨æ ¼å¼å’Œå‹ç¼©æ ¼å¼</p><p>18.å¯¹sparkäº†è§£å—ï¼Ÿ å½“æ—¶é«˜çº§ç­è¿˜æ²¡å­¦ã€‚ã€‚</p><p>19.hiveäºå…³ç³»å‹æ•°æ®åº“çš„åŒºåˆ«</p><p>20.å„ç§æ’åº æ‰‹å†™å †æ’åº,è¯´è¯´åŸç†</p><p>21.é“¾è¡¨é—®é¢˜ï¼Œæµè§ˆå™¨è®¿é—®è®°å½•ï¼Œå‰è¿›åé€€å½¢æˆé“¾è¡¨ï¼Œæ–°åŠ ä¸€ä¸ªè®°å½•ï¼Œå¤šå‡ºä¸€ä¸ªåˆ†æ”¯ï¼Œåˆ é™¤ä»¥å‰çš„åˆ†æ”¯ã€‚è®¾è®¡ç»“æ„ï¼Œå¦‚æœè¿™ä¸ªç»“æ„å†™åœ¨å‡½æ•°ä¸­æ€ä¹ˆç»´æŠ¤ã€‚</p><p>22ä¸­é—´ä¹Ÿç©¿æ’äº†é¡¹ç›®ã€‚</p><p>æ— è®ºæ˜¯å·²ç»æ‰¾åˆ°å·¥ä½œçš„è¿˜æ˜¯æ­£åœ¨å·¥ä½œçš„ï¼Œæˆ‘çš„è§‰çš„é¢è¯•é¢˜éƒ½å¯ä»¥ç»™æ‚¨ä»¬å¸¦æ¥ä¸€äº›å¯å‘ã€‚å¯ä»¥äº†è§£å¤§æ•°æ®è¡Œä¸šéœ€è¦ä»€ä¹ˆæ ·çš„äººæ‰ï¼Œä»€ä¹ˆæŠ€èƒ½ï¼Œå¯¹åº”å»è¡¥å……è‡ªå·±çš„ä¸è¶³ä¹‹å¤„ï¼Œä¸ºä¸‹ä¸€ä¸ªé«˜è–ªå·¥ä½œåšå‡†å¤‡ã€‚</p><p>è‹¥æ³½å¤§æ•°æ®åé¢ä¼šéšæ—¶æ›´æ–°å­¦å‘˜é¢è¯•é¢˜ï¼Œè®©å¤§å®¶äº†è§£å¤§æ•°æ®è¡Œä¸šçš„å‘å±•è¶‹åŠ¿ï¼Œæ—¨åœ¨å¸®åŠ©æ­£åœ¨è‰°è¾›æ‰“æ‹¼çš„æ‚¨æŒ‡å‡ºä¸€æ¡åŒºç›´çš„æœªæ¥ä¹‹è·¯ï¼ï¼ˆå°‘èµ°å¼¯è·¯å™¢å™¢ã€‚ã€‚ï¼‰</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> é¢è¯•é¢˜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¤§æ•°æ®é¢è¯•é¢˜ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä¸€æ¬¡è·³æ§½ç»å†ï¼ˆé˜¿é‡Œ/ç¾å›¢/å¤´æ¡/ç½‘æ˜“/æœ‰èµ...)</title>
      <link href="/2018/05/24/%E6%9C%89%E8%B5%9E...)/"/>
      <url>/2018/05/24/%E6%9C%89%E8%B5%9E...)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h6 id="ä¸ºå•¥è·³æ§½"><a href="#ä¸ºå•¥è·³æ§½" class="headerlink" title="ä¸ºå•¥è·³æ§½"></a>ä¸ºå•¥è·³æ§½</h6><p>æ¯æ¬¡è¯´å› ä¸ºç”Ÿæ´»æˆæœ¬çš„æ—¶å€™é¢è¯•å®˜éƒ½ä¼šå¾ˆæƒŠå¥‡ï¼Œéš¾é“æœ‰æˆ‘ä»¬è¿™é‡Œè´µï¼Ÿå¥½æƒ³ç›´æ¥ç»™å‡ºä¸‹é¢è¿™å¼ å›¾ï¼Œå¦é—¨çš„æˆ¿ä»·çœŸçš„å¥½è´µå¥½è´µå¥½è´µã€‚ã€‚ã€‚<br><img src="./assets/blogImg/tiaocao524.png" alt="enter description here"><br><a id="more"></a></p><h6 id="é¢è¯•è¿‡ç¨‹"><a href="#é¢è¯•è¿‡ç¨‹" class="headerlink" title="é¢è¯•è¿‡ç¨‹"></a>é¢è¯•è¿‡ç¨‹</h6><p>ï¼ˆå…ˆæ‰“ä¸ªå¹¿å‘Šï¼Œæœ‰å…´è¶£åŠ å…¥é˜¿é‡Œçš„æ¬¢è¿å‘ç®€å†è‡³ <a href="mailto:zhangzb2007@gmail.com" target="_blank" rel="noopener">zhangzb2007@gmail.com</a>ï¼Œæˆ–ç®€ä¹¦ä¸Šç»™æˆ‘å‘ä¿¡æ¯ï¼‰<br>é¢çš„æ˜¯Javaå²—ï¼Œæ€»å…±é¢äº†7å®¶å…¬å¸ï¼Œé€šè¿‡äº†6å®¶ã€‚æŒ‰è‡ªå·±çš„ä¿¡å¿ƒæå‡åº¦æˆ‘æŠŠé¢è¯•è¿‡ç¨‹åˆ†ä¸ºä¸ŠåŠåœºå’Œä¸‹åŠåœºã€‚</p><h6 id="ä¸ŠåŠåœº"><a href="#ä¸ŠåŠåœº" class="headerlink" title="ä¸ŠåŠåœº"></a>ä¸ŠåŠåœº</h6><ul><li><p>æ›¹æ“ä¸“è½¦<br>è¿™æ˜¯å‰åˆ©é›†å›¢ä¸‹å±å­å…¬å¸ï¼Œå·²ç»æ˜¯ä¸€å®¶ç‹¬è§’å…½ã€‚ä¸€é¢ä¸­è§„ä¸­çŸ©ï¼Œæ²¡å•¥ç‰¹åˆ«çš„ã€‚äºŒé¢å¥½åƒæ˜¯ä¸ªä¸»ç®¡ï¼Œéš”äº†å¥½å‡ å¤©ï¼ŒåŸºæœ¬æ²¡é—®æŠ€æœ¯é—®é¢˜ï¼Œåè€Œæ˜¯é—®èŒä¸šè§„åˆ’ï¼Œå¯¹åŠ ç­æœ‰å•¥çœ‹æ³•ï¼Œæœ‰ç‚¹æªæ‰‹ä¸åŠï¼Œæ„Ÿè§‰å›ç­”çš„ä¸å¥½ã€‚ä½†æ˜¯è¿‡å‡ å¤©è¿˜æ˜¯æ”¶åˆ°HRçš„ç°åœºé¢è¯•é€šçŸ¥ã€‚ç°åœºæ˜¯æŠ€æœ¯é¢åŠ HRé¢ï¼ŒæŠ€æœ¯é¢è¢«é—®äº†å‡ ä¸ªé—®é¢˜æœ‰ç‚¹æ‡µé€¼ï¼ša. zookeeperçš„watcherä¹è§‚é”æ€ä¹ˆå®ç° b. ä¸€ä¸ªé¡¹ç›®çš„æ•´ä¸ªæµç¨‹ c. è¯´å‡ºä¸€ä¸ªç©ºé—´æ¢æ—¶é—´çš„åœºæ™¯ d. centos7çš„å†…å­˜åˆ†é…æ–¹å¼å’Œ6æœ‰å•¥ä¸åŒ f. ä½ å¯¹å…¬å¸æœ‰ä»€ä¹ˆä»·å€¼ã€‚HRè·Ÿæˆ‘è¯´èŠ‚åï¼ˆé‚£ä¼šå†è¿‡ä¸¤å¤©å°±æ˜¯æ¸…æ˜ï¼‰ä¼šç»™æˆ‘æ¶ˆæ¯ï¼Œç»“æœè¿‡äº†åŠä¸ªæœˆçªç„¶æ¥åˆ°ä»–ä»¬çš„ç”µè¯ï¼Œè¯´æˆ‘é€šè¿‡äº†ï¼Œç»™æˆ‘è®²äº†ä»–ä»¬çš„è–ªèµ„æ–¹æ¡ˆï¼Œæ²¡å¤ªå¤§å¸å¼•åŠ›ï¼Œå†åŠ ä¸Šè¿™ç§è«åå…¶å¦™çš„æ—¶é—´ç­‰å¾…ï¼Œç›´æ¥æ‹’äº†ã€‚</p></li><li><p>ç¾äºšæŸç§‘<br>ä¼°è®¡å¾ˆå¤šäººæ²¡å¬è¯´è¿‡è¿™å®¶å…¬å¸ï¼Œè¿™æ˜¯ä¸€å®¶å¦é—¨æœ¬åœŸå…¬å¸ï¼Œåšæ”¿åºœå®‰é˜²é¡¹ç›®çš„ï¼Œåœ¨å¦é—¨ä¹Ÿè¿˜æ˜¯å°æœ‰åæ°”ã€‚ä½†æ˜¯é¢è¯•å®Œç›´æ¥é¢ è¦†äº†æˆ‘å¯¹è¿™å®¶å…¬å¸çš„è®¤çŸ¥ã€‚è¿›é—¨æœ€æ˜¾çœ¼çš„åœ°æ–¹æ˜¯å…šæ´»åŠ¨å®¤ï¼Œåœ¨ç­‰é¢è¯•å®˜çš„ä¸€å°æ®µæ—¶é—´é‡Œæœ‰å¥½å‡ æ‹¨äººåˆ°é‡Œé¢å‚è§‚ã€‚é¢è¯•å‰åšäº†ä¸€ä»½ç¬”è¯•é¢˜ï¼ŒåŸºæœ¬éƒ½æ˜¯web/æ•°æ®åº“æ–¹é¢çš„ã€‚ç¬¬ä¸€é¢ç®€å•é—®äº†å‡ ä¸ªredisçš„é—®é¢˜ä¹‹åé¢è¯•å®˜ä»‹ç»äº†ä»–ä»¬çš„é¡¹ç›®ï¼Œä»–ä»¬éƒ½æ˜¯åšCå’ŒC++çš„ï¼Œæƒ³æ‰¾ä¸€ä¸ªäººæ­ä¸€å¥—å¤§æ•°æ®é›†ç¾¤ï¼Œå¤„ç†ä»–ä»¬æ¯å¤©å‡ ç™¾Gçš„æ•°æ®ï¼Œç„¶åæœåŠ¡å™¨å…¨éƒ¨æ˜¯windowsï¼äºŒé¢æ˜¯å¦ä¸€ä¸ªéƒ¨é—¨çš„ï¼Œå°è±¡ä¸­å°±é—®äº†kafkaä¸ºä»€ä¹ˆæ€§èƒ½è¿™ä¹ˆå¥½ï¼Œç„¶åå°±å¼€å§‹é—®ä¹°æˆ¿äº†æ²¡æœ‰ï¼Œç»“å©šäº†æ²¡æœ‰ï¼Œä»–å¯¹æˆ‘ç°åœ¨çš„å…¬å¸æ¯”è¾ƒäº†è§£ï¼Œåˆæ‰¯äº†æŒºä¹…ã€‚ä¸‰é¢åº”è¯¥æ˜¯ä¸ªéƒ¨é—¨è€å¤§äº†ï¼Œæ²¡æœ‰é—®æŠ€æœ¯é—®é¢˜ï¼Œä¹Ÿæ˜¯é—®ä¹°æˆ¿äº†æ²¡ï¼Œç»“å©šæ²¡ï¼Œé—®å„ç§ç”Ÿæ´»é—®é¢˜ï¼Œæœ‰ç‚¹åƒäººå£æ™®æŸ¥ã€‚æˆ‘æœ‰ç‚¹å¥½å¥‡ï¼Œé—®ä»–ä»¬ä¸ºå•¥è¿™ä¹ˆå…³å¿ƒè¿™äº›é—®é¢˜ï¼Œä»–ç›´æ¥è¯´ä»–ä»¬æ›´å¼ºè°ƒå‘˜å·¥çš„ç¨³å®šæ€§ï¼Œé¡¹ç›®æ¯”è¾ƒç®€å•ï¼Œèƒ½åŠ›ä¸ç”¨è¦æ±‚å¤ªé«˜ï¼Œä¸è¦å¤ªå·®å°±è¡Œã€‚æ±—ï¼Œç›´æ¥æ‹’äº†ã€‚</p></li><li><p>æœ‰èµ<br>ç»å¯¹æ¨èçš„ä¸€å®¶å…¬å¸ï¼Œæ•ˆç‡è¶…é«˜ã€‚ä¸­åˆæ‰¾äº†ä¸€ä¸ªç½‘å‹å¸®å¿™å†…æ¨ï¼Œæ™šä¸Šå°±å¼€å§‹ä¸€é¢ï¼Œç¬¬äºŒå¤©æ—©ä¸ŠäºŒé¢ï¼Œç¬¬ä¸‰å¤©HRå°±çº¦ç°åœºé¢è¯•æ—¶é—´ï¼Œå¿«çš„è¶…ä¹æƒ³è±¡ã€‚ç°åœºé¢ä¹Ÿæ˜¯å…ˆä¸€ä¸ªæŠ€æœ¯é¢ï¼Œæœ€åæ‰HRé¢ã€‚é¢è¯•çš„æ•´ä½“éš¾åº¦ä¸­ç­‰ã€‚ç°åœ¨å°±è®°å¾—å‡ ä¸ªé—®é¢˜ï¼šG1å’ŒCMSçš„åŒºåˆ«ï¼ŒG1æœ‰å•¥åŠ£åŠ¿ï¼›Kafkaçš„æ•´ä½“æ¶æ„ï¼›Nettyçš„ä¸€æ¬¡è¯·æ±‚è¿‡ç¨‹ï¼›è‡ªæ—‹é”/åå‘é”/è½»é‡çº§é”ï¼ˆè¿™ä¸ªé—®é¢˜åœ¨å¤´æ¡çš„é¢è¯•é‡Œä¹Ÿå‡ºç°äº†ä¸€æ¬¡ï¼‰ã€hbaseçº¿ä¸Šé—®é¢˜æ’æŸ¥ï¼ˆåˆšå¥½é‡åˆ°è¿‡NUMAæ¶æ„ä¸‹çš„ä¸€ä¸ªé—®é¢˜ï¼Œå€Ÿæ­¤æŠŠhbaseçš„å†…æ ¸ä»‹ç»äº†ä¸‹ï¼‰ã€‚<br>è¿™é‡Œä¸å¾—ä¸è¯´ä¸‹æœ‰èµçš„äººï¼ŒçœŸçš„å¾ˆèµã€‚ç»ˆé¢çš„é¢è¯•å®˜æ˜¯ä¸€ä¸ªç ”å‘å›¢é˜Ÿçš„è´Ÿè´£äººï¼Œå…¨ç¨‹ä¸€ç›´å¾®ç¬‘ï¼Œä¸­é—´ç”µè¯å“äº†ä¸€æ¬¡ï¼Œä¸€ç›´è·Ÿæˆ‘é“æ­‰ã€‚é¢å®Œä¹‹åè¿˜æä¾›äº†å›¢é˜Ÿçš„ä¸‰ä¸ªç ”å‘æ–¹å‘è®©æˆ‘è‡ªå·±é€‰æ‹©ã€‚åé¢çœ‹ä»–çš„æœ‹å‹åœˆçŠ¶æ€ï¼Œä»–é‚£å¤©é«˜çƒ§ï¼Œé¢å®Œæˆ‘å°±å»æ‰“ç‚¹æ»´äº†ï¼Œä½†æ˜¯æ•´ä¸ªè¿‡ç¨‹å®Œå…¨çœ‹ä¸å‡ºæ¥ã€‚å¸®æˆ‘å†…æ¨çš„ç½‘å‹æ˜¯åœ¨å¾®ä¿¡ç¾¤é‡Œæ‰¾åˆ°çš„ï¼ŒçŸ¥é“æˆ‘è¿‡äº†ä¹‹åä¸»åŠ¨æ‰¾æˆ‘ï¼Œè®©æˆ‘è¿‡å»æ­å·æœ‰å•¥é—®é¢˜éšæ—¶æ‰¾ä»–ã€‚è™½ç„¶æœ€ç»ˆæ²¡æœ‰å»ï¼Œä½†è¿˜æ˜¯å¯ä»¥æ˜æ˜¾æ„Ÿå—åˆ°ä»–ä»¬çš„çƒ­æƒ…ã€‚</p></li><li><p>å­—èŠ‚è·³åŠ¨(ä»Šæ—¥å¤´æ¡)<br>HRç¾çœ‰æ‰“ç”µè¯è¿‡æ¥è¯´æ˜¯å­—èŠ‚è·³åŠ¨å…¬å¸ï¼Œæƒ³çº¦ä¸‹è§†é¢‘é¢è¯•æ—¶é—´ã€‚é‚£ä¼šæ˜¯æœ‰ç‚¹æ‡µçš„ï¼Œæˆ‘åªçŸ¥é“ä»Šæ—¥å¤´æ¡å’ŒæŠ–éŸ³ã€‚åé¢æƒ³åˆ°åŒ—äº¬çš„å·ç æ‰æƒ³èµ·æ¥ã€‚å¤´æ¡å¯ä»¥è¯´æ˜¯è¿™æ¬¡æ‰€æœ‰é¢è¯•é‡Œæµç¨‹æœ€è§„èŒƒçš„ï¼Œæ”¶åˆ°ç®€å†åæœ‰é‚®ä»¶é€šçŸ¥ï¼Œé¢„çº¦é¢è¯•æ—¶é—´åé‚®ä»¶çŸ­ä¿¡é€šçŸ¥ï¼Œé¢è¯•å®Œåä¸è¶…è¿‡ä¸€å¤©é€šçŸ¥é¢è¯•ç»“æœï¼Œæ¯æ¬¡é¢è¯•æœ‰é¢è¯•åé¦ˆã€‚è¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒç‰¹åˆ«çš„ï¼Œå¤§éƒ¨åˆ†å…¬å¸çš„ç”µè¯æˆ–è€…è§†é¢‘é¢è¯•åŸºæœ¬æ˜¯ä¸‹ç­åï¼Œå¤´æ¡éƒ½æ˜¯ä¸Šç­æ—¶é—´ï¼Œè¿˜ä¸ç»™çº¦ä¸‹ç­æ—¶é—´ï¼ˆéš¾é“ä»–ä»¬ä¸åŠ ç­ï¼Ÿï¼‰ã€‚<br>ä¸€é¢é¢è¯•å®˜åˆšä¸Šæ¥å°±è¯´ä»–ä»¬æ˜¯åšgoçš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰å…´è¶£ï¼Œä»–è‡ªå·±ä¹Ÿæ˜¯Javaè½¬çš„ã€‚æˆ‘è¯´æ²¡é—®é¢˜ï¼Œä»–å…ˆé—®äº†ä¸€äº›JavaåŸºç¡€é—®é¢˜ï¼Œç„¶åæœ‰ä¸€é“ç¼–ç¨‹é¢˜ï¼Œæ±‚ä¸€æ£µæ ‘ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€è¿‘çš„å…¬å…±çˆ¶èŠ‚ç‚¹ã€‚æ€è·¯åŸºæœ¬æ˜¯å¯¹çš„ï¼Œä½†æ˜¯æœ‰äº›ç»†èŠ‚æœ‰é—®é¢˜ï¼Œé¢è¯•å®˜äººå¾ˆå¥½ï¼Œè¾¹çœ‹è¾¹è·Ÿæˆ‘è®¨è®ºï¼Œæˆ‘è¾¹æ”¹è¿›ï¼Œå‰å‰ååä¼°è®¡ç”¨æ¥å¿«åŠå°æ—¶ã€‚ç„¶ååˆç»§ç»­é—®é—®é¢˜ï¼ŒHTTP 301 302æœ‰å•¥åŒºåˆ«ï¼Ÿè®¾è®¡ä¸€ä¸ªçŸ­é“¾æ¥ç®—æ³•ï¼›md5é•¿åº¦æ˜¯å¤šå°‘ï¼Ÿæ•´ä¸ªé¢è¯•è¿‡ç¨‹ä¸€ä¸ªå¤šå°æ—¶ï¼Œè‡ªæˆ‘æ„Ÿè§‰ä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä»¥ä¸ºè¿™æ¬¡åº”è¯¥æŒ‚äº†ï¼Œç»“æœæ™šä¸Šæ”¶åˆ°é¢è¯•é€šè¿‡çš„é€šçŸ¥ã€‚<br>äºŒé¢æ˜¯åœ¨ä¸€ä¸ªä¸Šåˆè¿›è¡Œçš„ï¼Œæˆ‘ä»¥ä¸ºzoomè§†é¢‘ç³»ç»Ÿä¼šè‡ªåŠ¨è¿ä¸Šï¼ˆä¸€é¢å°±æ˜¯è‡ªåŠ¨è¿ä¸Šï¼‰ï¼Œå°±åœ¨é‚£è¾¹ç­‰ï¼Œè¿‡äº†5åˆ†é’Ÿè¿˜æ˜¯ä¸è¡Œï¼Œæˆ‘å°±è”ç³»HRï¼ŒåŸæ¥è¦æ”¹idï¼Œç»ˆäºè¿ä¸Šåé¢è¯•å®˜çš„è¡¨æƒ…ä¸æ˜¯å¾ˆå¥½çœ‹ï¼Œæœ‰ç‚¹ä¸è€çƒ¦çš„æ ·å­ï¼Œä¸æ‡‚æ˜¯ä¸æ˜¯å› ä¸ºæˆ‘è€½è¯¯äº†å‡ åˆ†é’Ÿï¼Œè¿™ç§è¡¨æƒ…å»¶ç»­äº†æ•´ä¸ªé¢è¯•è¿‡ç¨‹ï¼Œå…¨ç¨‹æœ‰ç‚¹å‹æŠ‘ã€‚é—®çš„é—®é¢˜å¤§éƒ¨åˆ†å¿˜äº†ï¼Œåªè®°å¾—é—®äº†ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„é—®é¢˜ï¼ŒThreadLocalå¦‚æœå¼•ç”¨ä¸€ä¸ªstaticå˜é‡æ˜¯ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Ÿé—®ç€é—®ç€çªç„¶è¯´ä»Šå¤©é¢è¯•åˆ°æ­¤ä¸ºæ­¢ï¼Œä¸€çœ‹æ—¶é—´æ‰è¿‡å»äºŒåå‡ åˆ†é’Ÿã€‚ç¬¬äºŒå¤©å°±æ”¶åˆ°é¢è¯•æ²¡è¿‡çš„é€šçŸ¥ï¼Œæ„Ÿè§‰è‡ªå·±äºŒé¢ç­”çš„æ¯”ä¸€é¢å¥½å¤šäº†ï¼Œå®åœ¨æƒ³ä¸é€šã€‚</p></li></ul><h6 id="ä¸‹åŠåœº"><a href="#ä¸‹åŠåœº" class="headerlink" title="ä¸‹åŠåœº"></a>ä¸‹åŠåœº</h6><p>ä¸€ç›´æ„Ÿè§‰è‡ªå·±å¤ªæ°´äº†ï¼Œä»£ç é‡ä¸å¤§ï¼Œä¸‰å¹´åŠçš„ITç»éªŒè¿˜æœ‰ä¸€å¹´å»åšäº†äº§å“ï¼Œéƒ½ä¸æ•¢æŠ•å¤§å‚ã€‚ä¸ŠåŠåœºçš„æŠ€æœ¯é¢åŸºæœ¬è¿‡äº†ä¹‹åè‡ªä¿¡å¿ƒå¤§å¤§æå‡ï¼Œå¼€å§‹æŒ‘æˆ˜æ›´é«˜éš¾åº¦çš„ã€‚</p><ul><li><p>ç¾å›¢<br>è¿™ä¸ªæ˜¯å¦é—¨ç¾å›¢ï¼Œä»–ä»¬åœ¨è¿™è¾¹åšäº†ä¸€ä¸ªå«æ¦›æœæ°‘å®¿çš„APPï¼ŒåŠå…¬åœ°ç‚¹åœ¨JFCé«˜æ¡£å†™å­—æ¥¼ï¼Œä¼‘æ¯åŒºå¯ä»¥é¢æœå¤§æµ·ï¼Œç¯å¢ƒæ˜¯å¾ˆä¸é”™ï¼Œé¢è¯•å°±æœ‰ç‚¹è™å¿ƒäº†ã€‚<br>ä¸¤ç‚¹åŠè¿›å»ã€‚<br>ä¸€é¢ã€‚æˆ‘çš„ç®€å†å¤§éƒ¨åˆ†æ˜¯å¤§æ•°æ®ç›¸å…³çš„ï¼Œä»–ä¸æ˜¯å¾ˆäº†è§£ï¼Œé—®äº†ä¸€äº›åŸºç¡€é—®é¢˜å’Œnettyçš„å†™æµç¨‹ï¼Œè¿˜é—®äº†ä¸€ä¸ªredisæ•°æ®ç»“æ„çš„å®ç°ï¼Œç»“æ„ä»–é—®äº†é‡Œé¢å­—ç¬¦ä¸²æ˜¯æ€ä¹ˆå®ç°çš„ï¼Œæœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚ä¸€ç›´æ„Ÿè§‰è¿™ä¸ªå¤ªç®€å•ï¼Œæ²¡å¥½å¥½çœ‹ï¼Œåªè®°å¾—æœ‰æ ‡è®°é•¿åº¦ï¼Œå¯ä»¥ç›´æ¥å–ã€‚ç„¶åå°±æ¥ä¸¤é“ç¼–ç¨‹é¢˜ã€‚ç¬¬ä¸€é¢˜æ˜¯æ±‚ä¸€æ£µæ ‘æ‰€æœ‰å·¦å¶å­èŠ‚ç‚¹çš„å’Œï¼Œæ¯”è¾ƒç®€å•ï¼Œä¸€ä¸ªæ·±åº¦ä¼˜å…ˆå°±å¯ä»¥æå®šã€‚ç¬¬äºŒé¢˜æ˜¯ç»™å®šä¸€ä¸ªå€¼Kï¼Œä¸€ä¸ªæ•°åˆ—ï¼Œæ±‚æ•°åˆ—ä¸­ä¸¤ä¸ªå€¼aå’Œbï¼Œä½¿å¾—a+b=kã€‚æˆ‘æƒ³åˆ°äº†ä¸€ä¸ªä½¿ç”¨æ•°ç»„ä¸‹æ ‡çš„æ–¹æ³•ï¼ˆæ„Ÿè§‰æ˜¯åœ¨å“ªé‡Œæœ‰è§è¿‡ï¼Œä¸ç„¶ä¼°è®¡æ˜¯æƒ³ä¸å‡ºæ¥ï¼‰ï¼Œè¿™ç§å¯æ˜¯è¾¾åˆ°O(n)çš„å¤æ‚åº¦ï¼›ä»–åˆåŠ äº†ä¸ªé™åˆ¶æ¡ä»¶ï¼Œä¸èƒ½ä½¿ç”¨æ›´å¤šå†…å­˜ï¼Œæˆ‘æƒ³åˆ°äº†å¿«æ’+éå†ï¼Œä»–é—®æœ‰æ²¡æœ‰æ›´ä¼˜çš„ï¼Œå®åœ¨æƒ³ä¸å‡ºæ¥ï¼Œä»–æäº†ä¸€ä¸ªå¯ä»¥ä¸¤ç«¯é€¼è¿‘ï¼Œæ„Ÿè§‰å¾ˆå·§å¦™ã€‚<br>äºŒé¢ã€‚é¢è¯•å®˜é«˜é«˜ç˜¦ç˜¦çš„ï¼Œæˆ‘å¯¹è¿™ç§äººçš„å°è±¡éƒ½æ˜¯è‚¯å®šå¾ˆç‰›é€¼ï¼Œå¯èƒ½æ˜¯æºäºå¤§å­¦æ—¶ä»£é‚£äº›å¤§ç‰›éƒ½é•¿è¿™æ ·ã€‚å…ˆè®©æˆ‘è®²ä¸‹kafkaçš„ç»“æ„ï¼Œç„¶åæ€ä¹ˆé˜²æ­¢è®¢å•é‡å¤æäº¤ï¼Œç„¶åå¼€å§‹å›´ç»•ç¼“å­˜åŒæ­¥é—®é¢˜å±•å¼€äº†é•¿è¾¾åŠå°æ—¶çš„è®¨è®ºï¼šå…ˆå†™æ•°æ®åº“ï¼Œå†å†™ç¼“å­˜æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå…ˆå†™ç¼“å­˜å†å†™æ•°æ®åº“æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå†™åº“æˆåŠŸç¼“å­˜æ›´æ–°å¤±è´¥æ€ä¹ˆåŠï¼Ÿç¼“å­˜æ›´æ–°æˆåŠŸå†™åº“å¤±è´¥æ€ä¹ˆåŠï¼Ÿä»–å’Œæˆ‘ä¸€èµ·åœ¨ä¸€å¼ çº¸ä¸Šå„ç§ç”»ï¼Œæ„Ÿè§‰ä¸æ˜¯é¢è¯•ï¼Œè€Œæ˜¯åœ¨è®¾è®¡æ–¹æ¡ˆã€‚<br>ä¸‰é¢ã€‚è¿™æ˜¯åç«¯å›¢é˜Ÿè´Ÿè´£äººäº†ï¼Œå¾ˆå’Œè”¼ï¼Œä¸€ç›´ç¬‘å‘µå‘µã€‚é—®äº†æˆ‘ä¸€äº›å¾®æœåŠ¡çš„é—®é¢˜ï¼Œæˆ‘æåˆ°äº†istioï¼Œä»‹ç»äº†è®¾è®¡ç†å¿µï¼Œæ„Ÿè§‰ä»–æœ‰ç‚¹æ„å¤–ã€‚ç„¶åä»–é—®java8çš„æ–°ç‰¹æ€§ï¼Œé—®æˆ‘çŸ¥ä¸çŸ¥é“lambdaè¡¨è¾¾å¼æ€ä¹ˆæ¥çš„ï¼Œæˆ‘ä»lambdaæ¼”ç®—è¯´åˆ°lispè¯´åˆ°scalaï¼Œæ„Ÿè§‰ä»–æ›´æ„å¤–ã€‚æ­¤å¤„æœ‰ç‚¹å¹ç‰›äº†ã€‚æˆ‘é—®äº†ä¸€äº›å›¢é˜Ÿçš„é—®é¢˜ï¼Œé¡¹ç›®æœªæ¥è§„åˆ’ç­‰ï¼Œæ„Ÿè§‰æ¦›æœè¿˜æ˜¯æŒºä¸é”™çš„ã€‚<br>å››é¢ã€‚è¿™ä¸ªåº”è¯¥æ˜¯æ¦›æœå¦é—¨çš„è´Ÿè´£äººäº†ï¼ŒæŠ€æœ¯é—®é¢˜é—®çš„ä¸å¤šï¼Œæ›´å¤šæ˜¯ä¸€äº›èŒä¸šè§„åˆ’ï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ç­‰ã€‚é¢è¯•ç»“æŸçš„æ—¶å€™ä»–å…ˆå‡ºå»ï¼Œæˆ‘æ”¶æ‹¾ä¸‹ä¸œè¥¿ï¼Œå‡ºå»çš„æ—¶å€™å‘ç°ä»–åœ¨ç”µæ¢¯æ—å¸®æˆ‘å¼€ç”µæ¢¯ï¼Œå¯¹å¾…é¢è¯•è€…çš„è¿™ç§æ€åº¦å®åœ¨è®©äººå¾ˆæœ‰å¥½æ„Ÿã€‚<br>å‡ºæ¥çš„æ—¶å€™å·²ç»æ˜¯å…­ç‚¹åŠã€‚</p></li><li><p>ç½‘æ˜“<br>é¢çš„æ˜¯ç½‘æ˜“äº‘éŸ³ä¹ï¼Œå¹³æ—¶ç»å¸¸ç”¨ï¼Œæ„Ÿè§‰å¦‚æœå¯ä»¥å‚ä¸ç ”å‘åº”è¯¥æ˜¯ç§æŒºç¾å¦™çš„æ„Ÿè§‰ã€‚<br>ä¸€é¢ã€‚ä¸‹åˆæ‰“è¿‡æ¥çš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç©ºï¼Œæˆ‘è¯´æœ‰ï¼Œä»–è¯´ä½ ä¸ç”¨ä¸Šç­å—ï¼Ÿæœ‰æ€åº¦çœŸçš„å¯ä»¥ä¸ºæ‰€æ¬²ä¸ºï¼ˆè‹¦ç¬‘ï¼‰ã€‚ç„¶åé—®äº†ä¸ºä»€ä¹ˆç¦»èŒï¼ŒèŠäº†ä¼šæˆ¿ä»·ï¼Œé—®äº†å‡ ä¸ªnettyçš„é—®é¢˜ï¼Œgcçš„é—®é¢˜ï¼Œæœ€åé—®ä¸‹å¯¹ä¸šåŠ¡çš„çœ‹æ³•ã€‚<br>ç„¶åçº¦äº†ä¸ªäºŒé¢çš„æ—¶é—´ï¼Œç»“æœæ—¶é—´åˆ°äº†æ²¡äººè”ç³»æˆ‘ï¼Œç¬¬äºŒå¤©æ‰“ç”µè¯è·Ÿæˆ‘é“æ­‰é‡æ–°çº¦äº†æ—¶é—´ï¼Œä¸å¾—ä¸è¯´æ€åº¦è¿˜æ˜¯å¾ˆå¥½çš„ã€‚äºŒé¢é—®çš„åè€Œå¾ˆåŸºç¡€ï¼Œæ²¡å¤ªå¤šç‰¹åˆ«çš„ã€‚è®©æˆ‘æé—®çš„æ—¶å€™æˆ‘æŠŠç¾å›¢äºŒé¢é‡Œçš„ç¼“å­˜é—®é¢˜æ‹¿å‡ºæ¥é—®ä»–ï¼Œå¾ˆè€å¿ƒçš„ç»™æˆ‘è§£ç­”äº†å¥½å‡ åˆ†é’Ÿï¼Œäººå¾ˆå¥½ã€‚</p></li><li><p>é˜¿é‡Œ<br>è¿™ä¸ªå…¶å®ä¸æ˜¯æœ€åé¢è¯•çš„ï¼Œä½†æ˜¯æ˜¯æœ€åç»“æŸçš„ï¼Œä¸å¾—ä¸è¯´é˜¿é‡ŒäººçœŸçš„å¥½å¿™ï¼Œå‘¨ä¸‰è·Ÿæˆ‘é¢„çº¦æ—¶é—´ï¼Œç„¶åå·²ç»æ’åˆ°ä¸‹ä¸€å‘¨çš„å‘¨ä¸€ã€‚æ€»ä½“ä¸Šæ„Ÿè§‰é˜¿é‡Œçš„é¢è¯•é£æ ¼æ˜¯å–œæ¬¢åœ¨æŸä¸ªç‚¹ä¸Šä¸æ–­æ·±å…¥ï¼Œç›´åˆ°ä½ è¯´ä¸çŸ¥é“ã€‚<br>ä¸€é¢ã€‚è‡ªæˆ‘ä»‹ç»ï¼Œç„¶åä»‹ç»ç°åœ¨çš„é¡¹ç›®æ¶æ„ï¼Œç¬¬ä¸€éƒ¨åˆ†å°±æ˜¯æ—¥å¿—ä¸Šä¼ å’Œæ¥æ”¶ï¼Œç„¶åå°±å¦‚ä½•ä¿è¯æ—¥å¿—ä¸Šä¼ çš„å¹‚ç­‰æ€§å¼€å§‹ä¸æ–­æ·±å…¥ï¼Œå…ˆè®©æˆ‘è®¾è®¡ä¸€ä¸ªæ–¹æ¡ˆï¼Œç„¶åé—®æœ‰æ²¡æœ‰ä»€ä¹ˆæ”¹è¿›çš„ï¼Œç„¶åå¦‚ä½•åœ¨ä¿è¯å¹‚ç­‰çš„å‰æä¸‹æé«˜æ€§èƒ½ï¼Œä¸­é—´ç©¿æ’åˆ†å¸ƒå¼é”ã€redisã€mqã€æ•°æ®åº“é”ç­‰å„ç§é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜è®¨è®ºäº†å·®ä¸å¤šåŠå°æ—¶ã€‚ç„¶åå°±é—®æˆ‘æœ‰æ²¡æœ‰ä»€ä¹ˆè¦äº†è§£çš„ï¼ŒèŠ±äº†åå‡ åˆ†é’Ÿä»‹ç»ä»–ä»¬ç°åœ¨åšçš„äº‹æƒ…ã€æŠ€æœ¯æ ˆã€æœªæ¥çš„ä¸€äº›è®¡åˆ’ï¼Œéå¸¸è€å¿ƒã€‚<br>äºŒé¢ã€‚ä¹Ÿæ˜¯ä»ä»‹ç»é¡¹ç›®å¼€å§‹ï¼Œç„¶åæŠ“ä½ä¸€ä¸ªç‚¹ï¼Œç»“åˆç§’æ€çš„åœºæ™¯æ·±å…¥ï¼Œå¦‚ä½•å®ç°åˆ†å¸ƒå¼é”ã€å¦‚ä½•ä¿è¯å¹‚ç­‰æ€§ã€åˆ†å¸ƒå¼äº‹åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚é—®æˆ‘åˆ†å¸ƒå¼é”çš„ç¼ºç‚¹ï¼Œæˆ‘è¯´æ€§èƒ½ä¼šå‡ºç°ç“¶é¢ˆï¼Œä»–é—®æ€ä¹ˆè§£å†³ï¼Œæˆ‘æƒ³äº†æ¯”è¾ƒä¹…ï¼Œä»–æç¤ºè¯´å‘æ•£ä¸‹æ€ç»´ï¼Œæˆ‘æœ€åæƒ³äº†ä¸ªç®€å•çš„æ–¹æ¡ˆï¼Œç›´æ¥ä¸ä½¿ç”¨åˆ†å¸ƒå¼é”ï¼Œä»–å¥½åƒæŒºæ»¡æ„ã€‚æ„Ÿè§‰ä»–ä»¬æ›´çœ‹é‡æ€è€ƒçš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯å…·ä½“æ–¹æ¡ˆã€‚è¿˜é—®äº†ä¸€è‡´æ€§hashå¦‚ä½•ä¿è¯è´Ÿè½½å‡è¡¡ï¼Œkafkaå’Œrocketmqå„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼Œdubboçš„ä¸€ä¸ªè¯·æ±‚è¿‡ç¨‹ã€åºåˆ—åŒ–æ–¹å¼ï¼Œåºåˆ—åŒ–æ¡†æ¶ã€PBçš„ç¼ºç‚¹ã€å¦‚ä½•ä»æ•°æ®åº“å¤§æ‰¹é‡å¯¼å…¥æ•°æ®åˆ°hbaseã€‚<br>ä¸‰é¢ã€‚æ˜¯HRå’Œä¸»ç®¡çš„è”åˆè§†é¢‘é¢è¯•ã€‚è¿™ç§é¢è¯•è¿˜ç¬¬ä¸€æ¬¡é‡åˆ°ï¼Œæœ‰ç‚¹ç´§å¼ ã€‚ä¸»ç®¡å…ˆé¢ï¼Œä¹Ÿæ˜¯è®©æˆ‘å…ˆä»‹ç»é¡¹ç›®ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç”¨è¿‡mqï¼Œå¦‚ä½•ä¿è¯æ¶ˆæ¯å¹‚ç­‰æ€§ã€‚æˆ‘å°±æŠŠkafka0.11ç‰ˆæœ¬çš„å¹‚ç­‰æ€§æ–¹æ¡ˆè¯´äº†ä¸‹ï¼Œå°±æ²¡å†é—®æŠ€æœ¯é—®é¢˜äº†ã€‚åé¢åˆé—®äº†ä¸ºå•¥ç¦»èŒï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ä¹‹ç±»çš„ã€‚ç„¶åå°±äº¤ç»™HRï¼Œåªé—®äº†å‡ ä¸ªé—®é¢˜ï¼Œç„¶åå°±ç»“æŸäº†ï¼Œå…¨ç¨‹ä¸åˆ°åŠå°æ—¶ã€‚<br>ä¸æ‡‚æ˜¯ä¸æ˜¯è·Ÿé¢è¯•çš„éƒ¨é—¨æœ‰å…³ï¼Œé˜¿é‡Œå¯¹å¹‚ç­‰æ€§è¿™ä¸ªé—®é¢˜å¾ˆæ‰§ç€ï¼Œä¸‰æ¬¡éƒ½é—®åˆ°ï¼Œè€Œä¸”è¿˜æ˜¯ä»ä¸åŒè§’åº¦ã€‚</p></li></ul><h6 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h6><p>ä»é¢è¯•çš„éš¾æ˜“ç¨‹åº¦çœ‹é˜¿é‡Œ &gt; ç¾å›¢ &gt; å¤´æ¡ &gt; æœ‰èµ &gt; ç½‘æ˜“ &gt; æ›¹æ“ä¸“è½¦ &gt; ç¾äºšæŸç§‘ã€‚<strong>æ•´ä¸ªè¿‡ç¨‹çš„ä½“ä¼šæ˜¯åŸºç¡€çœŸçš„å¾ˆé‡è¦ï¼ŒåŸºç¡€å¥½äº†å¾ˆå¤šé—®é¢˜å³ä½¿æ²¡é‡åˆ°è¿‡ä¹Ÿå¯ä»¥ä¸¾ä¸€åä¸‰ã€‚</strong> å¦å¤–å¯¹ä¸€æ ·æŠ€æœ¯ä¸€å®šè¦æ‡‚åŸç†ï¼Œè€Œä¸ä»…ä»…æ˜¯æ€ä¹ˆä½¿ç”¨ï¼Œå°¤å…¶æ˜¯ç¼ºç‚¹ï¼Œå¯¹é€‰å‹å¾ˆå…³é”®ï¼Œå¯ä»¥å¾ˆå¥½çš„ç”¨æ¥å›ç­”ä¸ºä»€ä¹ˆä¸é€‰xxxã€‚å¦å¤–å¯¹ä¸€äº›æ¯”è¾ƒæ–°çš„æŠ€æœ¯æœ‰æ‰€äº†è§£ä¹Ÿæ˜¯ä¸€ä¸ªåŠ åˆ†é¡¹ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> é¢è¯•é¢˜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¤§æ•°æ®é¢è¯•é¢˜ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveä¸­è‡ªå®šä¹‰UDAFå‡½æ•°ç”Ÿäº§å°æ¡ˆä¾‹</title>
      <link href="/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="ä¸€ã€UDAF-å›é¡¾"><a href="#ä¸€ã€UDAF-å›é¡¾" class="headerlink" title="ä¸€ã€UDAF å›é¡¾"></a>ä¸€ã€UDAF å›é¡¾</h4><ul><li>1.å®šä¹‰ï¼šUDAF(User Defined Aggregation Funcation ) ç”¨æˆ·è‡ªå®šä¹‰èšç±»æ–¹æ³•ï¼Œå’Œgroup byè”åˆä½¿ç”¨ï¼Œæ¥å—å¤šä¸ªè¾“å…¥æ•°æ®è¡Œï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªè¾“å‡ºæ•°æ®è¡Œã€‚</li><li>2.Hiveæœ‰ä¸¤ç§UDAFï¼šç®€å•å’Œé€šç”¨<br>ç®€å•ï¼šåˆ©ç”¨æŠ½è±¡ç±»UDAFå’ŒUDAFEvaluatorï¼Œä½¿ç”¨Javaåå°„å¯¼è‡´æ€§èƒ½æŸå¤±ï¼Œä¸”æœ‰äº›ç‰¹æ€§ä¸èƒ½ä½¿ç”¨ï¼Œå¦‚å¯å˜é•¿åº¦å‚æ•°åˆ—è¡¨ ã€‚<br>é€šç”¨ï¼šåˆ©ç”¨æ¥å£GenericUDAFResolver2ï¼ˆæˆ–æŠ½è±¡ç±»AbstractGenericUDAFResolverï¼‰å’ŒæŠ½è±¡ç±»GenericUDAFEvaluatorï¼Œå¯ä»¥ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½ï¼Œä½†æ¯”è¾ƒå¤æ‚ï¼Œä¸ç›´è§‚ã€‚</li><li>3.ä¸€ä¸ªè®¡ç®—å‡½æ•°å¿…é¡»å®ç°çš„5ä¸ªæ–¹æ³•çš„å…·ä½“å«ä¹‰å¦‚ä¸‹ï¼š<br>init()ï¼šä¸»è¦æ˜¯è´Ÿè´£åˆå§‹åŒ–è®¡ç®—å‡½æ•°å¹¶ä¸”é‡è®¾å…¶å†…éƒ¨çŠ¶æ€ï¼Œä¸€èˆ¬å°±æ˜¯é‡è®¾å…¶å†…éƒ¨å­—æ®µã€‚ä¸€èˆ¬åœ¨é™æ€ç±»ä¸­å®šä¹‰ä¸€ä¸ªå†…éƒ¨å­—æ®µæ¥å­˜æ”¾æœ€ç»ˆçš„ç»“æœã€‚<br>iterate()ï¼šæ¯ä¸€æ¬¡å¯¹ä¸€ä¸ªæ–°å€¼è¿›è¡Œèšé›†è®¡ç®—æ—¶å€™éƒ½ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œè®¡ç®—å‡½æ•°ä¼šæ ¹æ®èšé›†è®¡ç®—ç»“æœæ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚å½“è¾“ å…¥å€¼åˆæ³•æˆ–è€…æ­£ç¡®è®¡ç®—äº†ï¼Œåˆ™å°±è¿”å›trueã€‚<br>terminatePartial()ï¼šHiveéœ€è¦éƒ¨åˆ†èšé›†ç»“æœçš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œå¿…é¡»è¦è¿”å›ä¸€ä¸ªå°è£…äº†èšé›†è®¡ç®—å½“å‰çŠ¶æ€çš„å¯¹è±¡ã€‚<br>merge()ï¼šHiveè¿›è¡Œåˆå¹¶ä¸€ä¸ªéƒ¨åˆ†èšé›†å’Œå¦ä¸€ä¸ªéƒ¨åˆ†èšé›†çš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚<br>terminate()ï¼šHiveæœ€ç»ˆèšé›†ç»“æœçš„æ—¶å€™å°±ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚è®¡ç®—å‡½æ•°éœ€è¦æŠŠçŠ¶æ€ä½œä¸ºä¸€ä¸ªå€¼è¿”å›ç»™ç”¨æˆ·ã€‚<h4 id="äºŒã€éœ€æ±‚"><a href="#äºŒã€éœ€æ±‚" class="headerlink" title="äºŒã€éœ€æ±‚"></a>äºŒã€éœ€æ±‚</h4>ä½¿ç”¨UDAFç®€å•æ–¹å¼å®ç°ç»Ÿè®¡åŒºåŸŸäº§å“ç”¨æˆ·è®¿é—®æ’å<a id="more"></a><h4 id="ä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®ç°"><a href="#ä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®ç°" class="headerlink" title="ä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®ç°"></a>ä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®ç°</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">package hive.org.ruozedata;</span><br><span class="line">import java.util.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">public class UserClickUDAF extends UDAF &#123;</span><br><span class="line">    // æ—¥å¿—å¯¹è±¡åˆå§‹åŒ–</span><br><span class="line">    public static Logger logger = Logger.getLogger(UserClickUDAF.class);</span><br><span class="line">    // é™æ€ç±»å®ç°UDAFEvaluator</span><br><span class="line">    public static class Evaluator implements UDAFEvaluator &#123;</span><br><span class="line">        // è®¾ç½®æˆå‘˜å˜é‡ï¼Œå­˜å‚¨æ¯ä¸ªç»Ÿè®¡èŒƒå›´å†…çš„æ€»è®°å½•æ•°</span><br><span class="line">        private static Map&lt;String, String&gt; courseScoreMap;</span><br><span class="line">        private static Map&lt;String, String&gt; city_info;</span><br><span class="line">        private static Map&lt;String, String&gt; product_info;</span><br><span class="line">        private static Map&lt;String, String&gt; user_click;</span><br><span class="line">        //åˆå§‹åŒ–å‡½æ•°,mapå’Œreduceå‡ä¼šæ‰§è¡Œè¯¥å‡½æ•°,èµ·åˆ°åˆå§‹åŒ–æ‰€éœ€è¦çš„å˜é‡çš„ä½œç”¨</span><br><span class="line">        public Evaluator() &#123;</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line">        // åˆå§‹åŒ–å‡½æ•°é—´ä¼ é€’çš„ä¸­é—´å˜é‡</span><br><span class="line">        public void init() &#123;</span><br><span class="line">            courseScoreMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">            city_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">            product_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">        //mapé˜¶æ®µï¼Œè¿”å›å€¼ä¸ºbooleanç±»å‹ï¼Œå½“ä¸ºtrueåˆ™ç¨‹åºç»§ç»­æ‰§è¡Œï¼Œå½“ä¸ºfalseåˆ™ç¨‹åºé€€å‡º</span><br><span class="line">        public boolean iterate(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (pcid == null || pcname == null || pccount == null) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (pccount.equals(&quot;-1&quot;)) &#123;</span><br><span class="line">                // åŸå¸‚è¡¨</span><br><span class="line">                city_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (pccount.equals(&quot;-2&quot;)) &#123;</span><br><span class="line">                // äº§å“è¡¨</span><br><span class="line">                product_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è”</span><br><span class="line">                unionCity_Prod_UserClic1(pcid, pcname, pccount);</span><br><span class="line">           &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è”</span><br><span class="line">        private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (product_info.containsKey(pcid)) &#123;</span><br><span class="line">                if (city_info.containsKey(pcname)) &#123;</span><br><span class="line">                    String city_name = city_info.get(pcname);</span><br><span class="line">                    String prod_name = product_info.get(pcid);</span><br><span class="line">                    String cp_name = city_name + prod_name;</span><br><span class="line">                    // å¦‚æœä¹‹å‰å·²ç»Putè¿‡Keyå€¼ä¸ºåŒºåŸŸä¿¡æ¯ï¼Œåˆ™æŠŠè®°å½•ç›¸åŠ å¤„ç†</span><br><span class="line">                    if (courseScoreMap.containsKey(cp_name)) &#123;</span><br><span class="line">                        int pcrn = 0;</span><br><span class="line">                        String strTemp = courseScoreMap.get(cp_name);</span><br><span class="line">                        String courseScoreMap_pn </span><br><span class="line">                         = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim();</span><br><span class="line">                        pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn);</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn));</span><br><span class="line">                    &#125;</span><br><span class="line">                    else &#123;</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * ç±»ä¼¼äºcombiner,åœ¨mapèŒƒå›´å†…åšéƒ¨åˆ†èšåˆï¼Œå°†ç»“æœä¼ ç»™mergeå‡½æ•°ä¸­çš„å½¢å‚mapOutput</span><br><span class="line">         * å¦‚æœéœ€è¦èšåˆï¼Œåˆ™å¯¹iteratorè¿”å›çš„ç»“æœå¤„ç†ï¼Œå¦åˆ™ç›´æ¥è¿”å›iteratorçš„ç»“æœå³å¯</span><br><span class="line">         */</span><br><span class="line">        public Map&lt;String, String&gt; terminatePartial() &#123;</span><br><span class="line">            return courseScoreMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // reduce é˜¶æ®µï¼Œç”¨äºé€ä¸ªè¿­ä»£å¤„ç†mapå½“ä¸­æ¯ä¸ªä¸åŒkeyå¯¹åº”çš„ terminatePartialçš„ç»“æœ</span><br><span class="line">        public boolean merge(Map&lt;String, String&gt; mapOutput) &#123;</span><br><span class="line">            this.courseScoreMap.putAll(mapOutput);</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        // å¤„ç†mergeè®¡ç®—å®Œæˆåçš„ç»“æœï¼Œå³å¯¹mergeå®Œæˆåçš„ç»“æœåšæœ€åçš„ä¸šåŠ¡å¤„ç†</span><br><span class="line">        public String terminate() &#123;</span><br><span class="line">            return courseScoreMap.toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°"><a href="#å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°" class="headerlink" title="å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°"></a>å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION user_click;</span><br><span class="line">add jar /data/hive_udf-1.0.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;</span><br></pre></td></tr></table></figure><h4 id="äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®"><a href="#äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®" class="headerlink" title="äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®"></a>äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from (</span><br><span class="line">  select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from (</span><br><span class="line">    select * from (</span><br><span class="line">      select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info</span><br><span class="line">      union all</span><br><span class="line">      select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info</span><br><span class="line">      union all</span><br><span class="line">      select count(1) as type,</span><br><span class="line">             product_id as pcid,</span><br><span class="line">             city_id as pcname</span><br><span class="line">        from user_click</span><br><span class="line">       where action_time=&apos;2016-05-05&apos;</span><br><span class="line">      group by product_id,city_id</span><br><span class="line">    ) a</span><br><span class="line">  order by type) b</span><br><span class="line">) c ;</span><br></pre></td></tr></table></figure><h4 id="å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨"><a href="#å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨" class="headerlink" title="å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨"></a>å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table tmp1(</span><br><span class="line">city_name string,</span><br><span class="line">product_name string,</span><br><span class="line">rn string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">location &apos;/works/tmp1&apos;;</span><br></pre></td></tr></table></figure><h4 id="ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æ’å"><a href="#ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æ’å" class="headerlink" title="ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æ’å"></a>ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æ’å</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">select city_name,</span><br><span class="line">       product_name,</span><br><span class="line">       floor(sum(rn)) visit_num,</span><br><span class="line">       row_number()over(partition by city_name order by sum(rn) desc) rn,</span><br><span class="line">       &apos;2016-05-05&apos; action_time</span><br><span class="line">  from tmp1 </span><br><span class="line"> group by city_name,product_name</span><br><span class="line">) a where rn &lt;=3 ;</span><br></pre></td></tr></table></figure><h4 id="å…«ã€æœ€ç»ˆç»“æœ"><a href="#å…«ã€æœ€ç»ˆç»“æœ" class="headerlink" title="å…«ã€æœ€ç»ˆç»“æœ"></a>å…«ã€æœ€ç»ˆç»“æœ</h4><p><img src="/assets/blogImg/hive523.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark History Server Web UIé…ç½®</title>
      <link href="/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="1-è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶"><a href="#1-è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶" class="headerlink" title="1.è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶"></a>1.è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd /opt/app/spark/conf</span><br><span class="line">[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="2-åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š-å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š"><a href="#2-åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š-å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š" class="headerlink" title="2.åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š(å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š)"></a>2.åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š(å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /Found 3 items</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line">drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 items</span><br><span class="line">drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog</span><br></pre></td></tr></table></figure><p>åœ¨HDFSä¸­åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œç”¨äºä¿å­˜Sparkè¿è¡Œæ—¥å¿—ä¿¡æ¯ã€‚Spark History Serverä»æ­¤ç›®å½•ä¸­è¯»å–æ—¥å¿—ä¿¡æ¯</p><h5 id="3-é…ç½®"><a href="#3-é…ç½®" class="headerlink" title="3.é…ç½®"></a>3.é…ç½®</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir             hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br></pre></td></tr></table></figure><p>spark.eventLog.dirä¿å­˜æ—¥å¿—ç›¸å…³ä¿¡æ¯çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯hdfs://å¼€å¤´çš„HDFSè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯file://å¼€å¤´çš„æœ¬åœ°è·¯å¾„ï¼Œéƒ½éœ€è¦æå‰åˆ›å»º<br>spark.yarn.historyServer.address : Spark history serverçš„åœ°å€(ä¸åŠ http://).<br>è¿™ä¸ªåœ°å€ä¼šåœ¨Sparkåº”ç”¨ç¨‹åºå®Œæˆåæäº¤ç»™YARN RMï¼Œç„¶åå¯ä»¥åœ¨RM UIä¸Šç‚¹å‡»é“¾æ¥è·³è½¬åˆ°history server UIä¸Š.</p><h5 id="4-æ·»åŠ SPARK-HISTORY-OPTSå‚æ•°"><a href="#4-æ·»åŠ SPARK-HISTORY-OPTSå‚æ•°" class="headerlink" title="4.æ·»åŠ SPARK_HISTORY_OPTSå‚æ•°"></a>4.æ·»åŠ SPARK_HISTORY_OPTSå‚æ•°</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 conf]# vi spark-env.sh</span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h5 id="5-å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹"><a href="#5-å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹" class="headerlink" title="5.å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹"></a>5.å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01  ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer</span><br><span class="line">[root@hadoop01  ~]# ps -ef|grep sparkroot     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop01  ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java</span><br></pre></td></tr></table></figure><p>ä»¥ä¸Šé…ç½®æ˜¯é’ˆå¯¹ä½¿ç”¨è‡ªå·±ç¼–è¯‘çš„Sparkéƒ¨ç½²åˆ°é›†ç¾¤ä¸­ä¸€åˆ°ä¸¤å°æœºå™¨ä¸Šä½œä¸ºæäº¤ä½œä¸šå®¢æˆ·ç«¯çš„ï¼Œå¦‚æœä½ æ˜¯CDHé›†ç¾¤ä¸­é›†æˆçš„Sparké‚£ä¹ˆå¯ä»¥åœ¨ç®¡ç†ç•Œé¢ç›´æ¥æŸ¥çœ‹ï¼</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark åŸºæœ¬æ¦‚å¿µ</title>
      <link href="/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><strong>åŸºäº Spark æ„å»ºçš„ç”¨æˆ·ç¨‹åºï¼ŒåŒ…å«äº† ä¸€ä¸ªdriver ç¨‹åºå’Œé›†ç¾¤ä¸Šçš„ executorsï¼›ï¼ˆèµ·äº†ä¸€ä¸ªä½œä¸šï¼Œå°±æ˜¯ä¸€ä¸ªApplicationï¼‰</strong><br><a id="more"></a></p><h5 id="sparkåè¯è§£é‡Š"><a href="#sparkåè¯è§£é‡Š" class="headerlink" title="sparkåè¯è§£é‡Š"></a>sparkåè¯è§£é‡Š</h5><ul><li><p>Application jarï¼šåº”ç”¨ç¨‹åºjaråŒ…<br>åŒ…å«äº†ç”¨æˆ·çš„ Spark ç¨‹åºçš„ä¸€ä¸ª jar åŒ…. åœ¨æŸäº›æƒ…å†µä¸‹ç”¨æˆ·å¯èƒ½æƒ³è¦åˆ›å»ºä¸€ä¸ªå›Šæ‹¬äº†åº”ç”¨åŠå…¶ä¾èµ–çš„ â€œèƒ–â€ jar åŒ…. ä½†å®é™…ä¸Š, ç”¨æˆ·çš„ jar ä¸åº”è¯¥åŒ…æ‹¬ Hadoop æˆ–æ˜¯ Spark çš„åº“, è¿™äº›åº“ä¼šåœ¨è¿è¡Œæ—¶è¢«è¿›è¡ŒåŠ è½½ï¼›</p></li><li><p>Driver Programï¼š<br>è¿™ä¸ªè¿›ç¨‹è¿è¡Œåº”ç”¨ç¨‹åºçš„ main æ–¹æ³•å¹¶ä¸”æ–°å»º SparkContext ï¼›</p></li><li><p>Cluster Managerï¼šé›†ç¾¤ç®¡ç†è€…<br>åœ¨é›†ç¾¤ä¸Šè·å–èµ„æºçš„å¤–éƒ¨æœåŠ¡ (ä¾‹å¦‚:standalone,Mesos,Yarn)ï¼›ï¼ˆâ€“masterï¼‰</p></li><li><p>Deploy modeï¼šéƒ¨ç½²æ¨¡å¼<br>å‘Šè¯‰ä½ åœ¨å“ªé‡Œå¯åŠ¨driver program. åœ¨ â€œclusterâ€ æ¨¡å¼ä¸‹, æ¡†æ¶åœ¨é›†ç¾¤å†…éƒ¨è¿è¡Œ driver. åœ¨ â€œclientâ€ æ¨¡å¼ä¸‹, æäº¤è€…åœ¨é›†ç¾¤å¤–éƒ¨è¿è¡Œ driver.ï¼›</p></li><li><p>Worker Nodeï¼šå·¥ä½œèŠ‚ç‚¹<br>é›†ç¾¤ä¸­ä»»ä½•å¯ä»¥è¿è¡Œåº”ç”¨ä»£ç çš„èŠ‚ç‚¹ï¼›ï¼ˆyarnä¸Šå°±æ˜¯node managerï¼‰</p></li><li><p>Executorï¼š<br>åœ¨ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šä¸ºæŸåº”ç”¨å¯åŠ¨çš„ä¸€ä¸ªè¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹è´Ÿè´£è¿è¡Œä»»åŠ¡ï¼Œå¹¶ä¸”è´Ÿè´£å°†æ•°æ®å­˜åœ¨å†…å­˜æˆ–è€…ç£ç›˜ä¸Šã€‚æ¯ä¸ªåº”ç”¨éƒ½æœ‰å„è‡ªç‹¬ç«‹çš„ executorsï¼›</p></li><li><p>Taskï¼šä»»åŠ¡<br>è¢«é€åˆ°æŸä¸ª executor ä¸Šæ‰§è¡Œçš„å·¥ä½œå•å…ƒï¼›</p></li><li><p>Jobï¼š<br>åŒ…å«å¾ˆå¤šå¹¶è¡Œè®¡ç®—çš„taskã€‚ä¸€ä¸ª action å°±ä¼šäº§ç”Ÿä¸€ä¸ªjobï¼›</p></li><li><p>Stageï¼š<br>ä¸€ä¸ª Job ä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªtaskçš„é›†åˆï¼Œæ¯ä¸ªtaské›†åˆè¢«ç§°ä¸º stageï¼Œstageä¹‹é—´æ˜¯ç›¸äº’ä¾èµ–çš„(å°±åƒ Mapreduce åˆ† mapå’Œ reduce stagesä¸€æ ·)ï¼Œå¯ä»¥åœ¨Driver çš„æ—¥å¿—ä¸Šçœ‹åˆ°ã€‚</p></li></ul><h5 id="sparkå·¥ä½œæµç¨‹"><a href="#sparkå·¥ä½œæµç¨‹" class="headerlink" title="sparkå·¥ä½œæµç¨‹"></a>sparkå·¥ä½œæµç¨‹</h5><p>1ä¸ªactionä¼šè§¦å‘1ä¸ªjobï¼Œ1ä¸ªjobåŒ…å«nä¸ªstageï¼Œæ¯ä¸ªstageåŒ…å«nä¸ªtaskï¼Œnä¸ªtaskä¼šé€åˆ°nä¸ªexecutorä¸Šæ‰§è¡Œï¼Œä¸€ä¸ªApplicationæ˜¯ç”±ä¸€ä¸ªdriver ç¨‹åºå’Œnä¸ª executorç»„æˆã€‚æäº¤çš„æ—¶å€™ï¼Œé€šè¿‡Cluster Managerå’ŒDeploy modeæ§åˆ¶ã€‚</p><p>sparkåº”ç”¨ç¨‹åºåœ¨é›†ç¾¤ä¸Šè¿è¡Œä¸€ç»„ç‹¬ç«‹çš„è¿›ç¨‹ï¼Œé€šè¿‡SparkContextåè°ƒçš„åœ¨mainæ–¹æ³•é‡Œé¢ã€‚<br>å¦‚æœè¿è¡Œåœ¨ä¸€ä¸ªé›†ç¾¤ä¹‹ä¸Šï¼ŒSparkContextèƒ½å¤Ÿè¿æ¥å„ç§çš„é›†ç¾¤ç®¡ç†è€…ï¼Œå»è·å–åˆ°ä½œä¸šæ‰€éœ€è¦çš„èµ„æºã€‚ä¸€æ—¦è¿æ¥æˆåŠŸï¼Œsparkåœ¨é›†ç¾¤èŠ‚ç‚¹ä¹‹ä¸Šè¿è¡Œexecutorè¿›ç¨‹ï¼Œæ¥ç»™ä½ çš„åº”ç”¨ç¨‹åºè¿è¡Œè®¡ç®—å’Œå­˜å‚¨æ•°æ®ã€‚å®ƒä¼šå‘é€ä½ çš„åº”ç”¨ç¨‹åºä»£ç åˆ°executorsä¸Šã€‚æœ€åï¼ŒSparkContextå‘é€tasksåˆ°executorsä¸Šå»è¿è¡Œ</p><ul><li>1ã€æ¯ä¸ªApplicationéƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„executorè¿›ç¨‹ï¼Œè¿™äº›è¿›ç¨‹åœ¨è¿è¡Œå‘¨æœŸå†…éƒ½æ˜¯å¸¸é©»çš„ä»¥å¤šçº¿ç¨‹çš„æ–¹å¼è¿è¡Œtasksã€‚å¥½å¤„æ˜¯æ¯ä¸ªè¿›ç¨‹æ— è®ºæ˜¯åœ¨è°ƒåº¦è¿˜æ˜¯æ‰§è¡Œéƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ„å‘³ç€æ•°æ®ä¸èƒ½è·¨åº”ç”¨ç¨‹åºè¿›è¡Œå…±äº«ï¼Œé™¤éå†™åˆ°å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿï¼ˆAlluxioï¼‰ã€‚</li><li>2ã€sparkå¹¶ä¸å…³å¿ƒåº•å±‚çš„é›†ç¾¤ç®¡ç†ã€‚</li><li>3ã€driver ç¨‹åºä¼šç›‘å¬å¹¶ä¸”æ¥æ”¶å¤–é¢çš„ä¸€äº›executorè¯·æ±‚ï¼Œåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸé‡Œé¢ã€‚æ‰€ä»¥ï¼Œdriver ç¨‹åºåº”è¯¥èƒ½è¢«Worker Nodeé€šè¿‡ç½‘ç»œè®¿é—®ã€‚</li><li>4ã€å› ä¸ºdriver åœ¨é›†ç¾¤ä¸Šè°ƒåº¦Tasksï¼Œdriver å°±åº”è¯¥é è¿‘Worker Nodeã€‚</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç¾å‘³ä¸ç”¨ç­‰å¤§æ•°æ®é¢è¯•é¢˜(201804æœˆ)</title>
      <link href="/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/"/>
      <url>/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h6 id="1-è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´ç°åœºé¢è¯•é¢˜æˆªå›¾"><a href="#1-è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´ç°åœºé¢è¯•é¢˜æˆªå›¾" class="headerlink" title="1.è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´ç°åœºé¢è¯•é¢˜æˆªå›¾:"></a><strong>1.è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´ç°åœºé¢è¯•é¢˜æˆªå›¾:</strong></h6><a id="more"></a><p><img src="/assets/blogImg/520_1.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_2.png" alt="enter description here"></p><h6 id="2-åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š"><a href="#2-åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š" class="headerlink" title="2.åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š"></a><strong>2.åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š</strong></h6><p><img src="/assets/blogImg/520_3.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> é¢è¯•é¢˜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¤§æ•°æ®é¢è¯•é¢˜ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkä¸å¾—ä¸ç†è§£çš„é‡è¦æ¦‚å¿µâ€”â€”ä»æºç è§’åº¦çœ‹RDD</title>
      <link href="/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
      <url>/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h4 id="1-RDDæ˜¯ä»€ä¹ˆ"><a href="#1-RDDæ˜¯ä»€ä¹ˆ" class="headerlink" title="1.RDDæ˜¯ä»€ä¹ˆ"></a>1.RDDæ˜¯ä»€ä¹ˆ</h4><p>Resilient Distributed Datasetï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ï¼Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¹¶è¡Œæ“ä½œä¸å¯å˜çš„åˆ†åŒºå…ƒç´ çš„é›†åˆ</p><h4 id="2-RDDäº”å¤§ç‰¹æ€§"><a href="#2-RDDäº”å¤§ç‰¹æ€§" class="headerlink" title="2.RDDäº”å¤§ç‰¹æ€§"></a>2.RDDäº”å¤§ç‰¹æ€§</h4><a id="more"></a><ol><li><p>A list of partitions<br>æ¯ä¸ªrddæœ‰å¤šä¸ªåˆ†åŒº<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>è®¡ç®—ä½œç”¨åˆ°æ¯ä¸ªåˆ†åŒº<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rddä¹‹é—´å­˜åœ¨ä¾èµ–ï¼ˆRDDçš„è¡€ç¼˜å…³ç³»ï¼‰å¦‚ï¼š<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>å¯é€‰ï¼Œé»˜è®¤å“ˆå¸Œçš„åˆ†åŒº<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„æœ€ä¼˜æ‰§è¡Œä½ç½®ï¼Œå°½é‡å®ç°æ•°æ®æœ¬åœ°åŒ–ï¼Œå‡å°‘IOï¼ˆè¿™å¾€å¾€æ˜¯ç†æƒ³çŠ¶æ€ï¼‰<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>æºç æ¥è‡ªgithubã€‚</p><h4 id="3-å¦‚ä½•åˆ›å»ºRDD"><a href="#3-å¦‚ä½•åˆ›å»ºRDD" class="headerlink" title="3.å¦‚ä½•åˆ›å»ºRDD"></a>3.å¦‚ä½•åˆ›å»ºRDD</h4><p>åˆ›å»ºRDDæœ‰ä¸¤ç§æ–¹å¼ parallelize() å’Œtextfile()ï¼Œå…¶ä¸­parallelizeå¯æ¥æ”¶é›†åˆç±»ï¼Œä¸»è¦ä½œä¸ºæµ‹è¯•ç”¨ã€‚textfileå¯è¯»å–æ–‡ä»¶ç³»ç»Ÿï¼Œæ˜¯å¸¸ç”¨çš„ä¸€ç§æ–¹å¼<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfileï¼ˆï¼‰</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><p><strong>æºç æ€»ç»“ï¼š<br>1ï¼‰.å–_2æ˜¯å› ä¸ºæ•°æ®ä¸ºï¼ˆkeyï¼ˆåç§»é‡ï¼‰ï¼Œvalueï¼ˆæ•°æ®ï¼‰ï¼‰</strong></p><h4 id="4-å¸¸è§çš„transformationå’Œaction"><a href="#4-å¸¸è§çš„transformationå’Œaction" class="headerlink" title="4.å¸¸è§çš„transformationå’Œaction"></a>4.å¸¸è§çš„transformationå’Œaction</h4><p>ç”±äºæ¯”è¾ƒç®€å•ï¼Œå¤§æ¦‚è¯´ä¸€ä¸‹å¸¸ç”¨çš„ç”¨å¤„ï¼Œä¸åšä»£ç æµ‹è¯•</p><p>transformation</p><ul><li>Mapï¼šå¯¹æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œ</li><li>FlatMapï¼šå…ˆå¯¹æ•°æ®é›†è¿›è¡Œæ‰å¹³åŒ–å¤„ç†ï¼Œç„¶åå†Map</li><li>Filterï¼šå¯¹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œä¸ºtrueåˆ™é€šè¿‡</li><li>destinctï¼šå»é‡æ“ä½œ</li></ul><p>action</p><ul><li>reduceï¼šå¯¹æ•°æ®è¿›è¡Œèšé›†</li><li>reduceBykeyï¼šå¯¹keyå€¼ç›¸åŒçš„è¿›è¡Œæ“ä½œ</li><li>collectï¼šæ²¡æœ‰æ•ˆæœçš„actionï¼Œä½†æ˜¯å¾ˆæœ‰ç”¨</li><li>saveAstextFileï¼šæ•°æ®å­˜å…¥æ–‡ä»¶ç³»ç»Ÿ</li><li>foreachï¼šå¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œfuncçš„æ“ä½œ</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDDã€DataFrameå’ŒDataSetçš„åŒºåˆ«</title>
      <link href="/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br>åœ¨Sparkä¸­ï¼ŒRDDã€DataFrameã€Datasetæ˜¯æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼Œä»Šå¤©è°ˆè°ˆä»–ä»¬çš„åŒºåˆ«ï¼</font><h5 id="ä¸€-ã€å…±æ€§"><a href="#ä¸€-ã€å…±æ€§" class="headerlink" title="ä¸€ ã€å…±æ€§"></a>ä¸€ ã€å…±æ€§</h5><p>1ã€RDDã€DataFrameã€Datasetå…¨éƒ½æ˜¯sparkå¹³å°ä¸‹çš„åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†ï¼Œä¸ºå¤„ç†è¶…å¤§å‹æ•°æ®æä¾›ä¾¿åˆ©</p><p>2ã€ä¸‰è€…éƒ½æœ‰æƒ°æ€§æœºåˆ¶ï¼Œåœ¨è¿›è¡Œåˆ›å»ºã€è½¬æ¢ï¼Œå¦‚mapæ–¹æ³•æ—¶ï¼Œä¸ä¼šç«‹å³æ‰§è¡Œï¼Œåªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶ï¼Œä¸‰è€…æ‰ä¼šå¼€å§‹éå†è¿ç®—ã€‚</p><p>3ã€ä¸‰è€…éƒ½ä¼šæ ¹æ®sparkçš„å†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—ï¼Œè¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º</p><p>4ã€ä¸‰è€…éƒ½æœ‰partitionçš„æ¦‚å¿µã€‚<br><a id="more"></a></p><h5 id="äºŒã€RDDä¼˜ç¼ºç‚¹"><a href="#äºŒã€RDDä¼˜ç¼ºç‚¹" class="headerlink" title="äºŒã€RDDä¼˜ç¼ºç‚¹"></a>äºŒã€RDDä¼˜ç¼ºç‚¹</h5><p><strong>ä¼˜ç‚¹ï¼š</strong></p><ul><li><p>1ã€ç›¸æ¯”äºä¼ ç»Ÿçš„MapReduceæ¡†æ¶ï¼ŒSparkåœ¨RDDä¸­å†…ç½®å¾ˆå¤šå‡½æ•°æ“ä½œï¼Œgroupï¼Œmapï¼Œfilterç­‰ï¼Œæ–¹ä¾¿å¤„ç†ç»“æ„åŒ–æˆ–éç»“æ„åŒ–æ•°æ®ã€‚</p></li><li><p>2ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹é£æ ¼</p></li><li><p>3ã€ç¼–è¯‘æ—¶ç±»å‹å®‰å…¨ï¼Œç¼–è¯‘æ—¶å°±èƒ½æ£€æŸ¥å‡ºç±»å‹é”™è¯¯</p></li></ul><p><strong>ç¼ºç‚¹ï¼š</strong></p><ul><li><p>1ã€åºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„æ€§èƒ½å¼€é”€</p></li><li><p>2ã€GCçš„æ€§èƒ½å¼€é”€ï¼Œé¢‘ç¹çš„åˆ›å»ºå’Œé”€æ¯å¯¹è±¡, åŠ¿å¿…ä¼šå¢åŠ GC</p></li></ul><h5 id="ä¸‰ã€DataFrame"><a href="#ä¸‰ã€DataFrame" class="headerlink" title="ä¸‰ã€DataFrame"></a>ä¸‰ã€DataFrame</h5><p>1ã€ä¸RDDå’ŒDatasetä¸åŒï¼ŒDataFrameæ¯ä¸€è¡Œçš„ç±»å‹å›ºå®šä¸ºRowï¼Œåªæœ‰é€šè¿‡è§£ææ‰èƒ½è·å–å„ä¸ªå­—æ®µçš„å€¼ã€‚å¦‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreach&#123;</span><br><span class="line">  x =&gt;</span><br><span class="line">    val v1=x.getAs[String](&quot;v1&quot;)</span><br><span class="line">    val v2=x.getAs[String](&quot;v2&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>2ã€DataFrameå¼•å…¥äº†schemaå’Œoff-heap</p><ul><li><p>schema : RDDæ¯ä¸€è¡Œçš„æ•°æ®, ç»“æ„éƒ½æ˜¯ä¸€æ ·çš„. è¿™ä¸ªç»“æ„å°±å­˜å‚¨åœ¨schemaä¸­. Sparké€šè¿‡schameå°±èƒ½å¤Ÿè¯»æ‡‚æ•°æ®, å› æ­¤åœ¨é€šä¿¡å’ŒIOæ—¶å°±åªéœ€è¦åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ•°æ®, è€Œç»“æ„çš„éƒ¨åˆ†å°±å¯ä»¥çœç•¥äº†.</p></li><li><p>off-heap : æ„å‘³ç€JVMå †ä»¥å¤–çš„å†…å­˜, è¿™äº›å†…å­˜ç›´æ¥å—æ“ä½œç³»ç»Ÿç®¡ç†ï¼ˆè€Œä¸æ˜¯JVMï¼‰ã€‚Sparkèƒ½å¤Ÿä»¥äºŒè¿›åˆ¶çš„å½¢å¼åºåˆ—åŒ–æ•°æ®(ä¸åŒ…æ‹¬ç»“æ„)åˆ°off-heapä¸­, å½“è¦æ“ä½œæ•°æ®æ—¶, å°±ç›´æ¥æ“ä½œoff-heapå†…å­˜. ç”±äºSparkç†è§£schema, æ‰€ä»¥çŸ¥é“è¯¥å¦‚ä½•æ“ä½œ.</p></li><li><p>off-heapå°±åƒåœ°ç›˜, schemaå°±åƒåœ°å›¾, Sparkæœ‰åœ°å›¾åˆæœ‰è‡ªå·±åœ°ç›˜äº†, å°±å¯ä»¥è‡ªå·±è¯´äº†ç®—äº†, ä¸å†å—JVMçš„é™åˆ¶, ä¹Ÿå°±ä¸å†æ”¶GCçš„å›°æ‰°äº†.</p></li></ul><p>3ã€ç»“æ„åŒ–æ•°æ®å¤„ç†éå¸¸æ–¹ä¾¿ï¼Œæ”¯æŒAvro, CSV, Elasticsearchæ•°æ®ç­‰ï¼Œä¹Ÿæ”¯æŒHive, MySQLç­‰ä¼ ç»Ÿæ•°æ®è¡¨</p><p>4ã€å…¼å®¹Hiveï¼Œæ”¯æŒHqlã€UDF</p><p><strong>æœ‰schemaå’Œoff-heapæ¦‚å¿µï¼ŒDataFrameè§£å†³äº†RDDçš„ç¼ºç‚¹, ä½†æ˜¯å´ä¸¢äº†RDDçš„ä¼˜ç‚¹. DataFrameä¸æ˜¯ç±»å‹å®‰å…¨çš„ï¼ˆåªæœ‰ç¼–è¯‘åæ‰èƒ½çŸ¥é“ç±»å‹é”™è¯¯ï¼‰, APIä¹Ÿä¸æ˜¯é¢å‘å¯¹è±¡é£æ ¼çš„.</strong></p><h5 id="å››ã€DataSet"><a href="#å››ã€DataSet" class="headerlink" title="å››ã€DataSet"></a>å››ã€DataSet</h5><p>1ã€DataSetæ˜¯åˆ†å¸ƒå¼çš„æ•°æ®é›†åˆã€‚DataSetæ˜¯åœ¨Spark1.6ä¸­æ·»åŠ çš„æ–°çš„æ¥å£ã€‚å®ƒé›†ä¸­äº†RDDçš„ä¼˜ç‚¹ï¼ˆå¼ºç±»å‹ å’Œå¯ä»¥ç”¨å¼ºå¤§lambdaå‡½æ•°ï¼‰ä»¥åŠSpark SQLä¼˜åŒ–çš„æ‰§è¡Œå¼•æ“ã€‚DataSetå¯ä»¥é€šè¿‡JVMçš„å¯¹è±¡è¿›è¡Œæ„å»ºï¼Œå¯ä»¥ç”¨å‡½æ•°å¼çš„è½¬æ¢ï¼ˆmap/flatmap/filterï¼‰è¿›è¡Œå¤šç§æ“ä½œã€‚</p><p>2ã€DataSetç»“åˆäº†RDDå’ŒDataFrameçš„ä¼˜ç‚¹, å¹¶å¸¦æ¥çš„ä¸€ä¸ªæ–°çš„æ¦‚å¿µEncoderã€‚DataSet é€šè¿‡Encoderå®ç°äº†è‡ªå®šä¹‰çš„åºåˆ—åŒ–æ ¼å¼ï¼Œä½¿å¾—æŸäº›æ“ä½œå¯ä»¥åœ¨æ— éœ€åºåˆ—åŒ–æƒ…å†µä¸‹è¿›è¡Œã€‚å¦å¤–Datasetè¿˜è¿›è¡Œäº†åŒ…æ‹¬Tungstenä¼˜åŒ–åœ¨å†…çš„å¾ˆå¤šæ€§èƒ½æ–¹é¢çš„ä¼˜åŒ–ã€‚</p><p>3ã€Dataset<row>ç­‰åŒäºDataFrameï¼ˆSpark 2.Xï¼‰</row></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¤§æ•°æ®ä¹‹å®æ—¶æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶--ç”Ÿäº§ä¸ŠCanalä¸Maxwellé¢ å³°å¯¹å†³</title>
      <link href="/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
      <url>/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="ä¸€-æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼š"><a href="#ä¸€-æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼š" class="headerlink" title="ä¸€.æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼š"></a>ä¸€.æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼š</h5><p>Canal<br><a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">https://github.com/alibaba/canal</a><br><a href="https://github.com/Hackeruncle/syncClient" target="_blank" rel="noopener">https://github.com/Hackeruncle/syncClient</a></p><p>Maxwell<br><a href="https://github.com/zendesk/maxwell" target="_blank" rel="noopener">https://github.com/zendesk/maxwell</a><br><img src="/assets/blogImg/514_1.png" alt="maxwell"><br><a id="more"></a></p><h5 id="äºŒ-æ¶æ„ä½¿ç”¨"><a href="#äºŒ-æ¶æ„ä½¿ç”¨" class="headerlink" title="äºŒ.æ¶æ„ä½¿ç”¨"></a>äºŒ.æ¶æ„ä½¿ç”¨</h5><p>MySQL â€”- ä¸­é—´ä»¶ mcp â€”&gt;KAFKAâ€”&gt;?â€”&gt;å­˜å‚¨HBASE/KUDU/Cassandra å¢é‡çš„<br>a.å…¨é‡ bootstrap<br>b.å¢é‡</p><h6 id="1-å¯¹æ¯”"><a href="#1-å¯¹æ¯”" class="headerlink" title="1.å¯¹æ¯”"></a>1.å¯¹æ¯”</h6><table><thead><tr><th></th><th></th><th>Canal(æœåŠ¡ç«¯)</th><th>Maxwell(æœåŠ¡ç«¯+å®¢æˆ·ç«¯)</th></tr></thead><tbody><tr><td>è¯­è¨€</td><td>Java</td><td>Java</td><td></td></tr><tr><td>æ´»è·ƒåº¦</td><td>æ´»è·ƒ</td><td>æ´»è·ƒ</td><td></td></tr><tr><td>HA</td><td>æ”¯æŒ</td><td>å®šåˆ¶ ä½†æ˜¯æ”¯æŒæ–­ç‚¹è¿˜åŸåŠŸèƒ½</td></tr><tr><td>æ•°æ®è½åœ°</td><td>å®šåˆ¶</td><td>è½åœ°åˆ°kafka</td></tr><tr><td>åˆ†åŒº</td><td>æ”¯æŒ</td><td>æ”¯æŒ</td></tr><tr><td>bootstrap(å¼•å¯¼)</td><td>ä¸æ”¯æŒ</td><td>æ”¯æŒ</td></tr><tr><td>æ•°æ®æ ¼å¼</td><td>æ ¼å¼è‡ªç”±</td><td>json(æ ¼å¼å›ºå®š) spark jsonâ€“&gt;DF</td></tr><tr><td>æ–‡æ¡£</td><td>è¾ƒè¯¦ç»†</td><td>è¾ƒè¯¦ç»†</td><td></td></tr><tr><td>éšæœºè¯»</td><td>æ”¯æŒ</td><td>æ”¯æŒ</td><td></td></tr></tbody></table><p><strong>ä¸ªäººé€‰æ‹©Maxwell</strong></p><p>a.æœåŠ¡ç«¯+å®¢æˆ·ç«¯ä¸€ä½“ï¼Œè½»é‡çº§çš„<br>b.æ”¯æŒæ–­ç‚¹è¿˜åŸåŠŸèƒ½+bootstrap+json<br>Can do SELECT * from table (bootstrapping) initial loads of a table.<br>supports automatic position recover on master promotion<br>flexible partitioning schemes for Kakfa - by database, table, primary key, or column<br>Maxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).</p><h6 id="2-å®˜ç½‘è§£è¯»"><a href="#2-å®˜ç½‘è§£è¯»" class="headerlink" title="2.å®˜ç½‘è§£è¯»"></a>2.å®˜ç½‘è§£è¯»</h6><p><a href="https://www.bilibili.com/video/av34778187?from=search&amp;seid=18393822973469412185" target="_blank" rel="noopener">Bç«™è§†é¢‘</a></p><h6 id="3-éƒ¨ç½²"><a href="#3-éƒ¨ç½²" class="headerlink" title="3.éƒ¨ç½²"></a>3.éƒ¨ç½²</h6><p><strong>3.1 MySQL Install</strong><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a><br><a href="https://ke.qq.com/course/262452?tuin=11cffd50" target="_blank" rel="noopener">https://ke.qq.com/course/262452?tuin=11cffd50</a></p><p><strong>3.2 ä¿®æ”¹</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">binlog_format=row</span><br><span class="line"></span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line">3.3 åˆ›å»ºMaxwellçš„dbå’Œç”¨æˆ·</span><br><span class="line">mysql&gt; create database maxwell;</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4è§£å‹</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz</span><br></pre></td></tr></table></figure><p></p><p><strong>3.5æµ‹è¯•STDOUT:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --user=&apos;maxwell&apos; \</span><br><span class="line">--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \</span><br><span class="line">--producer=stdout</span><br></pre></td></tr></table></figure><p></p><p>æµ‹è¯•1ï¼šinsert sqlï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br></pre></td></tr></table></figure><p></p><p>maxwellè¾“å‡ºï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;insert&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959044,</span><br><span class="line">    &quot;xid&quot;: 201,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>æµ‹è¯•1ï¼šupdate sql:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update ruozedata set age=29 where id=999;</span><br></pre></td></tr></table></figure><p></p><p><strong>é—®é¢˜: ROWï¼Œä½ è§‰å¾—binlogæ›´æ–°å‡ ä¸ªå­—æ®µï¼Ÿ</strong></p><p>maxwellè¾“å‡ºï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;update&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959208,</span><br><span class="line">    &quot;xid&quot;: 255,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 29,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;old&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="4-å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§"><a href="#4-å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§" class="headerlink" title="4.å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§"></a>4.å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§</h6><p><strong>4.1 kafka_version ç‰ˆæœ¬</strong><br>Using kafka version: 0.11.0.1 0.10<br>jar:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 kafka-clients]# ll</span><br><span class="line">total 4000</span><br><span class="line">-rw-r--r--. 1 ruoze games  746207 May  8 06:34 kafka-clients-0.10.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  951041 May  8 06:35 kafka-clients-0.10.2.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games 1419544 May  8 06:35 kafka-clients-0.11.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  324016 May  8 06:34 kafka-clients-0.8.2.2.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  641408 May  8 06:34 kafka-clients-0.9.0.1.jar</span><br><span class="line">[root@hadoop000 kafka-clients]#</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> å…¶ä»–ç»„ä»¶ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> maxwell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«</title>
      <link href="/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="ä¸€-YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«"><a href="#ä¸€-YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«" class="headerlink" title="ä¸€. YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«"></a>ä¸€. YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«</h5><p><img src="/assets/blogImg/512_1.png" alt><br>ï¼ˆ1ï¼‰SparkContextåˆå§‹åŒ–ä¸åŒï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Driveræ‰€åœ¨ä½ç½®çš„ä¸åŒï¼ŒYarnClusterçš„Driveræ˜¯åœ¨é›†ç¾¤çš„æŸä¸€å°NMä¸Šï¼Œä½†æ˜¯Yarn-Clientå°±æ˜¯åœ¨driveræ‰€åœ¨çš„æœºå™¨ä¸Šï¼›<br>ï¼ˆ2ï¼‰è€ŒDriverä¼šå’ŒExecutorsè¿›è¡Œé€šä¿¡ï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Yarn_clusteråœ¨æäº¤Appä¹‹åå¯ä»¥å…³é—­Clientï¼Œè€ŒYarn-Clientä¸å¯ä»¥ï¼›<br>ï¼ˆ3ï¼‰æœ€åå†æ¥è¯´åº”ç”¨åœºæ™¯ï¼ŒYarn-Clusteré€‚åˆç”Ÿäº§ç¯å¢ƒï¼ŒYarn-Clienté€‚åˆäº¤äº’å’Œè°ƒè¯•ã€‚<br><a id="more"></a></p><h5 id="äºŒ-yarn-client-æ¨¡å¼"><a href="#äºŒ-yarn-client-æ¨¡å¼" class="headerlink" title="äºŒ. yarn client æ¨¡å¼"></a>äºŒ. yarn client æ¨¡å¼</h5><p><img src="/assets/blogImg/512_2.png" alt></p><p><font color="#FF4200">yarn-client æ¨¡å¼çš„è¯ ï¼ŒæŠŠ å®¢æˆ·ç«¯å…³æ‰çš„è¯ ï¼Œæ˜¯ä¸èƒ½æäº¤ä»»åŠ¡çš„ ã€‚<br></font></p><h5 id="ä¸‰-yarn-cluster-æ¨¡å¼"><a href="#ä¸‰-yarn-cluster-æ¨¡å¼" class="headerlink" title="ä¸‰.yarn  cluster æ¨¡å¼"></a>ä¸‰.yarn cluster æ¨¡å¼</h5><p><img src="/assets/blogImg/512_3.png" alt></p><p><font color="#FF4200">yarn-cluster æ¨¡å¼çš„è¯ï¼Œ client å…³é—­æ˜¯å¯ä»¥æäº¤ä»»åŠ¡çš„ ï¼Œ<br></font></p><h5 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“:"></a>æ€»ç»“:</h5><p><strong>1.spark-shell/spark-sql åªæ”¯æŒ yarn-clientæ¨¡å¼ï¼›<br>2.spark-submitå¯¹äºä¸¤ç§æ¨¡å¼éƒ½æ”¯æŒã€‚</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> æ¶æ„ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç”Ÿäº§æ”¹é€ Spark1.6æºä»£ç ï¼Œcreate tableè¯­æ³•æ”¯æŒOracleåˆ—è¡¨åˆ†åŒº</title>
      <link href="/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/"/>
      <url>/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h5 id="1-éœ€æ±‚"><a href="#1-éœ€æ±‚" class="headerlink" title="1.éœ€æ±‚"></a>1.éœ€æ±‚</h5><p>é€šè¿‡Spark SQL JDBC æ–¹æ³•ï¼ŒæŠ½å–Oracleè¡¨æ•°æ®ã€‚</p><h5 id="2-é—®é¢˜"><a href="#2-é—®é¢˜" class="headerlink" title="2.é—®é¢˜"></a>2.é—®é¢˜</h5><p>å¤§æ•°æ®å¼€å‘äººå‘˜åæ˜ ï¼Œä½¿ç”¨æ•ˆæœä¸Šåˆ—è¡¨åˆ†åŒºä¼˜äºæ•£åˆ—åˆ†åŒºã€‚ä½†Spark SQL JDBCæ–¹æ³•åªæ”¯æŒæ•°å­—ç±»å‹åˆ†åŒºï¼Œè€Œä¸šåŠ¡è¡¨çš„åˆ—è¡¨åˆ†åŒºå­—æ®µæ˜¯ä¸ªå­—ç¬¦ä¸²ã€‚ç›®å‰Oracleè¡¨ä½¿ç”¨åˆ—è¡¨åˆ†åŒºï¼Œå¯¹çœçº§ä»£ç åˆ† åŒºã€‚<br>å‚è€ƒ <a href="http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases" target="_blank" rel="noopener">http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases</a><br><a id="more"></a></p><h5 id="3-Oracleçš„åˆ†åŒº"><a href="#3-Oracleçš„åˆ†åŒº" class="headerlink" title="3.Oracleçš„åˆ†åŒº"></a>3.Oracleçš„åˆ†åŒº</h5><h6 id="3-1åˆ—è¡¨åˆ†åŒº"><a href="#3-1åˆ—è¡¨åˆ†åŒº" class="headerlink" title="3.1åˆ—è¡¨åˆ†åŒº:"></a>3.1åˆ—è¡¨åˆ†åŒº:</h6><p>è¯¥åˆ†åŒºçš„ç‰¹ç‚¹æ˜¯æŸåˆ—çš„å€¼åªæœ‰å‡ ä¸ªï¼ŒåŸºäºè¿™æ ·çš„ç‰¹ç‚¹æˆ‘ä»¬å¯ä»¥é‡‡ç”¨åˆ—è¡¨åˆ†åŒºã€‚<br>ä¾‹ä¸€:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PROBLEM_TICKETS</span><br><span class="line">(</span><br><span class="line">PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),</span><br><span class="line">CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,</span><br><span class="line">STATUS VARCHAR2(20)</span><br><span class="line">)</span><br><span class="line">PARTITION BY LIST (STATUS)</span><br><span class="line">(</span><br><span class="line">PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,</span><br><span class="line">PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h6 id="3-2æ•£åˆ—åˆ†åŒº"><a href="#3-2æ•£åˆ—åˆ†åŒº" class="headerlink" title="3.2æ•£åˆ—åˆ†åŒº:"></a>3.2æ•£åˆ—åˆ†åŒº:</h6><p>è¿™ç±»åˆ†åŒºæ˜¯åœ¨åˆ—å€¼ä¸Šä½¿ç”¨æ•£åˆ—ç®—æ³•ï¼Œä»¥ç¡®å®šå°†è¡Œæ”¾å…¥å“ªä¸ªåˆ†åŒºä¸­ã€‚å½“åˆ—çš„å€¼æ²¡æœ‰åˆé€‚çš„æ¡ä»¶æ—¶ï¼Œå»ºè®®ä½¿ç”¨æ•£åˆ—åˆ†åŒºã€‚ æ•£åˆ—åˆ†åŒºä¸ºé€šè¿‡æŒ‡å®šåˆ†åŒºç¼–å·æ¥å‡åŒ€åˆ†å¸ƒæ•°æ®çš„ä¸€ç§åˆ†åŒºç±»å‹ï¼Œå› ä¸ºé€šè¿‡åœ¨I/Oè®¾å¤‡ä¸Šè¿›è¡Œæ•£åˆ—åˆ†åŒºï¼Œä½¿å¾—è¿™äº›åˆ†åŒºå¤§å°ä¸€è‡´ã€‚<br>ä¾‹ä¸€:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE HASH_TABLE</span><br><span class="line">(</span><br><span class="line">COL NUMBER(8),</span><br><span class="line">INF VARCHAR2(100) </span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (COL)</span><br><span class="line">(</span><br><span class="line">PARTITION PART01 TABLESPACE HASH_TS01, </span><br><span class="line">PARTITION PART02 TABLESPACE HASH_TS02, </span><br><span class="line">PARTITION PART03 TABLESPACE HASH_TS03</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h5 id="4-æ”¹é€ "><a href="#4-æ”¹é€ " class="headerlink" title="4.æ”¹é€ "></a>4.æ”¹é€ </h5><p>è“è‰²ä»£ç æ˜¯æ”¹é€ Sparkæºä»£ç ,åŠ è¯¾ç¨‹é¡¾é—®é¢†å–PDFã€‚</p><h6 id="1-Spark-SQL-JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚"><a href="#1-Spark-SQL-JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚" class="headerlink" title="1) Spark SQL JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚"></a>1) Spark SQL JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE TBLS_IN</span><br><span class="line">USING org.apache.spark.sql.jdbc OPTIONS (</span><br><span class="line">driver &quot;com.mysql.jdbc.Driver&quot;,</span><br><span class="line">url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,</span><br><span class="line">fetchSize &quot;1000&quot;,</span><br><span class="line">partitionColumn &quot;TBL_ID&quot;,</span><br><span class="line">numPartitions &quot;null&quot;,</span><br><span class="line">lowerBound &quot;null&quot;,</span><br><span class="line">upperBound &quot;null&quot;,</span><br><span class="line">user &quot;hive2user&quot;,</span><br><span class="line">password &quot;hive2user&quot;,</span><br><span class="line">partitionInRule &quot;1|15,16,18,19|20,21&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="2-ç¨‹åºå…¥å£org-apache-spark-sql-execution-datasources-jdbc-DefaultSourceï¼Œæ–¹æ³•createRelation"><a href="#2-ç¨‹åºå…¥å£org-apache-spark-sql-execution-datasources-jdbc-DefaultSourceï¼Œæ–¹æ³•createRelation" class="headerlink" title="2)ç¨‹åºå…¥å£org.apache.spark.sql.execution.datasources.jdbc.DefaultSourceï¼Œæ–¹æ³•createRelation"></a>2)ç¨‹åºå…¥å£org.apache.spark.sql.execution.datasources.jdbc.DefaultSourceï¼Œæ–¹æ³•createRelation</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">sqlContext: SQLContext,</span><br><span class="line">parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))</span><br><span class="line">val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)</span><br><span class="line">var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)</span><br><span class="line">var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)</span><br><span class="line"></span><br><span class="line">// add partition in rule</span><br><span class="line">val partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)</span><br><span class="line">// validind all the partition in rule </span><br><span class="line">if (partitionColumn != null</span><br><span class="line">&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)</span><br><span class="line">&amp;&amp; partitionInRule == null </span><br><span class="line">)&#123;</span><br><span class="line">   sys.error(&quot;Partitioning incompletely specified&quot;) </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val partitionInfo = </span><br><span class="line">if (partitionColumn == null) &#123; </span><br><span class="line">    null</span><br><span class="line">&#125; else &#123;</span><br><span class="line"></span><br><span class="line">val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123;</span><br><span class="line">val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot;</span><br><span class="line">upperBound = &quot;0&quot;</span><br><span class="line">inGroups &#125;</span><br><span class="line">else&#123;</span><br><span class="line">Array[String]() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JDBCPartitioningInfo( partitionColumn, </span><br><span class="line">lowerBound.toLong, </span><br><span class="line">upperBound.toLong, </span><br><span class="line">numPartitions.toInt, </span><br><span class="line">inPartitions)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val parts = JDBCRelation.columnPartition(partitionInfo)</span><br><span class="line">val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))</span><br><span class="line">// parameters is immutable</span><br><span class="line">if(numPartitions != null)&#123;</span><br><span class="line">properties.put(&quot;numPartitions&quot; , numPartitions) &#125;</span><br><span class="line">JDBCRelation(url, table, parts, properties)(sqlContext)</span><br><span class="line"></span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelationï¼Œæ–¹æ³•columnPartition"><a href="#3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelationï¼Œæ–¹æ³•columnPartition" class="headerlink" title="3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelationï¼Œæ–¹æ³•columnPartition"></a>3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelationï¼Œæ–¹æ³•columnPartition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;</span><br><span class="line">if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))</span><br><span class="line">val column = partitioning.column</span><br><span class="line">var i: Int = 0</span><br><span class="line">var ans = new ArrayBuffer[Partition]()</span><br><span class="line"></span><br><span class="line">// partition by long if(partitioning.inPartitions.length == 0)&#123;</span><br><span class="line"></span><br><span class="line">val numPartitions = partitioning.numPartitions</span><br><span class="line">if (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.</span><br><span class="line">// Here we get a little roundoff, but that&apos;s (hopefully) OK.</span><br><span class="line">val stride: Long = (partitioning.upperBound / numPartitions</span><br><span class="line"></span><br><span class="line">- partitioning.lowerBound / numPartitions)</span><br><span class="line">var currentValue: Long = partitioning.lowerBound</span><br><span class="line">while (i &lt; numPartitions) &#123;</span><br><span class="line">val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else null</span><br><span class="line">currentValue += stride</span><br><span class="line">val upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =</span><br><span class="line"></span><br><span class="line">if (upperBound == null) &#123; </span><br><span class="line">  lowerBound</span><br><span class="line"></span><br><span class="line">&#125; else if (lowerBound == null) &#123; </span><br><span class="line">  upperBound</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">  s&quot;$lowerBound AND $upperBound&quot; </span><br><span class="line">&#125;</span><br><span class="line">  ans += JDBCPartition(whereClause, i)</span><br><span class="line">   i= i+ 1 &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// partition by in </span><br><span class="line">else&#123;</span><br><span class="line">    while(i &lt; partitioning.inPartitions.length)&#123;</span><br><span class="line">           val inContent = partitioning.inPartitions(i)</span><br><span class="line">           val whereClause = s&quot;$column in ($inContent)&quot; </span><br><span class="line">           ans += JDBCPartition(whereClause, i)</span><br><span class="line">           i= i+ 1</span><br><span class="line">     &#125; </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ans.toArray </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="4-å¯¹å¤–æ–¹æ³•org-apache-spark-sql-SQLContext-æ–¹æ³•jdbc"><a href="#4-å¯¹å¤–æ–¹æ³•org-apache-spark-sql-SQLContext-æ–¹æ³•jdbc" class="headerlink" title="4)å¯¹å¤–æ–¹æ³•org.apache.spark.sql.SQLContext , æ–¹æ³•jdbc"></a>4)å¯¹å¤–æ–¹æ³•org.apache.spark.sql.SQLContext , æ–¹æ³•jdbc</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">url: String,</span><br><span class="line">table: String,</span><br><span class="line">columnName: String,</span><br><span class="line">lowerBound: Long,</span><br><span class="line">upperBound: Long,</span><br><span class="line">numPartitions: Int,</span><br><span class="line">inPartitions: Array[String] = Array[String]()</span><br><span class="line"></span><br><span class="line">): DataFrame = &#123;</span><br><span class="line">read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> æºç é˜…è¯» </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç”Ÿäº§ä¸­Hiveé™æ€å’ŒåŠ¨æ€åˆ†åŒºè¡¨ï¼Œè¯¥æ€æ ·æŠ‰æ‹©å‘¢ï¼Ÿ</title>
      <link href="/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/"/>
      <url>/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h6 id="ä¸€-éœ€æ±‚"><a href="#ä¸€-éœ€æ±‚" class="headerlink" title="ä¸€.éœ€æ±‚"></a>ä¸€.éœ€æ±‚</h6><p>æŒ‰ç…§ä¸åŒéƒ¨é—¨ä½œä¸ºåˆ†åŒºï¼Œå¯¼æ•°æ®åˆ°ç›®æ ‡è¡¨</p><h6 id="äºŒ-ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ"><a href="#äºŒ-ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ" class="headerlink" title="äºŒ.ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ"></a>äºŒ.ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ</h6><p>71.åˆ›å»ºé™æ€åˆ†åŒºè¡¨ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table emp_static_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><p>2.æ’å…¥æ•°æ®ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_static_partition partition(deptno=10)</span><br><span class="line">     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><p>3.æŸ¥è¯¢æ•°æ®ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_static_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_1.png" alt></p><h6 id="ä¸‰-ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ"><a href="#ä¸‰-ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ" class="headerlink" title="ä¸‰.ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ"></a>ä¸‰.ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ</h6><p>1.åˆ›å»ºåŠ¨æ€åˆ†åŒºè¡¨ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table emp_dynamic_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">ã€æ³¨æ„ã€‘åŠ¨æ€åˆ†åŒºè¡¨ä¸é™æ€åˆ†åŒºè¡¨çš„åˆ›å»ºï¼Œåœ¨è¯­æ³•ä¸Šæ˜¯æ²¡æœ‰ä»»ä½•åŒºåˆ«çš„</font><p>2.æ’å…¥æ•°æ®ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_dynamic_partition partition(deptno)     </span><br><span class="line">select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">ã€æ³¨æ„ã€‘åˆ†åŒºçš„å­—æ®µåç§°ï¼Œå†™åœ¨æœ€åï¼Œæœ‰å‡ ä¸ªå°±å†™å‡ ä¸ª ä¸é™æ€åˆ†åŒºç›¸æ¯”ï¼Œä¸éœ€è¦where</font><p>éœ€è¦è®¾ç½®å±æ€§çš„å€¼ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nonstrictï¼›</span><br></pre></td></tr></table></figure><p></p><p>å‡å¦‚ä¸è®¾ç½®ï¼ŒæŠ¥é”™å¦‚ä¸‹:<br><img src="/assets/blogImg/0506_2.png" alt><br>3.æŸ¥è¯¢æ•°æ®ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_dynamic_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_3.png" alt></p><p><font color="#FF4500">åˆ†åŒºåˆ—ä¸ºdeptnoï¼Œå®ç°äº†åŠ¨æ€åˆ†åŒº</font></p><h6 id="å››-æ€»ç»“"><a href="#å››-æ€»ç»“" class="headerlink" title="å››.æ€»ç»“"></a>å››.æ€»ç»“</h6><p>åœ¨ç”Ÿäº§ä¸Šæˆ‘ä»¬æ›´å€¾å‘æ˜¯é€‰æ‹©<strong>åŠ¨æ€åˆ†åŒº</strong>ï¼Œ<br>æ— éœ€æ‰‹å·¥æŒ‡å®šæ•°æ®å¯¼å…¥çš„å…·ä½“åˆ†åŒºï¼Œ<br>è€Œæ˜¯ç”±selectçš„å­—æ®µ(å­—æ®µå†™åœ¨æœ€åï¼Œæœ‰å‡ ä¸ªå†™å‡ ä¸ª)è‡ªè¡Œå†³å®šå¯¼å‡ºåˆ°å“ªä¸€ä¸ªåˆ†åŒºä¸­ï¼Œ å¹¶è‡ªåŠ¨åˆ›å»ºç›¸åº”çš„åˆ†åŒºï¼Œä½¿ç”¨ä¸Šæ›´åŠ æ–¹ä¾¿å¿«æ· ï¼Œåœ¨ç”Ÿäº§å·¥ä½œä¸­ç”¨çš„éå¸¸å¤šå¤šã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5minæŒæ¡ï¼ŒHiveçš„HiveServer2 å’ŒJDBCå®¢æˆ·ç«¯&amp;ä»£ç çš„ç”Ÿäº§ä½¿ç”¨</title>
      <link href="/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><img src="/assets/blogImg/504_1.png" alt><br><a id="more"></a></p><h6 id="1-ä»‹ç»ï¼š"><a href="#1-ä»‹ç»ï¼š" class="headerlink" title="1. ä»‹ç»ï¼š"></a>1. ä»‹ç»ï¼š</h6><p>ä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œé€šè¿‡HiveServeræˆ–è€…HiveServer2ï¼Œ<br>å®¢æˆ·ç«¯å¯ä»¥åœ¨ä¸å¯åŠ¨CLIçš„æƒ…å†µä¸‹å¯¹Hiveä¸­çš„æ•°æ®è¿›è¡Œæ“ä½œï¼Œ<br>ä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€å¦‚javaï¼Œpythonç­‰å‘hiveæäº¤è¯·æ±‚ï¼Œå–å›ç»“æœ<br>ï¼ˆä»hive0.15èµ·å°±ä¸å†æ”¯æŒhiveserveräº†ï¼‰ï¼Œä½†æ˜¯åœ¨è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯è¦è¯´ä¸€ä¸‹HiveServerã€‚</p><p>HiveServeræˆ–è€…HiveServer2éƒ½æ˜¯åŸºäºThriftçš„ï¼Œä½†HiveSeveræœ‰æ—¶è¢«ç§°ä¸ºThrift serverï¼Œ<br>è€ŒHiveServer2å´ä¸ä¼šã€‚æ—¢ç„¶å·²ç»å­˜åœ¨HiveServerï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦HiveServer2å‘¢ï¼Ÿ<br>è¿™æ˜¯å› ä¸ºHiveServerä¸èƒ½å¤„ç†å¤šäºä¸€ä¸ªå®¢æˆ·ç«¯çš„å¹¶å‘è¯·æ±‚ï¼Œè¿™æ˜¯ç”±äºHiveServerä½¿ç”¨çš„Thriftæ¥å£æ‰€å¯¼è‡´çš„é™åˆ¶ï¼Œ<br>ä¸èƒ½é€šè¿‡ä¿®æ”¹HiveServerçš„ä»£ç ä¿®æ­£ã€‚</p><p>å› æ­¤åœ¨Hive-0.11.0ç‰ˆæœ¬ä¸­é‡å†™äº†HiveServerä»£ç å¾—åˆ°äº†HiveServer2ï¼Œè¿›è€Œè§£å†³äº†è¯¥é—®é¢˜ã€‚<br>HiveServer2æ”¯æŒå¤šå®¢æˆ·ç«¯çš„å¹¶å‘å’Œè®¤è¯ï¼Œä¸ºå¼€æ”¾APIå®¢æˆ·ç«¯å¦‚é‡‡ç”¨jdbcã€odbcã€beelineçš„æ–¹å¼è¿›è¡Œè¿æ¥ã€‚</p><h6 id="2-é…ç½®å‚æ•°"><a href="#2-é…ç½®å‚æ•°" class="headerlink" title="2.é…ç½®å‚æ•°"></a>2.é…ç½®å‚æ•°</h6><p>Hiveserver2å…è®¸åœ¨é…ç½®æ–‡ä»¶hive-site.xmlä¸­è¿›è¡Œé…ç½®ç®¡ç†ï¼Œå…·ä½“çš„å‚æ•°ä¸ºï¼š<br>å‚æ•° | å«ä¹‰ |<br>-|-|<br>hive.server2.thrift.min.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º5ã€‚<br>hive.server2.thrift.max.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º500ã€‚<br>hive.server2.thrift.port| TCP çš„ç›‘å¬ç«¯å£ï¼Œé»˜è®¤ä¸º10000ã€‚<br>hive.server2.thrift.bind.host| TCPç»‘å®šçš„ä¸»æœºï¼Œé»˜è®¤ä¸ºlocalhost</p><p>é…ç½®ç›‘å¬ç«¯å£å’Œè·¯å¾„<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;192.168.48.130&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="3-å¯åŠ¨hiveserver2"><a href="#3-å¯åŠ¨hiveserver2" class="headerlink" title="3. å¯åŠ¨hiveserver2"></a>3. å¯åŠ¨hiveserver2</h6><p>ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/</span><br><span class="line">[hadoop@hadoop001 bin]$ hiveserver2 </span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br></pre></td></tr></table></figure><p></p><h6 id="4-é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿æ¥"><a href="#4-é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿æ¥" class="headerlink" title="4. é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿æ¥"></a>4. é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿æ¥</h6><ul><li>-n æŒ‡å®šæœºå™¨ç™»é™†çš„åå­—ï¼Œå½“å‰æœºå™¨çš„ç™»é™†ç”¨æˆ·å</li><li>-u æŒ‡å®šä¸€ä¸ªè¿æ¥ä¸²</li><li>æ¯æˆåŠŸè¿è¡Œä¸€ä¸ªå‘½ä»¤ï¼Œhiveserver2å¯åŠ¨çš„é‚£ä¸ªçª—å£ï¼Œåªè¦åœ¨å¯åŠ¨beelineçš„çª—å£ä¸­æ‰§è¡ŒæˆåŠŸä¸€æ¡å‘½ä»¤ï¼Œå¦å¤–ä¸ªçª—å£éšå³æ‰“å°ä¸€ä¸ªOK</li><li>å¦‚æœå‘½ä»¤é”™è¯¯ï¼Œhiveserver2é‚£ä¸ªçª—å£å°±ä¼šæŠ›å‡ºå¼‚å¸¸</li></ul><p>ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoop</span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br><span class="line">scan complete in 4ms</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.7.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.7.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.7.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt;</span><br></pre></td></tr></table></figure><p></p><p>ä½¿ç”¨SQL<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show databases;</span><br><span class="line">INFO  : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</span><br><span class="line">INFO  : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">1 row selected</span><br></pre></td></tr></table></figure><p></p><h6 id="5-ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿æ¥"><a href="#5-ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿æ¥" class="headerlink" title="5.ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿æ¥"></a>5.ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿æ¥</h6><p><strong>5.1</strong>ä½¿ç”¨mavenæ„å»ºé¡¹ç›®ï¼Œpom.xmlæ–‡ä»¶å¦‚ä¸‹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-train&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive-train&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;    </span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>5.2</strong>JdbcApp.javaæ–‡ä»¶ä»£ç :<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class JdbcApp &#123;</span><br><span class="line">     private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">         &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">             // TODO Auto-generated catch block</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">             System.exit(1);</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;);</span><br><span class="line">         Statement stmt = con.createStatement();</span><br><span class="line">         //select table:ename</span><br><span class="line">         String tableName = &quot;emp&quot;;</span><br><span class="line">         String sql = &quot;select ename from &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">          while(res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1));</span><br><span class="line">         &#125;</span><br><span class="line">         // describe table</span><br><span class="line">         sql = &quot;describe &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         res = stmt.executeQuery(sql);</span><br><span class="line">         while (res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2minå¿«é€Ÿäº†è§£ï¼ŒHiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨</title>
      <link href="/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
      <url>/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p><font color="#FF4500"><br></font><br>åœ¨äº†è§£å†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨åŒºåˆ«å‰ï¼Œ<br>æˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸‹<strong>Hiveæ¶æ„</strong> ï¼š</p><p><img src="/assets/blogImg/501_1.png" alt="Hiveæ¶æ„"><br><a id="more"></a><br>å¤§å®¶å¯ä»¥ç®€å•çœ‹ä¸€ä¸‹è¿™ä¸ªæ¶æ„å›¾ï¼Œæˆ‘ä»‹ç»å…¶ä¸­è¦ç‚¹ï¼š<br>Hiveçš„æ•°æ®åˆ†ä¸ºä¸¤ç§ï¼Œ<strong>ä¸€ç§ä¸ºæ™®é€šæ•°æ®ï¼Œä¸€ç§ä¸ºå…ƒæ•°æ®ã€‚</strong></p><ol><li>å…ƒæ•°æ®å­˜å‚¨ç€è¡¨çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¢åˆ æ”¹æŸ¥è®°å½•ï¼Œç±»ä¼¼äºHadoopæ¶æ„ä¸­çš„namespaceã€‚æ™®é€šæ•°æ®å°±æ˜¯è¡¨ä¸­çš„è¯¦ç»†æ•°æ®ã€‚</li><li>Hiveçš„å…ƒæ•°æ®é»˜è®¤å­˜å‚¨åœ¨derbyä¸­ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹å­˜å‚¨åœ¨MySQLä¸­ã€‚æ™®é€šæ•°æ®å¦‚æ¶æ„å›¾æ‰€ç¤ºå­˜å‚¨åœ¨hdfsä¸­ã€‚</li></ol><p>ä¸‹é¢æˆ‘ä»¬æ¥ä»‹ç»è¡¨çš„ä¸¤ç§ç±»å‹ï¼šå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨</p><ol><li><p>å†…éƒ¨è¡¨ï¼ˆMANAGEDï¼‰ï¼šhiveåœ¨hdfsä¸­å­˜åœ¨é»˜è®¤çš„å­˜å‚¨è·¯å¾„ï¼Œå³defaultæ•°æ®åº“ã€‚ä¹‹ååˆ›å»ºçš„æ•°æ®åº“åŠè¡¨ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„åº”éƒ½åœ¨/user/hive/warehouseä¸‹ï¼Œæ‰€ä»¥åœ¨è¯¥è·¯å¾„ä¸‹çš„è¡¨ä¸ºå†…éƒ¨è¡¨ã€‚</p></li><li><p>å¤–éƒ¨è¡¨ï¼ˆEXTERNALï¼‰ï¼šæŒ‡å®šäº†/user/hive/warehouseä»¥å¤–è·¯å¾„æ‰€åˆ›å»ºçš„è¡¨<br>è€Œå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„ä¸»è¦åŒºåˆ«å°±æ˜¯</p><ul><li>å†…éƒ¨è¡¨ï¼šå½“åˆ é™¤å†…éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®å’ŒHDFSä¸Šçš„æ™®é€šæ•°æ®éƒ½ä¼šåˆ é™¤ ï¼›</li><li>å¤–éƒ¨è¡¨ï¼šå½“åˆ é™¤å¤–éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®ä¼šè¢«åˆ é™¤ï¼ŒHDFSä¸Šçš„æ•°æ®ä¸ä¼šè¢«åˆ é™¤ï¼›</li></ul></li></ol><h6 id="1-å‡†å¤‡æ•°æ®-æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦"><a href="#1-å‡†å¤‡æ•°æ®-æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦" class="headerlink" title="1.å‡†å¤‡æ•°æ®:  æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦"></a>1.å‡†å¤‡æ•°æ®: æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦</h6><pre><code>cat /tmp/ruozedata.txt1   jepson  32  1102   ruoze   22  1123   www.ruozedata.com   18  120</code></pre><h6 id="2-å†…éƒ¨è¡¨æµ‹è¯•ï¼š"><a href="#2-å†…éƒ¨è¡¨æµ‹è¯•ï¼š" class="headerlink" title="2.å†…éƒ¨è¡¨æµ‹è¯•ï¼š"></a>2.å†…éƒ¨è¡¨æµ‹è¯•ï¼š</h6><ol><li><p>åœ¨Hiveé‡Œé¢åˆ›å»ºä¸€ä¸ªè¡¨ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table ruozedata(id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tele string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.759 seconds</span><br></pre></td></tr></table></figure></li><li><p>è¿™æ ·æˆ‘ä»¬å°±åœ¨Hiveé‡Œé¢åˆ›å»ºäº†ä¸€å¼ æ™®é€šçš„è¡¨ï¼Œç°åœ¨ç»™è¿™ä¸ªè¡¨å¯¼å…¥æ•°æ®ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>å†…éƒ¨è¡¨åˆ é™¤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table ruozedata;</span><br></pre></td></tr></table></figure></li></ol><h6 id="3-å¤–éƒ¨è¡¨æµ‹è¯•"><a href="#3-å¤–éƒ¨è¡¨æµ‹è¯•" class="headerlink" title="3.å¤–éƒ¨è¡¨æµ‹è¯•:"></a>3.å¤–éƒ¨è¡¨æµ‹è¯•:</h6><ol><li>åˆ›å»ºå¤–éƒ¨è¡¨å¤šäº†externalå…³é”®å­—è¯´æ˜ä»¥åŠhdfsä¸Šlocation â€˜/hive/externalâ€™<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table exter_ruozedata(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tel string)</span><br><span class="line">    &gt; location &apos;/hive/external&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.098 seconds</span><br></pre></td></tr></table></figure></li></ol><p>åˆ›å»ºå¤–éƒ¨è¡¨ï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨çš„æ—¶å€™åŠ ä¸Šexternalå…³é”®å­—ï¼ŒåŒæ—¶æŒ‡å®šå¤–éƒ¨è¡¨å­˜æ”¾æ•°æ®çš„è·¯å¾„<br>ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä¸æŒ‡å®šå¤–éƒ¨è¡¨çš„å­˜æ”¾è·¯å¾„ï¼Œè¿™æ ·Hiveå°† åœ¨HDFSä¸Šçš„/user/hive/warehouse/æ–‡ä»¶å¤¹ä¸‹ä»¥å¤–éƒ¨è¡¨çš„è¡¨ååˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå¹¶å°†å±äºè¿™ä¸ªè¡¨çš„æ•°æ®å­˜æ”¾åœ¨è¿™é‡Œï¼‰</p><ol start="2"><li><p>å¤–éƒ¨è¡¨å¯¼å…¥æ•°æ®å’Œå†…éƒ¨è¡¨ä¸€æ ·ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>åˆ é™¤å¤–éƒ¨è¡¨</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table exter_ruozedata;</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è°ˆè°ˆæˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜åŠå…¥é—¨</title>
      <link href="/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>&#8195;å½“å¹´æˆ‘æ˜¯åšC#+Javaè½¯ä»¶å¼€å‘ï¼Œç„¶åè€ƒå–OCPæ¥äº†ä¸Šæµ·ï¼Œç«‹å¿—è¦åšä¸€åDBAã€‚åªè®°å¾—å½“å¹´è¯•ç”¨æœŸåˆšè¿‡æ—¶ï¼Œé˜´å·®é˜³é”™è½®åˆ°æˆ‘è´Ÿè´£å…¬å¸çš„å¤§æ•°æ®å¹³å°è¿™å—ï¼Œåˆšå¼€å§‹å¾ˆç—›è‹¦ï¼Œä¸€ä¸ªé™Œç”Ÿçš„è¡Œä¸šï¼Œä¸€ä¸ªè®¨è®ºçš„å°ä¼™ä¼´éƒ½æ²¡æœ‰ï¼Œä¸€ä»½ç°æˆèµ„æ–™éƒ½æ²¡æœ‰ï¼Œå¿ƒæƒ…ç„¦è™‘ã€‚åæ¥æˆ‘è°ƒæ•´å¿ƒæ€ï¼Œä»DBè½¬ç§»åˆ°å¯¹å¤§æ•°æ®çš„ç ”ç©¶ï¼Œå†³å®šå•ƒä¸‹è¿™å—ç¡¬éª¨å¤´ï¼ŒæŠŠå®ƒåš¼ç¢ï¼ŒæŠŠå®ƒæ¶ˆåŒ–å¸æ”¶ã€‚</p><p>&#8195;ç”±äºå½“æ—¶å…¬å¸éƒ½æ˜¯CDHç¯å¢ƒï¼Œåˆšå¼€å§‹å®‰è£…å¡äº†å¾ˆä¹…éƒ½è¿‡ä¸å»ï¼Œåé¢é€‰æ‹©åœ¨çº¿å®‰è£…ï¼Œå¾ˆæ…¢ï¼Œæœ‰æ—¶éœ€è¦1å¤©ã€‚åæ¥å®‰è£…HDFS ,YARN,HIVEç»„ä»¶ï¼Œä¸è¿‡å¯¹å®ƒä»¬ä¸ç†è§£ï¼Œä¸æ˜ç™½ï¼Œæœ‰æ—¶å¾ˆå›°æƒ‘ã€‚è¿™æ ·çš„è¿‡ç¨‹å¤§æ¦‚æŒç»­ä¸‰ä¸ªæœˆäº†ã€‚<br><a id="more"></a><br>&#8195;åæ¥çœ‹äº†å¾ˆå¤šåšæ–‡ï¼Œéƒ½æ˜¯Apache Hadoopç‰ˆæœ¬æ­å»ºï¼Œäºæ˜¯æˆ‘å…ˆè¯•è¯•ç”¨Apache Hadoopæ­å»ºéƒ¨ç½²å•èŠ‚ç‚¹å’Œé›†ç¾¤ï¼Œç„¶åé…ç½®HAï¼Œæœ€åæˆ‘å‘ç°è‡ªå·±æ¯”è¾ƒå–œæ¬¢è¿™ç§æ–¹å¼ï¼Œå› ä¸ºæˆ‘èƒ½äº†è§£å…¶é…ç½®å‚æ•°ï¼Œé…ç½®æ–‡ä»¶å’Œå¸¸è§„å‘½ä»¤ç­‰ç­‰ï¼Œå†å›å¤´å»å¯¹æ¯”CDHå®‰è£…HDFSæœåŠ¡ï¼ŒçœŸæ˜¯å¤ªçˆ½äº†ï¼Œå› ä¸ºApache Hadoopç‰ˆæœ¬æœ‰çœŸæ­£ä½“éªŒæ„Ÿï¼Œè¿™æ—¶æˆ‘å°±è¿…é€Ÿè°ƒæ•´æ–¹å‘ : å…ˆApacheç‰ˆæœ¬ï¼Œå†CDHã€‚</p><p>&#8195;ç”±äºå…¬å¸é¡¹ç›®ç¯å¢ƒï¼Œæ¨è¿›è‡ªå·±å®åœ¨å¤ªæ…¢ï¼Œäºæ˜¯æˆ‘åœ¨ç½‘ä¸Šçœ‹å„ç§ç›¸å…³è§†é¢‘æ•™ç¨‹ï¼›åŠ nç§ç¾¤ï¼Œåœ¨ç¾¤é‡Œæ½œæ°´ï¼Œçœ‹æ°´å‹ä»¬æçš„é—®é¢˜è‡ªå·±ä¼šä¸ä¼šï¼Œä¸ä¼šå°±å»æŸ¥èµ„æ–™ï¼Œä¼šå°±å¸®åŠ©ä»–ä»¬ä¸€èµ·ç ”ç©¶å­¦ä¹ è¿›æ­¥ã€‚</p><p>&#8195;<strong>åæ¥è¿™æ ·çš„è¿›åº¦å¤ªæ…¢äº†</strong>ï¼Œå› ä¸ºå¾ˆå¤šç¾¤éƒ½æ˜¯æ‰“å¹¿å‘Šï¼Œæ½œæ°´ï¼Œæ²¡æœ‰çœŸæ­£çš„æŠ€æœ¯è®¨è®ºæ°›å›´ï¼Œäºæ˜¯æˆ‘è¿…é€Ÿè°ƒæ•´æ–¹å‘ï¼Œè‡ªå·±å»ºä¸ªQQç¾¤ï¼Œæ…¢æ…¢æ‹›å…µä¹°é©¬ï¼Œå’Œç®¡ç†å‘˜ä»¬ä¸€èµ·å»ç®¡ç†ï¼Œåœ¨è¿‡å»çš„ä¸¤å¹´é‡Œæˆ‘ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šçŸ¥è¯†å’Œè®¤è¯†å’Œæˆ‘ä¸€æ ·å‰è¿›çš„å°ä¼™ä¼´ä»¬ï¼Œç°åœ¨ä¹Ÿæœ‰å¾ˆå¤šå·²æˆä¸ºfriendsã€‚</p><p>&#8195;æ¯å½“å¤œæ™šï¼Œæˆ‘å°±ä¼šæ·±æ·±æ€è€ƒä»…å‡­å…¬å¸é¡¹ç›®,ç½‘ä¸Šå…è´¹è¯¾ç¨‹è§†é¢‘ï¼ŒQQç¾¤ç­‰ï¼Œè¿˜æ˜¯ä¸å¤Ÿçš„ï¼Œäºæ˜¯æˆ‘å¼€å§‹å’¨è¯¢åŸ¹è®­æœºæ„çš„è¯¾ç¨‹ï¼Œåœ¨è¿™é‡Œæé†’å„ä½å°ä¼™ä¼´ä»¬ï¼ŒæŠ¥ç­ä¸€å®šè¦æ“¦äº®çœ¼ç›ï¼Œé€‰æ‹©è€å¸ˆå¾ˆé‡è¦ï¼ŒçœŸå¿ƒå¾ˆé‡è¦ï¼Œè®¸å¤šåŸ¹è®­æœºæ„çš„è€å¸ˆéƒ½æ˜¯Javaè½¬çš„ï¼Œè®²çš„æ˜¯å…¨æ˜¯åŸºç¡€ï¼Œæ ¹æœ¬æ²¡æœ‰ä¼ä¸šé¡¹ç›®å®æˆ˜ç»éªŒï¼›è¿˜æœ‰ä¸è¦è·Ÿé£ï¼Œä¸€å®šçœ‹ä»”ç»†çœ‹æ¸…æ¥šè¯¾ç¨‹æ˜¯å¦ç¬¦åˆå½“å‰çš„ä½ ã€‚</p><p>&#8195;è¿™æ—¶è¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œäºæ˜¯æˆ‘å¼€å§‹æ¯å¤©ä¸Šä¸‹ç­åœ°é“ä¸Šçœ‹æŠ€æœ¯åšå®¢ï¼Œç§¯æåˆ†äº«ã€‚<strong>ç„¶åå†ç”³è¯·åšå®¢ï¼Œå†™åšæ–‡ï¼Œå†™æ€»ç»“ï¼ŒåšæŒæ¯æ¬¡åšå®Œä¸€æ¬¡å®éªŒå°±å°†åšæ–‡ï¼Œæ¢³ç†å¥½ï¼Œå†™å¥½ï¼Œè¿™æ ·ä¹…è€Œä¹…ä¹‹ï¼ŒçŸ¥è¯†ç‚¹å°±æ…¢æ…¢å¤¯å®ç§¯ç´¯äº†ã€‚</strong></p><p>&#8195;å†ç€åé¢å°±å¼€å§‹å—é‚€å‡ å¤§åŸ¹è®­æœºæ„åšå…¬å¼€è¯¾ï¼Œå†ä¸€æ¬¡å°†çŸ¥è¯†ç‚¹æ¢³ç†äº†ï¼Œä¹Ÿè®¤è¯†äº†æ–°çš„å°ä¼™ä¼´ä»¬ï¼Œæˆ‘ä»¬æœ‰ç€ç›¸åŒçš„æ–¹å‘å’Œç›®æ ‡ï¼Œæˆ‘ä»¬å°½æƒ…çš„è®¨è®ºç€å¤§æ•°æ®çš„çŸ¥è¯†ç‚¹ï¼Œæ…¢æ…¢æœç€æˆ‘ä»¬å¿ƒç›®ä¸­çš„ç›®æ ‡è€ŒåŠªåŠ›ç€ï¼</p><p><strong>ä»¥ä¸ŠåŸºæœ¬å°±æ˜¯æˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜ï¼Œä¸‹é¢æˆ‘æ¥è°ˆè°ˆæˆ‘å¯¹å¤§æ•°æ®å…¥é—¨çš„æ„Ÿæ‚Ÿã€‚</strong><br><strong>1. å¿ƒæ€è¦ç«¯æ­£ã€‚</strong><br>æ—¢ç„¶æƒ³è¦ä»äº‹è¿™è¡Œï¼Œé‚£ä¹ˆä¸€å®šè¦ä¸‹å®šå†³å¿ƒï¼Œå½“ç„¶ä»˜å‡ºæ˜¯è‚¯å®šå¤§å¤§çš„ï¼Œä¸å…‰å…‰æ˜¯æ¯›çˆ·çˆ·ï¼Œè€Œæ›´å¤šçš„ä»˜å‡ºæ˜¯è‡ªå·±çš„é‚£ä¸€ä»½åšæŒï¼Œå‡¡äº‹è´µåœ¨åšæŒï¼ŒçœŸçœŸä½“ç°åœ¨è¿™é‡Œã€‚<br>åæ¥æˆ‘å°†æˆ‘è€å©†ä»åŒ–å·¥å®éªŒå®¤åˆ†æå‘˜è½¬è¡Œï¼ŒåšPythonçˆ¬è™«å’Œæ•°æ®åˆ†æï¼Œå½“ç„¶è¿™ä¸ªä¸»è¦è¿˜æ˜¯é å¥¹çš„é‚£ä»½åšæŒã€‚</p><p><strong>2. å¿ƒç›®ä¸­è¦æœ‰è®¡åˆ’ã€‚</strong><br>å…ˆå­¦ä¹ Linuxå’ŒShellï¼Œå†å­¦ä¹ æ•°æ®åº“å’ŒSQLï¼Œå†å­¦ä¹ Javaå’ŒScalaï¼Œ<br>ç„¶åå­¦ä¹ Apache Haoopã€Hiveã€Kafkaã€Sparkï¼Œæœå¤§æ•°æ®ç ”å‘æˆ–å¼€å‘è€ŒåŠªåŠ›ç€ã€‚</p><p><strong>3. å„ç§æ–¹å¼å­¦ä¹ ã€‚</strong><br>QQç¾¤ï¼Œåšå®¢ï¼Œä¸Šä¸‹ç­çœ‹æŠ€æœ¯æ–‡ç« ï¼Œé€‰æ‹©å¥½çš„è€å¸ˆå’Œè¯¾ç¨‹åŸ¹è®­ï¼Œ</p><p><font color="#FF4500"><br>(æ“¦äº®çœ¼ç›ï¼Œå¾ˆå¤šè§†é¢‘ï¼Œå¾ˆå¤šå¤§æ•°æ®è€å¸ˆéƒ½æ˜¯çæ‰¯çš„ï¼Œæœ€ç»ˆæ€»ç»“ä¸€å¥è¯ï¼Œä¸åœ¨ä¼ä¸šä¸Šç­çš„æ•™å¤§æ•°æ®éƒ½æ˜¯è€æµæ°“çš„ã€‚)</font><br>å¯ä»¥åŠ é€Ÿè‡ªå·±å‰è¿›çš„é©¬æ‹‰æ¾é‡Œç¨‹ï¼Œå…¶å®ä¸€èˆ¬éƒ½è¦çœ‹å¤§å®¶æ€ä¹ˆè¡¡é‡åŸ¹è®­è¿™ä¸ªäº‹çš„ï¼Œtimeå’Œmoneyçš„æŠ‰æ‹©ï¼Œä»¥åŠå¿«é€Ÿjumpåçš„é«˜è–ªã€‚</p><p><strong>4. é¡¹ç›®ç»éªŒã€‚</strong><br>å¾ˆå¤šå°ç™½éƒ½æ²¡æœ‰é¡¹ç›®ç»éªŒä¹Ÿæ²¡æœ‰é¢è¯•ç»éªŒå’ŒæŠ€å·§ï¼Œå±¡å±¡é¢è¯•ä»¥å¤±è´¥å‘Šç»ˆï¼Œ<br>è¿™æ—¶å¤§å®¶å¯ä»¥æ‰¾ä½ ä»¬ç†Ÿæ‚‰çš„å°ä¼™ä¼´ä»¬çš„ï¼Œè®©ä»–ç»™ä½ åŸ¹è®­ä»–çš„é¡¹ç›®ï¼Œè¿™æ ·å°±æœ‰äº†ï¼Œå½“ç„¶å¯ä»¥ç›´æ¥äº’è”ç½‘æœç´¢ä¸€ä¸ªå°±è¡Œï¼Œä¸è¿‡ä¸€èˆ¬å¾ˆéš¾æœ‰å®Œæ•´çš„ã€‚<br>è€Œé¢è¯•ï¼Œå°±çœ‹çœ‹å…¶ä»–äººé¢è¯•åˆ†äº«ï¼Œå­¦ä¹ ä»–äººã€‚</p><p><strong>æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚<br>æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚<br>æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> æ„Ÿæƒ³ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> äººç”Ÿæ„Ÿæ‚Ÿ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Ÿ</title>
      <link href="/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Œä¸‰ç§æ–¹å¼ï¼<br><a id="more"></a></p><font color="#FF4500"><br></font><h6 id="ä¸€-ä¸´æ—¶å‡½æ•°"><a href="#ä¸€-ä¸´æ—¶å‡½æ•°" class="headerlink" title="ä¸€.ä¸´æ—¶å‡½æ•°"></a>ä¸€.ä¸´æ—¶å‡½æ•°</h6><ol><li>ideaç¼–å†™udf</li><li>æ‰“åŒ…<br>Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Build</li><li>rzä¸Šä¼ è‡³æœåŠ¡å™¨</li><li>æ·»åŠ jaråŒ…<br>hive&gt;add xxx.jar jar_filepath;</li><li>æŸ¥çœ‹jaråŒ…<br>hive&gt;list jars;</li><li>åˆ›å»ºä¸´æ—¶å‡½æ•°<br>hive&gt;create temporary function my_lower as â€˜com.example.hive.udf.Lowerâ€™;</li></ol><h6 id="äºŒ-æŒä¹…å‡½æ•°"><a href="#äºŒ-æŒä¹…å‡½æ•°" class="headerlink" title="äºŒ.æŒä¹…å‡½æ•°"></a>äºŒ.æŒä¹…å‡½æ•°</h6><ol><li>ideaç¼–å†™udf</li><li>æ‰“åŒ…<br>Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Build</li><li>rzä¸Šä¼ è‡³æœåŠ¡å™¨</li><li>ä¸Šä¼ åˆ°HDFS<br>$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jar</li><li>åˆ›å»ºæŒä¹…å‡½æ•°<br>hive&gt;CREATE FUNCTION myfunc AS â€˜myclassâ€™ USING JAR â€˜hdfs:///path/to/xxx.jarâ€™;</li></ol><p><strong>æ³¨æ„ç‚¹ï¼š</strong></p><ul><li><ol><li>æ­¤æ–¹æ³•åœ¨show functionsæ—¶æ˜¯çœ‹ä¸åˆ°çš„ï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨</li></ol></li><li><ol start="2"><li>éœ€è¦ä¸Šä¼ è‡³hdfs</li></ol></li></ul><h6 id="ä¸‰-æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†Œ"><a href="#ä¸‰-æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†Œ" class="headerlink" title="ä¸‰.æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†Œ"></a>ä¸‰.æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†Œ</h6><p>ç¯å¢ƒä»‹ç»ï¼šCentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>ä¸‹è½½æºç <br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a></p></li><li><p>è§£å‹æºç <br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>å°†HelloUDF.javaæ–‡ä»¶å¢åŠ åˆ°HIVEæºç ä¸­<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>ä¿®æ”¹FunctionRegistry.java æ–‡ä»¶</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">åœ¨importä¸­å¢åŠ ï¼šimport org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">åœ¨æ–‡ä»¶å¤´éƒ¨ static å—ä¸­æ·»åŠ ï¼šsystem.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>é‡æ–°ç¼–è¯‘<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>ç¼–è¯‘ç»“æœå…¨éƒ¨ä¸ºï¼šBUILD SUCCESS<br>æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼š/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>é…ç½®hiveç¯å¢ƒ<br>é…ç½®hiveç¯å¢ƒæ—¶ï¼Œå¯ä»¥å…¨æ–°é…ç½®æˆ–å°†ç¼–è¯‘åå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveç¯å¢ƒä¸­ï¼š<br>7.1. å…¨éƒ¨é…ç½®ï¼šå‚ç…§ä¹‹å‰æ–‡æ¡£ <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²</a></p><p>7.2. å°†ç¼–è¯‘åå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveç¯å¢ƒ<br>åˆ°/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libä¸‹ï¼Œæ‰¾åˆ°hive-exec-1.1.0-cdh5.7.0.jaråŒ…ï¼Œå¹¶å°†æ—§ç¯å¢ƒä¸­å¯¹ç…§çš„åŒ…æ›¿æ¢æ‰<br>å‘½ä»¤ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>æœ€ç»ˆå¯åŠ¨hive</p></li><li><p>æµ‹è¯•ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- èƒ½æŸ¥çœ‹åˆ°æœ‰ helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudfå‡½æ•°ç”Ÿæ•ˆ</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„ç¼–ç¨‹å¼€å‘ï¼Œä½ ä¼šå—ï¼Ÿ</title>
      <link href="/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šIntelliJ IDEA+Maven3.3.9<br><a id="more"></a></p><h6 id="1-åˆ›å»ºå·¥ç¨‹"><a href="#1-åˆ›å»ºå·¥ç¨‹" class="headerlink" title="1. åˆ›å»ºå·¥ç¨‹"></a>1. åˆ›å»ºå·¥ç¨‹</h6><p>æ‰“å¼€IntelliJ IDEA<br>Fileâ€“&gt;Newâ€“&gt;Projectâ€¦â€“&gt;Mavené€‰æ‹©Create from archetyeâ€“&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-é…ç½®"><a href="#2-é…ç½®" class="headerlink" title="2. é…ç½®"></a>2. é…ç½®</h6><p>åœ¨å·¥ç¨‹ä¸­æ‰¾åˆ°pom.xmlæ–‡ä»¶ï¼Œæ·»åŠ hadoopã€hiveä¾èµ–<br><img src="/assets/blogImg/425hive1.png" alt="Hiveå›¾1"></p><h6 id="3-åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF-javaï¼Œä»£ç å¦‚ä¸‹ï¼š"><a href="#3-åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF-javaï¼Œä»£ç å¦‚ä¸‹ï¼š" class="headerlink" title="3. åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF.javaï¼Œä»£ç å¦‚ä¸‹ï¼š"></a>3. åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF.javaï¼Œä»£ç å¦‚ä¸‹ï¼š</h6><p><img src="/assets/blogImg/425hive2.png" alt="Hiveå›¾2"></p><p><strong>é¦–å…ˆä¸€ä¸ªUDFå¿…é¡»æ»¡è¶³ä¸‹é¢ä¸¤ä¸ªæ¡ä»¶:</strong></p><ul><li><ol><li>ä¸€ä¸ªUDFå¿…é¡»æ˜¯org.apache.hadoop.hive.ql.exec.UDFçš„å­ç±»ï¼ˆæ¢å¥è¯è¯´å°±æ˜¯æˆ‘ä»¬ä¸€èˆ¬éƒ½æ˜¯å»ç»§æ‰¿è¿™ä¸ªç±»ï¼‰</li></ol></li><li><ol start="2"><li>ä¸€ä¸ªUDFå¿…é¡»è‡³å°‘å®ç°äº†evaluate()æ–¹æ³•</li></ol></li></ul><h6 id="4-æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun-â€˜HelloUDF-main-â€™"><a href="#4-æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun-â€˜HelloUDF-main-â€™" class="headerlink" title="4. æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun â€˜HelloUDF.main()â€™"></a>4. æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun â€˜HelloUDF.main()â€™</h6><h6 id="5-æ‰“åŒ…"><a href="#5-æ‰“åŒ…" class="headerlink" title="5. æ‰“åŒ…"></a>5. æ‰“åŒ…</h6><p>åœ¨IDEAèœå•ä¸­é€‰æ‹©viewâ€“&gt;Tool Windowsâ€“&gt;Maven Projectsï¼Œç„¶ååœ¨Maven Projectsçª—å£ä¸­é€‰æ‹©ã€å·¥ç¨‹åã€‘â€“&gt;Lifecycleâ€“&gt;packageï¼Œåœ¨packageä¸­å³é”®é€‰æ‹©Run Maven Buildå¼€å§‹æ‰“åŒ…<br>æ‰§è¡ŒæˆåŠŸååœ¨æ—¥å¿—ä¸­æ‰¾ï¼š<br><font color="#FF4500">[INFO] Building jar: (è·¯å¾„)/hive-1.0.jar</font></p><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive DDLï¼Œä½ çœŸçš„äº†è§£å—ï¼Ÿ</title>
      <link href="/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ å…¨é¢å‰–æHive DDLï¼</p><font color="#FF4500"><br></font><p><img src="/assets/blogImg/hive424.png" alt="Hiveæ¶æ„å›¾"><br><a id="more"></a></p><h5 id="æ¦‚å¿µ"><a href="#æ¦‚å¿µ" class="headerlink" title="æ¦‚å¿µ"></a>æ¦‚å¿µ</h5><p><strong>Database</strong><br>Hiveä¸­åŒ…å«äº†å¤šä¸ªæ•°æ®åº“ï¼Œé»˜è®¤çš„æ•°æ®åº“ä¸ºdefaultï¼Œå¯¹åº”äºHDFSç›®å½•æ˜¯/user/hadoop/hive/warehouseï¼Œå¯ä»¥é€šè¿‡hive.metastore.warehouse.dirå‚æ•°è¿›è¡Œé…ç½®ï¼ˆhive-site.xmlä¸­é…ç½®ï¼‰</p><p><strong>Table</strong><br>Hiveä¸­çš„è¡¨åˆåˆ†ä¸ºå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨ ,Hive ä¸­çš„æ¯å¼ è¡¨å¯¹åº”äºHDFSä¸Šçš„ä¸€ä¸ªç›®å½•ï¼ŒHDFSç›®å½•ä¸ºï¼š/user/hadoop/hive/warehouse/[databasename.db]/table</p><p><strong>Partition</strong><br>åˆ†åŒºï¼Œæ¯å¼ è¡¨ä¸­å¯ä»¥åŠ å…¥ä¸€ä¸ªåˆ†åŒºæˆ–è€…å¤šä¸ªï¼Œæ–¹ä¾¿æŸ¥è¯¢ï¼Œæé«˜æ•ˆç‡ï¼›å¹¶ä¸”HDFSä¸Šä¼šæœ‰å¯¹åº”çš„åˆ†åŒºç›®å½•ï¼š<br>/user/hadoop/hive/warehouse/[databasename.db]/table</p><h5 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h5><p><strong>Create Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><p></p><p>IF NOT EXISTSï¼šåŠ ä¸Šè¿™å¥è¯ä»£è¡¨åˆ¤æ–­æ•°æ®åº“æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±ä¼šåˆ›å»ºï¼Œå­˜åœ¨å°±ä¸ä¼šåˆ›å»ºã€‚<br>COMMENTï¼šæ•°æ®åº“çš„æè¿°<br>LOCATIONï¼šåˆ›å»ºæ•°æ®åº“çš„åœ°å€ï¼Œä¸åŠ é»˜è®¤åœ¨/user/hive/warehouse/è·¯å¾„ä¸‹<br>WITH DBPROPERTIESï¼šæ•°æ®åº“çš„å±æ€§</p><p><strong>Drop Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name </span><br><span class="line">[RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><p></p><p>RESTRICTï¼šé»˜è®¤æ˜¯restrictï¼Œå¦‚æœè¯¥æ•°æ®åº“è¿˜æœ‰è¡¨å­˜åœ¨åˆ™æŠ¥é”™ï¼›<br>CASCADEï¼šçº§è”åˆ é™¤æ•°æ®åº“(å½“æ•°æ®åº“è¿˜æœ‰è¡¨æ—¶ï¼Œçº§è”åˆ é™¤è¡¨ååœ¨åˆ é™¤æ•°æ®åº“)ã€‚</p><p><strong>Alter Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>Use Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br></pre></td></tr></table></figure><p></p><p><strong>Show Databases</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;</span><br><span class="line">â€œ | â€ï¼šå¯ä»¥é€‰æ‹©å…¶ä¸­ä¸€ç§</span><br><span class="line"></span><br><span class="line">â€œ[ ]â€ï¼šå¯é€‰é¡¹</span><br><span class="line"></span><br><span class="line">LIKE â€˜identifier_with_wildcardsâ€™ï¼šæ¨¡ç³ŠæŸ¥è¯¢æ•°æ®åº“</span><br></pre></td></tr></table></figure><p></p><p><strong>Describe Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE DATABASE [EXTENDED] db_name;</span><br><span class="line">DESCRIBE DATABASE db_nameï¼šæŸ¥çœ‹æ•°æ®åº“çš„æè¿°ä¿¡æ¯å’Œæ–‡ä»¶ç›®å½•ä½ç½®è·¯å¾„ä¿¡æ¯ï¼›</span><br><span class="line">EXTENDEDï¼šåŠ ä¸Šæ•°æ®åº“é”®å€¼å¯¹çš„å±æ€§ä¿¡æ¯ã€‚</span><br><span class="line">hive&gt; describe database default;</span><br><span class="line">OK</span><br><span class="line">default    Default Hive database    hdfs://hadoop1:9000/user/hive/warehouse    public    ROLE    </span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line"></span><br><span class="line">hive&gt; describe database extended hive2;</span><br><span class="line">OK</span><br><span class="line">hive2   it is my database       hdfs://hadoop1:9000/user/hive/warehouse/hive2.db        hadoop      USER    &#123;date=2018-08-08, creator=zhangsan&#125;</span><br><span class="line">Time taken: 0.135 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p></p><p><strong>Create Table</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line"> ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line"> [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line"> | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"></span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br></pre></td></tr></table></figure><p></p><p><strong>data_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">: primitive_type</span><br><span class="line">| array_type</span><br><span class="line">| map_type</span><br><span class="line">| struct_type</span><br><span class="line">| union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>primitive_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> : TINYINT</span><br><span class="line"> | SMALLINT</span><br><span class="line"> | INT</span><br><span class="line"> | BIGINT</span><br><span class="line"> | BOOLEAN</span><br><span class="line">| FLOAT</span><br><span class="line"> | DOUBLE</span><br><span class="line"> | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line"> | STRING</span><br><span class="line"> | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line"> | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>array_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: ARRAY &lt; data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>map_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: MAP &lt; primitive_type, data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>struct_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>union_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note:     Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>row_format</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p><strong>file_format:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p></p><p><strong>constraint_specification:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">TEMPORARYï¼ˆä¸´æ—¶è¡¨ï¼‰</span><br><span class="line">Hiveä»0.14.0å¼€å§‹æä¾›åˆ›å»ºä¸´æ—¶è¡¨çš„åŠŸèƒ½ï¼Œè¡¨åªå¯¹å½“å‰sessionæœ‰æ•ˆï¼Œsessioné€€å‡ºåï¼Œè¡¨è‡ªåŠ¨åˆ é™¤ã€‚</span><br><span class="line">è¯­æ³•ï¼šCREATE TEMPORARY TABLE â€¦</span><br></pre></td></tr></table></figure><p></p><h6 id="æ³¨æ„ï¼š"><a href="#æ³¨æ„ï¼š" class="headerlink" title="æ³¨æ„ï¼š"></a><strong>æ³¨æ„ï¼š</strong></h6><ol><li>å¦‚æœåˆ›å»ºçš„ä¸´æ—¶è¡¨è¡¨åå·²å­˜åœ¨ï¼Œé‚£ä¹ˆå½“å‰sessionå¼•ç”¨åˆ°è¯¥è¡¨åæ—¶å®é™…ç”¨çš„æ˜¯ä¸´æ—¶è¡¨ï¼Œåªæœ‰dropæˆ–renameä¸´æ—¶è¡¨åæ‰èƒ½ä½¿ç”¨åŸå§‹è¡¨</li><li>ä¸´æ—¶è¡¨é™åˆ¶ï¼šä¸æ”¯æŒåˆ†åŒºå­—æ®µå’Œåˆ›å»ºç´¢å¼•</li></ol><p>EXTERNALï¼ˆå¤–éƒ¨è¡¨ï¼‰<br>Hiveä¸Šæœ‰ä¸¤ç§ç±»å‹çš„è¡¨ï¼Œä¸€ç§æ˜¯Managed Table(é»˜è®¤çš„)ï¼Œå¦ä¸€ç§æ˜¯External Tableï¼ˆåŠ ä¸ŠEXTERNALå…³é”®å­—ï¼‰ã€‚å®ƒä¿©çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼šå½“æˆ‘ä»¬dropè¡¨æ—¶ï¼ŒManaged Tableä¼šåŒæ—¶åˆ å»dataï¼ˆå­˜å‚¨åœ¨HDFSä¸Šï¼‰å’Œmeta dataï¼ˆå­˜å‚¨åœ¨MySQLï¼‰ï¼Œè€ŒExternal Tableåªä¼šåˆ meta dataã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table external_table(</span><br><span class="line">  &gt; id int,</span><br><span class="line">&gt;  name string </span><br><span class="line">&gt; );</span><br></pre></td></tr></table></figure><p></p><p>PARTITIONED BYï¼ˆåˆ†åŒºè¡¨ï¼‰<br>äº§ç”ŸèƒŒæ™¯ï¼šå¦‚æœä¸€ä¸ªè¡¨ä¸­æ•°æ®å¾ˆå¤šï¼Œæˆ‘ä»¬æŸ¥è¯¢æ—¶å°±å¾ˆæ…¢ï¼Œè€—è´¹å¤§é‡æ—¶é—´ï¼Œå¦‚æœè¦æŸ¥è¯¢å…¶ä¸­éƒ¨åˆ†æ•°æ®è¯¥æ€ä¹ˆåŠå‘¢ï¼Œè¿™æ˜¯æˆ‘ä»¬å¼•å…¥åˆ†åŒºçš„æ¦‚å¿µã€‚</p><p>å¯ä»¥æ ¹æ®PARTITIONED BYåˆ›å»ºåˆ†åŒºè¡¨ï¼Œä¸€ä¸ªè¡¨å¯ä»¥æ‹¥æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºä»¥æ–‡ä»¶å¤¹çš„å½¢å¼å•ç‹¬å­˜åœ¨è¡¨æ–‡ä»¶å¤¹çš„ç›®å½•ä¸‹ï¼›</p><p>åˆ†åŒºæ˜¯ä»¥å­—æ®µçš„å½¢å¼åœ¨è¡¨ç»“æ„ä¸­å­˜åœ¨ï¼Œé€šè¿‡describe tableå‘½ä»¤å¯ä»¥æŸ¥çœ‹åˆ°å­—æ®µå­˜åœ¨ï¼Œä½†æ˜¯è¯¥å­—æ®µä¸å­˜æ”¾å®é™…çš„æ•°æ®å†…å®¹ï¼Œä»…ä»…æ˜¯åˆ†åŒºçš„è¡¨ç¤ºã€‚</p><p>åˆ†åŒºå»ºè¡¨åˆ†ä¸º2ç§ï¼Œä¸€ç§æ˜¯å•åˆ†åŒºï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨è¡¨æ–‡ä»¶å¤¹ç›®å½•ä¸‹åªæœ‰ä¸€çº§æ–‡ä»¶å¤¹ç›®å½•ã€‚å¦å¤–ä¸€ç§æ˜¯å¤šåˆ†åŒºï¼Œè¡¨æ–‡ä»¶å¤¹ä¸‹å‡ºç°å¤šæ–‡ä»¶å¤¹åµŒå¥—æ¨¡å¼ã€‚</p><p>å•åˆ†åŒºï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE order_partition (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">    &gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (event_month string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>å¤šåˆ†åŒºï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE TABLE order_partition2 (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">&gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt;  PARTITIONED BY (event_month string,every_day string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db</span><br><span class="line">18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2</span><br><span class="line">[hadoop@hadoop000 ~]$</span><br><span class="line">ROW FORMAT</span><br></pre></td></tr></table></figure><p>å®˜ç½‘è§£é‡Šï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char [ESCAPED BY char]]       [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   </span><br><span class="line">-- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p>DELIMITEDï¼šåˆ†éš”ç¬¦ï¼ˆå¯ä»¥è‡ªå®šä¹‰åˆ†éš”ç¬¦ï¼‰ï¼›</p><p>FIELDS TERMINATED BY char:æ¯ä¸ªå­—æ®µä¹‹é—´ä½¿ç”¨çš„åˆ†å‰²ï¼›</p><p>ä¾‹ï¼š-FIELDS TERMINATED BY â€˜\nâ€™ å­—æ®µä¹‹é—´çš„åˆ†éš”ç¬¦ä¸º\n;</p><p>COLLECTION ITEMS TERMINATED BY char:é›†åˆä¸­å…ƒç´ ä¸å…ƒç´ ï¼ˆarrayï¼‰ä¹‹é—´ä½¿ç”¨çš„åˆ†éš”ç¬¦ï¼ˆcollectionå•ä¾‹é›†åˆçš„è·Ÿæ¥å£ï¼‰ï¼›</p><p>MAP KEYS TERMINATED BY charï¼šå­—æ®µæ˜¯K-Vå½¢å¼æŒ‡å®šçš„åˆ†éš”ç¬¦ï¼›</p><p>LINES TERMINATED BY charï¼šæ¯æ¡æ•°æ®ä¹‹é—´ç”±æ¢è¡Œç¬¦åˆ†å‰²ï¼ˆé»˜è®¤[ \n ]ï¼‰</p><p>ä¸€èˆ¬æƒ…å†µä¸‹LINES TERMINATED BY charæˆ‘ä»¬å°±ä½¿ç”¨é»˜è®¤çš„æ¢è¡Œç¬¦\nï¼Œåªéœ€è¦æŒ‡å®šFIELDS TERMINATED BY charã€‚</p><p>åˆ›å»ºdemo1è¡¨ï¼Œå­—æ®µä¸å­—æ®µä¹‹é—´ä½¿ç”¨\tåˆ†å¼€ï¼Œæ¢è¡Œç¬¦ä½¿ç”¨é»˜è®¤\nï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo1(</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>åˆ›å»ºdemo2è¡¨ï¼Œå¹¶æŒ‡å®šå…¶ä»–å­—æ®µï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo2 (</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string,</span><br><span class="line">&gt; hobbies ARRAY &lt;string&gt;,</span><br><span class="line">&gt; address MAP &lt;string, string&gt;</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;</span><br><span class="line">&gt; MAP KEYS TERMINATED BY &apos;:&apos;;</span><br><span class="line">OK</span><br><span class="line">STORED ASï¼ˆå­˜å‚¨æ ¼å¼ï¼‰</span><br><span class="line">Create Table As Select</span><br></pre></td></tr></table></figure><p></p><p>åˆ›å»ºè¡¨ï¼ˆæ‹·è´è¡¨ç»“æ„åŠæ•°æ®ï¼Œå¹¶ä¸”ä¼šè¿è¡ŒMapReduceä½œä¸šï¼‰<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp (</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">salary double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure><p></p><p>åŠ è½½æ•°æ®<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;</span><br></pre></td></tr></table></figure><p></p><p>å¤åˆ¶æ•´å¼ è¡¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp2 as select * from emp;</span><br><span class="line">Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/</span><br><span class="line">Kill Command = /opt/software/hadoop/bin/hadoop job  -kill job_1514116522188_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2018-01-08 05:21:07,707 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2018-01-08 05:21:19,605 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 810 msec</span><br><span class="line">Ended Job = job_1514116522188_0003</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2</span><br><span class="line">Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 3927 HDFS Write: 730 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 810 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 33.322 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp2</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.071 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>å¤åˆ¶è¡¨ä¸­çš„ä¸€äº›å­—æ®µ<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table emp3 as select empno,ename from emp;</span><br></pre></td></tr></table></figure><p></p><p>LIKE<br>ä½¿ç”¨likeåˆ›å»ºè¡¨æ—¶ï¼Œåªä¼šå¤åˆ¶è¡¨çš„ç»“æ„ï¼Œä¸ä¼šå¤åˆ¶è¡¨çš„æ•°æ®<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp4 like emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.149 seconds</span><br><span class="line">hive&gt; select * from emp4;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.151 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>å¹¶æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®</p><p>desc formatted table_name<br>æŸ¥è¯¢è¡¨çš„è¯¦ç»†ä¿¡æ¯<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted emp;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>col_name data_type comment<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">empno                   int                                         </span><br><span class="line">ename                   string                                      </span><br><span class="line">job                     string                                      </span><br><span class="line">mgr                     int                                         </span><br><span class="line">hiredate                string                                      </span><br><span class="line">salary                  double                                      </span><br><span class="line">comm                    double                                      </span><br><span class="line">deptno                  int</span><br></pre></td></tr></table></figure><p></p><p>Detailed Table Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Database:               hive                     </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Mon Jan 08 05:17:54 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Protect Mode:           None                     </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">COLUMN_STATS_ACCURATE    true                </span><br><span class="line">numFiles                1                   </span><br><span class="line">numRows                 0                   </span><br><span class="line">rawDataSize             0                   </span><br><span class="line">totalSize               668                 </span><br><span class="line">transient_lastDdlTime    1515359982</span><br></pre></td></tr></table></figure><p></p><p>Storage Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">field.delim             \t                  </span><br><span class="line">serialization.format    \t                  </span><br><span class="line">Time taken: 0.228 seconds, Fetched: 39 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>é€šè¿‡æŸ¥è¯¢å¯ä»¥åˆ—å‡ºåˆ›å»ºè¡¨æ—¶çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥åœ¨mysqlä¸­æŸ¥è¯¢å‡ºè¿™äº›ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰select * from table_params;</p><p>æŸ¥è¯¢æ•°æ®åº“ä¸‹çš„æ‰€æœ‰è¡¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp1</span><br><span class="line">emp2</span><br><span class="line">emp3</span><br><span class="line">emp4</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.047 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>æŸ¥è¯¢åˆ›å»ºè¡¨çš„è¯­æ³•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show create table emp;</span><br><span class="line">OK</span><br><span class="line">CREATE TABLE `emp`(</span><br><span class="line">  `empno` int, </span><br><span class="line">  `ename` string, </span><br><span class="line">  `job` string, </span><br><span class="line">  `mgr` int, </span><br><span class="line">  `hiredate` string, </span><br><span class="line">  `salary` double, </span><br><span class="line">  `comm` double, </span><br><span class="line">  `deptno` int)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;\t&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;numRows&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;rawDataSize&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;668&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)</span><br><span class="line">Time taken: 0.192 seconds, Fetched: 24 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">Drop Table</span><br><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];     -- (Note: PURGE available in Hive 0.14.0 and later)</span><br></pre></td></tr></table></figure><p></p><p>æŒ‡å®šPURGEåï¼Œæ•°æ®ä¸ä¼šæ”¾åˆ°å›æ”¶ç®±ï¼Œä¼šç›´æ¥åˆ é™¤</p><p>DROP TABLEåˆ é™¤æ­¤è¡¨çš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚å¦‚æœé…ç½®äº†åƒåœ¾ç®±ï¼ˆå¹¶ä¸”æœªæŒ‡å®šPURGEï¼‰ï¼Œåˆ™å®é™…å°†æ•°æ®ç§»è‡³.Trash / Currentç›®å½•ã€‚å…ƒæ•°æ®å®Œå…¨ä¸¢å¤±</p><p>åˆ é™¤EXTERNALè¡¨æ—¶ï¼Œè¡¨ä¸­çš„æ•°æ®ä¸ä¼šä»æ–‡ä»¶ç³»ç»Ÿä¸­åˆ é™¤<br>Alter Table</p><p>é‡å‘½å<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table demo2 rename to new_demo2;</span><br><span class="line">OK</span><br><span class="line">Add Partitions</span><br><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line">  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)</span><br><span class="line">ç”¨æˆ·å¯ä»¥ç”¨ ALTER TABLE ADD PARTITION æ¥å‘ä¸€ä¸ªè¡¨ä¸­å¢åŠ åˆ†åŒºã€‚åˆ†åŒºåæ˜¯å­—ç¬¦ä¸²æ—¶åŠ å¼•å·ã€‚</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">æ³¨ï¼šæ·»åŠ åˆ†åŒºæ—¶å¯èƒ½å‡ºç°FAILED: SemanticException table is not partitioned but partition spec existsé”™è¯¯ã€‚</span><br><span class="line">åŸå› æ˜¯ï¼Œä½ åœ¨åˆ›å»ºè¡¨æ—¶å¹¶æ²¡æœ‰æ·»åŠ åˆ†åŒºï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨æ—¶åˆ›å»ºåˆ†åŒºï¼Œå†æ·»åŠ åˆ†åŒºã€‚</span><br><span class="line"></span><br><span class="line">hive&gt;  create table dept(</span><br><span class="line">&gt;  deptno int,</span><br><span class="line">&gt; dname string,</span><br><span class="line">&gt; loc string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (dt string)</span><br><span class="line">&gt;  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.953 seconds </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);</span><br><span class="line">Loading data to table default.dept partition (dt=2018-08-08)</span><br><span class="line">Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.147 seconds</span><br></pre></td></tr></table></figure><p></p><p>æŸ¥è¯¢ç»“æœ<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 0.481 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);</span><br><span class="line">OK</span><br><span class="line">Drop Partitions</span><br><span class="line">ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);</span><br></pre></td></tr></table></figure><p></p><p>æŸ¥çœ‹åˆ†åŒºè¯­å¥<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept;</span><br><span class="line">OK</span><br><span class="line">dt=2018-08-08</span><br><span class="line">dt=2018-09-09</span><br><span class="line">Time taken: 0.385 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><p></p><p>æŒ‰åˆ†åŒºæŸ¥è¯¢<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 2.323 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveç”Ÿäº§ä¸Šï¼Œå‹ç¼©å’Œå­˜å‚¨ç»“åˆä½¿ç”¨æ¡ˆä¾‹</title>
      <link href="/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>ä½ ä»¬Hiveç”Ÿäº§ä¸Šï¼Œå‹ç¼©å’Œå­˜å‚¨ï¼Œç»“åˆä½¿ç”¨äº†å—ï¼Ÿ</p><p>æ¡ˆä¾‹ï¼š<br>åŸæ–‡ä»¶å¤§å°ï¼š19M<br><img src="/assets/blogImg/423_1.png" alt="enter description here"><br><a id="more"></a></p><h6 id="1-ORC-Zlipç»“åˆ"><a href="#1-ORC-Zlipç»“åˆ" class="headerlink" title="1. ORC+Zlipç»“åˆ"></a>1. ORC+Zlipç»“åˆ</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc_zlib</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><font color="#FF4500">ç”¨ORC+Zlipä¹‹åçš„æ–‡ä»¶ä¸º2.8M<br><br></font><br>ç”¨ORC+Zlipä¹‹åçš„æ–‡ä»¶ä¸º2.8M<br><img src="/assets/blogImg/423_2.png" alt="enter description here"><br><br><br>###### 2. Parquet+gzipç»“åˆ<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       set parquet.compression=gzip;</span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><br><br><font color="#FF4500"><br>ç”¨Parquet+gzipä¹‹åçš„æ–‡ä»¶ä¸º3.9M<br></font><p><img src="/assets/blogImg/423_3.png" alt="enter description here"></p><h6 id="3-Parquet-Lzoç»“åˆ"><a href="#3-Parquet-Lzoç»“åˆ" class="headerlink" title="3. Parquet+Lzoç»“åˆ"></a>3. Parquet+Lzoç»“åˆ</h6><p><strong>3.1 å®‰è£…Lzo</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gz</span><br><span class="line">tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">cd lzo-2.06</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzo/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib/</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib64/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export PATH=/usr/local//hadoop/lzo/:$PATH</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.2 å®‰è£…Lzop</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.lzop.org/download/lzop-1.03.tar.gz</span><br><span class="line">tar -zxvf lzop-1.03.tar.gz</span><br><span class="line">cd lzop-1.03</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzop</span><br><span class="line">make  &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.3 è½¯è¿æ¥</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4 æµ‹è¯•lzop</strong><br>lzop xxx.log<br>è‹¥ç”Ÿæˆxxx.log.lzoæ–‡ä»¶ï¼Œåˆ™è¯´æ˜æˆåŠŸ<br><strong>3.5 å®‰è£…Hadoop-LZO</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   gitæˆ–svn ä¸‹è½½https://github.com/twitter/hadoop-lzo</span><br><span class="line">cd hadoop-lzo</span><br><span class="line">mvn clean package -Dmaven.test.skip=true </span><br><span class="line">tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/</span><br></pre></td></tr></table></figure><p></p><p><strong>3.6 é…ç½®</strong><br>åœ¨core-site.xmlé…ç½®<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">     org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">           &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">åœ¨mapred-site.xmlä¸­é…ç½®</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">           &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">åœ¨hadoop-env.shä¸­é…ç½®</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure><p></p><p><strong>3.7 æµ‹è¯•</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.compress.output=true;  </span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line">SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line">create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">TBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p></p><p><font color="#FF4500">ç”¨Parquet+Lzo(æœªå»ºç«‹ç´¢å¼•)ä¹‹åçš„æ–‡ä»¶ä¸º5.9M<br></font><br><img src="/assets/blogImg/423_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> æ¡ˆä¾‹ </tag>
            
            <tag> å‹ç¼©æ ¼å¼ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>åˆåˆåˆæ˜¯æºç ï¼RDD ä½œä¸šçš„DAGæ˜¯å¦‚ä½•åˆ‡åˆ†çš„ï¼Ÿ</title>
      <link href="/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/"/>
      <url>/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --><p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRDDå­˜åœ¨ç€ä¾èµ–å…³ç³»ï¼Œè¿™äº›ä¾èµ–å…³ç³»å½¢æˆäº†æœ‰å‘æ— ç¯å›¾DAGï¼ŒDAGé€šè¿‡DAGSchedulerè¿›è¡ŒStageçš„åˆ’åˆ†ï¼Œå¹¶åŸºäºæ¯ä¸ªStageç”Ÿæˆäº†TaskSetï¼Œæäº¤ç»™TaskSchedulerã€‚é‚£ä¹ˆè¿™æ•´ä¸ªè¿‡ç¨‹åœ¨æºç ä¸­æ˜¯å¦‚ä½•ä½“ç°çš„å‘¢ï¼Ÿ<br><a id="more"></a></p><h4 id="1-ä½œä¸šçš„æäº¤"><a href="#1-ä½œä¸šçš„æäº¤" class="headerlink" title="1.ä½œä¸šçš„æäº¤"></a>1.ä½œä¸šçš„æäº¤</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// SparkContext.scala</span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// DAGScheduler.scala</span><br><span class="line">   def runJob[T, U](</span><br><span class="line">    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure><p>å¯ä»¥çœ‹åˆ°ï¼ŒSparkContextçš„runjobæ–¹æ³•è°ƒç”¨äº†DAGSchedulerçš„runjobæ–¹æ³•æ­£å¼å‘é›†ç¾¤æäº¤ä»»åŠ¡ï¼Œæœ€ç»ˆè°ƒç”¨äº†submitJobæ–¹æ³•ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2   def submitJob[T, U](</span><br><span class="line"> 3      rdd: RDD[T],</span><br><span class="line"> 4      func: (TaskContext, Iterator[T]) =&gt; U,</span><br><span class="line"> 5      partitions: Seq[Int],</span><br><span class="line"> 6      callSite: CallSite,</span><br><span class="line"> 7      resultHandler: (Int, U) =&gt; Unit,</span><br><span class="line"> 8      properties: Properties): JobWaiter[U] = &#123;</span><br><span class="line"> 9    // Check to make sure we are not launching a task on a partition that does not exist.</span><br><span class="line">10    val maxPartitions = rdd.partitions.length</span><br><span class="line">11    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;</span><br><span class="line">12      throw new IllegalArgumentException(</span><br><span class="line">13        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +</span><br><span class="line">14          &quot;Total number of partitions: &quot; + maxPartitions)</span><br><span class="line">15    &#125;</span><br><span class="line">16</span><br><span class="line">17    val jobId = nextJobId.getAndIncrement()</span><br><span class="line">18    if (partitions.size == 0) &#123;</span><br><span class="line">19      // Return immediately if the job is running 0 tasks</span><br><span class="line">20      return new JobWaiter[U](this, jobId, 0, resultHandler)</span><br><span class="line">21    &#125;</span><br><span class="line">22</span><br><span class="line">23    assert(partitions.size &gt; 0)</span><br><span class="line">24    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</span><br><span class="line">25    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</span><br><span class="line">26    //ç»™eventProcessLoopå‘é€JobSubmittedæ¶ˆæ¯</span><br><span class="line">27    eventProcessLoop.post(JobSubmitted(</span><br><span class="line">28      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">29      SerializationUtils.clone(properties)))</span><br><span class="line">30    waiter</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><p>è¿™é‡Œå‘eventProcessLoopå¯¹è±¡å‘é€äº†JobSubmittedæ¶ˆæ¯ã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">1// DAGScheduler.scala</span><br><span class="line">2   private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)</span><br><span class="line">    eventProcessLoopæ˜¯DAGSchedulerEventProcessLoopç±»çš„ä¸€ä¸ªå¯¹è±¡ã€‚</span><br><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123;</span><br><span class="line"> 3    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line"> 4      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"> 5</span><br><span class="line"> 6    case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line"> 7      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"> 8</span><br><span class="line"> 9    case StageCancelled(stageId) =&gt;</span><br><span class="line">10      dagScheduler.handleStageCancellation(stageId)</span><br><span class="line">11</span><br><span class="line">12    case JobCancelled(jobId) =&gt;</span><br><span class="line">13      dagScheduler.handleJobCancellation(jobId)</span><br><span class="line">14</span><br><span class="line">15    case JobGroupCancelled(groupId) =&gt;</span><br><span class="line">16      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line">17</span><br><span class="line">18    case AllJobsCancelled =&gt;</span><br><span class="line">19      dagScheduler.doCancelAllJobs()</span><br><span class="line">20</span><br><span class="line">21    case ExecutorAdded(execId, host) =&gt;</span><br><span class="line">22      dagScheduler.handleExecutorAdded(execId, host)</span><br><span class="line">23</span><br><span class="line">24    case ExecutorLost(execId, reason) =&gt;</span><br><span class="line">25      val filesLost = reason match &#123;</span><br><span class="line">26        case SlaveLost(_, true) =&gt; true</span><br><span class="line">27        case _ =&gt; false</span><br><span class="line">28      &#125;</span><br><span class="line">29      dagScheduler.handleExecutorLost(execId, filesLost)</span><br><span class="line">30</span><br><span class="line">31    case BeginEvent(task, taskInfo) =&gt;</span><br><span class="line">32      dagScheduler.handleBeginEvent(task, taskInfo)</span><br><span class="line">33</span><br><span class="line">34    case GettingResultEvent(taskInfo) =&gt;</span><br><span class="line">35      dagScheduler.handleGetTaskResult(taskInfo)</span><br><span class="line">36</span><br><span class="line">37    case completion: CompletionEvent =&gt;</span><br><span class="line">38      dagScheduler.handleTaskCompletion(completion)</span><br><span class="line">39</span><br><span class="line">40    case TaskSetFailed(taskSet, reason, exception) =&gt;</span><br><span class="line">41      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span><br><span class="line">42</span><br><span class="line">43    case ResubmitFailedStages =&gt;</span><br><span class="line">44      dagScheduler.resubmitFailedStages()</span><br><span class="line">45  &#125;</span><br></pre></td></tr></table></figure><p></p><p>DAGSchedulerEventProcessLoopå¯¹æ¥æ”¶åˆ°çš„æ¶ˆæ¯è¿›è¡Œå¤„ç†ï¼Œåœ¨doOnReceiveæ–¹æ³•ä¸­å½¢æˆä¸€ä¸ªevent loopã€‚<br>æ¥ä¸‹æ¥å°†è°ƒç”¨submitStage()æ–¹æ³•è¿›è¡Œstageçš„åˆ’åˆ†ã€‚</p><h4 id="2-stageçš„åˆ’åˆ†"><a href="#2-stageçš„åˆ’åˆ†" class="headerlink" title="2.stageçš„åˆ’åˆ†"></a>2.stageçš„åˆ’åˆ†</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2 private def submitStage(stage: Stage) &#123;</span><br><span class="line"> 3    val jobId = activeJobForStage(stage)//æŸ¥æ‰¾è¯¥Stageçš„æ‰€æœ‰æ¿€æ´»çš„job</span><br><span class="line"> 4    if (jobId.isDefined) &#123;</span><br><span class="line"> 5      logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)</span><br><span class="line"> 6      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line"> 7        val missing = getMissingParentStages(stage).sortBy(_.id)//å¾—åˆ°Stageçš„çˆ¶Stageï¼Œå¹¶æ’åº</span><br><span class="line"> 8        logDebug(&quot;missing: &quot; + missing)</span><br><span class="line"> 9        if (missing.isEmpty) &#123;</span><br><span class="line">10          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)</span><br><span class="line">11          submitMissingTasks(stage, jobId.get)//å¦‚æœStageæ²¡æœ‰çˆ¶Stageï¼Œåˆ™æäº¤ä»»åŠ¡é›†</span><br><span class="line">12        &#125; else &#123;</span><br><span class="line">13          for (parent &lt;- missing) &#123;//å¦‚æœæœ‰çˆ¶Stageï¼Œé€’å½’è°ƒç”¨submiStage</span><br><span class="line">14            submitStage(parent)</span><br><span class="line">15          &#125;</span><br><span class="line">16          waitingStages += stage//å°†å…¶æ ‡è®°ä¸ºç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…ä¸‹æ¬¡æäº¤</span><br><span class="line">17        &#125;</span><br><span class="line">18      &#125;</span><br><span class="line">19    &#125; else &#123;</span><br><span class="line">20      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//å¦‚æœè¯¥Stageæ²¡æœ‰æ¿€æ´»çš„jobï¼Œåˆ™ä¸¢å¼ƒè¯¥Stage</span><br><span class="line">21    &#125;</span><br><span class="line">22  &#125;</span><br></pre></td></tr></table></figure><p>åœ¨submitStageæ–¹æ³•ä¸­åˆ¤æ–­Stageçš„çˆ¶Stageæœ‰æ²¡æœ‰è¢«æäº¤ï¼Œç›´åˆ°æ‰€æœ‰çˆ¶Stageéƒ½è¢«æäº¤ï¼Œåªæœ‰ç­‰çˆ¶Stageå®Œæˆåæ‰èƒ½è°ƒåº¦å­Stageã€‚<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123;</span><br><span class="line"> 3    val missing = new HashSet[Stage] //ç”¨äºå­˜æ”¾çˆ¶Stage</span><br><span class="line"> 4    val visited = new HashSet[RDD[_]] //ç”¨äºå­˜æ”¾å·²è®¿é—®è¿‡çš„RDD</span><br><span class="line"> 5</span><br><span class="line"> 6    val waitingForVisit = new Stack[RDD[_]]</span><br><span class="line"> 7    def visit(rdd: RDD[_]) &#123;</span><br><span class="line"> 8      if (!visited(rdd)) &#123; //å¦‚æœRDDæ²¡æœ‰è¢«è®¿é—®è¿‡ï¼Œåˆ™è¿›è¡Œè®¿é—®</span><br><span class="line"> 9        visited += rdd //æ·»åŠ åˆ°å·²è®¿é—®RDDçš„HashSetä¸­</span><br><span class="line">10        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)</span><br><span class="line">11        if (rddHasUncachedPartitions) &#123;</span><br><span class="line">12          for (dep &lt;- rdd.dependencies) &#123; //è·å–è¯¥RDDçš„ä¾èµ–</span><br><span class="line">13            dep match &#123;</span><br><span class="line">14              case shufDep: ShuffleDependency[_, _, _] =&gt;//è‹¥ä¸ºå®½ä¾èµ–ï¼Œåˆ™è¯¥RDDä¾èµ–çš„RDDæ‰€åœ¨çš„stageä¸ºçˆ¶stage</span><br><span class="line">15                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//ç”Ÿæˆçˆ¶Stage</span><br><span class="line">16                if (!mapStage.isAvailable) &#123;//è‹¥çˆ¶Stageä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ åˆ°çˆ¶Stageçš„HashSETä¸­</span><br><span class="line">17                  missing += mapStage</span><br><span class="line">18                &#125;</span><br><span class="line">19              case narrowDep: NarrowDependency[_] =&gt;//è‹¥ä¸ºçª„ä¾èµ–ï¼Œåˆ™ç»§ç»­è®¿é—®çˆ¶RDD</span><br><span class="line">20                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">21            &#125;</span><br><span class="line">22          &#125;</span><br><span class="line">23        &#125;</span><br><span class="line">24      &#125;</span><br><span class="line">25    &#125;</span><br><span class="line">26    waitingForVisit.push(stage.rdd)</span><br><span class="line">27    while (waitingForVisit.nonEmpty) &#123;//å¾ªç¯éå†æ‰€æœ‰RDD</span><br><span class="line">28      visit(waitingForVisit.pop())</span><br><span class="line">29    &#125;</span><br><span class="line">30    missing.toList</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="getmissingParentStages-æ–¹æ³•ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚"><a href="#getmissingParentStages-æ–¹æ³•ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚" class="headerlink" title="getmissingParentStages()æ–¹æ³•ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚"></a>getmissingParentStages()æ–¹æ³•ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚</h4><font color="#FF4500"><br><br>è¿™é‡Œæˆ‘ä»¬è¦æ‡‚å¾—è¿™æ ·ä¸€ä¸ªé€»è¾‘ï¼šæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒStageæ˜¯é€šè¿‡shuffleåˆ’åˆ†çš„ï¼Œæ‰€ä»¥ï¼Œæ¯ä¸€Stageéƒ½æ˜¯ä»¥shuffleå¼€å§‹çš„ï¼Œè‹¥ä¸€ä¸ªRDDæ˜¯å®½ä¾èµ–ï¼Œåˆ™å¿…ç„¶è¯´æ˜è¯¥RDDçš„çˆ¶RDDåœ¨å¦ä¸€ä¸ªStageä¸­ï¼Œè‹¥ä¸€ä¸ªRDDæ˜¯çª„ä¾èµ–ï¼Œåˆ™è¯¥RDDæ‰€ä¾èµ–çš„çˆ¶RDDè¿˜åœ¨åŒä¸€ä¸ªStageä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªé€»è¾‘ï¼Œæ‰¾åˆ°è¯¥Stageçš„çˆ¶Stageã€‚<br></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> æºç é˜…è¯» </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveå­˜å‚¨æ ¼å¼çš„ç”Ÿäº§åº”ç”¨</title>
      <link href="/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/"/>
      <url>/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p><strong>ç›¸åŒæ•°æ®ï¼Œåˆ†åˆ«ä»¥TextFileã€SequenceFileã€RcFileã€ORCå­˜å‚¨çš„æ¯”è¾ƒã€‚</strong></p><p>åŸå§‹å¤§å°: 19M</p><p><img src="/assets/blogImg/420_1.png" alt="enter description here"><br><a id="more"></a></p><h5 id="1-TextFile-é»˜è®¤-æ–‡ä»¶å¤§å°ä¸º18-1M"><a href="#1-TextFile-é»˜è®¤-æ–‡ä»¶å¤§å°ä¸º18-1M" class="headerlink" title="1. TextFile(é»˜è®¤) æ–‡ä»¶å¤§å°ä¸º18.1M"></a>1. TextFile(é»˜è®¤) æ–‡ä»¶å¤§å°ä¸º18.1M</h5><p><img src="/assets/blogImg/420_2.png" alt="enter description here"></p><h5 id="2-SequenceFile"><a href="#2-SequenceFile" class="headerlink" title="2. SequenceFile"></a>2. SequenceFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_seq( </span><br><span class="line">track_time string, </span><br><span class="line">url string, </span><br><span class="line">session_id string, </span><br><span class="line">referer string, </span><br><span class="line">ip string, </span><br><span class="line">end_user_id string, </span><br><span class="line">city_id string </span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY â€œ\tâ€ </span><br><span class="line">STORED AS SEQUENCEFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_seq select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>ç”¨SequenceFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º19.6M</strong><br><img src="/assets/blogImg/420_3.png" alt="enter description here"></p><h5 id="3-RcFile"><a href="#3-RcFile" class="headerlink" title="3. RcFile"></a>3. RcFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_rcfile(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS RCFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_rcfile select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>ç”¨RcFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º17.9M</strong><br><img src="/assets/blogImg/420_4.png" alt="enter description here"></p><h5 id="4-ORCFile"><a href="#4-ORCFile" class="headerlink" title="4. ORCFile"></a>4. ORCFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>ç”¨ORCFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º7.7M</strong><br><img src="/assets/blogImg/420_5.png" alt="enter description here"></p><h5 id="5-Parquet"><a href="#5-Parquet" class="headerlink" title="5. Parquet"></a>5. Parquet</h5><pre><code>create table page_views_parquetROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;</code></pre><p><strong>ç”¨ORCFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º13.1M</strong><br><img src="/assets/blogImg/420_6.png" alt="enter description here"></p><p><strong>æ€»ç»“ï¼šç£ç›˜ç©ºé—´å ç”¨å¤§å°æ¯”è¾ƒ</strong></p><font color="#FF4500">ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)</font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> å‹ç¼©æ ¼å¼ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¤§æ•°æ®å‹ç¼©ï¼Œä½ ä»¬çœŸçš„äº†è§£å—ï¼Ÿ</title>
      <link href="/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ ä»¬å‰–æå¤§æ•°æ®ä¹‹å‹ç¼©ï¼<br><a id="more"></a></p><h6 id="1-å‹ç¼©çš„å¥½å¤„å’Œåå¤„"><a href="#1-å‹ç¼©çš„å¥½å¤„å’Œåå¤„" class="headerlink" title="1. å‹ç¼©çš„å¥½å¤„å’Œåå¤„"></a>1. å‹ç¼©çš„å¥½å¤„å’Œåå¤„</h6><p><strong>å¥½å¤„</strong></p><ul><li>å‡å°‘å­˜å‚¨ç£ç›˜ç©ºé—´</li><li>é™ä½IO(ç½‘ç»œçš„IOå’Œç£ç›˜çš„IO)</li><li>åŠ å¿«æ•°æ®åœ¨ç£ç›˜å’Œç½‘ç»œä¸­çš„ä¼ è¾“é€Ÿåº¦ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦</li></ul><p><strong>åå¤„</strong></p><ul><li>ç”±äºä½¿ç”¨æ•°æ®æ—¶ï¼Œéœ€è¦å…ˆå°†æ•°æ®è§£å‹ï¼ŒåŠ é‡CPUè´Ÿè·</li></ul><h6 id="2-å‹ç¼©æ ¼å¼"><a href="#2-å‹ç¼©æ ¼å¼" class="headerlink" title="2. å‹ç¼©æ ¼å¼"></a>2. å‹ç¼©æ ¼å¼</h6><p><img src="/assets/blogImg/å‹ç¼©1.png" alt="enter description here"><br>å‹ç¼©æ¯”<br><img src="/assets/blogImg/å‹ç¼©2.png" alt="enter description here"><br>å‹ç¼©æ—¶é—´<br><img src="/assets/blogImg/yasuo3.png" alt="enter description here"></p><font color="#FF0000">å¯ä»¥çœ‹å‡ºï¼Œå‹ç¼©æ¯”è¶Šé«˜ï¼Œå‹ç¼©æ—¶é—´è¶Šé•¿ï¼Œå‹ç¼©æ¯”ï¼šSnappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</font><table><thead><tr><th>å‹ç¼©æ ¼å¼</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr></thead><tbody><tr><td><strong>gzip</strong></td><td>å‹ç¼©æ¯”åœ¨å››ç§å‹ç¼©æ–¹å¼ä¸­è¾ƒé«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œåœ¨åº”ç”¨ä¸­å¤„ç†gzipæ ¼å¼çš„æ–‡ä»¶å°±å’Œç›´æ¥å¤„ç†æ–‡æœ¬ä¸€æ ·ï¼›æœ‰hadoop nativeåº“ï¼›å¤§éƒ¨åˆ†linuxç³»ç»Ÿéƒ½è‡ªå¸¦gzipå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿</td><td>ä¸æ”¯æŒsplit</td></tr><tr><td><strong>lzo</strong></td><td>å‹ç¼©/è§£å‹é€Ÿåº¦ä¹Ÿæ¯”è¾ƒå¿«ï¼Œåˆç†çš„å‹ç¼©ç‡ï¼›æ”¯æŒsplitï¼Œæ˜¯hadoopä¸­æœ€æµè¡Œçš„å‹ç¼©æ ¼å¼ï¼›æ”¯æŒhadoop nativeåº“ï¼›éœ€è¦åœ¨linuxç³»ç»Ÿä¸‹è‡ªè¡Œå®‰è£…lzopå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿</td><td>å‹ç¼©ç‡æ¯”gzipè¦ä½ï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›lzoè™½ç„¶æ”¯æŒsplitï¼Œä½†éœ€è¦å¯¹lzoæ–‡ä»¶å»ºç´¢å¼•ï¼Œå¦åˆ™hadoopä¹Ÿæ˜¯ä¼šæŠŠlzoæ–‡ä»¶çœ‹æˆä¸€ä¸ªæ™®é€šæ–‡ä»¶ï¼ˆä¸ºäº†æ”¯æŒsplitéœ€è¦å»ºç´¢å¼•ï¼Œéœ€è¦æŒ‡å®šinputformatä¸ºlzoæ ¼å¼ï¼‰</td><td></td></tr><tr><td><strong>snappy</strong></td><td>å‹ç¼©é€Ÿåº¦å¿«ï¼›æ”¯æŒhadoop nativeåº“</td><td>ä¸æ”¯æŒsplitï¼›å‹ç¼©æ¯”ä½ï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›linuxç³»ç»Ÿä¸‹æ²¡æœ‰å¯¹åº”çš„å‘½ä»¤d. bzip2</td></tr><tr><td><strong>bzip2</strong></td><td>æ”¯æŒsplitï¼›å…·æœ‰å¾ˆé«˜çš„å‹ç¼©ç‡ï¼Œæ¯”gzipå‹ç¼©ç‡éƒ½é«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œä½†ä¸æ”¯æŒnativeï¼›åœ¨linuxç³»ç»Ÿä¸‹è‡ªå¸¦bzip2å‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿</td><td>å‹ç¼©/è§£å‹é€Ÿåº¦æ…¢ï¼›ä¸æ”¯æŒnative</td></tr></tbody></table><h5 id="æ€»ç»“ï¼š"><a href="#æ€»ç»“ï¼š" class="headerlink" title="æ€»ç»“ï¼š"></a><strong>æ€»ç»“ï¼š</strong></h5><p>ä¸åŒçš„åœºæ™¯é€‰æ‹©ä¸åŒçš„å‹ç¼©æ–¹å¼ï¼Œè‚¯å®šæ²¡æœ‰ä¸€ä¸ªä¸€åŠ³æ°¸é€¸çš„æ–¹æ³•ï¼Œå¦‚æœé€‰æ‹©é«˜å‹ç¼©æ¯”ï¼Œé‚£ä¹ˆå¯¹äºcpuçš„æ€§èƒ½è¦æ±‚è¦é«˜ï¼ŒåŒæ—¶å‹ç¼©ã€è§£å‹æ—¶é—´è€—è´¹ä¹Ÿå¤šï¼›é€‰æ‹©å‹ç¼©æ¯”ä½çš„ï¼Œå¯¹äºç£ç›˜ioã€ç½‘ç»œioçš„æ—¶é—´è¦å¤šï¼Œç©ºé—´å æ®è¦å¤šï¼›å¯¹äºæ”¯æŒåˆ†å‰²çš„ï¼Œå¯ä»¥å®ç°å¹¶è¡Œå¤„ç†ã€‚</p><h5 id="åº”ç”¨åœºæ™¯ï¼š"><a href="#åº”ç”¨åœºæ™¯ï¼š" class="headerlink" title="åº”ç”¨åœºæ™¯ï¼š"></a><strong>åº”ç”¨åœºæ™¯ï¼š</strong></h5><p>ä¸€èˆ¬åœ¨HDFS ã€Hiveã€HBaseä¸­ä¼šä½¿ç”¨ï¼›<br>å½“ç„¶ä¸€èˆ¬è¾ƒå¤šçš„æ˜¯ç»“åˆSpark æ¥ä¸€èµ·ä½¿ç”¨ã€‚</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å‹ç¼©æ ¼å¼ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘</title>
      <link href="/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼ŒSpark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘<br><a id="more"></a></p><h4 id="ç¯å¢ƒå‡†å¤‡"><a href="#ç¯å¢ƒå‡†å¤‡" class="headerlink" title="ç¯å¢ƒå‡†å¤‡"></a>ç¯å¢ƒå‡†å¤‡</h4><hr><p>JDKï¼š Spark 2.2.0åŠä»¥ä¸Šç‰ˆæœ¬åªæ”¯æŒJDK1.8</p><hr><p>Mavenï¼š3.3.9<br>è®¾ç½®mavenç¯å¢ƒå˜é‡æ—¶ï¼Œéœ€è®¾ç½®mavenå†…å­˜ï¼š<br>export MAVEN_OPTS=â€-Xmx2g -XX:ReservedCodeCacheSize=512mâ€</p><hr><p>Scalaï¼š2.11.8</p><hr><p>Git</p><h4 id="ç¼–è¯‘"><a href="#ç¼–è¯‘" class="headerlink" title="ç¼–è¯‘"></a>ç¼–è¯‘</h4><p>ä¸‹è½½sparkçš„taråŒ…ï¼Œå¹¶è§£å‹<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</span><br><span class="line">[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz</span><br></pre></td></tr></table></figure><p></p><p>ç¼–è¾‘dev/make-distribution.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh</span><br><span class="line">æ³¨é‡Šä»¥ä¸‹å†…å®¹ï¼š</span><br><span class="line">#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use &quot;set -o pipefail&quot;</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure><p></p><p>æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure><p></p><p>ç¼–è¾‘pom.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml</span><br><span class="line">æ·»åŠ åœ¨repositoryså†…</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">      &lt;id&gt;clouders&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;clouders Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br></pre></td></tr></table></figure><p></p><p>å®‰è£…<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn</span><br></pre></td></tr></table></figure><p></p><p>ç¨å¾®ç­‰å¾…å‡ å°æ—¶ï¼Œç½‘ç»œè¾ƒå¥½çš„è¯ï¼Œéå¸¸å¿«ã€‚<br>ä¹Ÿå¯ä»¥å‚è€ƒJå“¥åšå®¢ï¼š<br>åŸºäºCentOS6.4ç¯å¢ƒç¼–è¯‘Spark-2.1.0æºç  <a href="http://blog.itpub.net/30089851/viewspace-2140779/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2140779/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> ç¯å¢ƒæ­å»º </tag>
            
            <tag> åŸºç¡€ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨</title>
      <link href="/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
      <url>/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨<br><a id="more"></a></p><h6 id="1-å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡"><a href="#1-å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡" class="headerlink" title="1. å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡"></a><strong>1. å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡</strong></h6><table><thead><tr><th>åŠŸèƒ½</th><th style="text-align:center">å‘½ä»¤</th></tr></thead><tbody><tr><td><strong>å¯åŠ¨åç§°èŠ‚ç‚¹</strong></td><td style="text-align:center">hadoop-daemon.sh start namenode</td></tr><tr><td><strong>å¯åŠ¨æ•°æ®èŠ‚ç‚¹</strong></td><td style="text-align:center">hadoop-daemons.sh start datanode slave</td></tr><tr><td><strong>å¯åŠ¨secondarynamenode</strong></td><td style="text-align:center">hadoop-daemon.sh start secondarynamenode</td></tr><tr><td><strong>å¯åŠ¨resourcemanager</strong></td><td style="text-align:center">yarn-daemon.sh start resourcemanager</td></tr><tr><td><strong>å¯åŠ¨nodemanager</strong></td><td style="text-align:center">bin/yarn-daemons.sh start nodemanager</td></tr><tr><td><strong>åœæ­¢æ•°æ®èŠ‚ç‚¹</strong></td><td style="text-align:center">hadoop-daemons.sh stop datanode</td></tr></tbody></table><h6 id="2-å¸¸ç”¨çš„å‘½ä»¤"><a href="#2-å¸¸ç”¨çš„å‘½ä»¤" class="headerlink" title="2. å¸¸ç”¨çš„å‘½ä»¤"></a><strong>2. å¸¸ç”¨çš„å‘½ä»¤</strong></h6><table><thead><tr><th>åŠŸèƒ½</th><th style="text-align:center">å‘½ä»¤</th></tr></thead><tbody><tr><td><strong>åˆ›å»ºç›®å½•</strong></td><td style="text-align:center">hdfs dfs -mkdir /input</td></tr><tr><td><strong>æŸ¥çœ‹</strong></td><td style="text-align:center">hdfs dfs -ls</td></tr><tr><td><strong>é€’å½’æŸ¥çœ‹</strong></td><td style="text-align:center">hdfs dfs ls -R</td></tr><tr><td><strong>ä¸Šä¼ </strong></td><td style="text-align:center">hdfs dfs -put</td></tr><tr><td><strong>ä¸‹è½½</strong></td><td style="text-align:center">hdfs dfs -get</td></tr><tr><td><strong>åˆ é™¤</strong></td><td style="text-align:center">hdfs dfs -rm</td></tr><tr><td><strong>ä»æœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ°hdfs</strong></td><td style="text-align:center">hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>ä»hdfså‰ªåˆ‡ç²˜è´´åˆ°æœ¬åœ°</strong></td><td style="text-align:center">hdfs fs -moveToLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>è¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å¦ä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾</strong></td><td style="text-align:center">hdfs fs -appedToFile ./hello.txt /input/hello.txt</td></tr><tr><td><strong>æŸ¥çœ‹æ–‡ä»¶å†…å®¹</strong></td><td style="text-align:center">hdfs fs -cat /input/hello.txt</td></tr><tr><td><strong>æ˜¾ç¤ºä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾</strong></td><td style="text-align:center">hdfs fs -tail /input/hello.txt</td></tr><tr><td><strong>ä»¥å­—ç¬¦ä¸²çš„å½¢å¼æ‰“å°æ–‡ä»¶çš„å†…å®¹</strong></td><td style="text-align:center">hdfs fs -text /input/hello.txt</td></tr><tr><td><strong>ä¿®æ”¹æ–‡ä»¶æƒé™</strong></td><td style="text-align:center">hdfs fs -chmod 666 /input/hello.txt</td></tr><tr><td><strong>ä¿®æ”¹æ–‡ä»¶æ‰€å±</strong></td><td style="text-align:center">hdfs fs -chown ruoze.ruoze /input/hello.txt</td></tr><tr><td><strong>ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‹·è´åˆ°hdfsé‡Œ</strong></td><td style="text-align:center">hdfs fs -copyFromLocal /input/hello.txt /input/</td></tr><tr><td><strong>ä»hdfsæ‹·è´åˆ°æœ¬åœ°</strong></td><td style="text-align:center">hdfs fs -copyToLocal /input/hello.txt /input/</td></tr><tr><td><strong>ä»hdfsåˆ°ä¸€ä¸ªè·¯å¾„æ‹·è´åˆ°å¦ä¸€ä¸ªè·¯å¾„</strong></td><td style="text-align:center">hdfs fs -cp /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>ä»hdfsåˆ°ä¸€ä¸ªè·¯å¾„ç§»åŠ¨åˆ°å¦ä¸€ä¸ªè·¯å¾„</strong></td><td style="text-align:center">hdfs fs -mv /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>ç»Ÿè®¡æ–‡ä»¶ç³»ç»Ÿçš„å¯ç”¨ç©ºé—´ä¿¡æ¯</strong></td><td style="text-align:center">hdfs fs -df -h /</td></tr><tr><td><strong>ç»Ÿè®¡æ–‡ä»¶å¤¹çš„å¤§å°ä¿¡æ¯</strong></td><td style="text-align:center">hdfs fs -du -s -h /</td></tr><tr><td><strong>ç»Ÿè®¡ä¸€ä¸ªæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶èŠ‚ç‚¹æ•°é‡</strong></td><td style="text-align:center">hadoop fs -count /aaa</td></tr><tr><td><strong>è®¾ç½®hdfsçš„æ–‡ä»¶å‰¯æœ¬æ•°é‡</strong></td><td style="text-align:center">hadoop fs -setrep 3 /input/xx.txt</td></tr></tbody></table><h5 id="æ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©"><a href="#æ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©" class="headerlink" title="æ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©"></a>æ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©</h5><p><strong>1.hadoopå‘½ä»¤ç›´æ¥å›è½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©<br>2.hdfså‘½ä»¤ã€hdfs dfså‘½ä»¤ç›´æ¥å›è½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©<br>3.hadoop fs ç­‰ä»· hdfs dfså‘½ä»¤ï¼Œå’ŒLinuxçš„å‘½ä»¤å·®ä¸å¤šã€‚</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnæ¨¡å¼ï¼Ÿ</title>
      <link href="/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/"/>
      <url>/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnï¼Ÿ<br><a id="more"></a><br>å¼€å‘ä¸Šæˆ‘ä»¬é€‰æ‹©local[2]æ¨¡å¼<br>ç”Ÿäº§ä¸Šè·‘ä»»åŠ¡Jobï¼Œæˆ‘ä»¬é€‰æ‹©Spark On Yarnæ¨¡å¼ ï¼Œ</p><p>å°†Spark Applicationéƒ¨ç½²åˆ°yarnä¸­ï¼Œæœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š</p><p>1.éƒ¨ç½²Applicationå’ŒæœåŠ¡æ›´åŠ æ–¹ä¾¿</p><ul><li>åªéœ€è¦yarnæœåŠ¡ï¼ŒåŒ…æ‹¬Sparkï¼ŒStormåœ¨å†…çš„å¤šç§åº”ç”¨ç¨‹åºä¸è¦è¦è‡ªå¸¦æœåŠ¡ï¼Œå®ƒä»¬ç»ç”±å®¢æˆ·ç«¯æäº¤åï¼Œç”±yarnæä¾›çš„åˆ†å¸ƒå¼ç¼“å­˜æœºåˆ¶åˆ†å‘åˆ°å„ä¸ªè®¡ç®—èŠ‚ç‚¹ä¸Šã€‚</li></ul><p>2.èµ„æºéš”ç¦»æœºåˆ¶</p><ul><li>yarnåªè´Ÿè´£èµ„æºçš„ç®¡ç†å’Œè°ƒåº¦ï¼Œå®Œå…¨ç”±ç”¨æˆ·å’Œè‡ªå·±å†³å®šåœ¨yarné›†ç¾¤ä¸Šè¿è¡Œå“ªç§æœåŠ¡å’ŒApplicatioinï¼Œæ‰€ä»¥åœ¨yarnä¸Šæœ‰å¯èƒ½åŒæ—¶è¿è¡Œå¤šä¸ªåŒç±»çš„æœåŠ¡å’ŒApplicationã€‚Yarnåˆ©ç”¨Cgroupså®ç°èµ„æºçš„éš”ç¦»ï¼Œç”¨æˆ·åœ¨å¼€å‘æ–°çš„æœåŠ¡æˆ–è€…Applicationæ—¶ï¼Œä¸ç”¨æ‹…å¿ƒèµ„æºéš”ç¦»æ–¹é¢çš„é—®é¢˜ã€‚</li></ul><p>3.èµ„æºå¼¹æ€§ç®¡ç†</p><ul><li>Yarnå¯ä»¥é€šè¿‡é˜Ÿåˆ—çš„æ–¹å¼ï¼Œç®¡ç†åŒæ—¶è¿è¡Œåœ¨yarné›†ç¾¤ç§çš„å¤šä¸ªæœåŠ¡ï¼Œå¯æ ¹æ®ä¸åŒç±»å‹çš„åº”ç”¨ç¨‹åºå‹åŠ›æƒ…å†µï¼Œè°ƒæ•´å¯¹åº”çš„èµ„æºä½¿ç”¨é‡ï¼Œå®ç°èµ„æºå¼¹æ€§ç®¡ç†ã€‚</li></ul><p>Spark On Yarnæœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯clusteræ¨¡å¼ï¼Œä¸€ç§æ˜¯clientæ¨¡å¼ã€‚</p><p><strong>è¿è¡Œclientæ¨¡å¼ï¼š</strong></p><ul><li><p>â€œ./spark-shell â€“master yarnâ€</p></li><li><p>â€œ./spark-shell â€“master yarn-clientâ€</p></li><li><p>â€œ./spark-shell â€“master yarn â€“deploy-mode clientâ€</p></li></ul><p><strong>è¿è¡Œçš„æ˜¯clusteræ¨¡å¼</strong></p><ul><li><p>â€œ./spark-shell â€“master yarn-clusterâ€</p></li><li><p>â€œ./spark-shell â€“master yarn â€“deploy-mode clusterâ€</p></li></ul><p><strong>clientå’Œclusteræ¨¡å¼çš„ä¸»è¦åŒºåˆ«ï¼š<br>a. clientçš„driveræ˜¯è¿è¡Œåœ¨å®¢æˆ·ç«¯è¿›ç¨‹ä¸­<br>b. clusterçš„driveræ˜¯è¿è¡Œåœ¨Application Masterä¹‹ä¸­</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é«˜çº§ </tag>
            
            <tag> spark </tag>
            
            <tag> æ¶æ„ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²</title>
      <link href="/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼ŒHiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²<br><a id="more"></a></p><h6 id="ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶"><a href="#ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶" class="headerlink" title="ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶"></a>ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶</h6><ul><li><p>ç›¸å…³ç¯å¢ƒï¼š</p><ul><li><p>jdk-7u80</p><ul><li>hadoop-2.6.0-cdh5.7.1 ä¸æ”¯æŒjdk1.8ï¼Œå› æ­¤æ­¤å¤„ä¹Ÿå»¶ç»­jdk1.7</li></ul></li><li><p>apache-maven-3.3.9</p></li><li><p>mysql5.1</p></li><li><p>hadoopä¼ªåˆ†å¸ƒé›†ç¾¤å·²å¯åŠ¨</p></li></ul></li></ul><h6 id="äºŒã€å®‰è£…jdk"><a href="#äºŒã€å®‰è£…jdk" class="headerlink" title="äºŒã€å®‰è£…jdk"></a>äºŒã€å®‰è£…jdk</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /usr/java &amp;&amp; cd  /usr/java/    </span><br><span class="line"></span><br><span class="line">tar -zxvf  /tmp/server-jre-7u80-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line">chown -R root:root  /usr/java/jdk1.7.0_80/ </span><br><span class="line"></span><br><span class="line">echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="ä¸‰ã€å®‰è£…maven"><a href="#ä¸‰ã€å®‰è£…maven" class="headerlink" title="ä¸‰ã€å®‰è£…maven"></a>ä¸‰ã€å®‰è£…maven</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">unzip /tmp/apache-maven-3.3.9-bin.zip</span><br><span class="line"></span><br><span class="line">chown root: /usr/local/apache-maven-3.3.9 -R</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="å››ã€å®‰è£…mysql"><a href="#å››ã€å®‰è£…mysql" class="headerlink" title="å››ã€å®‰è£…mysql"></a>å››ã€å®‰è£…mysql</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server mysql</span><br><span class="line"></span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">chkconfig mysqld on</span><br><span class="line"></span><br><span class="line">mysqladmin -u root password 123456</span><br><span class="line"></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;</span><br><span class="line"></span><br><span class="line">delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line"></span><br><span class="line">delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line"></span><br><span class="line">drop database test;</span><br><span class="line"></span><br><span class="line">DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h6 id="äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼š"><a href="#äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼š" class="headerlink" title="äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼š"></a>äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼š</h6><p>è¾“å…¥ï¼š<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>æ ¹æ®cdhç‰ˆæœ¬é€‰æ‹©å¯¹åº”hiveè½¯ä»¶åŒ…ï¼š<br>hive-1.1.0-cdh5.7.1-src.tar.gz<br>è§£å‹åä½¿ç”¨mavenå‘½ä»¤ç¼–è¯‘æˆå®‰è£…åŒ…</p><h6 id="å…­ã€ç¼–è¯‘"><a href="#å…­ã€ç¼–è¯‘" class="headerlink" title="å…­ã€ç¼–è¯‘:"></a>å…­ã€ç¼–è¯‘:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line"></span><br><span class="line">tar -xf hive-1.1.0-cdh5.7.1-src.tar.gz</span><br><span class="line"></span><br><span class="line">cd /tmp/hive-1.1.0-cdh5.7.1</span><br><span class="line"></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br><span class="line"></span><br><span class="line"># ç¼–è¯‘ç”Ÿæˆçš„åŒ…åœ¨ä»¥ä¸‹ä½ç½®ï¼š</span><br><span class="line"></span><br><span class="line"># packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br></pre></td></tr></table></figure><h6 id="ä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åæµ‹è¯•"><a href="#ä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åæµ‹è¯•" class="headerlink" title="ä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åæµ‹è¯•"></a>ä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åæµ‹è¯•</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-1.1.0-cdh5.7.1-bin hive</span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin </span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop hive </span><br><span class="line"></span><br><span class="line">echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h6 id="å…«ã€æ›´æ”¹ç¯å¢ƒå˜é‡"><a href="#å…«ã€æ›´æ”¹ç¯å¢ƒå˜é‡" class="headerlink" title="å…«ã€æ›´æ”¹ç¯å¢ƒå˜é‡"></a>å…«ã€æ›´æ”¹ç¯å¢ƒå˜é‡</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">cd /usr/local/hive</span><br><span class="line"></span><br><span class="line">cd conf</span><br></pre></td></tr></table></figure><p>1ã€hive-env.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh&amp;&amp;vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure><p></p><p>2ã€hive-site.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt; </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;vincent&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°-HIVE-HOME-lib"><a href="#ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°-HIVE-HOME-lib" class="headerlink" title="ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°$HIVE_HOME/lib"></a>ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°$HIVE_HOME/lib</h6><p>ä¸Šæ–¹çš„hive-site.xmlä½¿ç”¨äº†javaçš„mysqlé©±åŠ¨åŒ…<br>éœ€è¦å°†è¿™ä¸ªåŒ…ä¸Šä¼ åˆ°hiveçš„libç›®å½•ä¹‹ä¸‹<br>è§£å‹ mysql-connector-java-5.1.45.zip å¯¹åº”çš„æ–‡ä»¶åˆ°ç›®å½•å³å¯<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">unzip mysql-connector-java-5.1.45.zip</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure><p></p><p>æœªæ‹·è´æœ‰ç›¸å…³æŠ¥é”™ï¼š</p><p>The specified datastore driver (â€œcom.mysql.jdbc.Driverâ€) was not found in the CLASSPATH.</p><p>Please check your CLASSPATH specification,</p><p>and the name of the driver.</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> ç¯å¢ƒæ­å»º </tag>
            
            <tag> åŸºç¡€ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)</title>
      <link href="/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/"/>
      <url>/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)<br><a id="more"></a></p><ol><li><p>ä¿®æ”¹mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template  mapred-site.xml</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>ä¿®æ”¹yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>å¯åŠ¨</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# cd ../../</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>å…³é—­</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> ç¯å¢ƒæ­å»º </tag>
            
            <tag> åŸºç¡€ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)</title>
      <link href="/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/"/>
      <url>/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)<br><a id="more"></a></p><h6 id="1-æ·»åŠ hadoopç”¨æˆ·"><a href="#1-æ·»åŠ hadoopç”¨æˆ·" class="headerlink" title="1.æ·»åŠ hadoopç”¨æˆ·"></a>1.æ·»åŠ hadoopç”¨æˆ·</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 ~]# useradd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# vi /etc/sudoers</span><br><span class="line"># æ‰¾åˆ°root ALL=(ALL) ALLï¼Œæ·»åŠ </span><br><span class="line"></span><br><span class="line">hadoop ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure><h6 id="2-ä¸Šä¼ å¹¶è§£å‹"><a href="#2-ä¸Šä¼ å¹¶è§£å‹" class="headerlink" title="2.ä¸Šä¼ å¹¶è§£å‹"></a>2.ä¸Šä¼ å¹¶è§£å‹</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# rz #ä¸Šä¼ hadoop-2.8.1.tar.gz</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz</span><br></pre></td></tr></table></figure><h6 id="3-è½¯è¿æ¥"><a href="#3-è½¯è¿æ¥" class="headerlink" title="3.è½¯è¿æ¥"></a>3.è½¯è¿æ¥</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span><br></pre></td></tr></table></figure><h6 id="4-è®¾ç½®ç¯å¢ƒå˜é‡"><a href="#4-è®¾ç½®ç¯å¢ƒå˜é‡" class="headerlink" title="4.è®¾ç½®ç¯å¢ƒå˜é‡"></a>4.è®¾ç½®ç¯å¢ƒå˜é‡</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="5-è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„"><a href="#5-è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„" class="headerlink" title="5.è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„"></a>5.è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# cd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# rm -f *.txt</span><br></pre></td></tr></table></figure><h6 id="6-åˆ‡æ¢hadoopç”¨æˆ·"><a href="#6-åˆ‡æ¢hadoopç”¨æˆ·" class="headerlink" title="6.åˆ‡æ¢hadoopç”¨æˆ·"></a>6.åˆ‡æ¢hadoopç”¨æˆ·</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# ll</span><br><span class="line"></span><br><span class="line">total 32</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 bin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 etc</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 include</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 lib</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexec</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logs</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 sbin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop 4096 Jun  2 14:24 share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin:å¯æ‰§è¡Œæ–‡ä»¶</span><br><span class="line"></span><br><span class="line"># etc: é…ç½®æ–‡ä»¶</span><br><span class="line"></span><br><span class="line"># sbin:shellè„šæœ¬ï¼Œå¯åŠ¨å…³é—­hdfs,yarnç­‰</span><br></pre></td></tr></table></figure><h6 id="7-é…ç½®æ–‡ä»¶"><a href="#7-é…ç½®æ–‡ä»¶" class="headerlink" title="7.é…ç½®æ–‡ä»¶"></a>7.é…ç½®æ–‡ä»¶</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt;    # é…ç½®è‡ªå·±æœºå™¨çš„IP</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="8-é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»"><a href="#8-é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»" class="headerlink" title="8.é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»"></a>8.é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»</h6><p>8.1å…¬é’¥/å¯†é’¥ é…ç½®æ— å¯†ç ç™»å½•<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>8.2 æŸ¥çœ‹æ—¥æœŸï¼Œçœ‹æ˜¯å¦é…ç½®æˆåŠŸ<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # ç¬¬ä¸€æ¬¡å›è½¦è¾“å…¥yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date   #ä¸éœ€è¦å›è½¦è¾“å…¥yes,å³OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # ç¬¬ä¸€æ¬¡å›è½¦è¾“å…¥yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date   #ä¸éœ€è¦å›è½¦è¾“å…¥yes,å³OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br></pre></td></tr></table></figure><p></p><h6 id="9-æ ¼å¼åŒ–å’Œå¯åŠ¨"><a href="#9-æ ¼å¼åŒ–å’Œå¯åŠ¨" class="headerlink" title="9.æ ¼å¼åŒ–å’Œå¯åŠ¨"></a>9.æ ¼å¼åŒ–å’Œå¯åŠ¨</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">hadoop-01: Error: JAVA_HOME is not set and could not be found.</span><br><span class="line"></span><br><span class="line">localhost: Error: JAVA_HOME is not set and could not be found.</span><br></pre></td></tr></table></figure><p>9.1è§£å†³æ–¹æ³•:æ·»åŠ ç¯å¢ƒå˜é‡<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]#  vi etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line"># å°†export JAVA_HOME=$&#123;JAVA_HOME&#125;æ”¹ä¸º</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied</span><br></pre></td></tr></table></figure><p>9.2è§£å†³æ–¹æ³•:æ·»åŠ æƒé™<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# exit</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# cd ../</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# cd /opt/software/hadoop</span><br></pre></td></tr></table></figure><p></p><p>9.3 ç»§ç»­å¯åŠ¨<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><p>9.4æ£€æŸ¥æ˜¯å¦æˆåŠŸ<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# jps</span><br><span class="line"></span><br><span class="line">19536 DataNode</span><br><span class="line"></span><br><span class="line">19440 NameNode</span><br><span class="line"></span><br><span class="line">19876 Jps</span><br><span class="line"></span><br><span class="line">19740 SecondaryNameNode</span><br></pre></td></tr></table></figure><p></p><p>9.5è®¿é—®ï¼š <a href="http://192.168.137.130:50070" target="_blank" rel="noopener">http://192.168.137.130:50070</a></p><p>9.6ä¿®æ”¹dfså¯åŠ¨çš„è¿›ç¨‹ï¼Œä»¥hadoop-01å¯åŠ¨</p><p>å¯åŠ¨çš„ä¸‰ä¸ªè¿›ç¨‹ï¼š</p><p>namenode: hadoop-01 bin/hdfs getconf -namenodes</p><p>datanode: localhost datanodes (using default slaves file) etc/hadoop/slaves</p><p>secondarynamenode: 0.0.0.0<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# echo  &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">hadoop-01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50090&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50091&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><p>9.7é‡å¯<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> ç¯å¢ƒæ­å»º </tag>
            
            <tag> åŸºç¡€ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸€ï¼‰</title>
      <link href="/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/"/>
      <url>/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆä¸€ï¼‰<br><a id="more"></a></p><ol><li><p>æŸ¥çœ‹å½“å‰ç›®å½• pwd</p></li><li><p>æŸ¥çœ‹IP</p></li></ol><ul><li><p>ifconfig æŸ¥çœ‹è™šæ‹Ÿæœºip</p></li><li><p>hostname ä¸»æœºåå­—</p><ul><li>i æŸ¥çœ‹ä¸»æœºåæ˜ å°„çš„IP</li></ul></li></ul><ol start="3"><li>åˆ‡æ¢ç›®å½• cd</li></ol><ul><li><p>cd ~ åˆ‡æ¢å®¶ç›®å½•ï¼ˆrootä¸º/rootï¼Œæ™®é€šç”¨æˆ·ä¸º/home/ç”¨æˆ·åï¼‰</p></li><li><p>cd /filename ä»¥ç»å¯¹è·¯å¾„åˆ‡æ¢ç›®å½•</p></li><li><p>cd - è¿”å›ä¸Šä¸€æ¬¡æ“ä½œè·¯å¾„ï¼Œå¹¶è¾“å‡ºè·¯å¾„</p></li><li><p>cd ../ è¿”å›ä¸Šä¸€å±‚ç›®å½•</p></li></ul><ol start="4"><li><p>æ¸…ç†æ¡Œé¢ clear</p></li><li><p>æ˜¾ç¤ºå½“å‰ç›®å½•æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ ls</p></li></ol><ul><li><p>ls -l(ll) æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯</p></li><li><p>ls -la æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´ï¼Œä¾‹ï¼š.sshï¼‰</p></li><li><p>ls -lh æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æ–‡ä»¶å¤§å°</p></li><li><p>ls -lrt æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æŒ‰æ—¶é—´æ’åº</p></li></ul><ol start="6"><li><p>æŸ¥çœ‹æ–‡ä»¶å¤¹å¤§å° du -sh</p></li><li><p>å‘½ä»¤å¸®åŠ©</p></li></ol><ul><li><p>man å‘½ä»¤</p></li><li><p>å‘½ä»¤ â€“help</p></li></ul><ol start="8"><li>åˆ›å»ºæ–‡ä»¶å¤¹ mkdir</li></ol><ul><li>mkdir -p filename1/filename2 é€’å½’åˆ›å»ºæ–‡ä»¶å¤¹</li></ul><ol start="9"><li><p>åˆ›å»ºæ–‡ä»¶ touch/vi/echo xx&gt;filename</p></li><li><p>æŸ¥çœ‹æ–‡ä»¶å†…å®¹</p></li></ol><ul><li><p>cat filename ç›´æ¥æ‰“å°æ‰€æœ‰å†…å®¹</p></li><li><p>more filename æ ¹æ®çª—å£å¤§å°è¿›è¡Œåˆ†é¡µæ˜¾ç¤º</p></li></ul><ol start="11"><li>æ–‡ä»¶ç¼–è¾‘ vi</li></ol><ul><li><p>viåˆ†ä¸ºå‘½ä»¤è¡Œæ¨¡å¼ï¼Œæ’å…¥æ¨¡å¼ï¼Œå°¾è¡Œæ¨¡å¼</p></li><li><p>å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;æ’å…¥æ¨¡å¼ï¼šæŒ‰iæˆ–aé”®</p></li><li><p>æ’å…¥æ¨¡å¼â€”&gt;å‘½ä»¤è¡Œæ¨¡å¼ï¼šæŒ‰Escé”®</p></li><li><p>å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;å°¾è¡Œæ¨¡å¼ï¼šæŒ‰Shiftå’Œ:é”®</p><p>æ’å…¥æ¨¡å¼</p><ul><li><p>dd åˆ é™¤å…‰æ ‡æ‰€åœ¨è¡Œ</p></li><li><p>n+dd åˆ é™¤å…‰æ ‡ä»¥ä¸‹çš„nè¡Œ</p></li><li><p>dG åˆ é™¤å…‰æ ‡ä»¥ä¸‹è¡Œ</p></li><li><p>gg ç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯</p></li><li><p>G æœ€åä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯</p></li><li><p>shift+$ è¯¥è¡Œæœ€åä¸€ä¸ªå­—æ¯</p><p>å°¾è¡Œæ¨¡å¼</p></li><li><p>q! å¼ºåˆ¶é€€å‡º</p></li><li><p>qw å†™å…¥å¹¶é€€å‡º</p></li><li><p>qw! å¼ºåˆ¶å†™å…¥é€€å‡º</p></li><li><p>x é€€å‡ºï¼Œå¦‚æœå­˜åœ¨æ”¹åŠ¨ï¼Œåˆ™ä¿å­˜å†é€€å‡º</p></li></ul></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> åŸºç¡€ </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linuxå¸¸ç”¨å‘½ä»¤ï¼ˆäºŒï¼‰</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆäºŒï¼‰<br><a id="more"></a></p><ol><li>å®æ—¶æŸ¥çœ‹æ–‡ä»¶å†…å®¹ tail filename</li></ol><ul><li><p>tail -f filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åï¼Œä¸èƒ½ç›‘è§†æ–‡ä»¶å†…å®¹</p></li><li><p>tail -F filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åï¼Œä¾ç„¶å¯ä»¥ç›‘è§†æ–‡ä»¶å†…å®¹</p></li></ul><ol start="2"><li>å¤åˆ¶ã€ç§»åŠ¨æ–‡ä»¶</li></ol><ul><li><p>cp oldfilename newfilename å¤åˆ¶</p></li><li><p>mv oldfilename newfilename ç§»åŠ¨/é‡å‘½å</p></li></ul><ol start="3"><li>echo</li></ol><ul><li><p>echo â€œxxxâ€ è¾“å‡º</p></li><li><p>echo â€œxxxâ€ &gt; filename è¦†ç›–</p></li><li><p>echo â€œxxxâ€ &gt;&gt; filename è¿½åŠ </p></li></ul><ol start="4"><li>åˆ é™¤ rm</li></ol><ul><li><p>rm -f å¼ºåˆ¶åˆ é™¤</p></li><li><p>rm -rf å¼ºåˆ¶åˆ é™¤æ–‡ä»¶å¤¹ï¼Œr è¡¨ç¤ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹åŠæ–‡ä»¶å¤¹é‡Œé¢æ–‡ä»¶</p></li></ul><ol start="5"><li>åˆ«å alias</li></ol><ul><li><p>alias x=â€xxxxxxâ€ ä¸´æ—¶å¼•ç”¨åˆ«å</p></li><li><p>alias x=â€xxxxxxâ€ é…ç½®åˆ°ç¯å¢ƒå˜é‡ä¸­å³ä¸ºæ°¸ä¹…ç”Ÿæ•ˆ</p></li></ul><ol start="6"><li>æŸ¥çœ‹å†å²å‘½ä»¤ history</li></ol><ul><li><p>history æ˜¾ç¤ºå‡ºæ‰€æœ‰å†å²è®°å½•</p></li><li><p>history n æ˜¾ç¤ºå‡ºnæ¡è®°å½•</p></li><li><p>!n æ‰§è¡Œç¬¬næ¡è®°å½•</p></li></ul><ol start="7"><li>ç®¡é“å‘½ä»¤ ï¼ˆ | ï¼‰</li></ol><ul><li>ç®¡é“çš„ä¸¤è¾¹éƒ½æ˜¯å‘½ä»¤ï¼Œå·¦è¾¹çš„å‘½ä»¤å…ˆæ‰§è¡Œï¼Œæ‰§è¡Œçš„ç»“æœä½œä¸ºå³è¾¹å‘½ä»¤çš„è¾“å…¥</li></ul><ol start="8"><li>æŸ¥çœ‹è¿›ç¨‹ã€æŸ¥çœ‹idã€ç«¯å£</li></ol><ul><li><p>ps -ef ï½œgrep è¿›ç¨‹å æŸ¥çœ‹è¿›ç¨‹åŸºæœ¬ä¿¡æ¯</p></li><li><p>netstat -nplï½œgrep è¿›ç¨‹åæˆ–è¿›ç¨‹id æŸ¥çœ‹æœåŠ¡idå’Œç«¯å£</p></li></ul><ol start="9"><li>æ€æ­»è¿›ç¨‹ kill</li></ol><ul><li><p>kill -9 è¿›ç¨‹å/pid å¼ºåˆ¶åˆ é™¤</p></li><li><p>kill -9 $(pgrep è¿›ç¨‹å)ï¼šæ€æ­»ä¸è¯¥è¿›ç¨‹ç›¸å…³çš„æ‰€æœ‰è¿›ç¨‹</p></li></ul><ol start="10"><li>rpm æœç´¢ã€å¸è½½</li></ol><ul><li><p>rpm -qa | grep xxx æœç´¢xxx</p></li><li><p>rpm â€“nodeps -e xxx åˆ é™¤xxx</p></li><li><p>â€“nodeps ä¸éªŒè¯åŒ…çš„ä¾èµ–æ€§</p></li></ul><ol start="11"><li>æŸ¥è¯¢</li></ol><ul><li><p>find è·¯å¾„ -name xxx (æ¨è)</p></li><li><p>which xxx</p></li><li><p>local xxx</p></li></ul><ol start="12"><li>æŸ¥çœ‹ç£ç›˜ã€å†…å­˜ã€ç³»ç»Ÿçš„æƒ…å†µ</li></ol><ul><li><p>df -h æŸ¥çœ‹ç£ç›˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µ</p></li><li><p>free -m æŸ¥çœ‹å†…å­˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µ</p></li><li><p>top æŸ¥çœ‹ç³»ç»Ÿæƒ…å†µ</p></li></ul><ol start="13"><li>è½¯è¿æ¥</li></ol><ul><li>ln -s åŸå§‹ç›®å½• ç›®æ ‡ç›®å½•</li></ul><ol start="14"><li>å‹ç¼©ã€è§£å‹</li></ol><ul><li><p>tar -czf å‹ç¼© tar -xzvf è§£å‹</p></li><li><p>zip å‹ç¼© unzip è§£å‹</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> åŸºç¡€ </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸‰ï¼‰</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆä¸‰ï¼‰<br><a id="more"></a></p><ol><li><p>ç”¨æˆ·ã€ç”¨æˆ·ç»„</p><p>ç”¨æˆ·</p><ul><li><p>useradd ç”¨æˆ·å æ·»åŠ ç”¨æˆ·</p></li><li><p>userdel ç”¨æˆ·å åˆ é™¤ç”¨æˆ·</p></li><li><p>id ç”¨æˆ·å æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯</p></li><li><p>passwd ç”¨æˆ·å ä¿®æ”¹ç”¨æˆ·å¯†ç </p></li><li><p>su - ç”¨æˆ·å åˆ‡æ¢ç”¨æˆ·</p></li><li><p>ll /home/ æŸ¥çœ‹å·²æœ‰çš„ç”¨æˆ·</p><p>ç”¨æˆ·ç»„</p></li><li><p>groupadd ç”¨æˆ·ç»„ æ·»åŠ ç”¨æˆ·ç»„</p></li><li><p>cat /etc/group ç”¨æˆ·ç»„çš„æ–‡ä»¶</p></li><li><p>usermod -a -G ç”¨æˆ·ç»„ ç”¨æˆ· å°†ç”¨æˆ·æ·»åŠ åˆ°ç”¨æˆ·ç»„ä¸­</p><p>ç»™ä¸€ä¸ªæ™®é€šç”¨æˆ·æ·»åŠ sudoæƒé™</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span><br><span class="line">    #åœ¨root     ALL=(ALL)    ALL    ä¸‹é¢æ·»åŠ ä¸€è¡Œ</span><br><span class="line">    ç”¨æˆ·    ALL=(ALL)    NOPASSWD:ALL</span><br></pre></td></tr></table></figure></li></ul></li><li><p>ä¿®æ”¹æ–‡ä»¶æƒé™</p><p>chown ä¿®æ”¹æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„æ‰€å±ç”¨æˆ·å’Œç”¨æˆ·ç»„</p><ul><li><p>chown -R ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶å¤¹å -R ä¸ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹</p></li><li><p>chown ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶å</p><p>chmod: ä¿®æ”¹æ–‡ä»¶å¤¹æˆ–è€…æ–‡ä»¶çš„æƒé™</p></li><li><p>chmod -R 700 æ–‡ä»¶å¤¹å</p></li><li><p>chmod 700 æ–‡ä»¶å¤¹å</p></li></ul></li></ol><pre><code>r  =&gt;    4w  =&gt;    2x  =&gt;    1</code></pre><ol start="3"><li>åå°æ‰§è¡Œå‘½ä»¤</li></ol><ul><li><p>&amp;</p></li><li><p>nohup</p></li><li><p>screen</p></li></ul><ol start="4"><li>å¤šäººåˆä½œ screen</li></ol><ul><li><p>screen -list æŸ¥çœ‹ä¼šè¯</p></li><li><p>screen -S å»ºç«‹ä¸€ä¸ªåå°çš„ä¼šè¯</p></li><li><p>screen -r è¿›å…¥ä¼šè¯</p></li><li><p>ctrl+a+d é€€å‡ºä¼šè¯</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> åŸºç¡€ </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFSæ¶æ„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥</title>
      <link href="/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"/>
      <url>/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>HDFSæ¶æ„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥<br><a id="more"></a><br>HDFSä¸»è¦ç”±3ä¸ªç»„ä»¶æ„æˆï¼Œåˆ†åˆ«æ˜¯<strong>NameNodeã€SecondaryNameNodeå’ŒDataNode</strong>ï¼ŒHSFSæ˜¯ä»¥master/slaveæ¨¡å¼è¿è¡Œçš„ï¼Œå…¶ä¸­NameNodeã€SecondaryNameNode è¿è¡Œåœ¨masterèŠ‚ç‚¹ï¼ŒDataNodeè¿è¡ŒslaveèŠ‚ç‚¹ã€‚</p><h6 id="NameNodeå’ŒDataNodeæ¶æ„å›¾"><a href="#NameNodeå’ŒDataNodeæ¶æ„å›¾" class="headerlink" title="NameNodeå’ŒDataNodeæ¶æ„å›¾"></a>NameNodeå’ŒDataNodeæ¶æ„å›¾</h6><p><img src="/assets/blogImg/1.png" alt="1"><br>NameNode(åç§°èŠ‚ç‚¹)<br>å­˜å‚¨ï¼šå…ƒä¿¡æ¯çš„ç§ç±»ï¼ŒåŒ…å«:</p><ul><li>æ–‡ä»¶åç§°</li><li>æ–‡ä»¶ç›®å½•ç»“æ„</li><li>æ–‡ä»¶çš„å±æ€§[æƒé™,åˆ›å»ºæ—¶é—´,å‰¯æœ¬æ•°]</li><li>æ–‡ä»¶å¯¹åº”å“ªäº›æ•°æ®å—â€“&gt;æ•°æ®å—å¯¹åº”å“ªäº›datanodeèŠ‚ç‚¹</li><li>ä½œç”¨ï¼š</li><li>ç®¡ç†ç€æ–‡ä»¶ç³»ç»Ÿå‘½åç©ºé—´</li><li>ç»´æŠ¤è¿™æ–‡ä»¶ç³»ç»Ÿæ ‘åŠæ ‘ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•</li><li>ç»´æŠ¤æ‰€æœ‰è¿™äº›æ–‡ä»¶æˆ–ç›®å½•çš„æ‰“å¼€ã€å…³é—­ã€ç§»åŠ¨ã€é‡å‘½åç­‰æ“ä½œ</li></ul><p>DataNode(æ•°æ®èŠ‚ç‚¹)<br>å­˜å‚¨ï¼šæ•°æ®å—ã€æ•°æ®å—æ ¡éªŒã€ä¸NameNodeé€šä¿¡<br>ä½œç”¨ï¼š</p><ul><li>è¯»å†™æ–‡ä»¶çš„æ•°æ®å—</li><li>NameNodeçš„æŒ‡ç¤ºæ¥è¿›è¡Œåˆ›å»ºã€åˆ é™¤ã€å’Œå¤åˆ¶ç­‰æ“ä½œ</li><li>é€šè¿‡å¿ƒè·³å®šæœŸå‘NameNodeå‘é€æ‰€å­˜å‚¨æ–‡ä»¶å—åˆ—è¡¨ä¿¡æ¯</li><li>Scondary NameNode(ç¬¬äºŒåç§°èŠ‚ç‚¹)<br>å­˜å‚¨: å‘½åç©ºé—´é•œåƒæ–‡ä»¶fsimage+ç¼–è¾‘æ—¥å¿—editlog<br>ä½œç”¨: å®šæœŸåˆå¹¶fsimage+editlogæ–‡ä»¶ä¸ºæ–°çš„fsimageæ¨é€ç»™NamenNode<h6 id="å‰¯æœ¬æ”¾ç½®ç­–ç•¥"><a href="#å‰¯æœ¬æ”¾ç½®ç­–ç•¥" class="headerlink" title="å‰¯æœ¬æ”¾ç½®ç­–ç•¥"></a>å‰¯æœ¬æ”¾ç½®ç­–ç•¥</h6><img src="/assets/blogImg/2.png" alt="2"><br><strong>ç¬¬ä¸€å‰¯æœ¬</strong>ï¼šæ”¾ç½®åœ¨ä¸Šä¼ æ–‡ä»¶çš„DataNodeä¸Šï¼›å¦‚æœæ˜¯é›†ç¾¤å¤–æäº¤ï¼Œåˆ™éšæœºæŒ‘é€‰ä¸€å°ç£ç›˜ä¸å¤ªæ…¢ã€CPUä¸å¤ªå¿™çš„èŠ‚ç‚¹ä¸Š<br><strong>ç¬¬äºŒå‰¯æœ¬</strong>ï¼šæ”¾ç½®åœ¨ä¸ç¬¬ä¸€ä¸ªå‰¯æœ¬ä¸åŒçš„æœºæ¶çš„èŠ‚ç‚¹ä¸Š<br><strong>ç¬¬ä¸‰å‰¯æœ¬</strong>ï¼šä¸ç¬¬äºŒä¸ªå‰¯æœ¬ç›¸åŒæœºæ¶çš„ä¸åŒèŠ‚ç‚¹ä¸Š<br>å¦‚æœè¿˜æœ‰æ›´å¤šçš„å‰¯æœ¬ï¼šéšæœºæ”¾åœ¨èŠ‚ç‚¹ä¸­</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hdfs </tag>
            
            <tag> æ¶æ„ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>é…ç½®å¤šå°è™šæ‹Ÿæœºä¹‹é—´çš„SSHä¿¡ä»»</title>
      <link href="/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/"/>
      <url>/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="æœ¬æœºç¯å¢ƒ"><a href="#æœ¬æœºç¯å¢ƒ" class="headerlink" title="æœ¬æœºç¯å¢ƒ"></a>æœ¬æœºç¯å¢ƒ</h4><a id="more"></a><p><img src="/assets/blogImg/640.png" alt="1"></p><p>3å°æœºå™¨æ‰§è¡Œå‘½ä»¤ssh-keygen<br><img src="/assets/blogImg/641.png" alt="2"></p><p>é€‰å–ç¬¬ä¸€å°,ç”Ÿæˆauthorized_keysæ–‡ä»¶<br><img src="/assets/blogImg/642.png" alt="3"></p><p>hadoop002 hadoop003ä¼ è¾“id_rsa.pubæ–‡ä»¶åˆ°hadoop001<br><img src="/assets/blogImg/643.png" alt="4"><br><img src="/assets/blogImg/644.png" alt="5"></p><p>hadoop001æœºå™¨ åˆå¹¶id_rsa.pub2ã€id_rsa.pub3åˆ°authorized_keys<br><img src="/assets/blogImg/645.png" alt="6"></p><p>è®¾ç½®æ¯å°æœºå™¨çš„æƒé™<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 -R ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>å°†authorized_keysåˆ†å‘åˆ°hadoop002ã€hadoop003æœºå™¨<br><img src="/assets/blogImg/646.png" alt="7"></p><p><img src="/assets/blogImg/647.png" alt="8"></p><p>éªŒè¯(æ¯å°æœºå™¨ä¸Šæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œåªè¾“å…¥yesï¼Œä¸è¾“å…¥å¯†ç ï¼Œè¯´æ˜é…ç½®æˆåŠŸ)<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ssh root@hadoop002 date</span><br><span class="line">[root@hadoop002 ~]# ssh root@hadoop001 date</span><br><span class="line">[root@hadoop003 ~]# ssh root@hadoop001 date</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ç¯å¢ƒæ­å»º </tag>
            
            <tag> åŸºç¡€ </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
