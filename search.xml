<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
      <link href="/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
      <url>/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use test</span><br><span class="line">mysql&gt; create table hlw_offset(</span><br><span class="line">        topic varchar(32),</span><br><span class="line">        groupid varchar(50),</span><br><span class="line">        partitions int,</span><br><span class="line">        fromoffset bigint,</span><br><span class="line">        untiloffset bigint,</span><br><span class="line">        primary key(topic,groupid,partitions)</span><br><span class="line">        );</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;</span><br><span class="line">&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;</span><br><span class="line">--------------------------------------------------</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.27&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;config&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）StreamingContext</span><br><span class="line">2）从kafka中获取数据(从外部存储获取offset--&gt;根据offset获取kafka中的数据)</span><br><span class="line">3）根据业务进行逻辑处理</span><br><span class="line">4）将处理结果存到外部存储中--保存offset</span><br><span class="line">5）启动程序，等待程序结束</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import kafka.common.TopicAndPartition</span><br><span class="line">import kafka.message.MessageAndMetadata</span><br><span class="line">import kafka.serializer.StringDecoder</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">import scalikejdbc._</span><br><span class="line">import scalikejdbc.config._</span><br><span class="line">object JDBCOffsetApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建SparkStreaming入口</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)</span><br><span class="line">    val ssc = new StreamingContext(conf,Seconds(5))</span><br><span class="line">    //kafka消费主题</span><br><span class="line">    val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet</span><br><span class="line">    //kafka参数</span><br><span class="line">    //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改</span><br><span class="line">    val kafkaParams = Map[String,String](</span><br><span class="line">      &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),</span><br><span class="line">      &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),</span><br><span class="line">      &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)</span><br><span class="line">    )</span><br><span class="line">    //先使用scalikejdbc从MySQL数据库中读取offset信息</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来</span><br><span class="line">    //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到</span><br><span class="line">    DBs.setup()</span><br><span class="line">    val fromOffset = DB.readOnly( implicit session =&gt; &#123;</span><br><span class="line">      SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123;</span><br><span class="line">        (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))</span><br><span class="line">      &#125;).list().apply()</span><br><span class="line">    &#125;).toMap</span><br><span class="line">    //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费</span><br><span class="line">      val messages = if (fromOffset.isEmpty) &#123;</span><br><span class="line">        println(&quot;从头开始消费...&quot;)</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        println(&quot;从已存在记录开始消费...&quot;)</span><br><span class="line">        val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)</span><br><span class="line">      &#125;</span><br><span class="line">      messages.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">        if(!rdd.isEmpty())&#123;</span><br><span class="line">          //输出rdd的数据量</span><br><span class="line">          println(&quot;数据统计记录为：&quot;+rdd.count())</span><br><span class="line">          //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组</span><br><span class="line">//          trait HasOffsetRanges &#123;</span><br><span class="line">//            def offsetRanges: Array[OffsetRange]</span><br><span class="line">//          &#125;</span><br><span class="line">          val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">          offsetRanges.foreach(x =&gt; &#123;</span><br><span class="line">            //输出每次消费的主题，分区，开始偏移量和结束偏移量</span><br><span class="line">            println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;)</span><br><span class="line">           //将最新的偏移量信息保存到MySQL表中</span><br><span class="line">            DB.autoCommit( implicit session =&gt; &#123;</span><br><span class="line">              SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)</span><br><span class="line">            .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)</span><br><span class="line">              .update().apply()</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义的ValueUtils工具类如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import com.typesafe.config.ConfigFactory</span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">object ValueUtils &#123;</span><br><span class="line">val load = ConfigFactory.load()</span><br><span class="line">  def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;</span><br><span class="line">val value = load.getString(key)</span><br><span class="line">    if(StringUtils.isNotEmpty(value)) &#123;</span><br><span class="line">      value</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      defaultValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>application.conf内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">metadata.broker.list = &quot;192.168.137.251:9092&quot;</span><br><span class="line">auto.offset.reset = &quot;smallest&quot;</span><br><span class="line">group.id = &quot;hlw_offset_group&quot;</span><br><span class="line">kafka.topics = &quot;hlw_offset&quot;</span><br><span class="line">serializer.class = &quot;kafka.serializer.StringEncoder&quot;</span><br><span class="line">request.required.acks = &quot;1&quot;</span><br><span class="line"># JDBC settings</span><br><span class="line">db.default.driver = &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;</span><br><span class="line">db.default.user=&quot;root&quot;</span><br><span class="line">db.default.password=&quot;123456&quot;</span><br></pre></td></tr></table></figure></li><li><p>自定义kafka producer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.&#123;Date, Properties&#125;</span><br><span class="line">import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;</span><br><span class="line">object KafkaProducer &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val properties = new Properties()</span><br><span class="line">    properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))</span><br><span class="line">    properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))</span><br><span class="line">    properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))</span><br><span class="line">    val producerConfig = new ProducerConfig(properties)</span><br><span class="line">    val producer = new Producer[String,String](producerConfig)</span><br><span class="line">    val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)</span><br><span class="line">    //每次产生100条数据</span><br><span class="line">    var i = 0</span><br><span class="line">    for (i &lt;- 1 to 100) &#123;</span><br><span class="line">      val runtimes = new Date().toString</span><br><span class="line">     val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)</span><br><span class="line">      producer.send(messages)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;数据发送完毕...&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</span><br></pre></td></tr></table></figure></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台输出结果：</span><br><span class="line">从头开始消费...</span><br><span class="line">数据统计记录为：500</span><br><span class="line">---hlw_offset,0,0,500---</span><br></pre></td></tr></table></figure></li></ol><pre><code>查看MySQL表，offset记录成功<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |          0 |         500 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></code></pre><ol start="5"><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台结果输出：</span><br><span class="line">从已存在记录开始消费...</span><br><span class="line">数据统计记录为：300</span><br><span class="line">---hlw_offset,0,500,800---</span><br></pre></td></tr></table></figure></li><li><p>查看更新后的offset MySQL数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |        500 |         800 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> spark streaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之四</title>
      <link href="/2019/05/31/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B/"/>
      <url>/2019/05/31/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h2 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val acc = sc.accumulator(0, “Error Accumulator”)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">val newData = data.map(x =&gt; &#123;</span><br><span class="line">  if (x % 2 == 0) &#123;</span><br><span class="line"> accum += 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">newData.count</span><br><span class="line">acc.value</span><br><span class="line">newData.foreach(println)</span><br><span class="line">acc.value</span><br></pre></td></tr></table></figure><p>上述现象，会造成acc.value的最终值变为10</p><a id="more"></a><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>Spark中的一系列transform操作都会构造成一长串的任务链，此时就需要通过一个action操作来触发（lazy的特性），accumulator也是如此。</p><ul><li>因此在一个action操作之后，调用value方法查看，是没有任何变化</li><li>第一次action操作之后，调用value方法查看，变成了5</li><li>第二次action操作之后，调用value方法查看，变成了10</li></ul><p>原因就在于第二次action操作的时候，又执行了一次累加器的操作，同个累加器，在原有的基础上又加了5，从而变成了10</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过上述的现象描述，我们可以很快知道解决的方法：只进行一次action操作。基于此，我们只要切断任务之间的依赖关系就可以了，即使用cache、persist。这样操作之后，那么后续的累加器操作就不会受前面的transform操作影响了</p><h2 id="案例地址"><a href="#案例地址" class="headerlink" title="案例地址"></a>案例地址</h2><p>相关的工程案例地址在Github上：<a href="https://github.com/lemonahit/spark-train/tree/master/01-Accumulator" target="_blank" rel="noopener">https://github.com/lemonahit/spark-train/tree/master/01-Accumulator</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">/**</span><br><span class="line">  * 使用Spark Accumulators完成Job的数据量处理</span><br><span class="line">  * 统计emp表中NULL出现的次数以及正常数据的条数 &amp; 打印正常数据的信息</span><br><span class="line">  *</span><br><span class="line">  * 若泽数据学员-呼呼呼 on 2017/11/9.</span><br><span class="line">  */</span><br><span class="line">object AccumulatorsApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;AccumulatorsApp&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val lines = sc.textFile(&quot;E:/emp.txt&quot;)</span><br><span class="line">    // long类型的累加器值</span><br><span class="line">    val nullNum = sc.longAccumulator(&quot;NullNumber&quot;)</span><br><span class="line">    val normalData = lines.filter(line =&gt; &#123;</span><br><span class="line">      var flag = true</span><br><span class="line">      val splitLines = line.split(&quot;\t&quot;)</span><br><span class="line">      for (splitLine &lt;- splitLines)&#123;</span><br><span class="line">        if (&quot;&quot;.equals(splitLine))&#123;</span><br><span class="line">          flag = false</span><br><span class="line">          nullNum.add(1)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line">    // 使用cache方法，将RDD的第一次计算结果进行缓存；防止后面RDD进行重复计算，导致累加器的值不准确</span><br><span class="line">    normalData.cache()</span><br><span class="line">    // 打印每一条正常数据</span><br><span class="line">    normalData.foreach(println)</span><br><span class="line">    // 打印正常数据的条数</span><br><span class="line">    println(&quot;NORMAL DATA NUMBER: &quot; + normalData.count())</span><br><span class="line">    // 打印emp表中NULL出现的次数</span><br><span class="line">    println(&quot;NULL: &quot; + nullNum.value)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上海某公司的生产MySQL灾难性挽救</title>
      <link href="/2019/05/30/%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91/"/>
      <url>/2019/05/30/%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h3><p>本人(<a href="www.ruozedata.com">若泽数据</a>J哥)的媳妇，是个漂亮的妹子，同时也是一枚爬虫&amp;Spark开发工程师。</p><p>前天，她的公司MySQL(阿里云ECS服务器)，由于磁盘爆了加上人为的修复，导致各种问题，然后经过2天的折腾，终于公司的大神修复不了了。于是就丢给她了，顺理成章的就丢给我了。我想说，难道J哥这么出名吗？那为了在妹子面前不能丢我们真正大佬的神技，于是乎我就很爽快接了这个MySQL故障恢复，此次故障的是一个数据盘，1T。<br>这时的我，说真的并没有意识到，此事是如此的繁杂，特此写此博文记录一下，毕竟J哥我年纪也大了。</p><p>PS:<br>这里吐槽一下，并没有周日全备+周1~周6增量备份机制哟，不然恢复就爽歪歪了。<br><a id="more"></a></p><h3 id="2-故障现象"><a href="#2-故障现象" class="headerlink" title="2.故障现象"></a>2.故障现象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看表结构、查询表数据都如下抛错:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019530_1.png" alt="enter description here"></p><h3 id="3-尝试修复第一次，失败"><a href="#3-尝试修复第一次，失败" class="headerlink" title="3.尝试修复第一次，失败"></a>3.尝试修复第一次，失败</h3><p>3.1 使用repair命令修复表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; repair table wenshu.wenshu2018;  </span><br><span class="line">错误依旧:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p>3.2 谷歌一篇有指导意义的<br><a href="https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine" target="_blank" rel="noopener">https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine</a></p><ul><li>3.2.1 让其扩容数据磁盘为1.5T，试试，依旧这个错误；</li><li>3.2.2 临时目录修改为大的磁盘空间，试试，依旧这个错误；</li><li>3.2.3 取消磁盘限额，试试，依旧这个错误；</li><li>3.2.4 就是一开始的repair命令修复，试试，依旧这个错误；</li></ul><p>这时的我，也无语了，什么鬼！谷歌一页页搜索验证，没有用！</p><h3 id="4-先部署相同系统的相同版本的机器和MySQL"><a href="#4-先部署相同系统的相同版本的机器和MySQL" class="headerlink" title="4.先部署相同系统的相同版本的机器和MySQL"></a>4.先部署相同系统的相同版本的机器和MySQL</h3><p>于是J哥，快速在【若泽数据】的阿里云账号上买了1台Ubuntu 16.04.6的按量付费机器<br>迅速部署MySQL5.7.26。</p><ul><li>4.1 购买按量付费机器(假如不会购买，找J哥)</li><li>4.2 部署MySQL</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.更新apt-get</span><br><span class="line">$ apt-get update</span><br><span class="line"></span><br><span class="line">b.安装MySQL-Server</span><br><span class="line">$ apt-get install mysql-server</span><br><span class="line"></span><br><span class="line">之后会问你，是否要下载文件， 输入 y 就好了</span><br><span class="line">然后会出现让你设置 root 密码的界面</span><br><span class="line">输入密码: ruozedata123</span><br><span class="line">然后再重复一下，</span><br><span class="line">再次输入密码: ruozedata123</span><br><span class="line"></span><br><span class="line">c.安装MySQL-Client</span><br><span class="line">$ apt install mysql-client</span><br><span class="line"></span><br><span class="line">d.我们可以使用</span><br><span class="line">$ mysql -uroot -pruozedata123</span><br><span class="line">来连接服务器本地的 MySQL</span><br></pre></td></tr></table></figure><h3 id="5-尝试先通过frm文件恢复表结构，失败"><a href="#5-尝试先通过frm文件恢复表结构，失败" class="headerlink" title="5.尝试先通过frm文件恢复表结构，失败"></a>5.尝试先通过frm文件恢复表结构，失败</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a. 建立一个数据库，比如wenshu.</span><br><span class="line"></span><br><span class="line">b. 在ruozedata数据库下建立同名的数据表wenshu2018，表结构随意，这里只有一个id字段，操作过程片段如下：</span><br><span class="line"></span><br><span class="line">mysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------+</span><br><span class="line">| Tables_in_aa |</span><br><span class="line">+--------------+</span><br><span class="line">| wenshu2018   |</span><br><span class="line">+--------------+</span><br><span class="line">1 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; desc wenshu2018;</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| Field | Type       | Null | Key | Default | Extra |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| id    | bigint(20) | NO   |     | NULL    |       |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">c.停止mysql服务器，将wenshu2018.frm文件scp远程拷贝到新的正常数据库的数据目录wenshu下，覆盖掉下边同名的frm文件：</span><br><span class="line"></span><br><span class="line">d.重新启动MYSQL服务</span><br><span class="line"></span><br><span class="line">e.测试下是否恢复成功，进入wenshu数据库，用desc命令测试下，错误为:</span><br><span class="line">mysql Tablespace is missing for table `wenshu`.`wenshu2018`.</span><br></pre></td></tr></table></figure><h3 id="6-尝试有没有备份的表结构恢复数据，失败"><a href="#6-尝试有没有备份的表结构恢复数据，失败" class="headerlink" title="6.尝试有没有备份的表结构恢复数据，失败"></a>6.尝试有没有备份的表结构恢复数据，失败</h3><p>媳妇公司给出一个表结构,如下，经过测试无法恢复，原因就是无法和ibd文件匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS cpws_batch;</span><br><span class="line">CREATE TABLE cpws_batch  (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,</span><br><span class="line">  source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  crawl_time datetime NULL DEFAULT NULL,</span><br><span class="line">  status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id) USING BTREE,</span><br><span class="line">  INDEX ix_status(status) USING BTREE,</span><br><span class="line">  INDEX ix_doc_id(doc_id) USING BTREE</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;</span><br></pre></td></tr></table></figure><h3 id="7-如何获取正确的表结构，这是【成功的第一步】"><a href="#7-如何获取正确的表结构，这是【成功的第一步】" class="headerlink" title="7.如何获取正确的表结构，这是【成功的第一步】"></a>7.如何获取正确的表结构，这是【成功的第一步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s get.dbsake.net &gt; /tmp/dbsake</span><br><span class="line">$ chmod u+x /tmp/dbsake</span><br><span class="line">$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm </span><br><span class="line">--</span><br><span class="line">-- Table structure for table wenshu_0_1000</span><br><span class="line">-- Created with MySQL Version 5.7.25</span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br></pre></td></tr></table></figure><p>对比Step6的表结构，感觉就差分区设置而已，坑！<br>这时，J哥有种信心，恢复应该小菜了。</p><h3 id="8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"><a href="#8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！" class="headerlink" title="8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"></a>8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！</h3><h3 id="9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"><a href="#9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】" class="headerlink" title="9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"></a>9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">9.1首先需要一个跟要恢复的表结构完全一致的表，至关重要</span><br><span class="line">mysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;</span><br><span class="line">USE wenshu;</span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br><span class="line"></span><br><span class="line">9.2然后DISCARD TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.3把要恢复的ibd文件复制到mysql的data文件夹下，修改用户和用户组为mysql</span><br><span class="line">$ scp wenshu2018#P#p*.ibd  新建机器IP:/mnt/mysql_data/wenshu/</span><br><span class="line">$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd</span><br><span class="line"></span><br><span class="line">9.4然后执行IMPORT TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.5等待，有戏，耗时3h，这时我相信应该么问题的</span><br><span class="line"></span><br><span class="line">9.6查询数据，果然恢复有结果，心里暗暗自喜</span><br><span class="line">mysql&gt; select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"><a href="#10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择" class="headerlink" title="10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"></a>10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择</h3><ul><li>10.1 重新购买一台新的服务器，在初始化配置时，就加上1块1.5T的大磁盘。好处是无需挂盘操作，坏处是需要重新做第一个表，浪费3h；</li><li>10.2 购买1.5T的大磁盘，挂载这个机器上。好处是无需再做一次第一个表，坏处是需要修改mysql的数据目录指向为这个大磁盘。系统盘扩容最大也就500G，所以必须外加一个数据盘1.5T容量。</li></ul><p>所以J哥是职场老手了！贼笑！</p><h3 id="11-服务器加数据磁盘，1-5T，购买、挂载、格式化"><a href="#11-服务器加数据磁盘，1-5T，购买、挂载、格式化" class="headerlink" title="11.服务器加数据磁盘，1.5T，购买、挂载、格式化"></a>11.服务器加数据磁盘，1.5T，购买、挂载、格式化</h3><p>接下来的操作是我媳妇独立完成的，这里表扬一下:</p><ul><li>11.1 先买云盘 <a href="https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8</a></li><li>11.2 再挂载云盘 到对应机器 <a href="https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB</a></li><li>11.3 最后Linux格式化数据盘 <a href="https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr</a></li></ul><p>图2所示，df -h命令查看，大磁盘/dev/vdb1<br><img src="/assets/blogImg/2019530_2.png" alt="enter description here"></p><h3 id="12-MySQL修改数据目录为大磁盘，重新启动失败，解决"><a href="#12-MySQL修改数据目录为大磁盘，重新启动失败，解决" class="headerlink" title="12.MySQL修改数据目录为大磁盘，重新启动失败，解决"></a>12.MySQL修改数据目录为大磁盘，重新启动失败，解决</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">12.1 修改数据目录为大磁盘</span><br><span class="line">$ mkdir -p /mnt/mysql_data</span><br><span class="line">$ chown mysql:mysql /mnt/mysql_data</span><br><span class="line">$ vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br><span class="line">datadir         = /mnt/mysql_data</span><br><span class="line"></span><br><span class="line">12.2 无法启动mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">无法启动成功，查看日志</span><br><span class="line">2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().</span><br><span class="line">2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error</span><br><span class="line"></span><br><span class="line">12.3 百思不得其解，CentOS也没有这么麻烦，Ubuntu难道这么搞事吗？</span><br><span class="line">12.4 新增mysqld内容</span><br><span class="line">$ vi /etc/apparmor.d/local/usr.sbin.mysqld</span><br><span class="line"># Site-specific additions and overrides for usr.sbin.mysqld.</span><br><span class="line"># For more details, please see /etc/apparmor.d/local/README.</span><br><span class="line">/mnt/mysql_data/ r,</span><br><span class="line">/mnt/mysql_data/** rwk,</span><br><span class="line"></span><br><span class="line">12.5 reload apparmor的配置并重启</span><br><span class="line">$ service apparmor reload </span><br><span class="line">$ service apparmor restart </span><br><span class="line"> </span><br><span class="line">12.6 重启mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">如果启动不了，查看/var/log/mysql/error.log</span><br><span class="line">如果出现：InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable 仔细核对目录权限</span><br><span class="line"></span><br><span class="line">12.7 进mysql查询数据验证，成功</span><br><span class="line">select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"><a href="#13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。" class="headerlink" title="13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"></a>13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。</h3><h2 id="最后-若泽数据J哥总结一下"><a href="#最后-若泽数据J哥总结一下" class="headerlink" title="最后@若泽数据J哥总结一下:"></a>最后@若泽数据J哥总结一下:</h2><ul><li>表结构正确的获取；</li><li>机器磁盘规划提前思考；</li><li>ibd数据文件恢复；</li><li>最后加上一个聪明的媳妇！(PS:老板会给媳妇涨薪水不🙅‍♂️)</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 其他组件 </category>
          
          <category> 故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>入门Impala只需此篇</title>
      <link href="/2019/05/17/%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)/"/>
      <url>/2019/05/17/%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h2 id="学习路径"><a href="#学习路径" class="headerlink" title="学习路径"></a>学习路径</h2><ul><li>官网：<a href="http://impala.apache.org/" target="_blank" rel="noopener">http://impala.apache.org/</a></li><li>使用手册：<a href="http://impala.apache.org/docs/build/html/index.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/index.html</a></li><li>Sql：<a href="http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html</a></li><li>窗口函数：<a href="http://impala.apache.org/docs/build/html/topics/impala_functions.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_functions.html</a></li><li>基本操作：<a href="http://impala.apache.org/docs/build/html/topics/impala_tutorial.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_tutorial.html</a></li><li>impala-shell：<a href="http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html</a></li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ol><li>Apache Impala是Apache Hadoop的开源原生分析数据库;</li><li>Impala于2017年11月15日从Apache孵化成顶级项目。在以前称为“Cloudera Impala”的文档中，现在的官方名称是“Apache Impala”。</li><li>Impala为Hadoop上的BI /分析查询提供低延迟和高并发性（不是由Apache Hive等批处理框架提供）。即使在多租户环境中，Impala也可以线性扩展。</li><li>利用与Hadoop部署相同的文件和数据格式以及元数据，安全性和资源管理框架 - 无冗余基础架构或数据转换/复制。</li><li>对于Apache Hive用户，Impala使用相同的元数据和ODBC驱动程序。与Hive一样，Impala支持SQL</li><li>Impala与本机Hadoop安全性和Kerberos集成以进行身份验证，通过Sentry模块，您可以确保为正确的用户和应用程序授权使用正确的数据。</li><li>使用Impala，无论是使用SQL查询还是BI应用程序，更多用户都可以通过单个存储库和元数据存储进行交互</li></ol><h2 id="什么是Impala"><a href="#什么是Impala" class="headerlink" title="什么是Impala"></a>什么是Impala</h2><ol><li>Impala是一种面向实时或者面向批处理的框架;</li><li>Impala的数据可以存储在HDFS,HBase和Amazon Simple Storage Servive(S3)中;</li><li>Impala和Hive使用了相同的元数据存储;</li><li>可以通过SQL的语法,JDBC,ODBC和用户界面(Hue中的Impala进行查询);</li></ol><p>我们知道Hive底层是MapReduce,在这里就可以看出区别了,Impala并不是为了替换构建在MapReduce上的批处理框架,就像我们说的Hive,Hive适用于长时间运行的批处理作业,例如涉及到Extract,Transform和Load(ETL)类型的作业.而Impala是进行实时处理的.</p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li>通过sql进行大量数据处理;</li><li>可以进行分布式部署,进行分布式查询;</li><li>可以和不同组件之间进行数据共享,不需要复制或者导入,导出等步骤,例如:可以先使用hive对数据进行ETL操作然后使用Impala进行查询.因为Impala和hive公用同一个元数据,这样就可以方便的对hive生成的数据进行分析.</li></ol><h2 id="Impala如何与Apache-Hadoop一起使用"><a href="#Impala如何与Apache-Hadoop一起使用" class="headerlink" title="Impala如何与Apache Hadoop一起使用"></a>Impala如何与Apache Hadoop一起使用</h2><ul><li><p>Impala解决方案由以下组件组成：</p><ol><li>客户端 - 包括Hue，ODBC客户端，JDBC客户端和Impala Shell的实体都可以与Impala进行交互。这些接口通常用于发出查询或完成管理任务，例如连接到Impala。</li><li>Hive Metastore - 存储有关Impala可用数据的信息。例如，Metastore让Impala知道哪些数据库可用，以及这些数据库的结构是什么。在创建，删除和更改模式对象，将数据加载到表中等等时，通过Impala SQL语句，相关的元数据更改将通过Impala 1.2中引入的专用目录服务自动广播到所有Impala节点。</li><li>Impala - 此过程在DataNodes上运行，协调并执行查询。Impala的每个实例都可以接收，计划和协调来自Impala客户端的查询。</li><li><p>HBase和HDFS -数据的存储。</p><p>下面这幅图应该说的很清楚了:</p></li></ol></li></ul><p><img src="/assets/blogImg/impala.png" alt="enter description here"></p><h2 id="使用Impala执行的查询流程如下："><a href="#使用Impala执行的查询流程如下：" class="headerlink" title="使用Impala执行的查询流程如下："></a>使用Impala执行的查询流程如下：</h2><ul><li>用户应用程序通过ODBC或JDBC向Impala发送SQL查询，这些查询提供标准化的查询接口。用户应用程序可以连接到impalad群集中的任何应用程序。这impalad将成为查询的协调者。</li><li>Impala会解析查询并对其进行分析，以确定impalad整个群集中的实例需要执行哪些任务 。计划执行以实现最佳效率。</li><li>本地impalad实例访问HDFS和HBase等服务以提供数据。</li><li>每个都impalad将数据返回给协调impalad，协调将这些结果发送给客户端。</li></ul><h2 id="impala-shell"><a href="#impala-shell" class="headerlink" title="impala-shell"></a>impala-shell</h2><p>使用Impala shell工具（impala-shell）来设置数据库和表，插入数据和发出查询</p><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>-B or –delimited</td><td>导致使用分隔符分割的普通文本格式打印查询结果。当为其他 Hadoop 组件生成数据时有用。对于避免整齐打印所有输出的性能开销有用，特别是使用查询返回大量的结果集进行基准测试的时候。使用 –output_delimiter 选项指定分隔符。使用 -B 选项常用于保存所有查询结果到文件里而不是打印到屏幕上。在 Impala 1.0.1 中添加</td></tr><tr><td>–print_header</td><td>是否打印列名。整齐打印时是默认启用。同时使用 -B 选项时，在首行打印列名</td></tr><tr><td>-o filename or –output_file filename</td><td>保存所有查询结果到指定的文件。通常用于保存在命令行使用 -q 选项执行单个查询时的查询结果。对交互式会话同样生效；此时你只会看到获取了多少行数据，但看不到实际的数据集。当结合使用 -q 和 -o 选项时，会自动将错误信息输出到 /dev/null(To suppress these incidental messages when combining the -q and -o options, redirect stderr to /dev/null)。在 Impala 1.0.1 中添加</td></tr><tr><td>–output_delimiter=character</td><td>当使用 -B 选项以普通文件格式打印查询结果时，用于指定字段之间的分隔符(Specifies the character to use as a delimiter between fields when query results are printed in plain format by the -B option)。默认是制表符 tab (’\t’)。假如输出结果中包含了分隔符，该列会被引起且/或转义( If an output value contains the delimiter character, that field is quoted and/or escaped)。在 Impala 1.0.1 中添加</td></tr><tr><td>-p or –show_profiles</td><td>对 shell 中执行的每一个查询，显示其查询执行计划 (与 EXPLAIN 语句输出相同) 和发生低级故障(low-level breakdown)的执行步骤的更详细的信息</td></tr><tr><td>-h or –help</td><td>显示帮助信息</td></tr><tr><td>-i hostname or –impalad=hostname</td><td>指定连接运行 impalad 守护进程的主机。默认端口是 21000。你可以连接到集群中运行 impalad 的任意主机。假如你连接到 impalad 实例通过 –fe_port 标志使用了其他端口，则应当同时提供端口号，格式为 hostname:port</td></tr><tr><td>-q query or –query=query</td><td>从命令行中传递一个查询或其他 shell 命令。执行完这一语句后 shell 会立即退出。限制为单条语句，可以是 SELECT, CREATE TABLE, SHOW TABLES, 或其他 impala-shell 认可的语句。因为无法传递 USE 语句再加上其他查询，对于 default 数据库之外的表，应在表名前加上数据库标识符(或者使用 -f 选项传递一个包含 USE 语句和其他查询的文件)</td></tr><tr><td>-f query_file or –query_file=query_file</td><td>传递一个文件中的 SQL 查询。文件内容必须以分号分隔</td></tr><tr><td>-k or –kerberos</td><td>当连接到 impalad 时使用 Kerberos 认证。如果要连接的 impalad 实例不支持 Kerberos，将显示一个错误</td></tr><tr><td>-s kerberos_service_name or –kerberos_service_name=name</td><td>Instructs impala-shell to authenticate to a particular impalad service principal. 如何没有设置 kerberos_service_name ，默认使用 impala。如何启用了本选项，而试图建立不支持Kerberos 的连接时，返回一个错误(If this option is used in conjunction with a connection in which Kerberos is not supported, errors are returned)</td></tr><tr><td>-V or –verbose</td><td>启用详细输出</td></tr><tr><td>–quiet</td><td>关闭详细输出</td></tr><tr><td>-v or –version</td><td>显示版本信息</td></tr><tr><td>-c</td><td>查询执行失败时继续执行</td></tr><tr><td>-r or –refresh_after_connect</td><td>建立连接后刷新 Impala 元数据，与建立连接后执行 REFRESH 语句效果相同</td></tr><tr><td>-d default_db or –database=default_db</td><td>指定启动后使用的数据库，与建立连接后使用 USE 语句选择数据库作用相同，如果没有指定，那么使用 default 数据库</td></tr><tr><td>-l</td><td>启用 LDAP 认证</td></tr><tr><td>-u</td><td>当使用 -l 选项启用 LDAP 认证时，提供用户名(使用短用户名，而不是完整的 LDAP 专有名称(distinguished name)) ，shell 会提示输入密码</td></tr></tbody></table><h2 id="概念与架构"><a href="#概念与架构" class="headerlink" title="概念与架构"></a>概念与架构</h2><h3 id="Impala-Server的组件"><a href="#Impala-Server的组件" class="headerlink" title="Impala Server的组件"></a>Impala Server的组件</h3><p>Impala服务器是分布式，大规模并行处理（MPP）数据库引擎。它由在群集中的特定主机上运行的不同守护程序进程组成。</p><p><strong>The Impala Daemon</strong></p><p>Impala的核心组件是Impala daemon。Impala daemon执行的一些关键功能是：</p><ul><li>读取和写入数据文件。</li><li>接受从impala-shell命令，Hue，JDBC或ODBC传输的查询。</li><li>并行化查询并在群集中分配工作。</li><li>将中间查询结果发送回中央协调器。</li><li>可以通过以下方式之一部署Impala守护程序：<ol><li>HDFS和Impala位于同一位置，每个Impala守护程序与DataNode在同一主机上运行。</li><li>Impala单独部署在计算群集中，可从HDFS，S3，ADLS等远程读取。Impala守护进程与StateStore保持持续通信，以确认哪些守护进程是健康的并且可以接受新工作。</li></ol></li></ul><p><b>在Impala 2.9及更高版本中，您可以控制哪些主机充当查询协调器，哪些主机充当查询执行程序，以提高大型群集上高度并发工作负载的可伸缩性。</b></p><p><strong>Impala Statestore</strong></p><p>Impala Statestore进程检查集群中所有Impala daemon的运行状况，并把信息反馈给Impala daemon进程。您只需要在群集中的一台主机上执行此类过程。如果Impala守护程序由于硬件故障，网络错误，软件问题或其他原因而脱机，则StateStore会通知所有其他Impala daemon程序，以便将来的查询可以避免向无法访问的Impala守护程序发出请求。</p><p>因为StateStore的目的是在出现问题时提供帮助并向协调器广播元数据，因此对Impala集群的正常操作并不总是至关重要的。如果StateStore未运行或无法访问，则在处理Impala已知的数据时，Impala守护程序会像往常一样继续运行和分配工作。如果其他Impala守护程序失败，则群集变得不那么健壮，并且当StateStore脱机时，元数据变得不那么一致。当StateStore重新联机时，它会重新建立与Impala守护程序的通信并恢复其监视和广播功能。</p><p><strong>The Impala Catalog Service</strong></p><p>Impala Catalog Service进程可以把Impala SQL语句中的元数据更改信息反馈到集群中的所有Impala守护程序。只需要在群集中的一台主机上执行此类过程。因为请求是通过StateStore守护程序传递的，所以要在同一主机上运行statestored和catalogd服务。</p><p>当通过Impala发出的语句执行元数据更改时，Impala Catalog Service进程避免了REFRESH和INVALIDATE METADATA语句的使用,该进程可以为我们更新元数据信息。</p><p><strong>使用–load_catalog_in_background选项控制何时加载表的元数据。</strong></p><ul><li>如果设置为false，则在第一次引用表时会加载表的元数据。这意味着第一次运行可能比后续运行慢。在impala2.2开始，默认load_catalog_in_background是 false。</li><li>如果设置为true，即使没有查询需要该元数据，目录服务也会尝试加载表的元数据。因此，当运行需要它的第一个查询时，可能已经加载了元数据。但是，由于以下原因，我们建议不要将选项设置为true。</li></ul><p>后台加载可能会干扰查询特定的元数据加载。这可能在启动时或在使元数据无效之后发生，持续时间取决于元数据的数量，并且可能导致看似随机的长时间运行的查询难以诊断。</p><p>Impala可能会加载从未使用过的表的元数据，这会增加目录服务和Impala守护程序的目录大小，从而增加内存使用量。</p><p>负载均衡和高可用性的大多数注意事项适用于impalad守护程序。该statestored和catalogd守护进程不具备高可用性的特殊要求，因为这些守护进程的问题不会造成数据丢失。如果这些守护程序由于特定主机上的中断而变得不可用，则可以停止Impala服务，删除Impala StateStore和Impala目录服务器角色，在其他主机上添加角色，然后重新启动Impala服务。</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Impala支持一组数据类型，可用于表列，表达式值，函数参数和返回值。</p><p>注意： 目前，Impala仅支持标量类型，而不支持复合类型或嵌套类型。访问包含任何具有不受支持类型的列的表会导致错误。</p><p>有关Impala和Hive数据类型之间的差异，请参阅:<br><a href="http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_delta" target="_blank" rel="noopener">http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_delta</a></p><ul><li>ARRAY复杂类型（仅限Impala 2.3或更高版本）</li><li>BIGINT数据类型</li><li>BOOLEAN数据类型</li><li>CHAR数据类型（仅限Impala 2.0或更高版本）</li><li>DECIMAL数据类型（仅限Impala 3.0或更高版本）</li><li>双数据类型</li><li>FLOAT数据类型</li><li>INT数据类型</li><li>MAP复杂类型（仅限Impala 2.3或更高版本）</li><li>REAL数据类型</li><li>SMALLINT数据类型</li><li>STRING数据类型</li><li>STRUCT复杂类型（仅限Impala 2.3或更高版本）</li><li>TIMESTAMP数据类型</li><li>TINYINT数据类型</li><li>VARCHAR数据类型（仅限Impala 2.0或更高版本）</li><li>复杂类型（仅限Impala 2.3或更高版本）</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch常用操作解析</title>
      <link href="/2019/05/13/Elasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/05/13/Elasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="创建Maven管理的Java项目"><a href="#创建Maven管理的Java项目" class="headerlink" title="创建Maven管理的Java项目"></a>创建Maven管理的Java项目</h3><p>在pom.xml中添加依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;es.version&gt;6.1.1&lt;/es.version&gt;</span><br><span class="line"> </span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;transport&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;es.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>然后创建一个单元测试类ESApp：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private TransportClient client;</span><br><span class="line"> </span><br><span class="line">    @Before</span><br><span class="line">    public void setUp() throws Exception &#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;mycluster&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, &quot;true&quot;)//增加自动嗅探配置</span><br><span class="line">                .build();</span><br><span class="line"> </span><br><span class="line">        client = new PreBuiltTransportClient(settings);</span><br><span class="line">        client.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;10.8.24.94&quot;), 9300));</span><br><span class="line"> </span><br><span class="line">        System.out.println(client.toString());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行后报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonFactory</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>运行后成功拿到ES的client：</p><p><img src="/assets/blogImg/es.png" alt="enter description here"></p><h3 id="创建一个Index"><a href="#创建一个Index" class="headerlink" title="创建一个Index"></a>创建一个Index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@Test</span><br><span class="line">    public void createIndex() &#123;</span><br><span class="line">        client.admin().indices().prepareCreate(INDEX).get();</span><br><span class="line">        System.out.println(&quot;创建Index成功&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="删除一个Index"><a href="#删除一个Index" class="headerlink" title="删除一个Index"></a>删除一个Index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void deleteIndex() &#123;</span><br><span class="line">        client.admin().indices().prepareDelete(INDEX).get();</span><br><span class="line">        System.out.println(&quot;删除Index成功&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="放入数据的三种方式"><a href="#放入数据的三种方式" class="headerlink" title="放入数据的三种方式"></a>放入数据的三种方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">//不推荐使用，太繁琐拼json格式</span><br><span class="line"> @Test</span><br><span class="line">    public void createDoc() &#123;</span><br><span class="line">        String json = &quot;&#123;\&quot;name\&quot;:\&quot;若泽数据\&quot;&#125;&quot;;</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;100&quot;)</span><br><span class="line">                .setSource(json, XContentType.JSON)</span><br><span class="line">                .get();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    //推荐使用</span><br><span class="line">    @Test</span><br><span class="line">    public void test01() throws Exception &#123;</span><br><span class="line">        Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;();</span><br><span class="line">        json.put(&quot;name&quot;, &quot;ruozedata&quot;);</span><br><span class="line">        json.put(&quot;message&quot;, &quot;trying out Elasticsearch&quot;);</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;101&quot;).setSource(json).get();</span><br><span class="line">        System.out.println(response.getVersion());</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">//推荐使用</span><br><span class="line">    @Test</span><br><span class="line">    public void test02() throws Exception &#123;</span><br><span class="line"> </span><br><span class="line">        XContentBuilder builder = jsonBuilder()</span><br><span class="line">                .startObject()</span><br><span class="line">                .field(&quot;user&quot;, &quot;ruoze&quot;)</span><br><span class="line">                .field(&quot;postDate&quot;, new Date())</span><br><span class="line">                .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;)</span><br><span class="line">                .endObject();</span><br><span class="line"> </span><br><span class="line">        IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;102&quot;).setSource(builder).get();</span><br><span class="line">        System.out.println(response.getVersion());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="拿到一条数据"><a href="#拿到一条数据" class="headerlink" title="拿到一条数据"></a>拿到一条数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@Test</span><br><span class="line">    public void getDoc() &#123;</span><br><span class="line">        GetResponse response = client.prepareGet(INDEX, TYPE, &quot;100&quot;).get();</span><br><span class="line">        System.out.println(response.getSourceAsString());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="拿到多条数据"><a href="#拿到多条数据" class="headerlink" title="拿到多条数据"></a>拿到多条数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void getDocsByIds() &#123;</span><br><span class="line"> </span><br><span class="line">        MultiGetResponse responses = client.prepareMultiGet()</span><br><span class="line">                .add(INDEX, TYPE,&quot;100&quot;)</span><br><span class="line">                .add(INDEX, TYPE, &quot;101&quot;, &quot;102&quot;, &quot;1000&quot;)</span><br><span class="line">                .get();</span><br><span class="line"> </span><br><span class="line">        for (MultiGetItemResponse response : responses) &#123;</span><br><span class="line">            GetResponse res = response.getResponse();</span><br><span class="line">            if (res.isExists()) &#123;</span><br><span class="line">                System.out.println(res);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                System.out.println(&quot;没有这条数据&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</title>
      <link href="/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/"/>
      <url>/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h2 id="若泽数据"><a href="#若泽数据" class="headerlink" title="若泽数据"></a><a href="www.ruozedata.com">若泽数据</a></h2><h2 id="CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践"><a href="#CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践" class="headerlink" title="CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)"></a>CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</h2><p>视频:<a href="https://www.bilibili.com/video/av52167219" target="_blank" rel="noopener">https://www.bilibili.com/video/av52167219</a><br>PS:建议先看课程视频1-2篇，再根据视频或文档部署，<br>如有问题，及时与@若泽数据J哥联系。</p><a id="more"></a><hr><h2 id="一-准备工作"><a href="#一-准备工作" class="headerlink" title="一.准备工作"></a>一.准备工作</h2><h4 id="1-离线部署主要分为三块"><a href="#1-离线部署主要分为三块" class="headerlink" title="1.离线部署主要分为三块:"></a>1.离线部署主要分为三块:</h4><p>a.MySQL离线部署<br>b.CM离线部署<br>c.Parcel文件离线源部署</p><h4 id="2-规划"><a href="#2-规划" class="headerlink" title="2.规划:"></a>2.规划:</h4><table><thead><tr><th>节点</th><th>MySQL部署组件</th><th>Parcel文件离线源</th><th>CM服务进程</th><th>大数据组件</th></tr></thead><tbody><tr><td>hadoop001</td><td>MySQL</td><td>Parcel</td><td>Activity Monitor<br></td><td>NN RM DN NM</td></tr><tr><td>hadoop002</td><td></td><td></td><td>Alert Publisher<br>Event Server</td><td>DN NM</td></tr><tr><td>hadoop003</td><td></td><td></td><td>Host Monitor<br>Service Monitor</td><td>DN NM</td></tr></tbody></table><h3 id="3-下载源"><a href="#3-下载源" class="headerlink" title="3.下载源:"></a>3.下载源:</h3><ul><li>CM<br><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz" target="_blank" rel="noopener">cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</a></li><li>Parcel<br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json" target="_blank" rel="noopener">manifest.json</a></li><li><p>JDK<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a><br>下载jdk-8u202-linux-x64.tar.gz</p></li><li><p>MySQL<br><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a><br>下载mysql-5.7.26-el7-x86_64.tar.gz</p></li><li><p>MySQL jdbc jar<br><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.47.jar</a><br>下载完成后要重命名去掉版本号，<br>mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar</p></li></ul><hr><p>###准备好百度云,下载安装包:<br>链接:<a href="https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA" target="_blank" rel="noopener">https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA</a> 密码:viqp</p><h2 id="二-集群节点初始化"><a href="#二-集群节点初始化" class="headerlink" title="二.集群节点初始化"></a>二.集群节点初始化</h2><h3 id="1-阿里云上海区购买3台，按量付费虚拟机"><a href="#1-阿里云上海区购买3台，按量付费虚拟机" class="headerlink" title="1.阿里云上海区购买3台，按量付费虚拟机"></a>1.阿里云上海区购买3台，按量付费虚拟机</h3><p>CentOS7.2操作系统，2核8G最低配置</p><h3 id="2-当前笔记本或台式机配置hosts文件"><a href="#2-当前笔记本或台式机配置hosts文件" class="headerlink" title="2.当前笔记本或台式机配置hosts文件"></a>2.当前笔记本或台式机配置hosts文件</h3><ul><li>MAC: /etc/hosts</li><li>Window: C:\windows\system32\drivers\etc\hosts</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">公网地址: </span><br><span class="line">106.15.234.222 hadoop001  </span><br><span class="line">106.15.235.200 hadoop002  </span><br><span class="line">106.15.234.239 hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-设置所有节点的hosts文件"><a href="#3-设置所有节点的hosts文件" class="headerlink" title="3.设置所有节点的hosts文件"></a>3.设置所有节点的hosts文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">私有地铁、内网地址:</span><br><span class="line">echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="4-关闭所有节点的防火墙及清空规则"><a href="#4-关闭所有节点的防火墙及清空规则" class="headerlink" title="4.关闭所有节点的防火墙及清空规则"></a>4.关闭所有节点的防火墙及清空规则</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure><h3 id="5-关闭所有节点的selinux"><a href="#5-关闭所有节点的selinux" class="headerlink" title="5.关闭所有节点的selinux"></a>5.关闭所有节点的selinux</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">将SELINUX=enforcing改为SELINUX=disabled </span><br><span class="line">设置后需要重启才能生效</span><br></pre></td></tr></table></figure><h3 id="6-设置所有节点的时区一致及时钟同步"><a href="#6-设置所有节点的时区一致及时钟同步" class="headerlink" title="6.设置所有节点的时区一致及时钟同步"></a>6.设置所有节点的时区一致及时钟同步</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">6.1.时区</span><br><span class="line">[root@hadoop001 ~]# date</span><br><span class="line">Sat May 11 10:07:53 CST 2019</span><br><span class="line">[root@hadoop001 ~]# timedatectl</span><br><span class="line">      Local time: Sat 2019-05-11 10:10:31 CST</span><br><span class="line">  Universal time: Sat 2019-05-11 02:10:31 UTC</span><br><span class="line">        RTC time: Sat 2019-05-11 10:10:29</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br><span class="line">     NTP enabled: yes</span><br><span class="line">NTP synchronized: yes</span><br><span class="line"> RTC in local TZ: yes</span><br><span class="line">      DST active: n/a</span><br><span class="line"></span><br><span class="line">#查看命令帮助，学习至关重要，无需百度，太👎</span><br><span class="line">[root@hadoop001 ~]# timedatectl --help</span><br><span class="line">timedatectl [OPTIONS...] COMMAND ...</span><br><span class="line"></span><br><span class="line">Query or change system time and date settings.</span><br><span class="line"></span><br><span class="line">  -h --help                Show this help message</span><br><span class="line">     --version             Show package version</span><br><span class="line">     --no-pager            Do not pipe output into a pager</span><br><span class="line">     --no-ask-password     Do not prompt for password</span><br><span class="line">  -H --host=[USER@]HOST    Operate on remote host</span><br><span class="line">  -M --machine=CONTAINER   Operate on local container</span><br><span class="line">     --adjust-system-clock Adjust system clock when changing local RTC mode</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  status                   Show current time settings</span><br><span class="line">  set-time TIME            Set system time</span><br><span class="line">  set-timezone ZONE        Set system time zone</span><br><span class="line">  list-timezones           Show known time zones</span><br><span class="line">  set-local-rtc BOOL       Control whether RTC is in local time</span><br><span class="line">  set-ntp BOOL             Control whether NTP is enabled</span><br><span class="line"></span><br><span class="line">#查看哪些时区</span><br><span class="line">[root@hadoop001 ~]# timedatectl list-timezones</span><br><span class="line">Africa/Abidjan</span><br><span class="line">Africa/Accra</span><br><span class="line">Africa/Addis_Ababa</span><br><span class="line">Africa/Algiers</span><br><span class="line">Africa/Asmara</span><br><span class="line">Africa/Bamako</span><br><span class="line"></span><br><span class="line">#所有节点设置亚洲上海时区 </span><br><span class="line">[root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">6.2.时间</span><br><span class="line">#所有节点安装ntp</span><br><span class="line">[root@hadoop001 ~]# yum install -y ntp</span><br><span class="line"></span><br><span class="line">#选取hadoop001为ntp的主节点</span><br><span class="line">[root@hadoop001 ~]# vi /etc/ntp.conf </span><br><span class="line"></span><br><span class="line">#time</span><br><span class="line">server 0.asia.pool.ntp.org</span><br><span class="line">server 1.asia.pool.ntp.org</span><br><span class="line">server 2.asia.pool.ntp.org</span><br><span class="line">server 3.asia.pool.ntp.org</span><br><span class="line">#当外部时间不可用时，可使用本地硬件时间</span><br><span class="line">server 127.127.1.0 iburst local clock </span><br><span class="line">#允许哪些网段的机器来同步时间</span><br><span class="line">restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#开启ntpd及查看状态</span><br><span class="line">[root@hadoop001 ~]# systemctl start ntpd</span><br><span class="line">[root@hadoop001 ~]# systemctl status ntpd</span><br><span class="line"> ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago</span><br><span class="line"> Main PID: 18518 (ntpd)</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           └─18518 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line"></span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usec</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabled</span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.</span><br><span class="line"></span><br><span class="line">#验证</span><br><span class="line">[root@hadoop001 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> LOCAL(0)        .LOCL.          10 l  726   64    0    0.000    0.000   0.000</span><br><span class="line"></span><br><span class="line">#其他从节点停止禁用ntpd服务 </span><br><span class="line">[root@hadoop002 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop002 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop002 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001  </span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop004 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop003 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001</span><br></pre></td></tr></table></figure><h3 id="7-部署集群的JDK"><a href="#7-部署集群的JDK" class="headerlink" title="7.部署集群的JDK"></a>7.部署集群的JDK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/</span><br><span class="line">#切记必须修正所属用户及用户组</span><br><span class="line">chown -R root:root /usr/java/jdk1.8.0_45</span><br><span class="line"></span><br><span class="line">echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure><h3 id="8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档"><a href="#8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档" class="headerlink" title="8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)"></a>8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)</h3><ul><li>文档链接:<a href="https://github.com/Hackeruncle/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL</a></li><li>视频链接:<a href="https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ</a> 密码:whic</li></ul><h3 id="9-创建CDH的元数据库和用户、amon服务的数据库及用户"><a href="#9-创建CDH的元数据库和用户、amon服务的数据库及用户" class="headerlink" title="9.创建CDH的元数据库和用户、amon服务的数据库及用户"></a>9.创建CDH的元数据库和用户、amon服务的数据库及用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create database cmf DEFAULT CHARACTER SET utf8;</span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="10-hadoop001节点部署mysql-jdbc-jar"><a href="#10-hadoop001节点部署mysql-jdbc-jar" class="headerlink" title="10.hadoop001节点部署mysql jdbc jar"></a>10.hadoop001节点部署mysql jdbc jar</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java.jar /usr/share/java/</span><br></pre></td></tr></table></figure><h2 id="三-CDH部署"><a href="#三-CDH部署" class="headerlink" title="三.CDH部署"></a>三.CDH部署</h2><h3 id="1-离线部署cm-server及agent"><a href="#1-离线部署cm-server及agent" class="headerlink" title="1.离线部署cm server及agent"></a>1.离线部署cm server及agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1.1.所有节点创建目录及解压</span><br><span class="line">mkdir /opt/cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/</span><br><span class="line"></span><br><span class="line">1.2.所有节点修改agent的配置，指向server的节点hadoop001</span><br><span class="line">sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">1.3.主节点修改server的配置:</span><br><span class="line">vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties </span><br><span class="line">com.cloudera.cmf.db.type=mysql</span><br><span class="line">com.cloudera.cmf.db.host=hadoop001</span><br><span class="line">com.cloudera.cmf.db.name=cmf</span><br><span class="line">com.cloudera.cmf.db.user=cmf</span><br><span class="line">com.cloudera.cmf.db.password=Ruozedata123456!</span><br><span class="line">com.cloudera.cmf.db.setupType=EXTERNAL</span><br><span class="line"></span><br><span class="line">1.4.所有节点创建用户</span><br><span class="line">useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line"></span><br><span class="line">1.5.目录修改用户及用户组</span><br><span class="line">chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager</span><br></pre></td></tr></table></figure><h3 id="2-hadoop001节点部署离线parcel源"><a href="#2-hadoop001节点部署离线parcel源" class="headerlink" title="2.hadoop001节点部署离线parcel源"></a>2.hadoop001节点部署离线parcel源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2.1.部署离线parcel源</span><br><span class="line">$ mkdir -p /opt/cloudera/parcel-repo</span><br><span class="line">$ ll</span><br><span class="line">total 3081664</span><br><span class="line">-rw-r--r-- 1 root root 2127506677 May  9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</span><br><span class="line">-rw-r--r-- 1 root root         41 May  9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</span><br><span class="line">-rw-r--r-- 1 root root  841524318 May  9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  185515842 Aug 10  2017 jdk-8u144-linux-x64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root      66538 May  9 18:03 manifest.json</span><br><span class="line">-rw-r--r-- 1 root root     989495 May 25  2017 mysql-connector-java.jar</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">#切记cp时，重命名去掉1，不然在部署过程CM认为如上文件下载未完整，会持续下载</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha</span><br><span class="line">$ cp manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">2.2.目录修改用户及用户组</span><br><span class="line">$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/</span><br></pre></td></tr></table></figure><h3 id="3-所有节点创建软件安装目录、用户及用户组权限"><a href="#3-所有节点创建软件安装目录、用户及用户组权限" class="headerlink" title="3.所有节点创建软件安装目录、用户及用户组权限"></a>3.所有节点创建软件安装目录、用户及用户组权限</h3><p>mkdir -p /opt/cloudera/parcels<br>chown -R cloudera-scm:cloudera-scm /opt/cloudera/</p><h3 id="4-hadoop001节点启动Server"><a href="#4-hadoop001节点启动Server" class="headerlink" title="4.hadoop001节点启动Server"></a>4.hadoop001节点启动Server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.1.启动server</span><br><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start</span><br><span class="line"></span><br><span class="line">4.2.阿里云web界面，设置该hadoop001节点防火墙放开7180端口</span><br><span class="line">4.3.等待1min，打开 http://hadoop001:7180 账号密码:admin/admin</span><br><span class="line">4.4.假如打不开，去看server的log，根据错误仔细排查错误</span><br></pre></td></tr></table></figure><h3 id="5-所有节点启动Agent"><a href="#5-所有节点启动Agent" class="headerlink" title="5.所有节点启动Agent"></a>5.所有节点启动Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h3 id="6-接下来，全部Web界面操作"><a href="#6-接下来，全部Web界面操作" class="headerlink" title="6.接下来，全部Web界面操作"></a>6.接下来，全部Web界面操作</h3><p><a href="http://hadoop001:7180/" target="_blank" rel="noopener">http://hadoop001:7180/</a><br>账号密码:admin/admin</p><h3 id="7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选"><a href="#7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选" class="headerlink" title="7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选"></a>7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选</h3><p><img src="/assets/blogImg/CDH516_1.png" alt="enter description here"></p><h3 id="8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本"><a href="#8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本" class="headerlink" title="8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本"></a>8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本</h3><p><img src="/assets/blogImg/CDH516_2.png" alt="enter description here"></p><h3 id="9-感谢您选择Cloudera-Manager和CDH"><a href="#9-感谢您选择Cloudera-Manager和CDH" class="headerlink" title="9.感谢您选择Cloudera Manager和CDH"></a>9.感谢您选择Cloudera Manager和CDH</h3><p><img src="/assets/blogImg/CDH516_3.png" alt="enter description here"></p><h3 id="10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选"><a href="#10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选" class="headerlink" title="10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选"></a>10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选</h3><p><img src="/assets/blogImg/CDH516_4.png" alt="enter description here"></p><h3 id="11-选择存储库"><a href="#11-选择存储库" class="headerlink" title="11.选择存储库"></a>11.选择存储库</h3><p><img src="/assets/blogImg/CDH516_5.png" alt="enter description here"></p><h3 id="12-集群安装–正在安装选定Parcel假如"><a href="#12-集群安装–正在安装选定Parcel假如" class="headerlink" title="12.集群安装–正在安装选定Parcel假如"></a>12.集群安装–正在安装选定Parcel假如</h3><p>本地parcel离线源配置正确，则”下载”阶段瞬间完成，其余阶段视节点数与内部网络情况决定。<br><img src="/assets/blogImg/CDH516_6.png" alt="enter description here"></p><h3 id="13-检查主机正确性"><a href="#13-检查主机正确性" class="headerlink" title="13.检查主机正确性"></a>13.检查主机正确性</h3><p><img src="/assets/blogImg/CDH516_7.png" alt="enter description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">13.1.建议将/proc/sys/vm/swappiness设置为最大值10。</span><br><span class="line">swappiness值控制操作系统尝试交换内存的积极；</span><br><span class="line">swappiness=0：表示最大限度使用物理内存，之后才是swap空间；</span><br><span class="line">swappiness=100：表示积极使用swap分区，并且把内存上的数据及时搬迁到swap空间；</span><br><span class="line">如果是混合服务器，不建议完全禁用swap，可以尝试降低swappiness。</span><br><span class="line"></span><br><span class="line">临时调整：</span><br><span class="line">sysctl vm.swappiness=10</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># Adjust swappiness value</span><br><span class="line">vm.swappiness=10</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">13.2.已启用透明大页面压缩，可能会导致重大性能问题，建议禁用此设置。</span><br><span class="line">临时调整：</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local</span><br><span class="line"># Disable transparent_hugepage</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># centos7.x系统，需要为&quot;/etc/rc.d/rc.local&quot;文件赋予执行权限</span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务"><a href="#14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务" class="headerlink" title="14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务"></a>14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务</h3><p><img src="/assets/blogImg/CDH516_8.png" alt="enter description here"></p><h3 id="15-自定义角色分配"><a href="#15-自定义角色分配" class="headerlink" title="15.自定义角色分配"></a>15.自定义角色分配</h3><p><img src="/assets/blogImg/CDH516_9.png" alt="enter description here"></p><h3 id="16-数据库设置"><a href="#16-数据库设置" class="headerlink" title="16.数据库设置"></a>16.数据库设置</h3><p><img src="/assets/blogImg/CDH516_10.png" alt="enter description here"></p><h3 id="17-审改设置，默认即可"><a href="#17-审改设置，默认即可" class="headerlink" title="17.审改设置，默认即可"></a>17.审改设置，默认即可</h3><p><img src="/assets/blogImg/CDH516_11.png" alt="enter description here"></p><h3 id="18-首次运行"><a href="#18-首次运行" class="headerlink" title="18.首次运行"></a>18.首次运行</h3><p><img src="/assets/blogImg/CDH516_12.png" alt="enter description here"></p><h3 id="19-恭喜您"><a href="#19-恭喜您" class="headerlink" title="19.恭喜您!"></a>19.恭喜您!</h3><p><img src="/assets/blogImg/CDH516_13.png" alt="enter description here"></p><h3 id="20-主页"><a href="#20-主页" class="headerlink" title="20.主页"></a>20.主页</h3><p><img src="/assets/blogImg/CDH516_14.png" alt="enter description here"></p><hr><h3 id="CDH全套课程目录，如有buy，加微信-ruoze-star"><a href="#CDH全套课程目录，如有buy，加微信-ruoze-star" class="headerlink" title="CDH全套课程目录，如有buy，加微信(ruoze_star)"></a>CDH全套课程目录，如有buy，加微信(ruoze_star)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">0.青云环境介绍和使用 </span><br><span class="line">1.Preparation        </span><br><span class="line">谈谈怎样入门大数据 </span><br><span class="line">谈谈怎样做好一个大数据平台的运营工作 </span><br><span class="line">Linux机器,各软件版本介绍及安装(录播) </span><br><span class="line">2.Introduction      </span><br><span class="line">Cloudera、CM及CDH介绍 </span><br><span class="line">CDH版本选择 </span><br><span class="line">CDH安装几种方式解读 </span><br><span class="line">3.Install&amp;UnInstall  </span><br><span class="line">集群节点规划,环境准备(NTP,Jdk and etc) </span><br><span class="line">MySQL编译安装及常用命令 </span><br><span class="line">推荐:CDH离线安装(踩坑心得,全面剖析) </span><br><span class="line">解读暴力卸载脚本 </span><br><span class="line"></span><br><span class="line">4.CDH Management      </span><br><span class="line">CDH体系架构剖析 </span><br><span class="line">CDH配置文件深度解析 </span><br><span class="line">CM的常用命令 </span><br><span class="line">CDH集群正确启动和停止顺序 </span><br><span class="line">CDH Tsquery Language </span><br><span class="line">CDH常规管理(监控/预警/配置/资源/日志/安全) </span><br><span class="line"></span><br><span class="line">5.Maintenance Experiment  </span><br><span class="line">HDFS HA 配置 及hadoop/hdfs常规命令 </span><br><span class="line">Yarn HA 配置 及yarn常规命令 </span><br><span class="line">Other CDH Components HA 配置 </span><br><span class="line">CDH动态添加删除服务(hive/spark/hbase) </span><br><span class="line">CDH动态添加删除机器 </span><br><span class="line">CDH动态添加删除及迁移DataNode进程等 </span><br><span class="line">CDH升级(5.10.0--&gt;5.12.0) </span><br><span class="line"></span><br><span class="line">6.Resource Management    </span><br><span class="line">Linux Cgroups </span><br><span class="line">静态资源池 </span><br><span class="line">动态资源池 </span><br><span class="line">多租户案例 </span><br><span class="line"></span><br><span class="line">7.Performance Tunning    </span><br><span class="line">Memory/CPU/Network/Disk及集群规划 </span><br><span class="line">Linux参数 </span><br><span class="line">HDFS参数 </span><br><span class="line">MapReduce及Yarn参数 </span><br><span class="line">其他服务参数 </span><br><span class="line"></span><br><span class="line">8.Cases Share </span><br><span class="line">CDH4&amp;5之Alternatives命令 的研究 </span><br><span class="line">CDH5.8.2安装之Hash verification failed </span><br><span class="line">记录一次CDH4.8.6 配置HDFS HA 坑 </span><br><span class="line">CDH5.0集群IP更改 </span><br><span class="line">CDH的active namenode exit(GC)和彩蛋分享 </span><br><span class="line"></span><br><span class="line">9. Kerberos</span><br><span class="line">Kerberos简介</span><br><span class="line">Kerberos体系结构</span><br><span class="line">Kerberos工作机制</span><br><span class="line">Kerberos安装部署</span><br><span class="line">CDH启用kerberos</span><br><span class="line">Kerberos开发使用(真实代码)</span><br><span class="line"></span><br><span class="line">10.Summary         </span><br><span class="line">总结</span><br></pre></td></tr></table></figure><hr><h4 id="Join-us-if-you-have-a-dream"><a href="#Join-us-if-you-have-a-dream" class="headerlink" title="Join us if you have a dream."></a>Join us if you have a dream.</h4><h5 id="若泽数据官网-http-ruozedata-com"><a href="#若泽数据官网-http-ruozedata-com" class="headerlink" title="若泽数据官网: http://ruozedata.com"></a>若泽数据官网: <a href="http://ruozedata.com" target="_blank" rel="noopener">http://ruozedata.com</a></h5><h5 id="腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com"><a href="#腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com" class="headerlink" title="腾讯课堂，搜若泽数据: http://ruoze.ke.qq.com"></a>腾讯课堂，搜若泽数据: <a href="http://ruoze.ke.qq.com" target="_blank" rel="noopener">http://ruoze.ke.qq.com</a></h5><h5 id="Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323"><a href="#Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323" class="headerlink" title="Bilibili网站,搜若泽数据: https://space.bilibili.com/356836323"></a>Bilibili网站,搜若泽数据: <a href="https://space.bilibili.com/356836323" target="_blank" rel="noopener">https://space.bilibili.com/356836323</a></h5><h5 id="若泽大数据–官方博客"><a href="#若泽大数据–官方博客" class="headerlink" title="若泽大数据–官方博客"></a><a href="https://ruozedata.github.io" target="_blank" rel="noopener">若泽大数据–官方博客</a></h5><h5 id="若泽大数据–博客一览"><a href="#若泽大数据–博客一览" class="headerlink" title="若泽大数据–博客一览"></a><a href="https://github.com/ruozedata/BigData/blob/master/blog/BigDataBlogOverview.md" target="_blank" rel="noopener">若泽大数据–博客一览</a></h5><h5 id="若泽大数据–内部学员面试题"><a href="#若泽大数据–内部学员面试题" class="headerlink" title="若泽大数据–内部学员面试题"></a><a href="https://github.com/ruozedata/BigData/blob/master/interview/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.md" target="_blank" rel="noopener">若泽大数据–内部学员面试题</a></h5><h5 id="扫一扫，学一学"><a href="#扫一扫，学一学" class="headerlink" title="扫一扫，学一学:"></a>扫一扫，学一学:</h5><p><img src="//yoursite.com/2019/05/13/若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)/若泽数据--扫描入口.png" alt="avatar"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cdh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之三(自定义累加器)</title>
      <link href="/2019/05/10/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)/"/>
      <url>/2019/05/10/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h2 id="思路-amp-需求"><a href="#思路-amp-需求" class="headerlink" title="思路 &amp; 需求"></a>思路 &amp; 需求</h2><p>参考IntAccumulatorParam的实现思路（上述文章中有讲）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    // addInPlace有很多具体的实现类</span><br><span class="line">    // 如果想要实现自定义的话，就得实现这个方法</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>自定义也可以通过这个方法去实现，从而兼容我们自定义的累加器</p><h2 id="需求：这里实现一个简单的案例，用分布式的方法去实现随机数"><a href="#需求：这里实现一个简单的案例，用分布式的方法去实现随机数" class="headerlink" title="需求：这里实现一个简单的案例，用分布式的方法去实现随机数"></a>需求：这里实现一个简单的案例，用分布式的方法去实现随机数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">**</span><br><span class="line">  * 自定义的AccumulatorParam</span><br><span class="line">  *</span><br><span class="line">  * Created by lemon on 2018/7/28.</span><br><span class="line">  */</span><br><span class="line">object UniqueKeyAccumulator extends AccumulatorParam[Map[Int, Int]] &#123;</span><br><span class="line">  override def addInPlace(r1: Map[Int, Int], r2: Map[Int, Int]): Map[Int, Int] = &#123;</span><br><span class="line">      // ++用于两个集合相加</span><br><span class="line">      r1++r2</span><br><span class="line">    &#125;</span><br><span class="line">    override def zero(initialValue: Map[Int, Int]): Map[Int, Int] = &#123;</span><br><span class="line">      var data: Map[Int, Int] = Map()</span><br><span class="line">      data</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">  * 使用自定义的累加器，实现随机数</span><br><span class="line">  *</span><br><span class="line">  * Created by lemon on 2018/7/28.</span><br><span class="line">  */</span><br><span class="line">object CustomAccumulator &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;CustomAccumulator&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    val uniqueKeyAccumulator = sc.accumulable(Map[Int, Int]())(UniqueKeyAccumulator)</span><br><span class="line">    val distData = sc.parallelize(1 to 10)</span><br><span class="line">    val mapCount = distData.map(x =&gt; &#123;</span><br><span class="line">      val randomNum = new Random().nextInt(20)</span><br><span class="line">      // 构造一个k-v对</span><br><span class="line">      val map: Map[Int, Int] = Map[Int, Int](randomNum -&gt; randomNum)</span><br><span class="line">      uniqueKeyAccumulator += map</span><br><span class="line">    &#125;)</span><br><span class="line">    println(mapCount.count())</span><br><span class="line">    // 获取到累加器的值 中的key值，并进行打印</span><br><span class="line">    uniqueKeyAccumulator.value.keys.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图：<br><img src="/assets/blogImg/Spark累加器简单案例.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker常用命令以及安装mysql</title>
      <link href="/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/"/>
      <url>/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>Docker是一个开源的应用容器引擎；是一个轻量级容器技术；</p><p>Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；</p><p>运行中的这个镜像称为容器，容器启动是非常快速的。<br><a id="more"></a></p><h3 id="2-核心概念"><a href="#2-核心概念" class="headerlink" title="2.核心概念"></a>2.核心概念</h3><p>docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；</p><p>docker客户端(Client)：连接docker主机进行操作；</p><p>docker仓库(Registry)：用来保存各种打包好的软件镜像；</p><p>docker镜像(Images)：软件打包好的镜像；放在docker仓库中；</p><p>docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用</p><h3 id="3-安装环境"><a href="#3-安装环境" class="headerlink" title="3.安装环境"></a>3.安装环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VM ware Workstation10</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">uname -r</span><br><span class="line">3.10.0-862.el7.x86_64</span><br></pre></td></tr></table></figure><p><strong>检查内核版本，必须是3.10及以上</strong> 查看命令：uname -r</p><h3 id="4-在linux虚拟机上安装docker"><a href="#4-在linux虚拟机上安装docker" class="headerlink" title="4.在linux虚拟机上安装docker"></a>4.在linux虚拟机上安装docker</h3><p>步骤：</p><p>1、检查内核版本，必须是3.10及以上<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -r</span><br></pre></td></tr></table></figure><p></p><p>2、安装docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker</span><br></pre></td></tr></table></figure><p></p><p>3、输入y确认安装<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Dependency Updated:</span><br><span class="line">  audit.x86_64 0:2.8.1-3.el7_5.1                                  audit-libs.x86_64 0:2.8.1-3.el7_5.1                                 </span><br><span class="line"></span><br><span class="line">Complete!</span><br><span class="line">(成功标志)</span><br></pre></td></tr></table></figure><p></p><p>4、启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl start docker</span><br><span class="line">[root@hadoop000 ~]# docker -v</span><br><span class="line">Docker version 1.13.1, build 8633870/1.13.1</span><br></pre></td></tr></table></figure><p></p><p>5、开机启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><p></p><p>6、停止docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl stop docker</span><br></pre></td></tr></table></figure><p></p><h3 id="5-常用命令"><a href="#5-常用命令" class="headerlink" title="5.常用命令"></a>5.常用命令</h3><p>镜像操作</p><table><thead><tr><th>操作</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>检索</td><td>docker search 关键字 eg：docker search redis</td><td>我们经常去docker hub上检索镜像的详细信息，如镜像的TAG</td></tr><tr><td>拉取</td><td>docker pull 镜像名:tag</td><td>:tag是可选的，tag表示标签，多为软件的版本，默认是latest</td></tr><tr><td>列表</td><td>docker images</td><td>查看所有本地镜像</td></tr><tr><td>删除</td><td>docker rmi image-id</td><td>删除指定的本地镜像</td></tr></tbody></table><p>当然大家也可以在官网查找：<a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a></p><p>容器操作<br>软件镜像（QQ安装程序）—-运行镜像—-产生一个容器（正在运行的软件，运行的QQ）；</p><p>步骤：</p><ul><li>1、搜索镜像<br>[root@localhost ~]# docker search tomcat</li><li>2、拉取镜像<br>[root@localhost ~]# docker pull tomcat</li><li>3、根据镜像启动容器<br>docker run –name mytomcat -d tomcat:latest</li><li>4、docker ps<br>查看运行中的容器</li><li>5、 停止运行中的容器<br>docker stop 容器的id</li><li>6、查看所有的容器<br>docker ps -a</li><li>7、启动容器<br>docker start 容器id</li><li>8、删除一个容器<br>docker rm 容器id</li><li><p>9、启动一个做了端口映射的tomcat<br>[root@localhost ~]# docker run -d -p 8888:8080 tomcat<br>-d：后台运行<br>-p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口</p></li><li><p>10、为了演示简单关闭了linux的防火墙<br>service firewalld status ；查看防火墙状态<br>service firewalld stop：关闭防火墙<br>systemctl disable firewalld.service #禁止firewall开机启动</p></li><li>11、查看容器的日志<br>docker logs container-name/container-id</li></ul><p>更多命令参看<br><a href="https://docs.docker.com/engine/reference/commandline/docker/" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/docker/</a><br>可以参考镜像文档</p><h3 id="6-使用docker安装mysql"><a href="#6-使用docker安装mysql" class="headerlink" title="6.使用docker安装mysql"></a>6.使用docker安装mysql</h3><ul><li>docker pull mysql</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">docker pull mysql </span><br><span class="line">Using default tag: latest</span><br><span class="line">Trying to pull repository docker.io/library/mysql ... </span><br><span class="line">latest: Pulling from docker.io/library/mysql</span><br><span class="line">a5a6f2f73cd8: Pull complete </span><br><span class="line">936836019e67: Pull complete </span><br><span class="line">283fa4c95fb4: Pull complete </span><br><span class="line">1f212fb371f9: Pull complete </span><br><span class="line">e2ae0d063e89: Pull complete </span><br><span class="line">5ed0ae805b65: Pull complete </span><br><span class="line">0283dc49ef4e: Pull complete </span><br><span class="line">a7e1170b4fdb: Pull complete </span><br><span class="line">88918a9e4742: Pull complete </span><br><span class="line">241282fa67c2: Pull complete </span><br><span class="line">b0fecf619210: Pull complete </span><br><span class="line">bebf9f901dcc: Pull complete </span><br><span class="line">Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8</span><br><span class="line">Status: Downloaded newer image for docker.io/mysql:latest</span><br><span class="line">[root@hadoop000 ~]# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/mysql     latest              f991c20cb508        10 days ago         486 MB</span><br></pre></td></tr></table></figure><ul><li>启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/mysql     latest              f991c20cb508        10 days ago         486 MB</span><br><span class="line">[root@hadoop000 ~]# docker run --name mysql01 -d mysql</span><br><span class="line">756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a</span><br><span class="line">[root@hadoop000 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS               NAMES</span><br><span class="line">756620c8e583        mysql               &quot;docker-entrypoint...&quot;   9 seconds ago       Exited (1) 7 seconds ago                       mysql01</span><br><span class="line">[root@hadoop000 ~]# docker logs 756620c8e583</span><br><span class="line">error: database is uninitialized and password option is not specified </span><br><span class="line">  You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD</span><br></pre></td></tr></table></figure><p>可以看到上面启动的方式是错误的，提示我们要带上具体的密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysql</span><br><span class="line">eae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc06</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">–name：给新创建的容器命名，此处命名为pwc-mysql</span><br><span class="line">-e：配置信息，此处配置mysql的root用户的登陆密码</span><br><span class="line">-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口</span><br><span class="line">-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...</span><br></pre></td></tr></table></figure><ul><li>查看是否启动成功</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES</span><br><span class="line">eae86796e132        mysql               &quot;docker-entrypoint...&quot;   8 minutes ago       Up 8 minutes        0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql02</span><br></pre></td></tr></table></figure><ul><li>登陆MySQL</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql04 /bin/bash</span><br><span class="line">root@e34aba02c0c3:/# mysql -uroot -p123456 </span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 80</span><br><span class="line">Server version: 8.0.13 MySQL Community Server - GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><ul><li>其他的高级操作<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br><span class="line">把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面</span><br><span class="line">改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）</span><br><span class="line"></span><br><span class="line">docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br><span class="line">指定mysql的一些配置参数</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据课程一览</title>
      <link href="/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/"/>
      <url>/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h1 id="若泽数据课程系列"><a href="#若泽数据课程系列" class="headerlink" title="若泽数据课程系列"></a>若泽数据课程系列</h1><h2 id="基础班"><a href="#基础班" class="headerlink" title="基础班"></a>基础班</h2><h3 id="Liunx"><a href="#Liunx" class="headerlink" title="Liunx"></a>Liunx</h3><ul><li>VM虚拟机安装</li><li>Liunx常用命令（重点）</li><li>开发环境搭</li></ul><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ul><li>源码安装&amp;yum安装</li><li>CRUD编写</li><li>权限控制</li></ul><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>架构介绍&amp;&amp;源码编译</li><li>伪分布式安装&amp;&amp;企业应用</li><li><p>HDFS（重点）</p><ul><li>架构设计</li><li>副本放置策略</li><li>读写流程</li></ul></li><li><p>YARN（重点）</p><ul><li>架构设计</li><li>工作流程</li><li>调度管理&amp;&amp;常见参数配置（调优）</li></ul></li><li><p>MapReduce</p><ul><li>架构设计</li><li>wordcount原理&amp;&amp;join原理和案例<a id="more"></a><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3></li></ul></li><li><p>架构设计</p></li><li>Hive DDL&amp;DML</li><li>join在大数据中的使用</li><li>使用自带UDF和开发自定义UDF</li></ul><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><ul><li>架构设计</li><li>RDBMS导入导出</li></ul><h3 id="整合项目将所有组件合作使用。"><a href="#整合项目将所有组件合作使用。" class="headerlink" title="整合项目将所有组件合作使用。"></a>整合项目将所有组件合作使用。</h3><h3 id="人工智能基础"><a href="#人工智能基础" class="headerlink" title="人工智能基础"></a>人工智能基础</h3><ul><li>python基础</li><li>常用库——pandas、numpy、sklearn、keras</li></ul><h2 id="高级班"><a href="#高级班" class="headerlink" title="高级班"></a>高级班</h2><h3 id="scala编程（重点）"><a href="#scala编程（重点）" class="headerlink" title="scala编程（重点）"></a>scala编程（重点）</h3><h3 id="Spark（五星重点）"><a href="#Spark（五星重点）" class="headerlink" title="Spark（五星重点）"></a>Spark（五星重点）</h3><h3 id="Hadoop高级"><a href="#Hadoop高级" class="headerlink" title="Hadoop高级"></a>Hadoop高级</h3><h3 id="Hive高级"><a href="#Hive高级" class="headerlink" title="Hive高级"></a>Hive高级</h3><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h3 id="调度平台"><a href="#调度平台" class="headerlink" title="调度平台"></a>调度平台</h3><h2 id="线下班"><a href="#线下班" class="headerlink" title="线下班"></a>线下班</h2><p><img src="/assets/blogImg/若泽数据.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kudu与Spark 生产最佳实践</title>
      <link href="/2019/05/07/Kudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/05/07/Kudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> &lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">    &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;</span><br><span class="line">    &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object KuduApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">     //Read a table from Kudu</span><br><span class="line">    val df = spark.read</span><br><span class="line">          .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;))</span><br><span class="line">          .format(&quot;kudu&quot;).load</span><br><span class="line">        df.schema.printTreeString()</span><br><span class="line">//    // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">//    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    // The schema is encoded in a string</span><br><span class="line">//    val schemalString=&quot;id,age,name&quot;</span><br><span class="line">//</span><br><span class="line">//    // Generate the schema based on the string of schema</span><br><span class="line">//    val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))</span><br><span class="line">//    val schema=StructType(fields)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    val KuduTable = kuduContext.createTable(</span><br><span class="line">//     &quot;test_table&quot;, schema, Seq(&quot;id&quot;),</span><br><span class="line">//     new CreateTableOptions()</span><br><span class="line">//       .setNumReplicas(1)</span><br><span class="line">//       .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema</span><br><span class="line">//</span><br><span class="line">//    val  id  = KuduTable.getColumn(&quot;id&quot;)</span><br><span class="line">//    print(id)</span><br><span class="line">//</span><br><span class="line">//    kuduContext.tableExists(&quot;test_table&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现象:通过spark sql 操作报如下错误:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)</span><br><span class="line">    at cn.zhangyu.KuduApp$.main(KuduApp.scala:18)</span><br><span class="line">    at cn.zhangyu.KuduApp.main(KuduApp.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try.orElse(Try.scala:84)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure><p>而通过KuduContext是可以操作的没有报错,代码为上面注解部分</p><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>查询kudu官网:<a href="https://kudu.apache.org/docs/developing.html" target="_blank" rel="noopener">https://kudu.apache.org/docs/developing.html</a></p><p>官网中说出了版本的问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果将Spark 2与Scala 2.11一起使用，请使用kudu-spark2_2.11工件。</span><br><span class="line">kudu-spark版本1.8.0及更低版本的语法略有不同。有关有效示例，请参阅您的版本的文档。可以在发布页面上找到版本化文档。</span><br><span class="line">spark-shell --packages org.apache.kudu:kudu-spark2_2.11:1.9.0</span><br><span class="line">看到了 官网使用的是1.9.0的版本.</span><br></pre></td></tr></table></figure><font size="3" color="red">但是但是但是</font><p>官网下面说到了下面几个集成问题:</p><ol><li><font size="3" color="red"><b>Spark 2.2+在运行时需要Java 8，即使Kudu Spark 2.x集成与Java 7兼容。Spark 2.2是Kudu 1.5.0的默认依赖版本。</b></font></li><li>当注册为临时表时，必须为名称包含大写或非ascii字符的Kudu表分配备用名称。</li><li>包含大写或非ascii字符的列名的Kudu表不能与SparkSQL一起使用。可以在Kudu中重命名列以解决此问题。</li><li>&lt;&gt;并且OR谓词不会被推送到Kudu，而是由Spark任务进行评估。只有LIKE带有后缀通配符的谓词才会被推送到Kudu，这意味着它LIKE “FOO%”被推下但LIKE “FOO%BAR”不是。</li><li>Kudu不支持Spark SQL支持的每种类型。例如， Date不支持复杂类型。</li><li>Kudu表只能在SparkSQL中注册为临时表。使用HiveContext可能无法查询Kudu表。</li></ol><font size="3"><b><br>那就很奇怪了我用的1.5.0版本报错为:找不到类,数据源有问题<br><br>但是把kudu改成1.9.0 问题解决<br></b></font><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = false)</span><br><span class="line"> |-- age: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Spark集成最佳实践"><a href="#Spark集成最佳实践" class="headerlink" title="Spark集成最佳实践"></a>Spark集成最佳实践</h3><p>每个群集避免多个Kudu客户端。</p><p>一个常见的Kudu-Spark编码错误是实例化额外的KuduClient对象。在kudu-spark中，a KuduClient属于KuduContext。Spark应用程序代码不应创建另一个KuduClient连接到同一群集。相反，应用程序代码应使用KuduContext访问KuduClient使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KuduContext#syncClient。</span><br><span class="line">  // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">    val list = kuduContext.syncClient.getTablesList.getTablesList</span><br><span class="line">    if (list.iterator().hasNext)&#123;</span><br><span class="line">      print(list.iterator().next())</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>要诊断KuduClientSpark作业中的多个实例，请查看主服务器的日志中的符号，这些符号会被来自不同客户端的许多GetTableLocations或 GetTabletLocations请求过载，通常大约在同一时间。这种症状特别适用于Spark Streaming代码，其中创建KuduClient每个任务将导致来自新客户端的主请求的周期性波。</p><h3 id="Spark操作kudu-Scala-demo"><a href="#Spark操作kudu-Scala-demo" class="headerlink" title="Spark操作kudu(Scala demo)"></a>Spark操作kudu(Scala demo)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">package cn.zhangyu</span><br><span class="line">import org.apache.kudu.spark.kudu._</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;</span><br><span class="line">import org.slf4j.LoggerFactory</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object SparkTest &#123;</span><br><span class="line">  //kuduMasters and tableName</span><br><span class="line">  val kuduMasters = &quot;192.168.13.130:7051&quot;</span><br><span class="line">  val tableName = &quot;kudu_spark_table&quot;</span><br><span class="line">  //table column</span><br><span class="line">  val idCol = &quot;id&quot;</span><br><span class="line">  val ageCol = &quot;age&quot;</span><br><span class="line">  val nameCol = &quot;name&quot;</span><br><span class="line">  //replication</span><br><span class="line">  val tableNumReplicas = Integer.getInteger(&quot;tableNumReplicas&quot;, 1)</span><br><span class="line">  val logger = LoggerFactory.getLogger(SparkTest.getClass)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //create SparkSession</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    //create kuduContext</span><br><span class="line">    val kuduContext = new KuduContext(kuduMasters,spark.sparkContext)</span><br><span class="line">    //schema</span><br><span class="line">    val schema = StructType(</span><br><span class="line">      List(</span><br><span class="line">        StructField(idCol, IntegerType, false),</span><br><span class="line">        StructField(nameCol, StringType, false),</span><br><span class="line">        StructField(ageCol,StringType,false)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    var tableIsCreated = false</span><br><span class="line">    try&#123;</span><br><span class="line">    // Make sure the table does not exist</span><br><span class="line">    if (kuduContext.tableExists(tableName)) &#123;</span><br><span class="line">      throw new RuntimeException(tableName + &quot;: table already exists&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    //create</span><br><span class="line">    kuduContext.createTable(tableName, schema, Seq(idCol),</span><br><span class="line">      new CreateTableOptions()</span><br><span class="line">        .addHashPartitions(List(idCol).asJava, 3)</span><br><span class="line">        .setNumReplicas(tableNumReplicas))</span><br><span class="line">    tableIsCreated = true</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    //write</span><br><span class="line">    logger.info(s&quot;writing to table &apos;$tableName&apos;&quot;)</span><br><span class="line">        val data = Array(Person(1,&quot;12&quot;,&quot;zhangsan&quot;),Person(2,&quot;20&quot;,&quot;lisi&quot;),Person(3,&quot;30&quot;,&quot;wangwu&quot;))</span><br><span class="line">        val personRDD = spark.sparkContext.parallelize(data)</span><br><span class="line">        val personDF = personRDD.toDF()</span><br><span class="line">        kuduContext.insertRows(personDF,tableName)</span><br><span class="line">    //useing SparkSQL read table</span><br><span class="line">    val sqlDF = spark.sqlContext.read</span><br><span class="line">      .options(Map(&quot;kudu.master&quot; -&gt; kuduMasters, &quot;kudu.table&quot; -&gt; tableName))</span><br><span class="line">      .format(&quot;kudu&quot;).kudu</span><br><span class="line">    sqlDF.createOrReplaceTempView(tableName)</span><br><span class="line">    spark.sqlContext.sql(s&quot;SELECT * FROM $tableName &quot;).show</span><br><span class="line">    //upsert some rows</span><br><span class="line">    val upsertPerson = Array(Person(1,&quot;10&quot;,&quot;jack&quot;))</span><br><span class="line">    val upsertPersonRDD = spark.sparkContext.parallelize(upsertPerson)</span><br><span class="line">    val upsertPersonDF = upsertPersonRDD.toDF()</span><br><span class="line">    kuduContext.updateRows(upsertPersonDF,tableName)</span><br><span class="line">    //useing RDD read table</span><br><span class="line">    val readCols = Seq(idCol,ageCol,nameCol)</span><br><span class="line">    val readRDD = kuduContext.kuduRDD(spark.sparkContext, tableName, readCols)</span><br><span class="line">    val userTuple = readRDD.map &#123; case Row( id: Int,age: String,name: String) =&gt; (id,age,name) &#125;</span><br><span class="line">    println(&quot;count:&quot;+userTuple.count())</span><br><span class="line">    userTuple.collect().foreach(println(_))</span><br><span class="line">    //delete table</span><br><span class="line">    kuduContext.deleteTable(tableName)    </span><br><span class="line">    &#125;catch &#123;</span><br><span class="line">      // Catch, log and re-throw. Not the best practice, but this is a very</span><br><span class="line">      // simplistic example.</span><br><span class="line">      case unknown : Throwable =&gt; logger.error(s&quot;got an exception: &quot; + unknown)</span><br><span class="line">        throw unknown</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      // Clean up.</span><br><span class="line">      if (tableIsCreated) &#123;</span><br><span class="line">        logger.info(s&quot;deleting table &apos;$tableName&apos;&quot;)</span><br><span class="line">        kuduContext.deleteTable(tableName)</span><br><span class="line">      &#125;</span><br><span class="line">      logger.info(s&quot;closing down the session&quot;)</span><br><span class="line">      spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">case class Person(id: Int,age: String,name: String)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> Kudu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之二</title>
      <link href="/2019/04/26/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C/"/>
      <url>/2019/04/26/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font size="4"><b>Driver端</b></font><ol><li>Driver端初始化构建Accumulator并初始化，同时完成了Accumulator注册，Accumulators.register(this)时Accumulator会在序列化后发送到Executor端</li><li>Driver接收到ResultTask完成的状态更新后，会去更新Value的值 然后在Action操作执行后就可以获取到Accumulator的值了</li></ol><font size="4"><b>Executor端</b></font><ol><li>Executor端接收到Task之后会进行反序列化操作，反序列化得到RDD和function。同时在反序列化的同时也去反序列化Accumulator(在readObject方法中完成)，同时也会向TaskContext完成注册</li><li>完成任务计算之后，随着Task结果一起返回给Driver<a id="more"></a></li></ol><h2 id="结合源码分析"><a href="#结合源码分析" class="headerlink" title="结合源码分析"></a>结合源码分析</h2><font size="4"><b>Driver端初始化</b></font><p>&ensp;&ensp;Driver端主要经过以下步骤，完成初始化操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val accum = sparkContext.accumulator(0, “AccumulatorTest”)</span><br><span class="line">val acc = new Accumulator(initialValue, param, Some(name))</span><br><span class="line">Accumulators.register(this)</span><br></pre></td></tr></table></figure><font size="4"><b>Executor端反序列化得到Accumulator</b></font><p>&ensp;&ensp;反序列化是在调用ResultTask的runTask方式时候做的操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 会反序列化出来RDD和自己定义的function</span><br><span class="line">val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](</span><br><span class="line">   ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;在反序列化的过程中，会调用Accumulable中的readObject方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123;</span><br><span class="line">    in.defaultReadObject()</span><br><span class="line">    // value的初始值为zero；该值是会被序列化的</span><br><span class="line">    value_ = zero</span><br><span class="line">    deserialized = true</span><br><span class="line">    // Automatically register the accumulator when it is deserialized with the task closure.</span><br><span class="line">    //</span><br><span class="line">    // Note internal accumulators sent with task are deserialized before the TaskContext is created</span><br><span class="line">    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL</span><br><span class="line">    // metrics, still need to register here.</span><br><span class="line">    val taskContext = TaskContext.get()</span><br><span class="line">    if (taskContext != null) &#123;</span><br><span class="line">      // 当前反序列化所得到的对象会被注册到TaskContext中</span><br><span class="line">      // 这样TaskContext就可以获取到累加器</span><br><span class="line">      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor</span><br><span class="line">      taskContext.registerAccumulator(this)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><font size="4"><b>注意</b></font><p><strong>Accumulable.scala中的value_，是不会被序列化的，@transient关键词修饰了</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@volatile @transient private var value_ : R = initialValue // Current value on master</span><br></pre></td></tr></table></figure><h2 id="累加器在各个节点的累加操作"><a href="#累加器在各个节点的累加操作" class="headerlink" title="累加器在各个节点的累加操作"></a>累加器在各个节点的累加操作</h2><p>针对传入function中不同的操作，对应有不同的调用方法，以下列举几种（在Accumulator.scala中）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def += (term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;</span><br><span class="line">def add(term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;</span><br><span class="line">def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;</span><br></pre></td></tr></table></figure><p>根据不同的累加器参数，有不同实现的AccumulableParam（在Accumulator.scala中）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulableParam[R, T] extends Serializable &#123;</span><br><span class="line">  /**</span><br><span class="line">  def addAccumulator(r: R, t: T): R</span><br><span class="line">  def addInPlace(r1: R, r2: R): R</span><br><span class="line">  def zero(initialValue: R): R</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不同的实现如下图所示：</p><p><img src="/assets/blogImg/累加器在各个节点的操作.png" alt="enter description here"></p><p>以IntAccumulatorParam为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123;</span><br><span class="line">  def addInPlace(t1: Int, t2: Int): Int = t1 + t2</span><br><span class="line">  def zero(initialValue: Int): Int = 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现IntAccumulatorParam实现的是trait AccumulatorParam[T]：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在各个节点上的累加操作完成之后，就会紧跟着返回更新之后的Accumulators的value_值</p><h2 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h2><p>在Task.scala中的run方法，会执行如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 返回累加器，并运行task</span><br><span class="line">// 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map</span><br><span class="line">(runTask(context), context.collectAccumulators())</span><br></pre></td></tr></table></figure><p>在Executor端已经完成了一系列操作，需要将它们的值返回到Driver端进行聚合汇总，整个顺序如图累加器执行流程：</p><p><img src="/source/assets/pic/累加器执行流程图.png" alt="累加器执行流程"></p><p>根据执行流程，我们可以发现，在执行完collectAccumulators方法之后，最终会在DAGScheduler中调用updateAccumulators(event)，而在该方法中会调用Accumulators的add方法，从而完成聚合操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def add(values: Map[Long, Any]): Unit = synchronized &#123;</span><br><span class="line">  // 遍历传进来的值</span><br><span class="line">  for ((id, value) &lt;- values) &#123;</span><br><span class="line">    if (originals.contains(id)) &#123;</span><br><span class="line">      // Since we are now storing weak references, we must check whether the underlying data</span><br><span class="line">      // is valid.</span><br><span class="line">      // 根据id从注册的Map中取出对应的累加器</span><br><span class="line">      originals(id).get match &#123;</span><br><span class="line">        // 将值给累加起来，最终将结果加到value里面</span><br><span class="line">       // ++=是被重载了</span><br><span class="line">        case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value</span><br><span class="line">        case None =&gt;</span><br><span class="line">          throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="获取累加器的值"><a href="#获取累加器的值" class="headerlink" title="获取累加器的值"></a>获取累加器的值</h2><p>通过accum.value方法可以获取到累加器的值</p><font size="3"><b>至此，累加器执行完毕。</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark2.4.2详细介绍</title>
      <link href="/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Spark发布了最新的版本spark-2.4.2<br>根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的</p><h4 id="版本介绍"><a href="#版本介绍" class="headerlink" title="版本介绍"></a>版本介绍</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。<font color="#FF4500"> <strong>我们强烈建议所有2.4用户升级到此稳定版本。</strong></font><br><a id="more"></a></p><h4 id="显著的变化"><a href="#显著的变化" class="headerlink" title="显著的变化"></a>显著的变化</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。</li><li>还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。</li></ul><h4 id="详细更改"><a href="#详细更改" class="headerlink" title="详细更改"></a>详细更改</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>在Spark Driver中发现Java死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>使用选项logConf = true时密码将以conf的明文形式记录</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>用Snappy 1.1.7.1解压、压缩空序列化数据时失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferTo中的潜在损坏</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointData因文件系统已缓存而无法清理</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>在仅使用空值列的AggregateEstimation之后的错误outputRows估计</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>修复包名称不匹配</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>修复updateTableStats以使用新统计信息或无更新表统计信息</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1静默删除DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>松开在ExpressionInfo的’examples’字段中换行断言条件</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>将jquery更新为1.12.x以获取安全修复程序</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>隐藏“org.apache.spark.util.kvstore”的API文档</td></tr></tbody></table><h6 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产常用Spark累加器剖析之一</title>
      <link href="/2019/04/19/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80/"/>
      <url>/2019/04/19/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><p><strong>由于最近在项目中需要用到Spark的累加器，同时需要自己去自定义实现Spark的累加器，从而满足生产上的需求。对此，对Spark的累加器实现机制进行了追踪学习。</strong></p><p>本系列文章，将从以下几个方面入手，对Spark累加器进行剖析：</p><ol><li>Spark累加器的基本概念</li><li>累加器的重点类构成</li><li>累加器的源码解析</li><li>累加器的执行过程</li><li>累加器使用中的坑</li><li>自定义累加器的实现<a id="more"></a><h2 id="Spark累加器基本概念"><a href="#Spark累加器基本概念" class="headerlink" title="Spark累加器基本概念"></a>Spark累加器基本概念</h2></li></ol><p>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，只能累加，不能减少累加器只能在Driver端构建，并只能从Driver端读取结果，在Task端只能进行累加。</p><p>至于这里为什么只能在Task累加呢？下面的内容将会进行详细的介绍，先简单介绍下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Task节点，准确的就是说在executor上；</span><br><span class="line">每个Task都会有一个累加器的变量，被序列化传输到executor端运行之后再返回过来都是独立运行的；</span><br><span class="line">如果在Task端去获取值的话，只能获取到当前Task的，Task与Task之间不会有影响</span><br></pre></td></tr></table></figure><p>累加器不会改变Spark lazy计算的特点，只会在Job触发的时候进行相关的累加操作</p><p>现有累加器类型:</p><p><img src="/assets/blogImg/Spark累加器类型_1.png" alt="enter description here"></p><h2 id="累加器的重点类介绍"><a href="#累加器的重点类介绍" class="headerlink" title="累加器的重点类介绍"></a>累加器的重点类介绍</h2><font size="4"><b>class Accumulator extends Accumulable</b></font><p>源码（源码中已经对这个类的作用做了十分详细的解释）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * A simpler value of [[Accumulable]] where the result type being accumulated is the same</span><br><span class="line"> * as the types of elements being merged, i.e. variables that are only &quot;added&quot; to through an</span><br><span class="line"> * associative operation and can therefore be efficiently supported in parallel. They can be used</span><br><span class="line"> * to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric</span><br><span class="line"> * value types, and programmers can add support for new types.</span><br><span class="line"> *</span><br><span class="line"> * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].</span><br><span class="line"> * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.</span><br><span class="line"> * However, they cannot read its value. Only the driver program can read the accumulator&apos;s value,</span><br><span class="line"> * using its value method.</span><br><span class="line"> *</span><br><span class="line"> * @param initialValue initial value of accumulator</span><br><span class="line"> * @param param helper object defining how to add elements of type `T`</span><br><span class="line"> * @tparam T result type</span><br><span class="line"> */</span><br><span class="line">class Accumulator[T] private[spark] (</span><br><span class="line">    @transient private[spark] val initialValue: T,</span><br><span class="line">    param: AccumulatorParam[T],</span><br><span class="line">    name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Accumulable[T, T](initialValue, param, name, internal) &#123;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123;</span><br><span class="line">    this(initialValue, param, name, false)</span><br><span class="line">  &#125;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T]) = &#123;</span><br><span class="line">    this(initialValue, param, None, false)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>主要实现了累加器的初始化及封装了相关的累加器操作方法同时在类对象构建的时候向Accumulators注册累加器累加器的add操作的返回值类型和传入进去的值类型可以不一样所以一定要定义好两步操作（即add方法）：累加操作/合并操作</code></pre><font size="4"><b>object Accumulators</b></font><pre><code>该方法在Driver端管理着累加器，也包含了累加器的聚合操作</code></pre><font size="4"><b>trait AccumulatorParam[T] extends AccumulableParam[T, T]</b></font><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add</span><br><span class="line"> * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be</span><br><span class="line"> * available when you create Accumulators of a specific type.</span><br><span class="line"> *</span><br><span class="line"> * @tparam T type of value to accumulate</span><br><span class="line"> */</span><br><span class="line">trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123;</span><br><span class="line">  def addAccumulator(t1: T, t2: T): T = &#123;</span><br><span class="line">    addInPlace(t1, t2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>AccumulatorParam的addAccumulator操作的泛型封装具体的实现还是需要在具体实现类里面实现addInPlace方法自定义实现累加器的关键</code></pre><font size="4"><b>object AccumulatorParam</b></font><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object AccumulatorParam &#123;</span><br><span class="line">  // The following implicit objects were in SparkContext before 1.2 and users had to</span><br><span class="line">  // `import SparkContext._` to enable them. Now we move them here to make the compiler find</span><br><span class="line">  // them automatically. However, as there are duplicate codes in SparkContext for backward</span><br><span class="line">  // compatibility, please update them accordingly if you modify the following implicit objects.</span><br><span class="line">  implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] &#123;</span><br><span class="line">    def addInPlace(t1: Double, t2: Double): Double = t1 + t2</span><br><span class="line">    def zero(initialValue: Double): Double = 0.0</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123;</span><br><span class="line">    def addInPlace(t1: Int, t2: Int): Int = t1 + t2</span><br><span class="line">    def zero(initialValue: Int): Int = 0</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object LongAccumulatorParam extends AccumulatorParam[Long] &#123;</span><br><span class="line">    def addInPlace(t1: Long, t2: Long): Long = t1 + t2</span><br><span class="line">    def zero(initialValue: Long): Long = 0L</span><br><span class="line">  &#125;</span><br><span class="line">  implicit object FloatAccumulatorParam extends AccumulatorParam[Float] &#123;</span><br><span class="line">    def addInPlace(t1: Float, t2: Float): Float = t1 + t2</span><br><span class="line">    def zero(initialValue: Float): Float = 0f</span><br><span class="line">  &#125;</span><br><span class="line">  // TODO: Add AccumulatorParams for other types, e.g. lists and strings</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>从源码中大量的implicit关键词，可以发现该类主要进行隐式类型转换的操作</code></pre><font size="4"><b>TaskContextImpl</b></font><pre><code>在Executor端管理着我们的累加器，累加器是通过该类进行返回的</code></pre><h2 id="累加器的源码解析"><a href="#累加器的源码解析" class="headerlink" title="累加器的源码解析"></a>累加器的源码解析</h2><font size="4"><b>Driver端</b></font><p>&ensp;&ensp;<font size="3"><b>accumulator方法</b></font></p><p>以下列这段代码中的accumulator方法为入口点，进入到相应的源码中去</p><p><code>val acc = new Accumulator(initialValue, param, Some(name))</code></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Accumulator[T] private[spark] (</span><br><span class="line">    @transient private[spark] val initialValue: T,</span><br><span class="line">    param: AccumulatorParam[T],</span><br><span class="line">    name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Accumulable[T, T](initialValue, param, name, internal) &#123;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123;</span><br><span class="line">    this(initialValue, param, name, false)</span><br><span class="line">  &#125;</span><br><span class="line">  def this(initialValue: T, param: AccumulatorParam[T]) = &#123;</span><br><span class="line">    this(initialValue, param, None, false)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>继承的Accumulable[T, T]</b></font></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Accumulable[R, T] private[spark] (</span><br><span class="line">    initialValue: R,</span><br><span class="line">    param: AccumulableParam[R, T],</span><br><span class="line">    val name: Option[String],</span><br><span class="line">    internal: Boolean)</span><br><span class="line">  extends Serializable &#123;</span><br><span class="line">…</span><br><span class="line">// 这里的_value并不支持序列化</span><br><span class="line">// 注：有@transient的都不会被序列化</span><br><span class="line">@volatile @transient private var value_ : R = initialValue // Current value on master</span><br><span class="line">  …</span><br><span class="line">  // 注册了当前的累加器</span><br><span class="line">  Accumulators.register(this)</span><br><span class="line">  …,</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators.register()</b></font></p><p>源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 传入参数，注册累加器</span><br><span class="line">def register(a: Accumulable[_, _]): Unit = synchronized &#123;</span><br><span class="line">// 构造成WeakReference</span><br><span class="line">originals(a.id) = new WeakReference[Accumulable[_, _]](a)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font size="3"><b>至此，Driver端的初始化已经完成</b></font> <font size="4"><b>Executor端</b></font><pre><code>Executor端的反序列化是一个得到我们的对象的过程初始化是在反序列化的时候就完成的，同时反序列化的时候还完成了Accumulator向TaskContextImpl的注册</code></pre><p>&ensp;&ensp;<font size="3"><b>TaskRunner中的run方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// 在计算的过程中，会将RDD和function经过序列化之后传给Executor端</span><br><span class="line">private[spark] class Executor(</span><br><span class="line">    executorId: String,</span><br><span class="line">    executorHostname: String,</span><br><span class="line">    env: SparkEnv,</span><br><span class="line">    userClassPath: Seq[URL] = Nil,</span><br><span class="line">    isLocal: Boolean = false)</span><br><span class="line">  extends Logging &#123;</span><br><span class="line">...</span><br><span class="line">  class TaskRunner(</span><br><span class="line">      execBackend: ExecutorBackend,</span><br><span class="line">      val taskId: Long,</span><br><span class="line">      val attemptNumber: Int,</span><br><span class="line">      taskName: String,</span><br><span class="line">      serializedTask: ByteBuffer)</span><br><span class="line">    extends Runnable &#123;</span><br><span class="line">…</span><br><span class="line">override def run(): Unit = &#123;</span><br><span class="line">    …</span><br><span class="line">val (value, accumUpdates) = try &#123;</span><br><span class="line">         // 调用TaskRunner中的task.run方法，触发task的运行</span><br><span class="line">         val res = task.run(</span><br><span class="line">           taskAttemptId = taskId,</span><br><span class="line">           attemptNumber = attemptNumber,</span><br><span class="line">           metricsSystem = env.metricsSystem)</span><br><span class="line">         threwException = false</span><br><span class="line">         res</span><br><span class="line">       &#125; finally &#123;</span><br><span class="line">        …</span><br><span class="line">       &#125;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Task中的collectAccumulators()方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private[spark] abstract class Task[T](</span><br><span class="line">final def run(</span><br><span class="line">    taskAttemptId: Long,</span><br><span class="line">    attemptNumber: Int,</span><br><span class="line">    metricsSystem: MetricsSystem)</span><br><span class="line">  : (T, AccumulatorUpdates) = &#123;</span><br><span class="line">  …</span><br><span class="line">    try &#123;</span><br><span class="line">      // 返回累加器，并运行task</span><br><span class="line">      // 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map</span><br><span class="line">      (runTask(context), context.collectAccumulators())</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">  …</span><br><span class="line"> &#125;</span><br><span class="line"> …</span><br><span class="line"> &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>ResultTask中的runTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">override def runTask(context: TaskContext): U = &#123;</span><br><span class="line">  // Deserialize the RDD and the func using the broadcast variables.</span><br><span class="line">  val deserializeStartTime = System.currentTimeMillis()</span><br><span class="line">  val ser = SparkEnv.get.closureSerializer.newInstance()</span><br><span class="line">  // 反序列化是在调用ResultTask的runTask方法的时候做的</span><br><span class="line">  // 会反序列化出来RDD和自己定义的function</span><br><span class="line">  val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](</span><br><span class="line">    ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br><span class="line">  _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime</span><br><span class="line">  metrics = Some(context.taskMetrics)</span><br><span class="line">  func(context, rdd.iterator(partition, context))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulable中的readObject方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 在反序列化的过程中会调用Accumulable.readObject方法</span><br><span class="line">  // Called by Java when deserializing an object</span><br><span class="line">  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123;</span><br><span class="line">    in.defaultReadObject()</span><br><span class="line">    // value的初始值为zero；该值是会被序列化的</span><br><span class="line">    value_ = zero</span><br><span class="line">    deserialized = true</span><br><span class="line">    // Automatically register the accumulator when it is deserialized with the task closure.</span><br><span class="line">    //</span><br><span class="line">    // Note internal accumulators sent with task are deserialized before the TaskContext is created</span><br><span class="line">    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL</span><br><span class="line">    // metrics, still need to register here.</span><br><span class="line">    val taskContext = TaskContext.get()</span><br><span class="line">    if (taskContext != null) &#123;</span><br><span class="line">      // 当前反序列化所得到的对象会被注册到TaskContext中</span><br><span class="line">      // 这样TaskContext就可以获取到累加器</span><br><span class="line">      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor</span><br><span class="line">      taskContext.registerAccumulator(this)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Executor.scala</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 在executor端拿到accumuUpdates值之后，会去构造一个DirectTaskResult</span><br><span class="line">val directResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.orNull)</span><br><span class="line">val serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">val resultSize = serializedDirectResult.limit</span><br><span class="line">…</span><br><span class="line">// 最终由ExecutorBackend的statusUpdate方法发送至Driver端</span><br><span class="line">// ExecutorBackend为一个Trait，有多种实现</span><br><span class="line">execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>CoarseGrainedExecutorBackend中的statusUpdate方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 通过ExecutorBackend的一个实现类：CoarseGrainedExecutorBackend 中的statusUpdate方法</span><br><span class="line">// 将数据发送至Driver端</span><br><span class="line">override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) &#123;</span><br><span class="line">    val msg = StatusUpdate(executorId, taskId, state, data)</span><br><span class="line">    driver match &#123;</span><br><span class="line">      case Some(driverRef) =&gt; driverRef.send(msg)</span><br><span class="line">      case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>CoarseGrainedSchedulerBackend中的receive方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// Driver端在接收到消息之后，会调用CoarseGrainedSchedulerBackend中的receive方法</span><br><span class="line">override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">      case StatusUpdate(executorId, taskId, state, data) =&gt;</span><br><span class="line">        // 会在DAGScheduler的handleTaskCompletion方法中将结果返回</span><br><span class="line">        scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskSchedulerImpl的statusUpdate方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) &#123;</span><br><span class="line">  …</span><br><span class="line">            if (state == TaskState.FINISHED) &#123;</span><br><span class="line">              taskSet.removeRunningTask(tid)</span><br><span class="line">              // 将成功的Task入队</span><br><span class="line">              taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)</span><br><span class="line">            &#125; else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) &#123;</span><br><span class="line">              taskSet.removeRunningTask(tid)</span><br><span class="line">              taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)</span><br><span class="line">            &#125;</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskResultGetter的enqueueSuccessfulTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) &#123;</span><br><span class="line">…</span><br><span class="line">          result.metrics.setResultSize(size)</span><br><span class="line">          scheduler.handleSuccessfulTask(taskSetManager, tid, result)</span><br><span class="line">…</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>TaskSchedulerImpl的handleSuccessfulTask方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def handleSuccessfulTask(</span><br><span class="line">      taskSetManager: TaskSetManager,</span><br><span class="line">      tid: Long,</span><br><span class="line">      taskResult: DirectTaskResult[_]): Unit = synchronized &#123;</span><br><span class="line">    taskSetManager.handleSuccessfulTask(tid, taskResult)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的taskEnded方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def taskEnded(</span><br><span class="line">     task: Task[_],</span><br><span class="line">     reason: TaskEndReason,</span><br><span class="line">     result: Any,</span><br><span class="line">     accumUpdates: Map[Long, Any],</span><br><span class="line">     taskInfo: TaskInfo,</span><br><span class="line">     taskMetrics: TaskMetrics): Unit = &#123;</span><br><span class="line"> eventProcessLoop.post(</span><br><span class="line">     // 给自身的消息循环体发了个CompletionEvent</span><br><span class="line">     // 这个CompletionEvent会被handleTaskCompletion方法所接收到</span><br><span class="line">     CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的handleTaskCompletion方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// 与上述CoarseGrainedSchedulerBackend中的receive方法章节对应</span><br><span class="line">// 在handleTaskCompletion方法中，接收CompletionEvent</span><br><span class="line">// 不论是ResultTask还是ShuffleMapTask都会去调用updateAccumulators方法，更新累加器的值</span><br><span class="line">private[scheduler] def handleTaskCompletion(event: CompletionEvent) &#123;</span><br><span class="line">    …</span><br><span class="line">    event.reason match &#123;</span><br><span class="line">      case Success =&gt;</span><br><span class="line">        listenerBus.post(SparkListenerTaskEnd(stageId, stage.latestInfo.attemptId, taskType,</span><br><span class="line">          event.reason, event.taskInfo, event.taskMetrics))</span><br><span class="line">        stage.pendingPartitions -= task.partitionId</span><br><span class="line">        task match &#123;</span><br><span class="line">          case rt: ResultTask[_, _] =&gt;</span><br><span class="line">            // Cast to ResultStage here because it&apos;s part of the ResultTask</span><br><span class="line">            // TODO Refactor this out to a function that accepts a ResultStage</span><br><span class="line">            val resultStage = stage.asInstanceOf[ResultStage]</span><br><span class="line">            resultStage.activeJob match &#123;</span><br><span class="line">              case Some(job) =&gt;</span><br><span class="line">                if (!job.finished(rt.outputId)) &#123;</span><br><span class="line">                  updateAccumulators(event)</span><br><span class="line">          case smt: ShuffleMapTask =&gt;</span><br><span class="line">            val shuffleStage = stage.asInstanceOf[ShuffleMapStage]</span><br><span class="line">            updateAccumulators(event)</span><br><span class="line">&#125;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>DAGScheduler的updateAccumulators方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private def updateAccumulators(event: CompletionEvent): Unit = &#123;</span><br><span class="line">   val task = event.task</span><br><span class="line">   val stage = stageIdToStage(task.stageId)</span><br><span class="line">   if (event.accumUpdates != null) &#123;</span><br><span class="line">     try &#123;</span><br><span class="line">       // 调用了累加器的add方法</span><br><span class="line">       Accumulators.add(event.accumUpdates)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的add方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def add(values: Map[Long, Any]): Unit = synchronized &#123;</span><br><span class="line">    // 遍历传进来的值</span><br><span class="line">    for ((id, value) &lt;- values) &#123;</span><br><span class="line">      if (originals.contains(id)) &#123;</span><br><span class="line">        // Since we are now storing weak references, we must check whether the underlying data</span><br><span class="line">        // is valid.</span><br><span class="line">        // 根据id从注册的Map中取出对应的累加器</span><br><span class="line">        originals(id).get match &#123;</span><br><span class="line">          // 将值给累加起来，最终将结果加到value里面</span><br><span class="line">          // ++=是被重载了</span><br><span class="line">          case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value</span><br><span class="line">          case None =&gt;</span><br><span class="line">            throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的++=方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;<font size="3"><b>Accumulators的value方法</b></font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def value: R = &#123;</span><br><span class="line">   if (!deserialized) &#123;</span><br><span class="line">     value_</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     throw new UnsupportedOperationException(&quot;Can&apos;t read accumulator value in task&quot;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><font size="4"><b>此时我们的应用程序就可以通过 .value 的方式去获取计数器的值了</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 累加器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark2.4.0如何Debug源代码</title>
      <link href="/2019/04/17/%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81/"/>
      <url>/2019/04/17/%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h3 id="源码获取与编译"><a href="#源码获取与编译" class="headerlink" title="源码获取与编译"></a>源码获取与编译</h3><ol><li>直接从Spark官网获取源码或者从GitHub获取<br><img src="/assets/blogImg/2019-04-17-1.png" alt="enter description here"></li></ol><p><img src="/assets/blogImg/2019-04-17-2.png" alt="enter description here"></p><p>下载源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0.tgz</span><br></pre></td></tr></table></figure><p>解压源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf spark-2.4.0.tgz</span><br></pre></td></tr></table></figure><a id="more"></a><ol start="2"><li>Spark源码编译<br>此处不再啰嗦，直接去腾讯课堂，搜索“若泽大数据”即可找到编译视频。</li></ol><h3 id="源码导入IDEA"><a href="#源码导入IDEA" class="headerlink" title="源码导入IDEA"></a>源码导入IDEA</h3><p><img src="/assets/blogImg/2019-04-17-3.png" alt="enter description here"></p><h3 id="运行hive-thriftserver2"><a href="#运行hive-thriftserver2" class="headerlink" title="运行hive-thriftserver2"></a>运行hive-thriftserver2</h3><p>从spark-2.4.0-bin-2.6.0-cdh5.7.0/sbin/start-thriftserver.sh 脚本中找到 hive-thriftserver2 的入口类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.hive.thriftserver.HiveThriftServer2</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-4.png" alt="enter description here"></p><h3 id="配置运行环境"><a href="#配置运行环境" class="headerlink" title="配置运行环境"></a>配置运行环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Menu -&gt; Run -&gt; Edit Configurations -&gt; 选择 + -&gt; Application</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-5.png" alt="enter description here"></p><p>-Dspark.master=local[2] 代表使用本地模式运行Spark代码</p><p>运行之前需要做一件很重要的事情，将 hive-thriftserver 这个子项目的pom依赖全部由provided改为compile：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;</span><br><span class="line">    &lt;scope&gt;compile&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt;</span><br><span class="line">    &lt;scope&gt;compile&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="添加运行依赖的jars"><a href="#添加运行依赖的jars" class="headerlink" title="添加运行依赖的jars"></a>添加运行依赖的jars</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Menu -&gt; File -&gt; Project Structure -&gt; Modules -&gt; spark-hive-thriftserver_2.11 -&gt; Dependencies 添加依赖 jars -&gt; &#123;Spark_home&#125;/assembly/target/scala-2.11/jars/</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019-04-17-6.png" alt="enter description here"></p><h3 id="中间遇到的问题"><a href="#中间遇到的问题" class="headerlink" title="中间遇到的问题"></a>中间遇到的问题</h3><p>问题一</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark\sql\hive-thriftserver\src\main\java\org\apache\hive\service\cli\thrift\ThriftCLIService.java</span><br><span class="line"></span><br><span class="line">Error:(52, 75) not found: value TCLIService</span><br><span class="line"></span><br><span class="line">public abstract class ThriftCLIService extends AbstractService implements TCLIService.Iface, Runnable &#123;………..</span><br></pre></td></tr></table></figure><p>解决办法： 在spark\sql\hive-thriftserver\src\gen\java右键中点Mark Directory as -&gt; Sources Root即可</p><p>问题二</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal  </span><br><span class="line">    at java.lang.ClassLoader.defineClass1(Native Method)</span><br></pre></td></tr></table></figure><p>解决办法：在 hive-thriftserve 子项目的pom文件中添加依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;xml-apis&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;xml-apis&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.4.01&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>问题三</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.BindException: Cannot assign requested address: Service &apos;sparkDriver&apos; failed after 16 retries (starting from 0)! Consider explicitly setting the appropriate port for the service &apos;sparkDriver&apos; (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.</span><br></pre></td></tr></table></figure><p>解决办法： 在 /etc/hosts 文件中配置相应的地址映射。</p><h3 id="成功运行"><a href="#成功运行" class="headerlink" title="成功运行"></a>成功运行</h3><p>在 HiveThriftServer2 中打断点进行调试源码即可。</p><p>打一个断点如下所示：<br><img src="/assets/blogImg/2019-04-17-7.png" alt="enter description here"><br>就能看到断点所打印出来的信息。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之三 UnifiedMemoryManager分析</title>
      <link href="/2019/04/16/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90/"/>
      <url>/2019/04/16/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h3 id="acquireExecutionMemory方法"><a href="#acquireExecutionMemory方法" class="headerlink" title="acquireExecutionMemory方法"></a>acquireExecutionMemory方法</h3><p>UnifiedMemoryManager中的accquireExecutionMemory方法：</p><p><img src="/assets/blogImg/2019-04-16内存管理1.png" alt="enter description here"></p><p>当前的任务尝试从executor中<font size="3"><strong>获取numBytes这么大的内存</strong></font></p><p>该方法直接向ExecutionMemoryPool索要所需内存，索要内存有以下几个关注点：</p><ul><li>当ExecutionMemory 内存充足，则不会触发向Storage申请内存</li><li>每个Task能够被使用的内存是被限制的</li><li>索要内存的大小</li></ul><p>我们通过源码来进行分析<br><a id="more"></a><br><strong><code>UnifiedMemoryManager.scala中</code></strong><br><img src="/assets/blogImg/2019-04-16内存管理2.png" alt="enter description here"></p><p>我们点进去后会发现，会调用ExecutionMemoryPool.acquireMemory()方法</p><p><strong><code>ExecutionMemoryPool.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理3.png" alt="enter description here"></p><p>我们可以发现每Task能够被使用的内存被限制在：</p><p>poolSize / (2 * numActiveTasks) ~ maxPoolSize / numActiveTasks 之间</p><p>val maxMemoryPerTask = maxPoolSize /numActiveTasks</p><p>val minMemoryPerTask = poolSize / (2 * numActiveTasks)</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理4.png" alt="enter description here"></p><p>其中maxPoolSize = maxMemory - math.min(storageMemoryUsed, storageRegionSize)</p><p>maxMemory = storage + execution的最大内存</p><p>poolSize = 当前这个pool的大小</p><p>maxPoolSize = execution pool的最大内存</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理5.png" alt="enter description here"></p><p>从上述代码中我们可以知道索要内存的大小：</p><p>val memoryReclaimableFromStorage=math.max(storageMemoryPool.memoryFree, storageMemoryPool.poolSize -storageRegionSize)</p><p>取决于StorageMemoryPool的剩余内存和 storageMemoryPool 从ExecutionMemory借来的内存哪个大，取最大的那个，作为可以重新归还的最大内存</p><p>用公式表达出来就是这一个样子：</p><p>ExecutionMemory 能借到的最大内存 = StorageMemory 借的内存 + StorageMemory 空闲内存</p><p><strong>注意：</strong>如果实际需要的小于能够借到的最大值，则以实际需要值为准</p><p>能回收的内存大小为：</p><p>val spaceToReclaim =storageMemoryPool.freeSpaceToShrinkPool ( math.min(extraMemoryNeeded,memoryReclaimableFromStorage))</p><h3 id="ExecutionMemoryPool-acquireMemory-解析"><a href="#ExecutionMemoryPool-acquireMemory-解析" class="headerlink" title="ExecutionMemoryPool.acquireMemory()解析"></a>ExecutionMemoryPool.acquireMemory()解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">while (true) &#123;</span><br><span class="line">  val numActiveTasks = memoryForTask.keys.size</span><br><span class="line">  val curMem = memoryForTask(taskAttemptId)</span><br><span class="line">  maybeGrowPool(numBytes - memoryFree)</span><br><span class="line">  val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">  val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">  val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br><span class="line">  val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem))</span><br><span class="line">  val toGrant = math.min(maxToGrant, memoryFree)</span><br><span class="line">  if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;</span><br><span class="line">    logInfo(s&quot;TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free&quot;)</span><br><span class="line">    lock.wait()</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    memoryForTask(taskAttemptId) += toGrant</span><br><span class="line">    return toGrant</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>整体流程解析：</p><p>程序一直处理该task的请求，直到系统判定无法满足该请求或者已经为该请求分配到足够的内存为止；如果当前execution内存池剩余内存不足以满足此次请求时，会向storage部分请求释放出被借走的内存以满足此次请求</p><p>根据此刻execution内存池的总大小maxPoolSize，以及从memoryForTask中统计出的处于active状态的task的个数计算出：</p><p>每个task能够得到的最大内存数 maxMemoryPerTask = maxPoolSize / numActiveTasks</p><p>每个task能够得到的最少内存数 minMemoryPerTask = poolSize /(2 * numActiveTasks)</p><p>根据申请内存的task当前使用的execution内存大小决定分配给该task多少内存，总的内存不能超过maxMemoryPerTask；但是如果execution内存池能够分配的最大内存小于numBytes，并且如果把能够分配的内存分配给当前task，但是该task最终得到的execution内存还是小于minMemoryPerTask时，该task进入等待状态，等其他task申请内存时再将其唤醒，唤醒之后如果此时满足，就会返回能够分配的内存数，并且更新memoryForTask，将该task使用的内存调整为分配后的值</p><font size="3"><strong>一个Task最少需要minMemoryPerTask才能开始执行</strong></font><h3 id="acquireStorageMemory方法"><a href="#acquireStorageMemory方法" class="headerlink" title="acquireStorageMemory方法"></a>acquireStorageMemory方法</h3><p>流程和acquireExecutionMemory类似，当storage的内存不足时，同样会向execution借内存，但区别是当且仅当ExecutionMemory有空闲内存时，StorageMemory 才能借走该内存</p><p><strong><code>UnifiedMemoryManager.scala中</code></strong></p><p><img src="/assets/blogImg/2019-04-16内存管理6.png" alt="enter description here"></p><p>从上述代码中我们可以知道能借到的内存数为：</p><p>val memoryBorrowedFromExecution = Math.min(onHeapExecutionMemoryPool.memoryFree,numBytes)</p><p>所以StorageMemory从ExecutionMemory借走的内存，完全取决于当时ExecutionMemory是不是有空闲内存；借到内存后，storageMemoryPool增加借到的这部分内存，之后同上一样，会调用StorageMemoryPool的acquireMemory()方法</p><h3 id="StorageMemoryPool-acquireMemory"><a href="#StorageMemoryPool-acquireMemory" class="headerlink" title="StorageMemoryPool.acquireMemory"></a>StorageMemoryPool.acquireMemory</h3><p><img src="/assets/blogImg/2019-04-16内存管理7.png" alt="enter description here"></p><p>整体流程解析：</p><p>在申请内存时，如果numBytes大于此刻storage内存池的剩余内存，即if (numBytesToFree &gt; 0)，那么需要storage内存池释放一部分内存以满足申请需求</p><p><strong>注意：</strong>这里的numBytesToFree可以理解为numBytes大小减去Storage内存池剩余大小，大于0，即所需要申请的numBytes大于Storage内存池剩余的内存</p><p>释放内存后如果memoryFree &gt;= numBytes，就会把这部分内存分配给申请内存的task，并且更新storage内存池的使用情况</p><p>同时StorageMemoryPool与ExecutionMemoryPool不同的是，他不会像前者那样分不到资源就进行等待，acquireStorageMemory只会返回一个true或是false，告知内存分配是否成功</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之二 统一内存管理及设计理念</title>
      <link href="/2019/04/10/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/"/>
      <url>/2019/04/10/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>Spark 1.6之后引入的统一内存管理机制，与静态内存管理的区别在于Storage和Execution共享同一块内存空间，可以动态占用对方的空闲区域</p><p><img src="/assets/blogImg/2019-04-10-内存管理1.png" alt="enter description here"></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><ul><li>设定基本的Storage内存和Execution内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘，若己方空间不足而对方空余时，可借用对方的空间（存储空间不足是指不足以放下一个完整的 Block）</li><li>Execution的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li><li>Storage的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle过程中的很多因素，实现起来较为复杂<a id="more"></a><h3 id="动态内存占用机制"><a href="#动态内存占用机制" class="headerlink" title="动态内存占用机制"></a>动态内存占用机制</h3></li></ul><p>动态占用机制如下图所示：</p><p><img src="/assets/blogImg/2019-04-10-内存管理2.png" alt="enter description here"></p><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧</p><p>譬如：如果Storage的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理</p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>如下图所示，相较于静态内存管理，引入了动态占用机制</p><p><img src="/assets/blogImg/2019-04-10-内存管理3.png" alt="enter description here"></p><h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>spark从1.6版本以后，默认的内存管理方式就调整为统一内存管理模式</p><p>由UnifiedMemoryManager实现</p><p>Unified MemoryManagement模型，重点是打破运行内存和存储内存之间的界限，使spark在运行时，不同用途的内存之间可以实现互相的拆借</p><h3 id="Reserved-Memory"><a href="#Reserved-Memory" class="headerlink" title="Reserved Memory"></a>Reserved Memory</h3><p>这部分内存是预留给系统使用,在1.6.1默认为300MB，这一部分内存不计算在Execution和Storage中；可通过spark.testing.reservedMemory进行设置；然后把实际可用内存减去这个reservedMemor得到usableMemory</p><p>ExecutionMemory 和 StorageMemory 会共享usableMemory * spark.memory.fraction(默认0.75)</p><font color="red" size="3"><b>注意：</b></font><ul><li>在Spark 1.6.1 中spark.memory.fraction默认为0.75</li><li>在Spark 2.2.0 中spark.memory.fraction默认为0.6</li></ul><h3 id="User-Memory"><a href="#User-Memory" class="headerlink" title="User Memory"></a>User Memory</h3><p>分配Spark Memory剩余的内存，用户可以根据需要使用</p><p>在Spark 1.6.1中，默认占(Java Heap - Reserved Memory) * 0.25</p><p>在Spark 2.2.0中，默认占(Java Heap - Reserved Memory) * 0.4</p><h3 id="Spark-Memory"><a href="#Spark-Memory" class="headerlink" title="Spark Memory"></a>Spark Memory</h3><p>计算方式为：<code>(Java Heap – ReservedMemory) * spark.memory.fraction</code></p><p>在Spark 1.6.1中，默认为(Java Heap - 300M) * 0.75</p><p>在Spark 2.2.0中，默认为(Java Heap - 300M) * 0.6</p><p>Spark Memory又分为Storage Memory和Execution Memory两部分</p><p>两个边界由spark.memory.storageFraction设定，默认为0.5</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>相对于静态内存模型（即Storage和Execution相互隔离、彼此不可拆借），动态内存实现了存储和计算内存的动态拆借：</p><ul><li>当计算内存超了，它会从空闲的存储内存中借一部分内存使用</li><li>存储内存不够用的时候，也会向空闲的计算内存中拆借</li></ul><p>值得注意的地方是：</p><ul><li>被借走用来执行运算的内存，在执行完任务之前是不会释放内存的</li><li>通俗的讲，运行任务会借存储的内存，但是它直到执行完以后才能归还内存</li></ul><h3 id="和动态内存相关的参数"><a href="#和动态内存相关的参数" class="headerlink" title="和动态内存相关的参数"></a>和动态内存相关的参数</h3><ul><li><p>spark.memory.fraction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Spark 1.6.1 默认0.75，Spark 2.2.0 默认0.6 </span><br><span class="line">这个参数用来配置存储和计算内存占整个可用内存的比例 </span><br><span class="line">这个参数设置的越低，也就是存储和计算内存占可用的比例越低，就越可能频繁的发生内存的释放（将内存中的数据写磁盘或者直接丢弃掉） </span><br><span class="line">反之，如果这个参数越高，发生释放内存的可能性就越小 </span><br><span class="line">这个参数的目的是在jvm中留下一部分空间用来保存spark内部数据，用户数据结构，并且防止对数据的错误预估可能造成OOM的风险，这就是Other部分</span><br></pre></td></tr></table></figure></li><li><p>spark.memory.storageFraction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认 0.5；在统一内存中存储内存所占的比例，默认是0.5，如果使用的存储内存超过了这个范围，缓存的数据会被驱赶</span><br></pre></td></tr></table></figure></li><li><p>spark.memory.useLegacyMode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">默认false；设置是否使用saprk1.5及以前遗留的内存管理模型，即静态内存模型，前面的文章介绍过这个，主要是设置以下几个参数：</span><br><span class="line">spark.storage.memoryFraction</span><br><span class="line">spark.storage.safetyFraction</span><br><span class="line">spark.storage.unrollFraction</span><br><span class="line">spark.shuffle.memoryFraction</span><br><span class="line">spark.shuffle.safetyFraction</span><br></pre></td></tr></table></figure></li></ul><h3 id="动态内存设计中的取舍"><a href="#动态内存设计中的取舍" class="headerlink" title="动态内存设计中的取舍"></a>动态内存设计中的取舍</h3><p>因为内存可以被Execution和Storage拆借，我们必须明确在这种机制下，当内存压力上升的时候，该如何进行取舍？</p><p>从三个角度进行分析：</p><ul><li>倾向于优先释放计算内存</li><li>倾向于优先释放存储内存</li><li>不偏不倚，平等竞争</li></ul><h4 id="释放内存的代价"><a href="#释放内存的代价" class="headerlink" title="释放内存的代价"></a>释放内存的代价</h4><p>释放存储内存的代价取决于Storage Level.：</p><ul><li>如果数据的存储level是MEMORY_ONLY的话代价最高，因为当你释放在内存中的数据的时候，你下次再复用的话只能重新计算了</li><li>如果数据的存储level是MEMORY_AND_DIS_SER的时候，释放内存的代价最低，因为这种方式，当内存不够的时候，它会将数据序列化后放在磁盘上，避免复用的时候再计算，唯一的开销在I/O</li></ul><p><strong>综述：</strong></p><p>释放计算内存的代价不是很显而易见：</p><ul><li>这里没有复用数据重计算的代价，因为计算内存中的任务数据会被移到硬盘，最后再归并起来（后面会有文章介绍到这点）</li><li>最近的spark版本将计算的中间数据进行压缩使得序列化的代价降到了最低</li></ul><p>值得注意的是：</p><ul><li>移到硬盘的数据总会再重新读回来</li><li>从存储内存移除的数据也许不会被用到，所以当没有重新计算的风险时，释放计算的内存要比释放存储内存的代价更高（假使计算内存部分刚好用于计算任务的时候）</li></ul><h4 id="实现复杂度"><a href="#实现复杂度" class="headerlink" title="实现复杂度"></a>实现复杂度</h4><ul><li>实现释放存储内存的策略很简单：我们只需要用目前的内存释放策略释放掉存储内存中的数据就好了</li><li>实现释放计算内存却相对来说很复杂</li></ul><p>这里有2个释放计算内存的思路：</p><ul><li>当运行任务要拆借存储内存的时候，给所有这些任务注册一个回调函数以便日后调这个函数来回收内存</li><li>协同投票来进行内存的释放</li></ul><p>值得我们注意的一个地方是，以上无论哪种方式，都需要考虑一种特殊情况：</p><ul><li>即如果我要释放正在运行的计算任务的内存，同时我们想要cache到存储内存的一部分数据恰巧是由这个计算任务产生的</li><li>此时，如果我们现在释放掉正在运行的任务的计算内存，就需要考虑在这种环境下会造成的饥饿情况：即生成cache的数据的计算任务没有足够的内存空间来跑出cache的数据，而一直处于饥饿状态（因为计算内存已经不够了，再释放计算内存更加不可取）</li><li><p>此外，我们还需要考虑：一旦我们释放掉计算内存，那么那些需要cache的数据应该怎么办？有2种方案：</p><ol><li>最简单的方式就是等待，直到计算内存有足够的空闲，但是这样就可能会造成死锁，尤其是当新的数据块依赖于之前的计算内存中的数据块的时候</li><li>另一个可选的操作就是丢掉那些最新的正准备写入到磁盘中的块并且一旦当计算内存够了又马上加载回来。为了避免总是丢掉那些等待中的块，我们可以设置一个小的内存空间(比如堆内存的5%)去确保内存中至少有一定的比例的的数据块</li></ol></li></ul><p><strong>综述：</strong></p><p>所给的两种方法都会增加额外的复杂度，这两种方式在第一次的实现中都被排除了</p><p>综上目前看来，释放掉存储内存中的计算任务在实现上比较繁琐，目前暂不考虑</p><p>即计算内存借了存储内存用来计算任务，然后释放，这种不考虑；计算内存借来内存之后，是可以不还的</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>我们倾向于<font color="red">优先释放掉存储内存</font></p><p>即如果存储内存拆借了计算内存，当计算内存需要进行计算并且内存空间不足的时候，<font color="red">优先把计算内存中这部分被用来存储的内存释放掉</font></p><h3 id="可选设计"><a href="#可选设计" class="headerlink" title="可选设计"></a>可选设计</h3><h4 id="1-设计方案"><a href="#1-设计方案" class="headerlink" title="1.设计方案"></a><font size="4"><b>1.设计方案</b></font></h4><p>结合我们前面的描述，针对在内存压力下释放存储内存有以下几个可选设计：</p><ul><li><p>设计1：释放存储内存数据块，完全平滑</p><p>计算和存储内存共享一片统一的区域，没有进行统一的划分</p><ol><li>内存压力上升，优先释放掉存储内存部分中的数据</li><li>如果压力没有缓解，开始将计算内存中运行的任务数据进行溢写磁盘</li></ol></li><li><p>设计2：释放存储内存数据块，静态存储空间预留，存储空间的大小是定死的</p><p>这种设计和1设计很像，不同的是会专门划分一个预留存储内存区域：在这个内存区域内，存储内存不会被释放，只有当存储内存超出这个预留区域，才会被释放（即超过50%了就被释放，当然50%为默认值）。这个参数由spark.memory.storageFraction（默认值为0.5，即计算和存储内存的分割线）配置</p></li><li><p>设计3：释放存储内存数据块，动态存储空间预留</p><p>这种设计于设计2很相似，但是存储空间的那一部分区域不再是静态设置的了，而是动态分配；这样设置带来的不同是计算内存可以尽可能借走存储内存中可用的部分，因为存储内存是动态分配的</p></li></ul><p><strong>结论：最终采用的的是设计3</strong></p><h4 id="2-各个方案的优劣"><a href="#2-各个方案的优劣" class="headerlink" title="2.各个方案的优劣"></a><font size="4"><b>2.各个方案的优劣</b></font></h4><ul><li><p>设计1被拒绝的原因</p><p>设计1不适合那些对cache内存重度依赖的saprk任务，因为设计1中只要内存压力上升就释放存储内存</p></li><li><p>设计2被拒绝的原因</p><p>设计2在很多情况下需要用户去设置存储内存中那部分最小的区域<br>另外无论我们设置一个具体值，只要它非0，那么计算内存最终也会达到一个上限，比如，如果我们将存储内存设置为0.6，那么有效的执行内存就是：</p><ul><li>Spark 1.6.1 可用内存0.40.75</li><li><p>Spark 2.2.0 可用内存0.40.6</p><p>那么如果用户没有cache数据，或是cache的数据达不到设置的0.6，那么这种情况就又回到了静态内存模型那种情况，并没有改善什么</p></li></ul></li><li><p>最终选择设计3的原因</p><p>设计3就避免了2中的问题只要存储内存有空余的情况，那么计算内存就可以借用</p><p>需要关注的问题是：</p><ul><li>当计算内存已经使用了存储内存中的所有可用内存但是又需要cache数据的时候应该怎么处理</li><li>最早的版本中直接释放最新的block来避免引入执行驱赶策略（eviction策略，上述章节中有介绍）的复杂性</li></ul></li></ul><font size="4"><b>设计3是唯一一个同时满足下列条件的：</b></font><ol><li>存储内存没有上限</li><li>计算内存没有上限</li><li>保障了存储空间有一个小的保留区域</li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理之一 静态内存管理</title>
      <link href="/2019/04/03/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2019/04/03/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h3 id="Spark内存管理简介"><a href="#Spark内存管理简介" class="headerlink" title="Spark内存管理简介"></a>Spark内存管理简介</h3><p>Spark从<font color="blue"><strong>1.6</strong></font>开始引入了动态内存管理模式，即执行内存和存储内存之间可以相互抢占</p><p>Spark提供了2种内存分配模式：</p><ul><li>静态内存管理</li><li>统一内存管理</li></ul><p>本系列文章将分别对这两种内存管理模式的优缺点以及设计原理进行分析（主要基于Spark 1.6.1的内存管理进行分析）</p><font color="blue">在本篇文章中，将先对<font size="3" color="red">静态内存管理</font>进行介绍</font><a id="more"></a><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>在Spark最初采用的静态内存管理机制下，存储内存、执行内存和其它内存的大小在Spark应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置，堆内内存的分配如下图所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理1.png" alt="enter description here"></p><p>默认情况下，spark内存管理采用unified模式，如果要开启静态内存管理模式，需要将spark.memory.useLegacyMode参数调为true（默认为false），1.6.1版本的官网配置如下所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理2.png" alt="enter description here"></p><p>将参数调整为true之后，就会进入到静态内存管理中来，可以通过SparkEnv.scala中发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理3.png" alt="enter description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果spark.memory.useLegacyMode为true，就进入到StaticMemoryManager（静态内存管理）；</span><br><span class="line">如果为false，就进入到UnifiedMemoryManager（统一内存管理）；</span><br><span class="line">同时我们可以发现该参数的默认值为false，即默认情况下就会调用统一内存管理类。</span><br></pre></td></tr></table></figure><h3 id="Execution内存"><a href="#Execution内存" class="headerlink" title="Execution内存"></a>Execution内存</h3><p>####可用的Execution内存</p><p>用于shuffle操作的内存，取决于join、sort、aggregation等过程频繁的IO需要的Buffer临时数据存储</p><p>简单来说，spark在shuffle write的过程中，每个executor会将数据写到该executor的物理磁盘上，下一个stage的task会去上一个stage拉取其所需要处理的数据，并且是边拉取边进行处理的（和MapReduce的拉取合并数据基本一样），这个时候就会用到一个aggregate的数据结构，比如hashmap这种边拉取数据边进行聚合。这部分内存就被称为execution内存</p><p>从StaticMemoryManager.scala中的getMaxExecutionMemory方法中，我们可以发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理4.png" alt="enter description here"></p><p>每个executor分配给execution的内存为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ExecutionMemory = systemMaxMemory * memoryFraction * safetyFraction </span><br><span class="line">默认情况下为：systemMaxMemory * 0.2 * 0.8 = 0.16 * systemMaxMemory </span><br><span class="line">即默认为executor最大可用内存 * 0.16</span><br></pre></td></tr></table></figure><p>Execution内存再运行的时候会被分配给运行在JVM上的task；这里不同的是，分配给每个task的内存并不是固定的，而是动态的；spark不是一上来就分配固定大小的内存块给task，而是允许一个task占据JVM所有execution内存</p><p>每个JVM上的task可以最多申请至多1/N的execution内存，其中N为active task的个数，由spark.executor.cores指定；如果task的申请没有被批准，它会释放一部分内存，并且下次申请的时候，它会申请更小的一部分内存</p><p><strong>注：</strong></p><ul><li>每个Executor单独运行在一个JVM进程中，每个Task则是运行在Executor中的线程</li><li>spark.executor.cores设置的是每个executor的core数量</li><li>task的数量就是partition的数量</li><li>一般来说，一个core设置2~4个partition</li></ul><font color="red" size="3"><b>注意：</b></font> <font color="red"><br>为了防止过多的spilling数据，只有当一个task分配到的内存达到execution内存1/2N的时候才会spill，如果目前空闲的内存达不到1/2N的时候，内存申请会被阻塞直到其它的task spill掉它们的内存；<br><br>如果不这样限制，假设当前一个任务占据了绝大部分内存，那么新来的task会一直往硬盘spill数据，这样就会导致比较严重的I/O问题；而我们做了一定程度的限制，会进行一定程度的阻塞等待，对于频繁的小数据集的I/O会有一定的减缓<br><br>例子：某executor先启动一个task A，并在task B启动前快速占用了所有可用的内存；在B启用之后N变成了2，task B会阻塞直到task A spill，自己可以获得1/2N=1/4的execution内存的时候；而一大task B获取到了1/4的内存，A和B就都有可能spill了<br></font><h3 id="预留内存"><a href="#预留内存" class="headerlink" title="预留内存"></a>预留内存</h3><p>Spark之所以会有一个SafetyFraction这样的参数，是为了避免潜在的OOM。例如，进行计算时，有一个提前未预料到的比较大的数据，会导致计算时间延长甚至OOM，safetyFraction为storage和execution都提供了额外的buffer以防止此类的数据倾斜；这部分内存叫作预留内存</p><p>####Storage内存</p><p>####可用的Storage内存</p><p>该部分内存用作对RDD的缓存（如调用cache、persist等方法），节点间传输的广播变量</p><p>StaticMemoryManager.scala中的getMaxStorageMemory方法发现：</p><p><img src="/assets/blogImg/2019-04-03-内存管理5.png" alt="enter description here"></p><p>最后为每个executor分配到的storage的内存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">StorageMemory = systemMaxMemory * memoryFraction * safetyFraction </span><br><span class="line">默认情况下为：systemMaxMemory * 0.6 * 0.9 = 0.54 * systemMaxMemory </span><br><span class="line">即默认分配executor最大可用内存的0.54</span><br></pre></td></tr></table></figure><h4 id="预留内存-1"><a href="#预留内存-1" class="headerlink" title="预留内存"></a>预留内存</h4><p>同Execution内存中的预留部分</p><h3 id="Unroll"><a href="#Unroll" class="headerlink" title="Unroll"></a>Unroll</h3><p>Unroll是storage中比较特殊的一部分，它默认占据storage总内存的20%</p><p>BlockManager是spark自己实现的内部分布式文件系统，BlockManager接受数据（可能从本地或者其他节点）的时候是以iterator的形式，并且这些数据是有序列化和非序列化的，因此需要注意以下两点：</p><ul><li>Iterator在物理内存上是不连续的，如果后续spark要把数据装载进内存的话，就需要把这些数据放进一个array（物理上连续）</li><li>另外，序列化数据需要进行展开，如果直接展开序列化的数据，会造成OOM，所以BlockManager会逐渐的展开这个iterator，并逐渐检查内存里是否还有足够的空间用来展开数据放进array里</li></ul><p>StaticMemoryManager.scala中的maxUnrollMemory方法：</p><p><img src="/assets/blogImg/2019-04-03-内存管理6.png" alt="enter description here"></p><p>Unroll的优先级别还是比较高的，它使用的内存空间是可以从storage中借用的，如果在storage中没有现存的数据block，它甚至可以占据整个storage空间；如果storage中有数据block，它可以最大drop掉内存的数据是通过spark.storage.unrollFraction来控制的，通过源码可知这部分的默认值为0.2</p><font color="red" size="3"><b>注意：</b></font> <font color="red"><br>这个20%的空间并不是静态保留的，而是通过drop掉内存中的数据block来分配的（动态的分配过程）；如果unroll失败了，spark会把这部分数据evict到硬盘中去<br></font><h3 id="eviction策略"><a href="#eviction策略" class="headerlink" title="eviction策略"></a>eviction策略</h3><p>在spark技术文档中，eviction一词经常出现，eviction并不是单纯字面上驱逐的意思。说句题外话，spark通常被我们叫做内存计算框架，但是从严格意义上说，spark并不是内存计算的新技术；无论是cache还是persist这类算子，spark在内存安排上，绝大多数用的都是LRU策略（LRU可以说是一种算法，也可以算是一种原则，用来判断如何从Cache中清除对象，而LRU就是“近期最少使用”原则，当Cache溢出时，最近最少使用的对象将被从Cache中清除）。即当内存不够的时候，会evict掉最远使用过的内存数据block；当evict的时候，spark会将该数据块evict到硬盘，而不是单纯的抛弃掉</p><p>无论是storage还是execution的内存空间，当内存区域的空间不够用的时候，spark都会evict数据到硬盘</p><h3 id="Other部分"><a href="#Other部分" class="headerlink" title="Other部分"></a>Other部分</h3><p>这部分的内存用于程序本身运行所需要的内存，以及用户定义的数据结构和创建的对象，此内存由上面两部分：storage、execution决定的，默认为0.2</p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>Spark1.6开始引入了Off-heap memory（详见SPARK-11389）</p><p>堆外的空间分配较为简单，只有存储内存和执行内存，如图所示：</p><p><img src="/assets/blogImg/2019-04-03-内存管理7.png" alt="enter description here"></p><p>可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定（默认为0.5），由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域</p><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>在Spark的设计文档中，指出了静态内存管理的局限性：</p><p>没有适用于所有应用的默认配置，通常需要开发人员针对不同的应用进行不同的参数进行配置：比如根据任务的执行逻辑，调整shuffle和storage的内存占比来适应任务的需求</p><p>这样需要开发人员具备较高的spark原理知识</p><p>那些不cache数据的应用在运行的时候只会占用一小部分可用内存，而默认的内存配置中storage就用去了60%，造成了浪费</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark Executor Dead快速剖析</title>
      <link href="/2019/03/12/%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90/"/>
      <url>/2019/03/12/%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>通过Spark UI查看Executors，发现存在Executor Dead的情况</p><p><img src="/assets/blogImg/2019-03-12-1.png" alt="enter description here"></p><p>进一步查看dead Executor stderr日志，发现如下报错信息：</p><p><img src="/assets/blogImg/2019-03-12-2.png" alt="enter description here"></p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><a id="more"></a> <font color="#FF4500">打开GC日志，配置如下</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf &quot;spark.executor.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;</span><br><span class="line">--conf &quot;spark.driver.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;</span><br></pre></td></tr></table></figure><font color="#FF4500">打开exeutor gc日志，发现一直在<strong>full gc</strong>，几乎每秒1次，基本处于拒绝服务状态</font><p><img src="//yoursite.com/2019/03/12/生产Spark_Executor_Dead快速剖析/blogImg/2019-03-12-3.png" alt="enter description here"></p><font size="4"><b>至此找到问题原因，executor内存不够导致dead，调大executor内存即可 ，所以排错方法定位很重要！</b></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark Streaming 黑名单过滤案例</title>
      <link href="/2019/03/08/%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B/"/>
      <url>/2019/03/08/%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>测试数据(通过Socket传入)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">20180808,zs</span><br><span class="line">20180808,ls</span><br><span class="line">20180808,ww</span><br></pre></td></tr></table></figure><p>黑名单列表(生产存在表)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zs</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ol><li>原始日志可以通过Streaming直接读取成一个DStream</li><li><p>名单通过RDD来模拟一份</p><a id="more"></a><h3 id="逻辑实现"><a href="#逻辑实现" class="headerlink" title="逻辑实现"></a>逻辑实现</h3></li><li><p>将DStream转成以下格式(黑名单只有名字)</p><p><code>(zs,(20180808,zs))(ls,(20180808,ls))(ww,( 20180808,ww))</code></p></li><li><p>将黑名单转成</p><p><code>(zs, true)(ls, true)</code></p></li><li><p>DStram与RDD进行LeftJoin(DStream能与RDD进行Join就是借用的transform算子)</p></li></ol><h3 id="具体代码实现及注释"><a href="#具体代码实现及注释" class="headerlink" title="具体代码实现及注释"></a>具体代码实现及注释</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.spark.Streaming</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">/**</span><br><span class="line">  * @author soulChun</span><br><span class="line">  * @create 2019-01-10-16:12</span><br><span class="line">  */</span><br><span class="line">object TransformApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatafulFunApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf,Seconds(10))</span><br><span class="line">    //构建黑名单</span><br><span class="line">    val blacks = List(&quot;zs&quot;, &quot;ls&quot;)</span><br><span class="line">    //通过map操作将黑名单结构转换成(zs, true)(ls, true)</span><br><span class="line">    val blackRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, true))</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;localhost&quot;, 8769)</span><br><span class="line">    //lines (20180808,zs)</span><br><span class="line">    //lines 通过map.split(1)之后取得就是zs,然后加一个x就转成了(zs,(20180808,zs)).就可以和blackRDD进行Join了</span><br><span class="line">    val clicklog = lines.map(x =&gt; (x.split(&quot;,&quot;)(1), x)).transform(rdd =&gt; &#123;</span><br><span class="line">      //Join之后数据结构就变成了(zs,[(20180808,zs),true]),过滤掉第二个元素中的第二个元素等于true的</span><br><span class="line">      rdd.leftOuterJoin(blackRDD).filter(x =&gt; x._2._2.getOrElse(false) != true)</span><br><span class="line">        //我们最后要输出的格式是(20180808,zs)，所以取Join之后的第二个元素中的第一个元素</span><br><span class="line">        .map(x =&gt; x._2._1)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后输出：<br><img src="/assets/blogImg/2019-03-08-1.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> sparkstreaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>刚出炉的3家大数据面试题(含高级),你会吗？</title>
      <link href="/2019/03/07/%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97/"/>
      <url>/2019/03/07/%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="第一家大数据开发的面试题"><a href="#第一家大数据开发的面试题" class="headerlink" title="第一家大数据开发的面试题:"></a>第一家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试1.png" alt="enter description here"></p><h3 id="第二家大数据开发的面试题"><a href="#第二家大数据开发的面试题" class="headerlink" title="第二家大数据开发的面试题:"></a>第二家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试2.png" alt="enter description here"></p><h3 id="第三家大数据开发的面试题"><a href="#第三家大数据开发的面试题" class="headerlink" title="第三家大数据开发的面试题:"></a>第三家大数据开发的面试题:</h3><p><img src="/assets/blogImg/2019-03-07面试3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkShuffle详解剖析</title>
      <link href="/2019/03/06/SparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90/"/>
      <url>/2019/03/06/SparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h2 id="HashShuffle"><a href="#HashShuffle" class="headerlink" title="HashShuffle"></a>HashShuffle</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>所谓Shuffle就是将不同节点上相同的Key拉取到一个节点的过程。这之中涉及到各种IO，所以执行时间势必会较长，<font color="#FF4500">Spark的Shuffle在1.2之前默认的计算引擎是HashShuffleManager</font>，不过HashShuffleManager有一个十分严重的弊端，就是会产生大量的中间文件。<font color="#FF4500">在1.2之后默认Shuffle改为SortShuffleManager</font>，相对于之前，在每个Task虽然也会产生大量中间文件，但是最后会将所有的临时文件合并（merge）成一个文件。因此Shuffle read只需要读取时，根据索引拿到每个磁盘的部分数据就可以了<br><a id="more"></a></p><h3 id="测试条件"><a href="#测试条件" class="headerlink" title="测试条件"></a>测试条件</h3><p><code>每个Executor只有一个CUP（core），同一时间每个Executor只能执行一个task</code></p><h3 id="HashShuffleManager未优化版本"><a href="#HashShuffleManager未优化版本" class="headerlink" title="HashShuffleManager未优化版本"></a>HashShuffleManager未优化版本</h3><p>首先从shuffle write阶段，主要是在一个stage结束后，为了下一个stage可以执行shuffle，将每一个task的数据按照key进行分类，对key进行hash算法，从而使相同的key写入同一个文件，每个磁盘文件都由下游stage的一个task读取。在写入磁盘时，先将数据写入内存缓冲，当内存缓冲填满后，才会溢写到磁盘文件（似乎所以写文件都需要写入先写入缓冲区，然后再溢写，防止频繁IO）</p><p>我们可以先算一下当前stage的一个task会为下一个stage创建多少个磁盘文件。若下一个stage有100个task，则当前stage的每一个task都将创建100个文件，若当前stage要处理的task为50个，共有10个Executor，也就是说每个Executor共执行5个task，5x100x10=1000。也就是说这么一个小规模的操作会生产5000个文件。这是相当可观的。</p><p>而shuffle read 通常是一个stage一开始要做的事情。此时stage的每一个task去将上一个stage的计算结果的所有相同的key从不同节点拉到自己所在节点。进行聚合或join操作。在shuffle write过程，每个task给下游的每个task都创建了一个磁盘文件。在read过程task只需要去上游stage的task中拉取属于自己的磁盘文件。</p><p>shuffle read是边拉取边聚合。每一个read task都有一个buffer缓冲，然后通过内存中的Map进行聚合，每次只拉取buffer大小的数据，放到缓冲区中聚合，直到所有数据都拉取完。</p><p><img src="/source/assets/blogImg/2019-03-06-1.png" alt="HashShuffleManager未优化版本"></p><h3 id="优化版本"><a href="#优化版本" class="headerlink" title="优化版本"></a>优化版本</h3><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启这个机制之后，在shuffle write时，task并不是为下游的每一个task创建一个磁盘文件。引入了shuffleFileGroup的概念，每个shuffleFileGroup都对应一批磁盘文件。磁盘文件数量与下游task相同。只是仅仅第一批执行的task会创建一个shuffleFIleGroup，将数据写入到对应磁盘文件。</p><p>在执行下一批的task时，会复用已经创建好的shuffleFIleGroup和磁盘文件，即数据会继续写入到已有的磁盘文件。该机制会允许不同task复用同一个磁盘文件，对于多个task进行了一定程度的合并，大幅度减少shuffle write时，文件的数量，提升性能。</p><p>相对于优化前，每个Executor之前需要创建五百个磁盘文件，因为之前需要5个task线性执行，而使用参数优化之后，就每个Executor只需要100个就可以了，这样10个Executor就是1000个文件，这比优化前整整减少了4000个文件。</p><p><img src="/source/assets/blogImg/2019-03-06-2.png" alt="优化版本"></p><h2 id="SortShuffle"><a href="#SortShuffle" class="headerlink" title="SortShuffle"></a>SortShuffle</h2><p>在<font color="#FF4500">Spark1.2版本之后，出现了SortShuffle</font>，这种方式以更少的中间磁盘文件产生而远远优于HashShuffle。而它的运行机制主要分为两种。一种为普通机制，另一种为bypass机制。而bypass机制的启动条件为，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。即当read task不是那么多的时候，采用bypass机制是更好的选择。</p><h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一遍写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值，如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。</p><p>在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件。</p><p>最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引，start offset和end offset。</p><p>这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50个磁盘文件。</p><p><img src="/source/assets/blogImg/2019-03-06-3.png" alt="普通运行机制"></p><h3 id="bypass机制"><a href="#bypass机制" class="headerlink" title="bypass机制"></a>bypass机制</h3><font size="4"><b>bypass机制运行条件：</b></font><ol><li><strong>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</strong></li><li><strong>不是聚合类的shuffle算子（比如reduceByKey）。</strong></li></ol><p>在这种机制下，当前stage的task会为每个下游的task都创建临时磁盘文件。将数据按照key值进行hash，然后根据hash值，将key写入对应的磁盘文件中（个人觉得这也相当于一次另类的排序，将相同的key放在一起了）。最终，同样会将所有临时文件依次合并成一个磁盘文件，建立索引。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>该机制与未优化的hashshuffle相比，没有那么多磁盘文件，下游task的read操作相对性能会更好。</p><p>该机制与sortshuffle的普通机制相比，在readtask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。</p><p><img src="/source/assets/blogImg/2019-03-06-4.png" alt="优点"></p><font color="#FF4500"><br></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最佳实践之Spark写入Hfile经典案例</title>
      <link href="/2019/03/01/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/"/>
      <url>/2019/03/01/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><font size="5" color="blue"><b>本文由小伙伴提供</b></font><p>将HDFS上的数据解析出来，然后通过hfile方式批量写入Hbase(需要多列写入) 写⼊数据的关键api:<br><a id="more"></a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">saveAsNewAPIHadoopFile(</span><br><span class="line">        stagingFolder,</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[KeyValue],</span><br><span class="line">        classOf[HFileOutputFormat2],</span><br><span class="line">        job.getConfiguration)</span><br></pre></td></tr></table></figure><p></p><font size="4" color="red"><b>特殊地方：</b></font><ol><li><p><strong>最初写hfile警告</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Does it contain files in subdirectories that correspond to column family names</span><br></pre></td></tr></table></figure></li></ol><pre><code>这个原因大概三种* 代码问题* 数据源问题* setMapOutputKeyClass 和 saveAsNewAPIHadoopFile中的Class不不⼀一致这里是我的是数据源问题</code></pre><ol start="2"><li><p><strong>正常写put操作的时候，服务端自动帮助排序，因此在使用put操作的时候没有涉及到这样的错误</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure></li></ol><pre><code>但是在写hfile的时候如果出现报错:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure>这样的错误，一般会认为rowkey没有做好排序，然后傻fufu的去验证了一下，rowkey的确做了排序。真正原因:`spark写hfile时候是按照rowkey+列族+列名进⾏行排序的，因此在写⼊数据的时候，要做到整体有序 (事情还没完)`</code></pre><ol start="3"><li><p><strong>因为需要多列写入，最好的⽅式:要么反射来动态获取列名称和列值 、 要么通过datafame去获取(df.columns)</strong><br>反射方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] = rdd.map</span><br><span class="line">&#123;</span><br><span class="line">        line =&gt;</span><br><span class="line">          val rowkey = line.vintime</span><br><span class="line">          val clazz = Class.forName(XXXXXXXXXXXXXXXX)</span><br><span class="line">          val fields = clazz.getDeclaredFields</span><br><span class="line">          var list = new ListBuffer[String]()</span><br><span class="line">          var kvlist = new ListBuffer[KeyValue]()//</span><br><span class="line">          if (fields != null &amp;&amp; fields.size &gt; 0) &#123;</span><br><span class="line">            for (field &lt;- fields) &#123;</span><br><span class="line">              field.setAccessible(true)</span><br><span class="line">              val column = field.getName</span><br><span class="line">              list.append(column)</span><br><span class="line">&#125; &#125;</span><br><span class="line">          val newList = list.sortWith(_ &lt; _)</span><br><span class="line">          val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">          for(column &lt;- newList)&#123;</span><br><span class="line">            val declaredField: Field =</span><br><span class="line">line.getClass.getDeclaredField(column)</span><br><span class="line">&#125;</span><br><span class="line">  declaredField.setAccessible(true)</span><br><span class="line">  val value = declaredField.get(line).toString</span><br><span class="line">  val kv: KeyValue = new KeyValue(</span><br><span class="line">    Bytes.toBytes(rowkey),</span><br><span class="line">    Bytes.toBytes(columnFamily),</span><br><span class="line">    Bytes.toBytes(column),</span><br><span class="line">    Bytes.toBytes(value))</span><br><span class="line">  kvlist.append(kv)</span><br><span class="line">&#125;</span><br><span class="line">(ik, kvlist)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><pre><code>datafame的方式:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">val tmpData: RDD[(ImmutableBytesWritable, util.LinkedList[KeyValue])] =</span><br><span class="line">df.rdd.map(</span><br><span class="line">      line =&gt;&#123;</span><br><span class="line">        val rowkey = line.getAs[String](&quot;vintime&quot;)</span><br><span class="line">        val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">        var linkedList = new util.LinkedList[KeyValue]()</span><br><span class="line">        for (column &lt;- columns) &#123;</span><br><span class="line">        val kv: KeyValue = new KeyValue(</span><br><span class="line">            Bytes.toBytes(rowkey),</span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            Bytes.toBytes(column),</span><br><span class="line">            Bytes.toBytes(line.getAs[String](column)))</span><br><span class="line">          linkedList.add(kv)</span><br><span class="line">        &#125;</span><br><span class="line">        (ik, linkedList)</span><br><span class="line">      &#125;)</span><br><span class="line">    val result: RDD[(ImmutableBytesWritable, KeyValue)] =</span><br><span class="line">tmpData.flatMapValues(</span><br><span class="line">      s =&gt; &#123;</span><br><span class="line">        val values: Iterator[KeyValue] =</span><br><span class="line">JavaConverters.asScalaIteratorConverter(s.iterator()).asScala</span><br><span class="line">        values</span><br><span class="line">      &#125;</span><br><span class="line">    ).sortBy(x =&gt;x._1 , true)</span><br></pre></td></tr></table></figure>仔细观察可以发现，其实两者都做了排序操作，但是即便经过(1)步骤后仍然报错:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Added a key not lexically larger than previous</span><br></pre></td></tr></table></figure>那么再回想⼀下之前写hfile的要求:rowkey+列族+列都要有序，那么如果出现数据的重复，也不算是有序的操作! 因为，做一下数据的去重:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val key: RDD[(String, TransferTime)] = data.reduceByKey((x, y) =&gt; y)</span><br><span class="line">val unitData: RDD[TransferTime] = key.map(line =&gt; line._2)</span><br></pre></td></tr></table></figure>果然，这样解决了:Added a key not lexically larger than previous这个异常 但是会报如下另⼀个异常:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kryo serialization failed: Buffer overflow</span><br></pre></td></tr></table></figure>这个是因为在对⼀些类做kryo序列化时候，数据量的缓存⼤小超过了默认值，做⼀下调整即可<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(&quot;spark.kryoserializer.buffer.max&quot; , &quot;256m&quot;)</span><br><span class="line">sparkConf.set(&quot;spark.kryoserializer.buffer&quot; , &quot;64m&quot;)</span><br></pre></td></tr></table></figure></code></pre><font size="5"><b>完整代码</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">object WriteTransferTime extends WriteToHbase&#123;</span><br><span class="line">  /**</span><br><span class="line">* @param data 要插⼊入的数据 * @param tableName 表名</span><br><span class="line">**/</span><br><span class="line">  override def bulkLoadData(data: RDD[Any], tableName: String ,</span><br><span class="line">columnFamily:String): Unit = &#123;</span><br><span class="line">    val bean: RDD[TransferTime] = data.map(line =&gt;</span><br><span class="line">line.asInstanceOf[TransferTime])</span><br><span class="line">    val map: RDD[(String, TransferTime)] = bean.map(line =&gt; (line.vintime ,</span><br><span class="line">line))</span><br><span class="line">    val key: RDD[(String, TransferTime)] = map.reduceByKey((x, y) =&gt; y)</span><br><span class="line">    val map1: RDD[TransferTime] = key.map(line =&gt; line._2)</span><br><span class="line">    val by1: RDD[TransferTime] = map1.sortBy(f =&gt; f.vintime)</span><br><span class="line">    val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] =</span><br><span class="line">by1.map &#123;</span><br><span class="line">      line =&gt;</span><br><span class="line">        val rowkey = line.vintime</span><br><span class="line">        val clazz =</span><br><span class="line">Class.forName(&quot;com.dongfeng.code.Bean.message.TransferTime&quot;)</span><br><span class="line">        val fields = clazz.getDeclaredFields</span><br><span class="line">        var list = new ListBuffer[String]()</span><br><span class="line">        var kvlist = new ListBuffer[KeyValue]()//</span><br><span class="line">        if (fields != null &amp;&amp; fields.size &gt; 0) &#123;</span><br><span class="line">          for (field &lt;- fields) &#123;</span><br><span class="line">            field.setAccessible(true)</span><br><span class="line">            val column = field.getName</span><br><span class="line">            list.append(column)</span><br><span class="line">&#125; &#125;</span><br><span class="line">        val newList = list.sortWith(_ &lt; _)</span><br><span class="line">        val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey))</span><br><span class="line">        for(column &lt;- newList)&#123;</span><br><span class="line">          val declaredField: Field = line.getClass.getDeclaredField(column)</span><br><span class="line">          declaredField.setAccessible(true)</span><br><span class="line">          val value = declaredField.get(line).toString</span><br><span class="line">          val kv: KeyValue = new KeyValue(</span><br><span class="line">            Bytes.toBytes(rowkey),</span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            Bytes.toBytes(column),</span><br><span class="line">            Bytes.toBytes(value))</span><br><span class="line">          kvlist.append(kv)</span><br><span class="line">        &#125;</span><br><span class="line">        (ik, kvlist)</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">       val result: RDD[(ImmutableBytesWritable, KeyValue)] =</span><br><span class="line">listData.flatMapValues(</span><br><span class="line">      s =&gt; &#123;</span><br><span class="line">        val values: Iterator[KeyValue] = s.iterator</span><br><span class="line">        values</span><br><span class="line">&#125; )</span><br><span class="line">    val resultDD: RDD[(ImmutableBytesWritable, KeyValue)] = result.sortBy(x</span><br><span class="line">=&gt;x._1 , true)</span><br><span class="line">    WriteToHbaseDB.hfile_load(result , TableName.valueOf(tableName) ,</span><br><span class="line">columnFamily)</span><br><span class="line">&#125; &#125;</span><br><span class="line">    def hfile_load(rdd:RDD[(ImmutableBytesWritable , KeyValue)] , tableName:</span><br><span class="line">TableName , columnFamily:String): Unit =&#123;</span><br><span class="line">//声明表的信息</span><br><span class="line">var table: Table = null try&#123;</span><br><span class="line">val startTime = System.currentTimeMillis() println(s&quot;开始时间:--------&gt;$&#123;startTime&#125;&quot;) //⽣生成的HFile的临时保存路路径</span><br><span class="line">val stagingFolder = &quot;hdfs://cdh1:9000/hfile/&quot;+tableName+new</span><br><span class="line">Date().getTime//</span><br><span class="line">table = connection.getTable(tableName) //如果表不不存在，则创建表 if(!admin.tableExists(tableName))&#123;</span><br><span class="line">        createTable(tableName , columnFamily)</span><br><span class="line">      &#125;</span><br><span class="line">//开始导⼊</span><br><span class="line">val job = Job.getInstance(config) job.setJobName(&quot;DumpFile&quot;) job.setMapOutputKeyClass(classOf[ImmutableBytesWritable]) job.setMapOutputValueClass(classOf[KeyValue])</span><br><span class="line">      rdd.sortBy(x =&gt; x._1, true).saveAsNewAPIHadoopFile(</span><br><span class="line">        stagingFolder,</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[KeyValue],</span><br><span class="line">        classOf[HFileOutputFormat2],</span><br><span class="line">        job.getConfiguration)</span><br><span class="line">      val load = new LoadIncrementalHFiles(config)</span><br><span class="line">      val regionLocator = connection.getRegionLocator(tableName)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">HFileOutputFormat2.configureIncrementalLoad(job, table,</span><br><span class="line">regionLocator)</span><br><span class="line">      load.doBulkLoad(new Path(stagingFolder), table.asInstanceOf[HTable])</span><br><span class="line">//      load.doBulkLoad(new Path(stagingFolder) , connection.getAdmin ,</span><br><span class="line">table , regionLocator)</span><br><span class="line">val endTime = System.currentTimeMillis() println(s&quot;结束时间:--------&gt;$&#123;endTime&#125;&quot;) println(s&quot;花费的时间:-----------------&gt;$&#123;(endTime - startTime)&#125;ms&quot;)</span><br><span class="line">    &#125;catch&#123;</span><br><span class="line">      case e:IOException =&gt;</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">    &#125;finally &#123;</span><br><span class="line">      if (table != null) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">// 关闭HTable对象 table.close(); &#125; catch &#123;</span><br><span class="line">          case e: IOException =&gt;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">&#125; &#125;</span><br><span class="line">if (connection != null) &#123; try &#123; //关闭hbase连接. connection.close();</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: IOException =&gt;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">&#125; &#125;</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
            <tag> Hfile </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产SparkSQL如何读写本地外部数据源及排错</title>
      <link href="/2019/03/01/%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99/"/>
      <url>/2019/03/01/%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><p><a href="https://spark-packages.org/" target="_blank" rel="noopener">https://spark-packages.org/</a>里有很多third-party数据源的package，spark把包加载进来就可以使用了</p><p><img src="/assets/blogImg/2019-03-01-1.png" alt="enter description here"></p><p>csv格式在spark2.0版本之后是内置的，2.0之前属于第三方数据源<br><a id="more"></a></p><h3 id="读取本地外部数据源"><a href="#读取本地外部数据源" class="headerlink" title="读取本地外部数据源"></a>读取本地外部数据源</h3><h4 id="直接读取一个json文件"><a href="#直接读取一个json文件" class="headerlink" title="直接读取一个json文件"></a>直接读取一个json文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.27.jar </span><br><span class="line">scala&gt; spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;).show</span><br></pre></td></tr></table></figure><p>运行报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: file:/home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [49, 57, 125, 10]</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)</span><br><span class="line">  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)</span><br><span class="line">  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)</span><br><span class="line">  ... 32 more</span><br></pre></td></tr></table></figure><p>查看load方法的源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by</span><br><span class="line">   * a local or distributed file system).</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def load(path: String): DataFrame = &#123;</span><br><span class="line">    option(&quot;path&quot;, path).load(Seq.empty: _*) // force invocation of `load(...varargs...)`</span><br><span class="line">  &#125;</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">/**</span><br><span class="line">   * Loads input in as a `DataFrame`, for data sources that support multiple paths.</span><br><span class="line">   * Only works if the source is a HadoopFsRelationProvider.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def load(paths: String*): DataFrame = &#123;</span><br><span class="line">    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) &#123;</span><br><span class="line">      throw new AnalysisException(&quot;Hive data source can only be used with tables, you can not &quot; +</span><br><span class="line">        &quot;read files of Hive data source directly.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)</span><br><span class="line">    if (classOf[DataSourceV2].isAssignableFrom(cls)) &#123;</span><br><span class="line">      val ds = cls.newInstance()</span><br><span class="line">      val options = new DataSourceOptions((extraOptions ++</span><br><span class="line">        DataSourceV2Utils.extractSessionConfigs(</span><br><span class="line">          ds = ds.asInstanceOf[DataSourceV2],</span><br><span class="line">          conf = sparkSession.sessionState.conf)).asJava)</span><br><span class="line">      // Streaming also uses the data source V2 API. So it may be that the data source implements</span><br><span class="line">      // v2, but has no v2 implementation for batch reads. In that case, we fall back to loading</span><br><span class="line">      // the dataframe as a v1 source.</span><br><span class="line">      val reader = (ds, userSpecifiedSchema) match &#123;</span><br><span class="line">        case (ds: ReadSupportWithSchema, Some(schema)) =&gt;</span><br><span class="line">          ds.createReader(schema, options)</span><br><span class="line">        case (ds: ReadSupport, None) =&gt;</span><br><span class="line">          ds.createReader(options)</span><br><span class="line">        case (ds: ReadSupportWithSchema, None) =&gt;</span><br><span class="line">          throw new AnalysisException(s&quot;A schema needs to be specified when using $ds.&quot;)</span><br><span class="line">        case (ds: ReadSupport, Some(schema)) =&gt;</span><br><span class="line">          val reader = ds.createReader(options)</span><br><span class="line">          if (reader.readSchema() != schema) &#123;</span><br><span class="line">            throw new AnalysisException(s&quot;$ds does not allow user-specified schemas.&quot;)</span><br><span class="line">          &#125;</span><br><span class="line">          reader</span><br><span class="line">        case _ =&gt; null // fall back to v1</span><br><span class="line">      &#125;</span><br><span class="line">      if (reader == null) &#123;</span><br><span class="line">        loadV1Source(paths: _*)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Dataset.ofRows(sparkSession, DataSourceV2Relation(reader))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      loadV1Source(paths: _*)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  private def loadV1Source(paths: String*) = &#123;</span><br><span class="line">    // Code path for data source v1.</span><br><span class="line">    sparkSession.baseRelationToDataFrame(</span><br><span class="line">      DataSource.apply(</span><br><span class="line">        sparkSession,</span><br><span class="line">        paths = paths,</span><br><span class="line">        userSpecifiedSchema = userSpecifiedSchema,</span><br><span class="line">        className = source,</span><br><span class="line">        options = extraOptions.toMap).resolveRelation())</span><br><span class="line">  &#125;</span><br><span class="line">------------------------------------------------------</span><br><span class="line">private var source: String = sparkSession.sessionState.conf.defaultDataSourceName</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">def defaultDataSourceName: String = getConf(DEFAULT_DATA_SOURCE_NAME)</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">// This is used to set the default data source</span><br><span class="line">  val DEFAULT_DATA_SOURCE_NAME = buildConf(&quot;spark.sql.sources.default&quot;)</span><br><span class="line">    .doc(&quot;The default data source to use in input/output.&quot;)</span><br><span class="line">    .stringConf</span><br><span class="line">    .createWithDefault(&quot;parquet&quot;)</span><br></pre></td></tr></table></figure><p>从源码中可以看出，如果不指定format，load默认读取的是parquet文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val users = spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">scala&gt; users.show()</span><br><span class="line">+------+--------------+----------------+                                        </span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><p>读取其他格式的文件，必须通过format指定文件格式，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//windows idea环境下</span><br><span class="line">val df1 = spark.read.format(&quot;json&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).load(&quot;hdfs://192.168.137.141:9000/data/people.json&quot;)</span><br><span class="line">df1.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><font color="#FF4500">option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)必须带上，不然报错</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX</span><br></pre></td></tr></table></figure><h4 id="读取CSV格式文件"><a href="#读取CSV格式文件" class="headerlink" title="读取CSV格式文件"></a>读取CSV格式文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">//源文件内容如下：</span><br><span class="line">[hadoop@hadoop001 ~]$ hadoop fs -text /data/people.csv</span><br><span class="line">name;age;job</span><br><span class="line">Jorge;30;Developer</span><br><span class="line">Bob;32;Developer</span><br><span class="line"></span><br><span class="line">//windows idea环境下</span><br><span class="line">val df2 = spark.read.format(&quot;csv&quot;)</span><br><span class="line">      .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">      .option(&quot;header&quot;,&quot;true&quot;)     //use first line of all files as header</span><br><span class="line">      .option(&quot;inferSchema&quot;,&quot;true&quot;)</span><br><span class="line">      .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)</span><br><span class="line">df2.show()</span><br><span class="line">df2.printSchema()</span><br><span class="line">//输出结果：</span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| 30|Developer|</span><br><span class="line">|  Bob| 32|Developer|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- job: string (nullable = true)</span><br><span class="line">-----------------------------------------------------------</span><br><span class="line">//如果不指定option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">+------------------+</span><br><span class="line">|      name;age;job|</span><br><span class="line">+------------------+</span><br><span class="line">|Jorge;30;Developer|</span><br><span class="line">|  Bob;32;Developer|</span><br><span class="line">+------------------+</span><br><span class="line">//如果不指定option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|  _c0|_c1|      _c2|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">|Jorge| 30|Developer|</span><br><span class="line">|  Bob| 32|Developer|</span><br><span class="line">+-----+---+---------+</span><br></pre></td></tr></table></figure><p>读取csv格式文件还可以自定义schema</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">val peopleschema = StructType(Array(</span><br><span class="line">StructField(&quot;hlwname&quot;,StringType,true), </span><br><span class="line">StructField(&quot;hlwage&quot;,IntegerType,true), </span><br><span class="line">StructField(&quot;hlwjob&quot;,StringType,true)))</span><br><span class="line">val df2 = spark.read.format(&quot;csv&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).option(&quot;sep&quot;,&quot;;&quot;)</span><br><span class="line">        .option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">        .schema(peopleschema)</span><br><span class="line">        .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)</span><br><span class="line">      //打印测试</span><br><span class="line">      df2.show()</span><br><span class="line">      df2.printSchema()</span><br><span class="line">输出结果：</span><br><span class="line">+-------+------+---------+</span><br><span class="line">|hlwname|hlwage|   hlwjob|</span><br><span class="line">+-------+------+---------+</span><br><span class="line">|  Jorge|    30|Developer|</span><br><span class="line">|    Bob|    32|Developer|</span><br><span class="line">+-------+------+---------+</span><br><span class="line">root</span><br><span class="line"> |-- hlwname: string (nullable = true)</span><br><span class="line"> |-- hlwage: integer (nullable = true)</span><br><span class="line"> |-- hlwjob: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="将读取的文件以其他格式写出"><a href="#将读取的文件以其他格式写出" class="headerlink" title="将读取的文件以其他格式写出"></a>将读取的文件以其他格式写出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//将上文读取的users.parquet以json格式写出</span><br><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br><span class="line">[hadoop@hadoop000 ~]$ cd /home/hadoop/tmp/parquet2json</span><br><span class="line">[hadoop@hadoop000 parquet2json]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 56 Sep 24 10:15 part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Sep 24 10:15 _SUCCESS</span><br><span class="line">[hadoop@hadoop000 parquet2json]$ cat part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Alyssa&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Ben&quot;,&quot;favorite_color&quot;:&quot;red&quot;&#125;</span><br><span class="line"></span><br><span class="line">//将上文读取的people.json以csv格式写出</span><br><span class="line">df1.write.format(&quot;csv&quot;)</span><br><span class="line">     .mode(&quot;overwrite&quot;)</span><br><span class="line">     .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;)</span><br><span class="line">     .save(&quot;hdfs://192.168.137.141:9000/data/formatconverttest/&quot;)</span><br><span class="line">------------------------------------------</span><br><span class="line">[hadoop@hadoop001 ~]$ hadoop fs -text /data/formatconverttest/part-00000-6fd65eff-d0d3-43e5-9549-2b11bc3ca9de-c000.csv</span><br><span class="line">,Michael</span><br><span class="line">30,Andy</span><br><span class="line">19,Justin</span><br><span class="line">//发现若没有.option(&quot;header&quot;,&quot;true&quot;)，写出的csv丢失了首行的age,name信息</span><br><span class="line">//若不指定.option(&quot;sep&quot;,&quot;;&quot;)，默认逗号为分隔符</span><br></pre></td></tr></table></figure><p>此操作的目的在于学会类型转换，生产上最开始进来的数据大多都是text，json等行式存储的文件，一般都要转成ORC，parquet列式存储的文件，加上压缩，能把文件大小减小到10%左右，大幅度减小IO和数据处理量，提高性能</p><p>此时如果再执行一次save，路径不变，则会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br><span class="line">org.apache.spark.sql.AnalysisException: path file:/home/hadoop/tmp/parquet2json already exists.;</span><br><span class="line">  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)</span><br><span class="line">  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)</span><br><span class="line">.........................................................</span><br></pre></td></tr></table></figure><p>可以通过设置savemode来解决这个问题</p><p><img src="/assets/blogImg/2019-03-01-2.png" alt="enter description here"></p><p>默认是errorifexists</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产Spark开发读取云主机HDFS异常剖析流程</title>
      <link href="/2019/02/26/%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/02/26/%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h3 id="问题背景："><a href="#问题背景：" class="headerlink" title="问题背景："></a>问题背景：</h3><h4 id="云主机是-Linux-环境，搭建-Hadoop-伪分布式"><a href="#云主机是-Linux-环境，搭建-Hadoop-伪分布式" class="headerlink" title="云主机是 Linux 环境，搭建 Hadoop 伪分布式"></a>云主机是 Linux 环境，搭建 Hadoop 伪分布式</h4><ul><li>公网 IP：139.198.xxx.xxx</li><li>内网 IP：192.168.137.2</li><li>主机名：hadoop001</li></ul><h4 id="本地的core-site-xml配置如下："><a href="#本地的core-site-xml配置如下：" class="headerlink" title="本地的core-site.xml配置如下："></a>本地的core-site.xml配置如下：</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:9001&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:9001/hadoop/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="本地的hdfs-site-xml配置如下："><a href="#本地的hdfs-site-xml配置如下：" class="headerlink" title="本地的hdfs-site.xml配置如下："></a>本地的hdfs-site.xml配置如下：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="云主机hosts文件配置："><a href="#云主机hosts文件配置：" class="headerlink" title="云主机hosts文件配置："></a>云主机hosts文件配置：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"># hostname loopback address</span><br><span class="line">  192.168.137.2   hadoop001</span><br></pre></td></tr></table></figure><p>云主机将内网IP和主机名hadoop001做了映射</p><h4 id="本地hosts文件配置"><a href="#本地hosts文件配置" class="headerlink" title="本地hosts文件配置"></a>本地hosts文件配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">139.198.18.XXX     hadoop001</span><br></pre></td></tr></table></figure><p>本地已经将公网IP和域名hadoop001做了映射</p><h3 id="问题症状"><a href="#问题症状" class="headerlink" title="问题症状"></a>问题症状</h3><ol><li><strong>在云主机上开启 HDFS，JPS 查看进程都没有异常，通过 Shell 操作 HDFS 文件也没有问题</strong></li><li><strong>通过浏览器访问 50070 端口管理界面也没有问题</strong></li><li><p><strong>在本地机器上使用 Java API 操作远程 HDFS 文件，URI 使用公网 IP，代码如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val uri = new URI(&quot;hdfs://hadoop001:9001&quot;)</span><br><span class="line">val fs = FileSystem.get(uri,conf)</span><br><span class="line">val listfiles = fs.listFiles(new Path(&quot;/data&quot;),true)</span><br><span class="line">    while (listfiles.hasNext) &#123;</span><br><span class="line">    val nextfile = listfiles.next()</span><br><span class="line">    println(&quot;get file path:&quot; + nextfile.getPath().toString())</span><br><span class="line">    &#125;</span><br><span class="line">------------------------------运行结果---------------------------------</span><br><span class="line">get file path:hdfs://hadoop001:9001/data/infos.txt</span><br></pre></td></tr></table></figure></li><li><p>在本地机器使用SparkSQL读取hdfs上的文件并转换为DF的过程中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">object SparkSQLApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">  val spark = SparkSession.builder().appName(&quot;SparkSQLApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">  val info = spark.sparkContext.textFile(&quot;/data/infos.txt&quot;)</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val infoDF = info.map(_.split(&quot;,&quot;)).map(x=&gt;Info(x(0).toInt,x(1),x(2).toInt)).toDF()</span><br><span class="line">  infoDF.show()</span><br><span class="line">  spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Info(id:Int,name:String,age:Int)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><pre><code>出现如下报错信息：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">   ....</span><br><span class="line">   ....</span><br><span class="line">   19/02/23 16:07:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)</span><br><span class="line">   19/02/23 16:07:00 INFO HadoopRDD: Input split: hdfs://hadoop001:9001/data/infos.txt:0+17</span><br><span class="line">   19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   .....</span><br><span class="line">   ....</span><br><span class="line">   19/02/23 16:07:21 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">   19/02/23 16:07:21 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 272.617680460432 msec.</span><br><span class="line">   19/02/23 16:07:42 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">       at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">       at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3499)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:12 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">       at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">       at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:12 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">   19/02/23 16:08:12 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11918.913311370841 msec.</span><br><span class="line">   19/02/23 16:08:45 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">   java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException</span><br><span class="line">   19/02/23 16:08:45 WARN DFSClient: DFS Read</span><br><span class="line">   org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:648)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   19/02/23 16:08:45 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job</span><br><span class="line">   19/02/23 16:08:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool </span><br><span class="line">   19/02/23 16:08:45 INFO TaskSchedulerImpl: Cancelling stage 0</span><br><span class="line">   19/02/23 16:08:45 INFO DAGScheduler: ResultStage 0 (show at SparkSQLApp.scala:30) failed in 105.618 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br><span class="line">   Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt</span><br><span class="line">       at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001)</span><br><span class="line">   ...</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure></code></pre><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><ol><li><strong>本地 Shell 可以正常操作，排除集群搭建和进程没有启动的问题</strong></li><li><strong>云主机没有设置防火墙，排除防火墙没关的问题</strong></li><li><strong>云服务器防火墙开放了 DataNode 用于数据传输服务端口 默认是 50010</strong></li><li><strong>我在本地搭建了另一台虚拟机，该虚拟机和本地在同一局域网，本地可以正常操作该虚拟机的hdfs，基本确定了是由于内外网的原因。</strong></li><li><strong>查阅资料发现 HDFS 中的文件夹和文件名都是存放在 NameNode 上，操作不需要和 DataNode 通信，因此可以正常创建文件夹和创建文件说明本地和远程 NameNode 通信没有问题。那么很可能是本地和远程 DataNode 通信有问题</strong></li></ol><h3 id="问题猜想"><a href="#问题猜想" class="headerlink" title="问题猜想"></a>问题猜想</h3><p>由于本地测试和云主机不在一个局域网，hadoop配置文件是以内网ip作为机器间通信的ip。在这种情况下,我们能够访问到namenode机器，namenode会给我们数据所在机器的ip地址供我们访问数据传输服务，但是当写数据的时候，NameNode 和DataNode 是通过内网通信的，返回的是datanode内网的ip,我们无法根据该IP访问datanode服务器。</p><p>我们来看一下其中一部分报错信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">...</span><br><span class="line">19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue....</span><br></pre></td></tr></table></figure><p>从报错信息中可以看出，连接不到192.168.137.2:50010，也就是datanode的地址，因为外网必须访问“139.198.18.XXX:50010”才能访问到datanode。</p><p>为了能够让开发机器访问到hdfs，我们可以通过域名访问hdfs，让namenode返回给我们datanode的域名。</p><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><h4 id="尝试一："><a href="#尝试一：" class="headerlink" title="尝试一："></a>尝试一：</h4><p>在开发机器的hosts文件中配置datanode对应的外网ip和域名（上文已经配置），并且在与hdfs交互的程序中添加如下代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val conf = new Configuration()</span><br><span class="line">conf.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure><p>报错依旧</p><h4 id="尝试二："><a href="#尝试二：" class="headerlink" title="尝试二："></a>尝试二：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;SparkSQLApp&quot;)</span><br><span class="line">       .master(&quot;local[2]&quot;)</span><br><span class="line">      .config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure><p>报错依旧</p><h4 id="尝试三："><a href="#尝试三：" class="headerlink" title="尝试三："></a>尝试三：</h4><p>在hdfs-site.xml中添加如下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>运行成功</p><font color="#FF4500">通过查阅资料，建议在<strong>hdfs-site.xml</strong>中增加<strong><em>dfs.datanode.use.datanode.hostname</em></strong>属性，表示datanode之间的通信也通过域名方式</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这样能够使得更换内网IP变得十分简单、方便，而且可以让特定datanode间的数据交换变得更容易。但与此同时也<font color="#FF4500">存在一个副作用</font>，当DNS解析失败时会导致整个Hadoop不能正常工作，所以要保证DNS的可靠</p><font size="5"><b>总结：将默认的通过IP访问，改为通过域名方式访问。</b></font><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://blog.csdn.net/vaf714/article/details/82996860" target="_blank" rel="noopener">https://blog.csdn.net/vaf714/article/details/82996860</a></p><p><a href="https://www.cnblogs.com/krcys/p/9146329.html" target="_blank" rel="noopener">https://www.cnblogs.com/krcys/p/9146329.html</a></p><p><a href="https://blog.csdn.net/dominic_tiger/article/details/71773656" target="_blank" rel="noopener">https://blog.csdn.net/dominic_tiger/article/details/71773656</a></p><p><a href="https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/" target="_blank" rel="noopener">https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark UI界面实现原理</title>
      <link href="/2019/02/22/Spark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>/2019/02/22/Spark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><p>当Spark程序在运行时，会提供一个Web页面查看Application运行状态信息。是否开启UI界面由参数spark.ui.enabled(默认为true)来确定。下面列出Spark UI一些相关配置参数，默认值，以及其作用。</p><p><img src="/assets/blogImg/2019-02-22-1.png" alt="enter description here"></p><p>本文接下来分成两个部分，第一部分基于Spark-1.6.0的源码，结合第二部分的图片内容来描述UI界面在Spark中的实现方式。第二部分以实例展示Spark UI界面显示的内容。</p><a id="more"></a><h2 id="Spark-UI界面实现方式"><a href="#Spark-UI界面实现方式" class="headerlink" title="Spark UI界面实现方式"></a>Spark UI界面实现方式</h2><h3 id="UI组件结构"><a href="#UI组件结构" class="headerlink" title="UI组件结构"></a>UI组件结构</h3><p>这部分先讲UI界面的实现方式，UI界面的实例在本文最后一部分。如果对这部分中的某些概念不清楚，那么最好先把第二部分了解一下。</p><p>从下面UI界面的实例可以看出，不同的内容以Tab的形式展现在界面上，对应每一个Tab在下方显示具体内容。基本上Spark UI界面也是按这个层次关系实现的。</p><p>以SparkUI类为容器，各个Tab，如JobsTab, StagesTab, ExecutorsTab等镶嵌在SparkUI上，对应各个Tab，有页面内容实现类JobPage, StagePage, ExecutorsPage等页面。这些类的继承和包含关系如下图所示：</p><p><img src="/assets/blogImg/2019-02-22-2.png" alt="enter description here"></p><h3 id="初始化过程"><a href="#初始化过程" class="headerlink" title="初始化过程"></a>初始化过程</h3><p>从上面可以看出，SparkUI类型的对象是UI界面的根对象，它是在SparkContext类中构造出来的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">private var _ui: Option[SparkUI] = None //定义</span><br><span class="line">_ui = //SparkUI对象的生成</span><br><span class="line">  if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123;</span><br><span class="line">    Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,</span><br><span class="line">      _env.securityManager, appName, startTime = startTime))</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // For tests, do not enable the UI</span><br><span class="line">    None</span><br><span class="line">  &#125;</span><br><span class="line">_ui.foreach(_.bind())  //启动jetty。bind方法继承自WebUI，该类负责和真实的Jetty Server API打交道</span><br></pre></td></tr></table></figure><p>上面这段代码中可以看到SparkUI对象的生成过程，结合上面的类结构图，可以看到bind方法继承自WebUI类，进入WebUI类中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">protected val handlers = ArrayBuffer[ServletContextHandler]() // 这个对象在下面bind方法中会使用到。</span><br><span class="line">  protected val pageToHandlers = new HashMap[WebUIPage, ArrayBuffer[ServletContextHandler]] // 将page绑定到handlers上</span><br><span class="line">  /** 将Http Server绑定到这个Web页面 */</span><br><span class="line">  def bind() &#123;</span><br><span class="line">    assert(!serverInfo.isDefined, &quot;Attempted to bind %s more than once!&quot;.format(className))</span><br><span class="line">    try &#123;</span><br><span class="line">      serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name))</span><br><span class="line">      logInfo(&quot;Started %s at http://%s:%d&quot;.format(className, publicHostName, boundPort))</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logError(&quot;Failed to bind %s&quot;.format(className), e)</span><br><span class="line">        System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>上面代码中handlers对象维持了WebUIPage和Jetty之间的关系，org.eclipse.jetty.servlet.ServletContextHandler是标准jetty容器的handler。而对象pageToHandlers维持了WebUIPage到ServletContextHandler的对应关系。</p><p>各Tab页以及该页内容的实现，基本上大同小异。接下来以AllJobsPage页面为例仔细梳理页面展示的过程。</p><h3 id="SparkUI中Tab的绑定"><a href="#SparkUI中Tab的绑定" class="headerlink" title="SparkUI中Tab的绑定"></a>SparkUI中Tab的绑定</h3><p>从上面的类结构图中看到WebUIPage提供了两个重要的方法，render和renderJson用于相应页面请求，在WebUIPage的实现类中，具体实现了这两个方法。在SparkContext中构造出SparkUI的实例后，会执行SparkUI#initialize方法进行初始化。如下面代码中，调用SparkUI从WebUI继承的attacheTab方法，将各Tab页面绑定到UI上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def initialize() &#123;</span><br><span class="line">    attachTab(new JobsTab(this))</span><br><span class="line">    attachTab(stagesTab)</span><br><span class="line">    attachTab(new StorageTab(this))</span><br><span class="line">    attachTab(new EnvironmentTab(this))</span><br><span class="line">    attachTab(new ExecutorsTab(this))</span><br><span class="line">    attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;))</span><br><span class="line">    attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath))</span><br><span class="line">    attachHandler(ApiRootResource.getServletHandler(this))</span><br><span class="line">    // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs</span><br><span class="line">    attachHandler(createRedirectHandler(</span><br><span class="line">      &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest,</span><br><span class="line">      httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="页面内容绑定到Tab"><a href="#页面内容绑定到Tab" class="headerlink" title="页面内容绑定到Tab"></a>页面内容绑定到Tab</h3><p>在上一节中，JobsTab标签绑定到SparkUI上之后，在JobsTab上绑定了AllJobsPage和JobPage类。AllJobsPage页面即访问SparkUI页面时列举出所有Job的那个页面，JobPage页面则是点击单个Job时跳转的页面。通过调用JobsTab从WebUITab继承的attachPage方法与JobsTab进行绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123;</span><br><span class="line">  val sc = parent.sc</span><br><span class="line">  val killEnabled = parent.killEnabled</span><br><span class="line">  val jobProgresslistener = parent.jobProgressListener</span><br><span class="line">  val executorListener = parent.executorsListener</span><br><span class="line">  val operationGraphListener = parent.operationGraphListener</span><br><span class="line">  def isFairScheduler: Boolean =</span><br><span class="line">    jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR)</span><br><span class="line">  attachPage(new AllJobsPage(this))</span><br><span class="line">  attachPage(new JobPage(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="页面内容的展示"><a href="#页面内容的展示" class="headerlink" title="页面内容的展示"></a>页面内容的展示</h3><p>知道了AllJobsPage页面如何绑定到SparkUI界面后，接下来分析这个页面的内容是如何显示的。进入AllJobsPage类，主要观察render方法。在页面展示上Spark直接利用了Scala对html/xml的语法支持，将页面的Html代码嵌入Scala程序中。具体的页面生成过程可以查看下面源码中的注释。这里可以结合第二部分的实例进行查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">def render(request: HttpServletRequest): Seq[Node] = &#123;</span><br><span class="line">    val listener = parent.jobProgresslistener //获取jobProgresslistener对象，页面展示的数据都是从这里读取</span><br><span class="line">    listener.synchronized &#123;</span><br><span class="line">      val startTime = listener.startTime // 获取application的开始时间，默认值为-1L</span><br><span class="line">      val endTime = listener.endTime // 获取application的结束时间，默认值为-1L</span><br><span class="line">      val activeJobs = listener.activeJobs.values.toSeq // 获取当前application中处于active状态的job</span><br><span class="line">      val completedJobs = listener.completedJobs.reverse.toSeq // 获取当前application中完成状态的job</span><br><span class="line">      val failedJobs = listener.failedJobs.reverse.toSeq  // 获取当前application中失败状态的job</span><br><span class="line">      val activeJobsTable =</span><br><span class="line">        jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val completedJobsTable =</span><br><span class="line">        jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val failedJobsTable =</span><br><span class="line">        jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</span><br><span class="line">      val shouldShowActiveJobs = activeJobs.nonEmpty</span><br><span class="line">      val shouldShowCompletedJobs = completedJobs.nonEmpty</span><br><span class="line">      val shouldShowFailedJobs = failedJobs.nonEmpty</span><br><span class="line">      val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123;</span><br><span class="line">        s&quot;$&#123;completedJobs.size&#125;&quot;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot;</span><br><span class="line">      &#125;</span><br><span class="line">      val summary: NodeSeq =</span><br><span class="line">        &lt;div&gt;</span><br><span class="line">          &lt;ul class=&quot;unstyled&quot;&gt;</span><br><span class="line">            &lt;li&gt;</span><br><span class="line">              &lt;strong&gt;Total Uptime:&lt;/strong&gt; // 显示当前Spark应用运行时间</span><br><span class="line">              &#123;// 如果还没有结束，就用系统当前时间减开始时间。如果已经结束，就用结束时间减开始时间</span><br><span class="line">                if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123;</span><br><span class="line">                  UIUtils.formatDuration(System.currentTimeMillis() - startTime)</span><br><span class="line">                &#125; else if (endTime &gt; 0) &#123;</span><br><span class="line">                  UIUtils.formatDuration(endTime - startTime)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &lt;/li&gt;</span><br><span class="line">            &lt;li&gt;</span><br><span class="line">              &lt;strong&gt;Scheduling Mode: &lt;/strong&gt; // 显示调度模式，FIFO或FAIR</span><br><span class="line">              &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125;</span><br><span class="line">            &lt;/li&gt;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowActiveJobs) &#123; // 如果有active状态的job，则显示Active Jobs有多少个</span><br><span class="line">                &lt;li&gt;</span><br><span class="line">                  &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;activeJobs.size&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowCompletedJobs) &#123; // 如果有完成状态的job，则显示Completed Jobs的个数</span><br><span class="line">                &lt;li id=&quot;completed-summary&quot;&gt;</span><br><span class="line">                  &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;completedJobNumStr&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#123;</span><br><span class="line">              if (shouldShowFailedJobs) &#123; // 如果有失败状态的job，则显示Failed Jobs的个数</span><br><span class="line">                &lt;li&gt;</span><br><span class="line">                  &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt;</span><br><span class="line">                  &#123;listener.numFailedJobs&#125;</span><br><span class="line">                &lt;/li&gt;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &lt;/ul&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">      var content = summary // 将上面的html代码写入content变量，在最后统一显示content中的内容</span><br><span class="line">      val executorListener = parent.executorListener // 这里获取EventTimeline中的信息</span><br><span class="line">      content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs,</span><br><span class="line">          executorListener.executorIdToData, startTime)</span><br><span class="line">// 然后根据当前application中是否存在active， failed， completed状态的job，将这些信息显示在页面上。</span><br><span class="line">      if (shouldShowActiveJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++</span><br><span class="line">          activeJobsTable // 生成active状态job的展示表格，具体形式可参看第二部分。按提交时间倒序排列</span><br><span class="line">      &#125;</span><br><span class="line">      if (shouldShowCompletedJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++</span><br><span class="line">          completedJobsTable</span><br><span class="line">      &#125;</span><br><span class="line">      if (shouldShowFailedJobs) &#123;</span><br><span class="line">        content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++</span><br><span class="line">          failedJobsTable</span><br><span class="line">      &#125;</span><br><span class="line">      val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; +</span><br><span class="line">        &quot; Click on a job to see information about the stages of tasks inside it.&quot;</span><br><span class="line">      UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText)) // 最后将content中的所有内容全部展示在页面上</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>接下来以activeJobsTable代码为例分析Jobs信息展示表格的生成。这里主要的方法是makeRow，接收的是上面代码中的activeJobs, completedJobs, failedJobs。这三个对象都是包含在JobProgressListener对象中的，在JobProgressListener中的定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 这三个对象用于存储数据的主要是JobUIData类型，</span><br><span class="line">  val activeJobs = new HashMap[JobId, JobUIData]</span><br><span class="line">  val completedJobs = ListBuffer[JobUIData]()</span><br><span class="line">  val failedJobs = ListBuffer[JobUIData]()</span><br></pre></td></tr></table></figure><p>将上面三个对象传入到下面这段代码中，继续执行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123;</span><br><span class="line">    val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined)</span><br><span class="line">    val columns: Seq[Node] = &#123; // 显示的信息包括，Job Id(Job Group)以及Job描述，Job提交时间，Job运行时间，总的Stage/Task数，成功的Stage/Task数，以及一个进度条</span><br><span class="line">      &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Description&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Submitted&lt;/th&gt;</span><br><span class="line">      &lt;th&gt;Duration&lt;/th&gt;</span><br><span class="line">      &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt;</span><br><span class="line">      &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    def makeRow(job: JobUIData): Seq[Node] = &#123;</span><br><span class="line">      val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job)</span><br><span class="line">      val duration: Option[Long] = &#123;</span><br><span class="line">        job.submissionTime.map &#123; start =&gt; // Job运行时长为系统时间，或者结束时间减去开始时间</span><br><span class="line">          val end = job.completionTime.getOrElse(System.currentTimeMillis())</span><br><span class="line">          end - start</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      val formattedDuration = duration.map(d =&gt;  // 格式化任务运行时间，显示为a h:b m:c s格式UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;)</span><br><span class="line">      val formattedSubmissionTime = // 获取Job提交时间job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;)</span><br><span class="line">      val jobDescription = UIUtils.makeDescription(lastStageDescription, parent.basePath) // 获取任务描述</span><br><span class="line">      val detailUrl = // 点击单个Job下面链接跳转到JobPage页面，传入参数为jobId</span><br><span class="line">        &quot;%s/jobs/job?id=%s&quot;.format(UIUtils.prependBaseUri(parent.basePath), job.jobId)</span><br><span class="line">      &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt;</span><br><span class="line">          &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td&gt;</span><br><span class="line">          &#123;jobDescription&#125;</span><br><span class="line">          &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt;</span><br><span class="line">          &#123;formattedSubmissionTime&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt;</span><br><span class="line">        &lt;td class=&quot;stage-progress-cell&quot;&gt;</span><br><span class="line">          &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125;</span><br><span class="line">          &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125;</span><br><span class="line">          &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">        &lt;td class=&quot;progress-cell&quot;&gt; // 进度条</span><br><span class="line">          &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks,</span><br><span class="line">           failed = job.numFailedTasks, skipped = job.numSkippedTasks,</span><br><span class="line">           total = job.numTasks - job.numSkippedTasks)&#125;</span><br><span class="line">        &lt;/td&gt;</span><br><span class="line">      &lt;/tr&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt;</span><br><span class="line">      &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt; // 显示列名</span><br><span class="line">      &lt;tbody&gt;</span><br><span class="line">        &#123;jobs.map(makeRow)&#125; // 调用上面的row生成方法，具体显示Job信息</span><br><span class="line">      &lt;/tbody&gt;</span><br><span class="line">    &lt;/table&gt;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>从上面这些代码中可以看到，Job页面显示的所有数据，都是从JobProgressListener对象中获得的。SparkUI可以理解成一个JobProgressListener对象的消费者，页面上显示的内容都是JobProgressListener内在的展现。</p><h2 id="Spark-UI界面实例"><a href="#Spark-UI界面实例" class="headerlink" title="Spark UI界面实例"></a>Spark UI界面实例</h2><p>默认情况下，当一个Spark Application运行起来后，可以通过访问hostname:4040端口来访问UI界面。hostname是提交任务的Spark客户端ip地址，端口号由参数spark.ui.port(默认值4040，如果被占用则顺序往后探查)来确定。由于启动一个Application就会生成一个对应的UI界面，所以如果启动时默认的4040端口号被占用，则尝试4041端口，如果还是被占用则尝试4042，一直找到一个可用端口号为止。</p><p>下面启动一个Spark ThriftServer服务，并用beeline命令连接该服务，提交sql语句运行。则ThriftServer对应一个Application，每个sql语句对应一个Job，按照Job的逻辑划分Stage和Task。</p><h3 id="Jobs页面"><a href="#Jobs页面" class="headerlink" title="Jobs页面"></a>Jobs页面</h3><p><img src="/assets/blogImg/2019-02-22-3.png" alt="enter description here"></p><p>连接上该端口后，显示的就是上面的页面，也是Job的主页面。这里会显示所有Active，Completed, Cancled以及Failed状态的Job。默认情况下总共显示1000条Job信息，这个数值由参数spark.ui.retainedJobs(默认值1000)来确定。</p><p>从上面还看到，除了Jobs选项卡之外，还可显示Stages, Storage, Enviroment, Executors, SQL以及JDBC/ODBC Server选项卡。分别如下图所示。</p><h3 id="Stages页面"><a href="#Stages页面" class="headerlink" title="Stages页面"></a>Stages页面</h3><p><img src="/assets/blogImg/2019-02-22-4.png" alt="enter description here"></p><h3 id="Storage页面"><a href="#Storage页面" class="headerlink" title="Storage页面"></a>Storage页面</h3><p><img src="/assets/blogImg/2019-02-22-5.png" alt="enter description here"></p><h3 id="Enviroment页面"><a href="#Enviroment页面" class="headerlink" title="Enviroment页面"></a>Enviroment页面</h3><p><img src="/assets/blogImg/2019-02-22-6.png" alt="enter description here"></p><h3 id="Executors页面"><a href="#Executors页面" class="headerlink" title="Executors页面"></a>Executors页面</h3><p><img src="/assets/blogImg/2019-02-22-7.png" alt="enter description here"></p><h3 id="单个Job包含的Stages页面"><a href="#单个Job包含的Stages页面" class="headerlink" title="单个Job包含的Stages页面"></a>单个Job包含的Stages页面</h3><p><img src="/assets/blogImg/2019-02-22-8.png" alt="enter description here"></p><h3 id="Task页面"><a href="#Task页面" class="headerlink" title="Task页面"></a>Task页面</h3><p><img src="/assets/blogImg/2019-02-22-9.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark监控报错javax.servlet.http.HttpServletRequest.isAsyncStarted</title>
      <link href="/2019/02/16/Spark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted/"/>
      <url>/2019/02/16/Spark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h4><ul><li>Spark2.2.1</li><li>Hadoop2.6</li><li>Intellj</li><li>Scala2.11</li></ul><h4 id="pom文件"><a href="#pom文件" class="headerlink" title="pom文件"></a>pom文件</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.common.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.hdfs.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.client.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h4 id="报错信息如下所示："><a href="#报错信息如下所示：" class="headerlink" title="报错信息如下所示："></a>报错信息如下所示：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">at org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)</span><br><span class="line">at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)</span><br><span class="line">at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)</span><br><span class="line">at org.spark_project.jetty.server.Server.handle(Server.java:499)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN HttpChannel: Could not send response error 500: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">16/11/08 21:37:43 WARN HttpChannel: /jobs/</span><br><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br><span class="line">at org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)</span><br><span class="line">at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)</span><br><span class="line">at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)</span><br><span class="line">at org.spark_project.jetty.server.Server.handle(Server.java:499)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN QueuedThreadPool: </span><br><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletResponse.getStatus()I</span><br><span class="line">at org.spark_project.jetty.server.handler.ErrorHandler.handle(ErrorHandler.java:112)</span><br><span class="line">at org.spark_project.jetty.server.Response.sendError(Response.java:597)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handleException(HttpChannel.java:487)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection$HttpChannelOverHttp.handleException(HttpConnection.java:594)</span><br><span class="line">at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:387)</span><br><span class="line">at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)</span><br><span class="line">at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)</span><br><span class="line">at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">16/11/08 21:37:43 WARN QueuedThreadPool: Unexpected thread death: org.spark_project.jetty.util.thread.QueuedThreadPool$3@3ec5063f in SparkUI&#123;STARTED,8&lt;=8&lt;=200,i=4,q=0&#125;</span><br></pre></td></tr></table></figure><h4 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h4><h5 id="查看报错信息"><a href="#查看报错信息" class="headerlink" title="查看报错信息"></a>查看报错信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z</span><br></pre></td></tr></table></figure><p>未找到HttpServletRequest类中的isAsyncStarted方法。</p><h5 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h5><p>使用搜索功能，查看该类存在于哪些包下。</p><p><img src="/source/assets/blogImg/2019-02-16-Spark监控报错.png" alt="Spark监控报错"></p><h5 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h5><p><img src="/source/assets/blogImg/2019-02-16-Spark监控问题解决1.png" alt="Spark监控问题解决1"></p><p><img src="/source/assets/blogImg/2019-02-16-Spark监控问题解决2.png" alt="Spark监控问题解决2"></p><p>所有涉及到该类jar文件且版本低于3.0的均需要进行删除。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每天起床第一句，看看Spark调度器</title>
      <link href="/2019/01/18/Spark%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
      <url>/2019/01/18/Spark%E8%B0%83%E5%BA%A6%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>之前呢，我们详细地分析了DAGScheduler的执行过程，我们知道，RDD形成的DAG经过DAGScheduler，依据shuffle将DAG划分为若干个stage，再由taskScheduler提交task到executor中执行，那么执行task的过程，就需要调度器来参与了。</p><p>Spark调度器主要有两种模式，也是大家耳熟能详的FIFO和FAIR模式。默认情况下，Spark是FIFO（先入先出）模式，即谁先提交谁先执行。而FAIR（公平调度）模式会在调度池中为任务进行分组，可以有不同的权重，根据权重来决定执行顺序。</p><p>那么源码中是怎么实现的呢？<br><a id="more"></a><br>首先，当Stage划分好，会调用TaskSchedulerImpl.submitTasks()方法，以TaskSet的形式提交给TaskScheduler，并创建一个TaskSetManger对象添加进调度池。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">override def submitTasks(taskSet: TaskSet) &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    //....</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">    //.....</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br></pre></td></tr></table></figure><p>SchedulerBulider通过TaskSchedulerImpl.initialize()进行了实例化，并调用了SchedulerBulider.buildPools()方法。具体怎么个build，就要看用户选择的schedulingMode了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def initialize(backend: SchedulerBackend) &#123;</span><br><span class="line">    this.backend = backend</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode match &#123;</span><br><span class="line">        case SchedulingMode.FIFO =&gt;</span><br><span class="line">          new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">        case SchedulingMode.FAIR =&gt;</span><br><span class="line">          new FairSchedulableBuilder(rootPool, conf)</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; +</span><br><span class="line">          s&quot;$schedulingMode&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>然后我们来看一下两个调度器的buildPools()方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">override def buildPools() &#123;</span><br><span class="line">    // nothing</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>FIFO什么也没干~~</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">override def buildPools() &#123;</span><br><span class="line">    var fileData: Option[(InputStream, String)] = None</span><br><span class="line">    try &#123;</span><br><span class="line">      fileData = schedulerAllocFile.map &#123; f =&gt;</span><br><span class="line">        val fis = new FileInputStream(f)</span><br><span class="line">        logInfo(s&quot;Creating Fair Scheduler pools from $f&quot;)</span><br><span class="line">        Some((fis, f))</span><br><span class="line">      &#125;.getOrElse &#123;</span><br><span class="line">        val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE)</span><br><span class="line">        if (is != null) &#123;</span><br><span class="line">          logInfo(s&quot;Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE&quot;)</span><br><span class="line">          Some((is, DEFAULT_SCHEDULER_FILE))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          logWarning(&quot;Fair Scheduler configuration file not found so jobs will be scheduled in &quot; +</span><br><span class="line">            s&quot;FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or &quot; +</span><br><span class="line">            s&quot;set $SCHEDULER_ALLOCATION_FILE_PROPERTY to a file that contains the configuration.&quot;)</span><br><span class="line">          None</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; buildFairSchedulerPool(is, fileName) &#125;</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(t) =&gt;</span><br><span class="line">        val defaultMessage = &quot;Error while building the fair scheduler pools&quot;</span><br><span class="line">        val message = fileData.map &#123; case (is, fileName) =&gt; s&quot;$defaultMessage from $fileName&quot; &#125;</span><br><span class="line">          .getOrElse(defaultMessage)</span><br><span class="line">        logError(message, t)</span><br><span class="line">        throw t</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; is.close() &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // finally create &quot;default&quot; pool</span><br><span class="line">    buildDefaultPool()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再谈，某头条公司Spark结构化流的SQL实现</title>
      <link href="/2019/01/10/%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/01/10/%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>前面介绍了大概的使用语句，接下来讲解基本的功能点的实现。</p><h2 id="SQL语句的解析-解析部分为开源项目flinkStreamSQL内容，直接拿过来用"><a href="#SQL语句的解析-解析部分为开源项目flinkStreamSQL内容，直接拿过来用" class="headerlink" title="SQL语句的解析(解析部分为开源项目flinkStreamSQL内容，直接拿过来用)"></a>SQL语句的解析(解析部分为开源项目flinkStreamSQL内容，直接拿过来用)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SocketTable(</span><br><span class="line">    word String,</span><br><span class="line">    valuecount int</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;socket&apos;,</span><br><span class="line">    host=&apos;hadoop-sh1-core1&apos;,</span><br><span class="line">    port=&apos;9998&apos;,</span><br><span class="line">    delimiter=&apos; &apos;</span><br><span class="line">);</span><br><span class="line">create SINK console(</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;console&apos;,</span><br><span class="line">    outputmode=&apos;complete&apos;</span><br><span class="line">);</span><br><span class="line">insert into console select word,count(*) from SocketTable group by word;</span><br></pre></td></tr></table></figure><a id="more"></a><p>将create的内容根据正则解析出来，将field和配置相关的内容解析出来。</p><p>insert into部分的内容则使用calsite解析出insert部分的target表和已经create的source表内容。</p><p>因为spark没有定义好表之后直接可以insert的内容，所以要将需要sink的target解析出来另外处理。</p><h2 id="创建source输入"><a href="#创建source输入" class="headerlink" title="创建source输入"></a>创建source输入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SocketTable(</span><br><span class="line">    word String,</span><br><span class="line">    valuecount int</span><br><span class="line">)WITH(</span><br><span class="line">    type=&apos;socket&apos;,</span><br><span class="line">    host=&apos;hadoop-sh1-core1&apos;,</span><br><span class="line">    port=&apos;9998&apos;,</span><br><span class="line">    delimiter=&apos; &apos;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>解析出type中的内容，使用反射寻找到对应的处理类，解析各个参数是否合法，获得默认参数等。</p><p>这里就会使用format(‘socket’)的方式，option中分别是host和port，分隔符是’ ‘空格。</p><h2 id="schema的定义"><a href="#schema的定义" class="headerlink" title="schema的定义"></a>schema的定义</h2><p>schema的定义<br>spark.readStream创建的是dataframe，比如socket，它创建的df只有一个列，schema是value，如果是kafka的话就更多了。</p><p>接下来就是将定义的表中的field赋给df。</p><p>本项目中采用的是json的方式传schema，具体原因也很简单，tuple不行，case class的话需要动态变化，难度大，rdd方式在里面行不通，就通过json来做了。</p><h2 id="窗口的定义"><a href="#窗口的定义" class="headerlink" title="窗口的定义"></a>窗口的定义</h2><p>flink中其实也有在sql中添加窗口相关的字段，比如group by proctime 之类的。</p><p>在StructuredStreamingInSQL中添加，eventtime或者processtime的window sql，看源码中，其实定义一个窗口，就是为这个df添加了一个window的字段，window中有start、end等字段，知道了这个，我们在df中只要定义窗口的字段覆盖掉默认的window字段，就能使用processtime和eventtime的sql语句啦！</p><h2 id="sink的处理"><a href="#sink的处理" class="headerlink" title="sink的处理"></a>sink的处理</h2><p>将create的source加上定义field，加上window字段之后，就是将insert into的sql解析，把target的表拿出来，select后的内容是逻辑的主体，sql执行的内容结束之后，就和前面一样，根据type中的内容，找到对应的sink内容，执行writeStream。</p><h2 id="动态添加"><a href="#动态添加" class="headerlink" title="动态添加"></a>动态添加</h2><p>在处理中可能有这样的情况，想要更新执行的sql，但又不希望spark程序停止，这个时候就可以通过在zk上创建监听器的方式来实现sql的动态添加。</p><p>动态的替换的实现方式是，结构化流把所有的查询存在一个map中，key是jobid，value是query，通过获取旧的query的id，将其stop，新的query就会无缝对接，由于是新的query，bachid等内容都会从头开始计算。</p><h2 id="后续监控、自定义函数、压测、调优等功能-待分享"><a href="#后续监控、自定义函数、压测、调优等功能-待分享" class="headerlink" title="后续监控、自定义函数、压测、调优等功能(待分享)"></a>后续监控、自定义函数、压测、调优等功能(待分享)</h2><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 结构化流 </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我司Kafka+Flink+MySQL生产完整案例代码</title>
      <link href="/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
      <url>/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h6 id="1-版本信息："><a href="#1-版本信息：" class="headerlink" title="1.版本信息："></a>1.版本信息：</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flink实战</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="3-工程pom-xml"><a href="#3-工程pom-xml" class="headerlink" title="3.工程pom.xml"></a>3.工程pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConf类 定义与MySQL连接的JDBC的参数<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="5-MySQLSlink类"><a href="#5-MySQLSlink类" class="headerlink" title="5.MySQLSlink类"></a>5.MySQLSlink类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-数据清洗日期工具类"><a href="#6-数据清洗日期工具类" class="headerlink" title="6.数据清洗日期工具类"></a>6.数据清洗日期工具类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //测试一下</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQL建表"><a href="#7-MySQL建表" class="headerlink" title="7.MySQL建表"></a>7.MySQL建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-主程序："><a href="#8-主程序：" class="headerlink" title="8.主程序："></a>8.主程序：</h6><p>主要是将time的格式转成yyyyMMddHHmmss,</p><p>还有取URL中的课程ID，将不是/class开头的过滤掉。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="9-启动主程序，查看MySQL表数据在递增"><a href="#9-启动主程序，查看MySQL表数据在递增" class="headerlink" title="9.启动主程序，查看MySQL表数据在递增"></a>9.启动主程序，查看MySQL表数据在递增</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafka过来的消息是我模拟的，一分钟产生100条。</p><p>以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark在携程的实践（二）</title>
      <link href="/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>以下内容来自第三届携程大数据沙龙</p><h3 id="七、遇到的问题"><a href="#七、遇到的问题" class="headerlink" title="七、遇到的问题"></a>七、遇到的问题</h3><h5 id="orc-split"><a href="#orc-split" class="headerlink" title="orc split"></a>orc split</h5><p>Spark读取Hive表用的各个文件格式的InuptFormat，计算读取表需要的task数量依赖于InputFormat#getSplits<br>由于大部分表的存储格式主要使用的是orc，当一个orc文件超过256MB，split算法并行去读取orc元数据，有时候Driver内存飙升，OOM crash，Full GC导致network timeout，spark context stop<br>Hive读这些大表为何没有问题？因为Hive默认使用的是CombineHiveInputFormat，split是基于文件大小的。<br>Spark也需要实现类似于Hive的CombineInputFormat，还能解决小文件过多导致提交task数量过多的问题。<br>Executor Container killed<br>Executor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead<br><a id="more"></a></p><h5 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h5><p>1.Shuffle Read时netty堆外内存的使用<br>2.Window function spill threshold过小，导致每4096条或者64MB为一个文件写到磁盘<br>外部排序同时打开每个文件，每个文件占用1MB的堆外内存，导致container使用的内存远超过申请的内存，遂被yarn kill。<br>解决：<br>Patch：<br>[SPARK-19659] Fetch big blocks to disk when shuffle-read<br>[SPARK-21369][CORE] Don’t use Scala Tuple2 in common/network-<em><br>参数：spark.reducer.maxReqSizeShuffleToMem=209715200<br>Patch：<br>[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArray<br>参数：<br>spark.sql.windowExec.buffer.in.memory.threshold=4096<br>spark.sql.windowExec.buffer.spill.threshold= 1024 </em>1024 * 1024 / 2</p><h5 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h5><p>Spark写数据时生成很多小文件，对NameNode产生巨大的压力，在一开始Spark灰度上线的时候，文件数和Block数飙升，文件变小导致压缩率降低，容量也跟着上去。</p><h5 id="移植Hive-MergeFileTask的实现"><a href="#移植Hive-MergeFileTask的实现" class="headerlink" title="移植Hive MergeFileTask的实现"></a>移植Hive MergeFileTask的实现</h5><p>在Spark最后写目标表的阶段追加入了一个MergeFileTask，参考了Hive的实现<br>org.apache.hadoop.hive.ql.io.merge.MergeFileTask<br>org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator</p><h5 id="无数据的情况下不创建空文件"><a href="#无数据的情况下不创建空文件" class="headerlink" title="无数据的情况下不创建空文件"></a>无数据的情况下不创建空文件</h5><p>[SPARK-21435][SQL]<br>Empty files should be skipped while write to file</p><h3 id="八、优化"><a href="#八、优化" class="headerlink" title="八、优化"></a>八、优化</h3><p>1.查询分区表时支持broadcast join，加速查询<br>2.减少Broadcast join的内存压力 SPARK-22170<br>3.Fetch失败后能快速失败，以免作业卡几个小时 SPARK-19753<br>4.Spark Thrift Server稳定性<br>经常挂掉，日志里异常，more than one active taskSet for stage<br>Apply SPARK-23433仍有少数挂掉的情况，<br>提交SPARK-24677到社区，修复之<br>5.作业hang住 SPARK-21834 SPARK-19326 SPARK-11334</p><h3 id="九、未来计划"><a href="#九、未来计划" class="headerlink" title="九、未来计划"></a>九、未来计划</h3><h5 id="自动调优内存"><a href="#自动调优内存" class="headerlink" title="自动调优内存"></a>自动调优内存</h5><p>手机spark driver和executor内存使用情况<br>根据作业历史的内存使用情况，在调度系统端自动设置合适的内存<br><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener">https://github.com/uber-common/jvm-profiler</a></p><h5 id="spark-adaptive"><a href="#spark-adaptive" class="headerlink" title="spark adaptive"></a>spark adaptive</h5><p>动态调整执行计划 SortMergeJoin转化为BroadcastHashJoin<br>动态处理数据倾斜<br><a href="https://issues.apache.org/jira/browse/SPARK-23128" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-23128</a><br><a href="https://github.com/Intel-bigdata/spark-adaptive" target="_blank" rel="noopener">https://github.com/Intel-bigdata/spark-adaptive</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark在携程的实践（一）</title>
      <link href="/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="一、Spark在携程应用的现状"><a href="#一、Spark在携程应用的现状" class="headerlink" title="一、Spark在携程应用的现状"></a>一、Spark在携程应用的现状</h3><h6 id="集群规模："><a href="#集群规模：" class="headerlink" title="集群规模："></a>集群规模：</h6><p>平均每天MR任务数：30W+</p><h6 id="开发平台："><a href="#开发平台：" class="headerlink" title="开发平台："></a>开发平台：</h6><p>调度系统运行的任务数：10W+<br>每天运行任务实例数：23W+<br>ETL/计算任务：~58%</p><h6 id="查询平台"><a href="#查询平台" class="headerlink" title="查询平台:"></a>查询平台:</h6><p>adhoc查询：2W+<br>支持Spark/Hive/Presto<br><img src="/assets/blogImg/1209_1.png" alt="enter description here"></p><h3 id="二、Hive与Spark的区别"><a href="#二、Hive与Spark的区别" class="headerlink" title="二、Hive与Spark的区别"></a>二、Hive与Spark的区别</h3><h6 id="Hive："><a href="#Hive：" class="headerlink" title="Hive："></a>Hive：</h6><p>优点：运行稳定，客户端内存消耗小。<br>存在问题：生成多个MapReduce作业；中间结果落地，IO开销大；频繁申请和释放container，资源没有合理充分利用</p><h6 id="Spark："><a href="#Spark：" class="headerlink" title="Spark："></a>Spark：</h6><p>快：高效的DAG执行引擎，可以基于内存来高效的处理数据流，节省大量IO开销<br>通用性：SparkSQL能直接使用HiveQL语法，Hive Metastore，Serdes，UDFs<br><img src="/assets/blogImg/1209_2.png" alt="enter description here"></p><h3 id="三、迁移SparkSQL的挑战"><a href="#三、迁移SparkSQL的挑战" class="headerlink" title="三、迁移SparkSQL的挑战"></a>三、迁移SparkSQL的挑战</h3><h6 id="兼容性："><a href="#兼容性：" class="headerlink" title="兼容性："></a>兼容性：</h6><p>Hive原先的权限控制<br>SQL语法，UDF和Hive的兼容性</p><h6 id="稳定性："><a href="#稳定性：" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明，低优先级用户无感知<br>监控作业迁移后成功率及运行时长对比</p><h6 id="准确性："><a href="#准确性：" class="headerlink" title="准确性："></a>准确性：</h6><p>数据一致<br>功能增强：<br>用户体验，是否易用，报错信息是否可读<br>潜在Bug<br>周边系统配合改造<br>血缘收集</p><h3 id="四、兼容性改造"><a href="#四、兼容性改造" class="headerlink" title="四、兼容性改造"></a>四、兼容性改造</h3><h6 id="移植hive权限"><a href="#移植hive权限" class="headerlink" title="移植hive权限"></a>移植hive权限</h6><p>Spark没有权限认证模块，可对任意表进行查询，有安全隐患<br>需要与Hive共享同一套权限</p><h6 id="方案："><a href="#方案：" class="headerlink" title="方案："></a>方案：</h6><p>执行SQL时，对SQL解析得到LogicalPlan，对LogicalPlan进行遍历，提取读取的表及写入的表，调用Hvie的认证方法进行检查，如果有权限则继续执行，否则拒绝该用户的操作。<br>SQL语法和hive兼容<br>Spark创建的某些视图，在Hive查询时报错，Spark创建的视图不会对SQL进行展开，视图定义没有当前的DB信息，Hive不兼容读取这样的视图</p><h6 id="方案：、"><a href="#方案：、" class="headerlink" title="方案：、"></a>方案：、</h6><p>保持与Hive一致，在Spark创建和修改视图时，使用hive cli driver去执行create/alter view sql<br>UDF与hive兼容<br>UDF计算结果不一样，即使是正常数据，Spark返回null，Hive结果正确；异常数据，Spark抛exception导致作业失败，Hive返回的null。</p><h6 id="方案：-1"><a href="#方案：-1" class="headerlink" title="方案："></a>方案：</h6><p>Spark函数修复，比如round函数<br>将hive一些函数移植，并注册成永久函数<br>整理Spark和Hive语法和UDF差异<br>五、稳定性和准确性</p><h6 id="稳定性：-1"><a href="#稳定性：-1" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明：调度系统对低优先级作业，按作业粒度切换成Spark执行，失败后再切换成hive<br>灰度变更，多种变更规则：支持多版本Spark，自动切换引擎，Spark v2 -&gt; Spark v1 -&gt; Hive；灰度推送参数，调优参数，某些功能<br>监控：每日统计spark和hive运行对比，每时收集作业粒度失败的Spark作业，分析失败原因<br>准确性：<br>数据质量系统：校验任务，检查数据准确性</p><h3 id="六、功能增强"><a href="#六、功能增强" class="headerlink" title="六、功能增强"></a>六、功能增强</h3><h6 id="Spark-Thrift-Server："><a href="#Spark-Thrift-Server：" class="headerlink" title="Spark Thrift Server："></a>Spark Thrift Server：</h6><ul><li>1.基于delegation token的impersontion<br>Driver：<br>为不同的用户拿delegation token，写到staging目录，记录User-&gt;SQL-&gt;Job映射关系，分发task带上对应的username<br>Executor：<br>根据task信息带的username找到staging目录下的token，加到当前proxy user的ugi，实现impersonate</li><li>2.基于zookeeper的服务发现，支持多台server<br>这一块主要移植了Hive zookeeper的实现</li><li>3.限制大查询作业，防止driver OOM<br>限制每个job产生的task最大数量<br>限制查询SQL的最大行数，客户端查询大批量数据，数据挤压在Thrift Server，堆内内存飙升，强制在只有查的SQL加上limit<br>限制查询SQL的结果集数据大小</li><li>4.监控<br>对每个server定时查询，检测是否可用<br>多运行时长较久的作业，主动kill<h6 id="用户体验"><a href="#用户体验" class="headerlink" title="用户体验"></a>用户体验</h6>用户看到的是类似Hive MR进度的日志，INFO级别日志收集到ES，可供日志的分析和排查问题<br>收集生成的表或者分区的numRows numFile totalSize，输出到日志<br>对简单的语句，如DDL语句，自动使用–master=local方式启动<h6 id="Combine-input-Format"><a href="#Combine-input-Format" class="headerlink" title="Combine input Format"></a>Combine input Format</h6>在HadoopTableReader#makeRDDForTable，拿到对应table的InputFormatClass，转换成对应格式的CombineInputFormat<br>通过开关来决定是否启用这个特性<br>set spark.sql.combine.input.splits.enable=true<br>通过参数来调整每个split的total input size<br>mapreduce.input.fileinputformat.split.maxsize=256MB <em>1024</em>1024<br>之前driver读大表高峰时段split需要30分钟不止，才把任务提交上，现在只要几分钟就算好split的数量并提交任务，也解决了一些表不大，小文件多，能合并到同一个task进行读取</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码 | Spark读取mongoDB数据写入Hive普通表和分区表</title>
      <link href="/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/"/>
      <url>/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="版本："><a href="#版本：" class="headerlink" title="版本："></a>版本：</h3><p>spark 2.2.0<br>hive 1.1.0<br>scala 2.11.8<br>hadoop-2.6.0-cdh5.7.0<br>jdk 1.8<br>MongoDB 3.6.4</p><h3 id="一-原始数据及Hive表"><a href="#一-原始数据及Hive表" class="headerlink" title="一 原始数据及Hive表"></a>一 原始数据及Hive表</h3><h5 id="MongoDB数据格式"><a href="#MongoDB数据格式" class="headerlink" title="MongoDB数据格式"></a>MongoDB数据格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;),</span><br><span class="line">    &quot;id&quot; : &quot;1&quot;,</span><br><span class="line">    &quot;name&quot; : &quot;lisi&quot;,</span><br><span class="line">    &quot;age&quot; : &quot;18&quot;,</span><br><span class="line">    &quot;deptno&quot; : &quot;01&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="Hive普通表"><a href="#Hive普通表" class="headerlink" title="Hive普通表"></a>Hive普通表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table mg_hive_test(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string,</span><br><span class="line">deptno string</span><br><span class="line">)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h5 id="Hive分区表"><a href="#Hive分区表" class="headerlink" title="Hive分区表"></a>Hive分区表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table  mg_hive_external(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string</span><br><span class="line">)</span><br><span class="line">partitioned by (deptno string)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h3 id="二-IDEA-Maven-Java"><a href="#二-IDEA-Maven-Java" class="headerlink" title="二 IDEA+Maven+Java"></a>二 IDEA+Maven+Java</h3><h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.6.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.2.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/2 21:00</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import com.mongodb.spark.MongoSpark;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import org.bson.Document;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class sparkreadmgtohive &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        //spark 2.x</span><br><span class="line">        String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath();</span><br><span class="line">        SparkSession spark = SparkSession.builder()</span><br><span class="line">                .master(&quot;local[2]&quot;)</span><br><span class="line">                .appName(&quot;SparkReadMgToHive&quot;)</span><br><span class="line">                .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">                .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line"></span><br><span class="line">        //spark 1.x</span><br><span class="line">//        JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);</span><br><span class="line">//        SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);</span><br><span class="line">//        conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);</span><br><span class="line">//        conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);</span><br><span class="line">//        HiveContext sqlContext = new HiveContext(sc);</span><br><span class="line">//        //create df from mongo</span><br><span class="line">//        Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();</span><br><span class="line">//        df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">        String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;;</span><br><span class="line">        String opType =&quot;P&quot;;</span><br><span class="line"></span><br><span class="line">        SQLUtils sqlUtils = new SQLUtils();</span><br><span class="line">        List&lt;String&gt; column = sqlUtils.getColumns(querysql);</span><br><span class="line"></span><br><span class="line">        //create rdd from mongo</span><br><span class="line">        JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc);</span><br><span class="line">        //将Document转成Object</span><br><span class="line">        JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123;</span><br><span class="line">            public Object call(Document document)&#123;</span><br><span class="line">                List list = new ArrayList();</span><br><span class="line">                for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">                    list.add(String.valueOf(document.get(column.get(i))));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line"></span><br><span class="line">//                return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(Ordd.first());</span><br><span class="line">        //通过编程方式将RDD转成DF</span><br><span class="line">        List ls= new ArrayList();</span><br><span class="line">        for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">            ls.add(column.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">        String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;);</span><br><span class="line">        System.out.println(schemaString);</span><br><span class="line"></span><br><span class="line">        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();</span><br><span class="line">        for (String fieldName : schemaString.split(&quot;,&quot;)) &#123;</span><br><span class="line">            StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);</span><br><span class="line">            fields.add(field);</span><br><span class="line">        &#125;</span><br><span class="line">        StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123;</span><br><span class="line">            List fileds = (List) record;</span><br><span class="line">//            String[] attributes = record.toString().split(&quot;,&quot;);</span><br><span class="line">            return RowFactory.create(fileds.toArray());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema);</span><br><span class="line"></span><br><span class="line">        //将DF写入到Hive中</span><br><span class="line">        //选择Hive数据库</span><br><span class="line">        spark.sql(&quot;use datalake&quot;);</span><br><span class="line">        //注册临时表</span><br><span class="line">        df.registerTempTable(&quot;mgtable&quot;);</span><br><span class="line"></span><br><span class="line">        if (&quot;O&quot;.equals(opType.trim())) &#123;</span><br><span class="line">            System.out.println(&quot;数据插入到Hive ordinary table&quot;);</span><br><span class="line">            Long t1 = System.currentTimeMillis();</span><br><span class="line">            spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;);</span><br><span class="line">            Long t2 = System.currentTimeMillis();</span><br><span class="line">            System.out.println(&quot;共耗时：&quot; + (t2 - t1) / 60000 + &quot;分钟&quot;);</span><br><span class="line">        &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123;</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;数据插入到Hive  dynamic partition table&quot;);</span><br><span class="line">        Long t3 = System.currentTimeMillis();</span><br><span class="line">        //必须设置以下参数 否则报错</span><br><span class="line">        spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;);</span><br><span class="line">        //depton为分区字段   select语句最后一个字段必须是deptno</span><br><span class="line">        spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;);</span><br><span class="line">        Long t4 = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;共耗时：&quot;+(t4 -t3)/60000+ &quot;分钟&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/3 23:20</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class SQLUtils &#123;</span><br><span class="line"></span><br><span class="line">    public List&lt;String&gt; getColumns(String querysql)&#123;</span><br><span class="line">        List&lt;String&gt; column = new ArrayList&lt;String&gt;();</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6,</span><br><span class="line">                querysql.indexOf(&quot;from&quot;)).trim();</span><br><span class="line">        if (tmp.indexOf(&quot;*&quot;) == -1)&#123;</span><br><span class="line">            String cols[] = tmp.split(&quot;,&quot;);</span><br><span class="line">            for (String c:cols)&#123;</span><br><span class="line">                column.add(c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return column;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTBname(String querysql)&#123;</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim();</span><br><span class="line">        int sx = tmp.indexOf(&quot; &quot;);</span><br><span class="line">        if(sx == -1)&#123;</span><br><span class="line">            return tmp;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return tmp.substring(0,sx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三-错误解决办法"><a href="#三-错误解决办法" class="headerlink" title="三 错误解决办法"></a>三 错误解决办法</h3><p>1 IDEA会获取不到Hive的数据库和表，将hive-site.xml放入resources文件中。并且将resources设置成配置文件(设置成功文件夹是蓝色否则是灰色)<br>file–&gt;Project Structure–&gt;Modules–&gt;Source<br><img src="/assets/blogImg/1120_1.png" alt="enter description here"><br>2 上面错误处理完后如果报JDO类型的错误，那么检查HIVE_HOME/lib下时候否mysql驱动，如果确定有，那么就是IDEA获取不到。解决方法如下：</p><p>将mysql驱动拷贝到jdk1.8.0_171.jdk/Contents/Home/jre/lib/ext路径下(jdk/jre/lib/ext)<br>在IDEA项目External Libraries下的&lt;1.8&gt;里面添加mysql驱动<br><img src="/assets/blogImg/1120_2.png" alt="enter description here"></p><h3 id="四-注意点"><a href="#四-注意点" class="headerlink" title="四 注意点"></a>四 注意点</h3><p>由于将MongoDB数据表注册成了临时表和Hive表进行了关联，所以要将MongoDB中的id字段设置成索引字段，否则性能会很慢。<br>MongoDB设置索引方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;</span><br></pre></td></tr></table></figure><p></p><p>查看索引：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).getIndexes()</span><br><span class="line">MongoSpark网址：https://docs.mongodb.com/spark-connector/current/java-api/</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)</title>
      <link href="/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
      <url>/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h5 id="1-下载Flink安装包"><a href="#1-下载Flink安装包" class="headerlink" title="1.下载Flink安装包"></a>1.下载Flink安装包</h5><p>flink下载地址</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可</p><p>上传至机器的/opt目录下<br><a id="more"></a></p><h5 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-配置master节点"><a href="#3-配置master节点" class="headerlink" title="3.配置master节点"></a>3.配置master节点</h5><p>选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。</p><p>jobmanager.rpc.address: node1</p><p>(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)</p><p>rest.port: 8088</p><p>本次安装 master节点为node1，因为单机，slave节点也为node1</p><h5 id="4-配置slaves"><a href="#4-配置slaves" class="headerlink" title="4.配置slaves"></a>4.配置slaves</h5><p>将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。</p><h5 id="5-启动flink集群"><a href="#5-启动flink集群" class="headerlink" title="5.启动flink集群"></a>5.启动flink集群</h5><p>bin/start-cluster.sh</p><p>打开 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 查看web页面<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managers代表当前的flink只有一个节点，每个task还有两个slots</p><h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6.测试"></a>6.测试</h5><p><strong>依赖</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p></p><h5 id="7-Socket测试代码"><a href="#7-Socket测试代码" class="headerlink" title="7.Socket测试代码"></a>7.Socket测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)</p><p>在命令行set MAVEN_OPTS= -Xms128m -Xmx512m</p><p>继续执行mvn clean install</p><p>生成FlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>找到打成的jar，并upload，开始上传<br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>运行参数介绍<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个</p><p>发送数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>点开running的job，你可以看见接收的字节数等信息</p><p>到log目录下可以清楚的看见输出<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure><p></p><p>除了可以在界面提交，还可以将jar上传的linux中进行提交任务</p><p>运行flink上传的jar<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure><p></p><p>其他步骤一致。</p><h5 id="8-使用kafka作为source"><a href="#8-使用kafka作为source" class="headerlink" title="8.使用kafka作为source"></a>8.使用kafka作为source</h5><p>加上依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //可以通过正则表达式来匹配合适的topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //配置从最新的地方开始消费</span><br><span class="line">        kafkaSource.setStartFromLatest();        //使用addsource，将kafka的输入转变为datastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-使用mysql作为sink"><a href="#9-使用mysql作为sink" class="headerlink" title="9.使用mysql作为sink"></a>9.使用mysql作为sink</h5><p>flink本身并没有提供datastream输出到mysql，需要我们自己去实现</p><p>首先，导入依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。</p><p>使用这个mysqlsink也非常简单<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure><p></p><h5 id="10-总结"><a href="#10-总结" class="headerlink" title="10.总结"></a>10.总结</h5><p>本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>19生产预警平台项目之sparkdemo.jar运行在yarn上过程</title>
      <link href="/2018/09/28/19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/09/28/19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-将之前打包的jar包上传"><a href="#1-将之前打包的jar包上传" class="headerlink" title="1.将之前打包的jar包上传"></a>1.将之前打包的jar包上传</h4><p>[root@sht-sgmhadoopnn-01 spark]# pwd<br>/root/learnproject/app/spark<br>[root@sht-sgmhadoopnn-01 spark]# rz<br>rz waiting to receive.<br>Starting zmodem transfer. Press Ctrl+C to cancel.<br>Transferring sparkdemo.jar…<br>100% 164113 KB 421 KB/sec 00:06:29 0 Errors</p><h5 id="2-以下是错误"><a href="#2-以下是错误" class="headerlink" title="2.以下是错误"></a>2.以下是错误</h5><h6 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR1: Exception in thread &quot;main&quot; </span><br><span class="line">java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</span><br></pre></td></tr></table></figure><p>IDEA打包的jar包,需要使用zip删除指定文件<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -d sparkdemo.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</span><br></pre></td></tr></table></figure><p></p><h6 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR2: Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/learn/java/main/OnLineLogAnalysis2 : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure><p>yarn环境的jdk版本低于编译jar包的jdk版本(需要一致或者高于;每个节点需要安装jdk,同时修改每个节点的hadoop-env.sh文件的JAVA_HOME参数指向)</p><h6 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ERROR3: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line"> 17/02/15 17:30:35 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line"> java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;</span><br><span class="line">  at org.influxdb.impl.InfluxDBImpl.ping(InfluxDBImpl.java:178)</span><br><span class="line">  at org.influxdb.impl.InfluxDBImpl.version(InfluxDBImpl.java:201)</span><br><span class="line">  at com.learn.java.main.OnLineLogAnalysis2.main(OnLineLogAnalysis2.java:69)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">  at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">  at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)</span><br></pre></td></tr></table></figure><p>抛错信息为NoSuchMethodError，表示 guava可能有多版本，则低版本<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# pwd</span><br><span class="line"> /root/learnproject/app</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# ll</span><br><span class="line"> total 470876</span><br><span class="line"> -rw-r--r--  1 root root   7509833 Jan 16 22:11 AdminLTE.zip</span><br><span class="line"> drwxr-xr-x 12 root root      4096 Feb 14 11:21 hadoop</span><br><span class="line"> -rw-r--r--  1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz</span><br><span class="line"> drwxr-xr-x  7 root root      4096 Feb  7 11:16 kafka-manager-1.3.2.1</span><br><span class="line"> -rw-r--r--  1 root root  59682993 Dec 26 14:44 kafka-manager-1.3.2.1.zip</span><br><span class="line"> drwxr-xr-x  2 root root      4096 Jan  7 16:21 kafkaoffsetmonitor</span><br><span class="line"> drwxr-xr-x  2  777 root      4096 Feb 14 14:48 pid</span><br><span class="line"> drwxrwxr-x  4 1000 1000      4096 Oct 29 01:46 sbt</span><br><span class="line"> -rw-r--r--  1 root root   1049906 Dec 25 21:29 sbt-0.13.13.tgz</span><br><span class="line"> drwxrwxr-x  6 root root      4096 Mar  4  2016 scala</span><br><span class="line"> -rw-r--r--  1 root root  28678231 Mar  4  2016 scala-2.11.8.tgz</span><br><span class="line"> drwxr-xr-x 13 root root      4096 Feb 15 17:01 spark</span><br><span class="line"> -rw-r--r--  1 root root 187426587 Nov 12 06:54 spark-2.0.2-bin-hadoop2.7.tgz</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# </span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# find ./ -name *guava*</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/yarn/lib/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# mv ./spark/jars/guava-14.0.1.jar ./spark/jars/guava-14.0.1.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./spark/jars/</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar.bak</span><br><span class="line"> [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/common/lib/</span><br></pre></td></tr></table></figure><p></p><h5 id="3-后台提交jar包运行"><a href="#3-后台提交jar包运行" class="headerlink" title="3.后台提交jar包运行"></a>3.后台提交jar包运行</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# nohup /root/learnproject/app/spark/bin/spark-submit \</span><br><span class="line">&gt; --name onlineLogsAnalysis \</span><br><span class="line">&gt; --master yarn    \</span><br><span class="line">&gt; --deploy-mode cluster     \</span><br><span class="line">&gt; --conf &quot;spark.scheduler.mode=FAIR&quot; \</span><br><span class="line">&gt; --conf &quot;spark.sql.codegen=true&quot; \</span><br><span class="line">&gt; --driver-memory 2G \</span><br><span class="line">&gt; --executor-memory 2G \</span><br><span class="line">&gt; --executor-cores 1 \</span><br><span class="line">&gt; --num-executors 3 \</span><br><span class="line">&gt; --class com.learn.java.main.OnLineLogAnalysis2     \</span><br><span class="line">&gt; /root/learnproject/app/spark/sparkdemo.jar &amp;</span><br><span class="line">[1] 22926</span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# nohup: ignoring input and appending output to `nohup.out&apos;</span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 spark]# tail -f nohup.out</span><br></pre></td></tr></table></figure><h5 id="4-yarn-web界面查看运行log"><a href="#4-yarn-web界面查看运行log" class="headerlink" title="4.yarn web界面查看运行log"></a>4.yarn web界面查看运行log</h5><p><img src="/assets/blogImg/928_1.jpg" alt="enter description here"><br>ApplicationMaster：打开为spark history server web界面</p><p>logs： 查看stderr 和 stdout日志 (system.out.println方法输出到stdout日志中)<br><img src="/assets/blogImg/928_2.jpg" alt="enter description here"><br><img src="/assets/blogImg/928_3.jpg" alt="enter description here"><br><img src="/assets/blogImg/928_4.jpg" alt="enter description here"></p><h5 id="5-查看spark-history-web"><a href="#5-查看spark-history-web" class="headerlink" title="5.查看spark history web"></a>5.查看spark history web</h5><p><img src="/assets/blogImg/928_5.jpg" alt="enter description here"></p><h5 id="6-查看DashBoard-实时可视化"><a href="#6-查看DashBoard-实时可视化" class="headerlink" title="6.查看DashBoard ,实时可视化"></a>6.查看DashBoard ,实时可视化</h5><p><img src="/assets/blogImg/928_6.jpg" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>18Spark on Yarn配置日志Web UI(HistoryServer服务)</title>
      <link href="/2018/09/26/18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)/"/>
      <url>/2018/09/26/18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 ~]# cd /root/learnproject/app/spark/conf</span><br><span class="line">[root@sht-sgmhadoopnn-01 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><h4 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h4><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /</span><br><span class="line"> Found 3 items</span><br><span class="line"> drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line"> drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line"> drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line"> You have new mail in /var/spool/mail/root</span><br><span class="line"> [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /spark</span><br><span class="line"> Found 1 items</span><br><span class="line"> drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@sht-sgmhadoopnn-01 conf]# hdfs dfs -mkdir /spark/historylog</span><br><span class="line">#在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息</span><br></pre></td></tr></table></figure><h4 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir               hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br><span class="line">#spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建</span><br><span class="line">#spark.yarn.historyServer.address : Spark history server的地址(不加http://).</span><br><span class="line">这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</span><br></pre></td></tr></table></figure><h4 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 conf]# vi spark-env.sh</span><br><span class="line"> #!/usr/bin/env bash</span><br><span class="line"> </span><br><span class="line"> export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line"> export JAVA_HOME=/usr/java/jdk1.8.0_111</span><br><span class="line"> export SPARK_MASTER_IP=172.16.101.55</span><br><span class="line"> export SPARK_WORKER_MEMORY=1g</span><br><span class="line"> export SPARK_PID_DIR=/root/learnproject/app/pid</span><br><span class="line"> export HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoop</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h4 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> [root@sht-sgmhadoopnn-01 spark]# ./sbin/start-history-server.sh </span><br><span class="line"> starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out</span><br><span class="line"> [root@sht-sgmhadoopnn-01 ~]# jps</span><br><span class="line"> 28905 HistoryServer</span><br><span class="line"> 30407 ProdServerStart</span><br><span class="line"> 30373 ResourceManager</span><br><span class="line"> 30957 NameNode</span><br><span class="line"> 16949 Jps</span><br><span class="line"> 30280 DFSZKFailoverController</span><br><span class="line">31445 JobHistoryServer</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# ps -ef|grep spark</span><br><span class="line">root     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java          </span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>17生产预警平台项目之使用IDEA将工程Build成jar包</title>
      <link href="/2018/09/25/17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85/"/>
      <url>/2018/09/25/17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-File–-gt-Project-Structure"><a href="#1-File–-gt-Project-Structure" class="headerlink" title="1.File–&gt;Project Structure"></a>1.File–&gt;Project Structure</h4><p><img src="/assets/blogImg/0925_1.png" alt="enter description here"></p><h4 id="2-Artifacts–-gt-–-gt-JAR–-gt-From-modules-with-dependencies"><a href="#2-Artifacts–-gt-–-gt-JAR–-gt-From-modules-with-dependencies" class="headerlink" title="2.Artifacts–&gt;+–&gt;JAR–&gt;From modules with dependencies"></a>2.Artifacts–&gt;+–&gt;JAR–&gt;From modules with dependencies</h4><p><img src="/assets/blogImg/0925_2.png" alt="enter description here"></p><h4 id="3-单击…-–-gt-选择OnLineLogAnalysis2"><a href="#3-单击…-–-gt-选择OnLineLogAnalysis2" class="headerlink" title="3. 单击… –&gt;选择OnLineLogAnalysis2"></a>3. 单击… –&gt;选择OnLineLogAnalysis2</h4><p><img src="/assets/blogImg/0925_3.png" alt="enter description here"><br><img src="/assets/blogImg/0925_4.png" alt="enter description here"></p><h4 id="4-选择项目的根目录"><a href="#4-选择项目的根目录" class="headerlink" title="4.选择项目的根目录"></a>4.选择项目的根目录</h4><p><img src="/assets/blogImg/0925_5.png" alt="enter description here"></p><h4 id="5-修改Name–-gt-选择输出目录–-gt-选择Output-directory–-gt-Apply–-gt-OK"><a href="#5-修改Name–-gt-选择输出目录–-gt-选择Output-directory–-gt-Apply–-gt-OK" class="headerlink" title="5.修改Name–&gt;选择输出目录–&gt;选择Output directory–&gt;Apply–&gt;OK"></a>5.修改Name–&gt;选择输出目录–&gt;选择Output directory–&gt;Apply–&gt;OK</h4><p><img src="/assets/blogImg/0925_6.png" alt="enter description here"></p><h4 id="6-Build–-gt-Build-Artifacts–-gt-Build"><a href="#6-Build–-gt-Build-Artifacts–-gt-Build" class="headerlink" title="6.Build–&gt;Build Artifacts–&gt;Build"></a>6.Build–&gt;Build Artifacts–&gt;Build</h4><p><img src="/assets/blogImg/0925_7.png" alt="enter description here"><br><img src="/assets/blogImg/0925_8.png" alt="enter description here"><br><img src="/assets/blogImg/0925_9.png" alt="enter description here"></p><p>===================================<br>说明:<br>1.打包方式很多，大家自行google.<br>2.由于我是引用influxdb的源码包,需要引入许多依赖jar包,所以我需要将相关依赖jar包全部打包到本程序的jar包,故该jar包大概160M。<br>(当然也可以只需要打本程序的jar包，只不过需要事先将相关的所有或者部分依赖jar包，前提上传到集群，然后spark-submit使用–jars引用即可)</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>16生产预警平台项目之grafana-4.1.1 Install和新建日志分析的DashBoard</title>
      <link href="/2018/09/19/16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard/"/>
      <url>/2018/09/19/16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h4><p>wget <a href="https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz" target="_blank" rel="noopener">https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz</a></p><h4 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h4><p>tar -zxvf grafana-4.1.1-1484211277.linux-x64.tar.gz</p><h4 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3.配置文件"></a>3.配置文件</h4><p>cd grafana-4.1.1-1484211277<br>cp conf/sample.ini conf/custom.ini</p><p>#make changes to conf/custom.ini then start grafana-server</p><h4 id="4-后台启动"><a href="#4-后台启动" class="headerlink" title="4.后台启动"></a>4.后台启动</h4><p>./bin/grafana-server &amp;</p><h4 id="5-打开web"><a href="#5-打开web" class="headerlink" title="5.打开web"></a>5.打开web</h4><p><a href="http://172.16.101.66:3000/" target="_blank" rel="noopener">http://172.16.101.66:3000/</a> admin/admin</p><h4 id="6-配置数据源influxdb"><a href="#6-配置数据源influxdb" class="headerlink" title="6.配置数据源influxdb"></a>6.配置数据源influxdb</h4><p><img src="/assets/blogImg/919_1.png" alt="enter description here"><br>还要填写Database 为 online_log_analysis</p><h4 id="7-IDEA本机运行OnLineLogAanlysis2-class，实时计算存储到influxdb"><a href="#7-IDEA本机运行OnLineLogAanlysis2-class，实时计算存储到influxdb" class="headerlink" title="7.IDEA本机运行OnLineLogAanlysis2.class，实时计算存储到influxdb"></a>7.IDEA本机运行OnLineLogAanlysis2.class，实时计算存储到influxdb</h4><h4 id="8-新建dashboard和-cdh-hdfs-warn曲线图"><a href="#8-新建dashboard和-cdh-hdfs-warn曲线图" class="headerlink" title="8.新建dashboard和 cdh_hdfs_warn曲线图"></a>8.新建dashboard和 cdh_hdfs_warn曲线图</h4><p><img src="/assets/blogImg/919_2.png" alt="enter description here"><br>参考:<br><a href="http://grafana.org/download/" target="_blank" rel="noopener">http://grafana.org/download/</a><br><a href="http://docs.grafana.org/" target="_blank" rel="noopener">http://docs.grafana.org/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15生产预警平台项目之基于Spark Streaming+Saprk SQL开发OnLineLogAanlysis2</title>
      <link href="/2018/09/18/15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming+Saprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2/"/>
      <url>/2018/09/18/15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming+Saprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h5 id="1-influxdb创建database"><a href="#1-influxdb创建database" class="headerlink" title="1.influxdb创建database"></a>1.influxdb创建database</h5><p>[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339<br>Connected to <a href="http://localhost:8086" target="_blank" rel="noopener">http://localhost:8086</a> version 1.2.0<br>InfluxDB shell version: 1.2.0</p><blockquote><p>create database online_log_analysis</p></blockquote><h5 id="2-导入源代码"><a href="#2-导入源代码" class="headerlink" title="2.导入源代码"></a>2.导入源代码</h5><p>项目中原本想将 influxdb-java <a href="https://github.com/influxdata/influxdb-java的InfluxDBTest.java" target="_blank" rel="noopener">https://github.com/influxdata/influxdb-java的InfluxDBTest.java</a> 文件的加到项目中，所以必须要引入 influxdb-java 的包；<br>但是由于GitHub的上的class文件的某些方法，是版本是2.6，而maven中的最高也就2.5版本，所以将Github的源代码下载导入到idea中，编译导出2.6.jar包；<br>可是 引入2.6jar包，其在InfluxDBTest.class文件的 无法import org.influxdb（百度谷歌很长时间，尝试很多方法不行）。<br>最后索性将 influx-java的源代码全部添加到项目中即可，如下图所示。</p><h5 id="3-运行OnLineLogAanlysis2-java"><a href="#3-运行OnLineLogAanlysis2-java" class="headerlink" title="3.运行OnLineLogAanlysis2.java"></a>3.运行OnLineLogAanlysis2.java</h5><p><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.java</a><br><img src="/assets/blogImg/0918_1.png" alt="enter description here"><br><strong>比如 logtype_count,host_service_logtype=hadoopnn-01_namenode_WARN</strong> count=12<br>logtype_count 是表<br>host_service_logtype=hadoopnn-01_namenode_WARN 是 tag–标签，在InfluxDB中，tag是一个非常重要的部分，表名+tag一起作为数据库的索引，是“key-value”的形式。<br>count=12 是 field–数据，field主要是用来存放数据的部分，也是“key-value”的形式。<br>tag、field 中间是要有空格的</p><h5 id="4-influxdb查询数据"><a href="#4-influxdb查询数据" class="headerlink" title="4.influxdb查询数据"></a>4.influxdb查询数据</h5><p><img src="/assets/blogImg/0918_2.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>13生产预警平台项目之舍弃Redis+echarts3,选择InfluxDB+Grafana</title>
      <link href="/2018/09/17/13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis+echarts3,%E9%80%89%E6%8B%A9InfluxDB+Grafana/"/>
      <url>/2018/09/17/13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis+echarts3,%E9%80%89%E6%8B%A9InfluxDB+Grafana/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-最初选择Redis作为存储，是主要有4个原因"><a href="#1-最初选择Redis作为存储，是主要有4个原因" class="headerlink" title="1.最初选择Redis作为存储，是主要有4个原因:"></a>1.最初选择Redis作为存储，是主要有4个原因:</h4><p>a.redis是一个key-value的存储系统，数据是存储在内存中，读写性能很高；<br>b.支持多种数据类型，如set,zset,list,hash,string；<br>c.key过期策略；<br>d.最主要是网上的博客全是sparkstreaming+redis，都互相模仿；<br>至于缺点，当时还没考虑到。</p><h4 id="2-然后开始添加CDHRolelog-class类和将redis模块加入代码中，"><a href="#2-然后开始添加CDHRolelog-class类和将redis模块加入代码中，" class="headerlink" title="2.然后开始添加CDHRolelog.class类和将redis模块加入代码中，"></a>2.然后开始添加CDHRolelog.class类和将redis模块加入代码中，</h4><p>使计算结果（本次使用spark streaming+spark sql，之前仅仅是spark streaming，具体看代码）存储到redis中，当然存储到redis中，有两种存储格式。</p><h5 id="2-1-key为机器名称-服务名称-日志级别拼接的字符串，"><a href="#2-1-key为机器名称-服务名称-日志级别拼接的字符串，" class="headerlink" title="2.1 key为机器名称,服务名称,日志级别拼接的字符串，"></a>2.1 key为机器名称,服务名称,日志级别拼接的字符串，</h5><p><strong>如hadoopnn-01_namenode_WARN，</strong><br>value为数据类型list，其存储为json格式的 [{“timeStamp”: “2017-02-09 17:16:14.249”,”hostName”: “hadoopnn-01”,”serviceName”: “namenode”,”logType”:”WARN”,”count”:”12” }]<br>代码url,下载导入idea,运行即可:<br><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java</a><br><img src="/assets/blogImg/917_1.png" alt="enter description here"></p><h4 id="2-2-key为timestamp"><a href="#2-2-key为timestamp" class="headerlink" title="2.2 key为timestamp"></a>2.2 key为timestamp</h4><p><strong>如 2017-02-09 18:09:02.462,</strong><br>value 为 [ {“host_service_logtype”: “hadoopnn-01_namenode_INFO”,”count”:”110” }, {“host_service_logtype”: “hadoopnn-01_namenode_DEBUG”,”count”:”678” }, {“host_service_logtype”: “hadoopnn-01_namenode_WARN”,”count”:”12” }]<br>代码url,下载导入idea,运行即可:<br><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java</a><br><img src="/assets/blogImg/917_2.png" alt="enter description here"></p><h4 id="3-做可视化这块，我们选择adminLTE-flask-echarts3-计划和编程开发尝试去从redis实时读取数据，动态绘制图表；"><a href="#3-做可视化这块，我们选择adminLTE-flask-echarts3-计划和编程开发尝试去从redis实时读取数据，动态绘制图表；" class="headerlink" title="3.做可视化这块，我们选择adminLTE+flask+echarts3, 计划和编程开发尝试去从redis实时读取数据，动态绘制图表；"></a>3.做可视化这块，我们选择adminLTE+flask+echarts3, 计划和编程开发尝试去从redis实时读取数据，动态绘制图表；</h4><p>后来开发调研大概1周，最终2.1 和2.2方法的存储格式都不能有效适合我们，进行开发可视化Dashboard，<br>所以我们最终调研采取InfluxDB+Grafana来做存储和可视化展示及预警。</p><h4 id="4-InfluxDB是时序数据库"><a href="#4-InfluxDB是时序数据库" class="headerlink" title="4.InfluxDB是时序数据库"></a>4.InfluxDB是时序数据库</h4><p><a href="https://docs.influxdata.com/influxdb/v1.2/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/</a></p><h4 id="5-Grafana是可视化组件"><a href="#5-Grafana是可视化组件" class="headerlink" title="5.Grafana是可视化组件"></a>5.Grafana是可视化组件</h4><p><a href="http://grafana.org/" target="_blank" rel="noopener">http://grafana.org/</a><br><a href="https://github.com/grafana/grafana" target="_blank" rel="noopener">https://github.com/grafana/grafana</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>14生产预警平台项目之influxdb-1.2.0 Install和概念，语法等学习</title>
      <link href="/2018/09/17/14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0/"/>
      <url>/2018/09/17/14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-下载rpm"><a href="#1-下载rpm" class="headerlink" title="1.下载rpm"></a>1.下载rpm</h4><p><a href="https://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpm" target="_blank" rel="noopener">https://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpm</a><br>我选择用window7 浏览器下载，然后rz上传到linux机器上</p><h4 id="2-安装"><a href="#2-安装" class="headerlink" title="2.安装"></a>2.安装</h4><p>yum install influxdb-1.2.0.x86_64.rpm</p><h4 id="3-启动"><a href="#3-启动" class="headerlink" title="3.启动"></a>3.启动</h4><p>service influxdb start</p><p>参考:<br><a href="https://docs.influxdata.com/influxdb/v1.2/introduction/installation/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/introduction/installation/</a><br>编译安装:<br><a href="https://anomaly.io/compile-influxdb/" target="_blank" rel="noopener">https://anomaly.io/compile-influxdb/</a></p><h4 id="4-进入"><a href="#4-进入" class="headerlink" title="4.进入"></a>4.进入</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339</span><br><span class="line">Connected to http://localhost:8086 version 1.2.0</span><br><span class="line">InfluxDB shell version: 1.2.0</span><br></pre></td></tr></table></figure><p>语法参考:<br><a href="https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/</a></p><p>学习url:<br><a href="http://www.linuxdaxue.com/influxdb-study-series-manual.html" target="_blank" rel="noopener">http://www.linuxdaxue.com/influxdb-study-series-manual.html</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12生产预警平台项目之RedisLive监控工具的详细安装</title>
      <link href="/2018/09/14/12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85/"/>
      <url>/2018/09/14/12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GitHub: https://github.com/nkrode/RedisLive</span><br></pre></td></tr></table></figure><h4 id="1-安装python2-7-5-和pip"><a href="#1-安装python2-7-5-和pip" class="headerlink" title="1.安装python2.7.5 和pip"></a>1.安装python2.7.5 和pip</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://blog.itpub.net/30089851/viewspace-2132450/</span><br></pre></td></tr></table></figure><h4 id="2-下载RedisLive"><a href="#2-下载RedisLive" class="headerlink" title="2.下载RedisLive"></a>2.下载RedisLive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# wget https://github.com/nkrode/RedisLive/archive/master.zip</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# unzip master </span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# mv RedisLive-master RedisLive</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# cd RedisLive/</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 design</span><br><span class="line">-rw-r--r-- 1 root root 1067 Aug 20  2015 MIT-LICENSE.txt</span><br><span class="line">-rw-r--r-- 1 root root  902 Aug 20  2015 README.md</span><br><span class="line">-rw-r--r-- 1 root root   58 Aug 20  2015 requirements.txt</span><br><span class="line">drwxr-xr-x 7 root root 4096 Aug 20  2015 src</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]#</span><br></pre></td></tr></table></figure><h4 id="3-查看版本要求-刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程"><a href="#3-查看版本要求-刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程" class="headerlink" title="3.查看版本要求(刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程)"></a>3.查看版本要求(刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cat requirements.txt</span><br><span class="line">argparse==1.2.1</span><br><span class="line">python-dateutil==1.5</span><br><span class="line">redis</span><br><span class="line">tornado==2.1.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cd ../</span><br></pre></td></tr></table></figure><h4 id="4-pip安装环境要求"><a href="#4-pip安装环境要求" class="headerlink" title="4.pip安装环境要求"></a>4.pip安装环境要求</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install python-dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 app]# pip install argparse</span><br></pre></td></tr></table></figure><h4 id="5-进入-root-learnproject-app-RedisLive-src目录-配置redis-live-conf文件"><a href="#5-进入-root-learnproject-app-RedisLive-src目录-配置redis-live-conf文件" class="headerlink" title="5.进入 /root/learnproject/app/RedisLive/src目录,配置redis-live.conf文件"></a>5.进入 /root/learnproject/app/RedisLive/src目录,配置redis-live.conf文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 app]# cd -</span><br><span class="line">/root/learnproject/app/RedisLive</span><br><span class="line">[root@sht-sgmhadoopdn-04 RedisLive]# cd src</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# ll</span><br><span class="line">total 40</span><br><span class="line">drwxr-xr-x 4 root root 4096 Aug 20  2015 api</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 dataprovider</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 db</span><br><span class="line">-rw-r--r-- 1 root root    0 Aug 20  2015 __init__.py</span><br><span class="line">-rw-r--r-- 1 root root  381 Aug 20  2015 redis-live.conf.example</span><br><span class="line">-rwxr-xr-x 1 root root 1343 Aug 20  2015 redis-live.py</span><br><span class="line">-rwxr-xr-x 1 root root 9800 Aug 20  2015 redis-monitor.py</span><br><span class="line">drwxr-xr-x 2 root root 4096 Aug 20  2015 util</span><br><span class="line">drwxr-xr-x 4 root root 4096 Aug 20  2015 www</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp redis-live.conf.example redis-live.conf</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# vi redis-live.conf</span><br><span class="line">&#123;</span><br><span class="line">        &quot;RedisServers&quot;:</span><br><span class="line">        [</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;server&quot;: &quot;172.16.101.66&quot;,</span><br><span class="line">                        &quot;port&quot; : 6379</span><br><span class="line">                &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;DataStoreType&quot; : &quot;redis&quot;,</span><br><span class="line">        &quot;RedisStatsServer&quot;:</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;server&quot; : &quot;172.16.101.66&quot;,</span><br><span class="line">          &quot;port&quot; : 6379</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-第一次尝试启动redis-monitor-py抛错-sqlite3"><a href="#6-第一次尝试启动redis-monitor-py抛错-sqlite3" class="headerlink" title="6.第一次尝试启动redis-monitor.py抛错 _sqlite3"></a>6.第一次尝试启动redis-monitor.py抛错 _sqlite3</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">ImportError: No module named _sqlite3</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite-devel</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# find / -name _sqlite3.so</span><br><span class="line">/usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so</span><br><span class="line">/usr/local/Python-2.7.5/build/lib.linux-x86_64-2.7/_sqlite3.so</span><br><span class="line">/usr/lib64/python2.6/lib-dynload/_sqlite3.so</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# cp /usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">[root@sht-sgmhadoopdn-04 ~]# python</span><br><span class="line">Python 2.7.5 (default, Sep 17 2016, 15:34:31) </span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import sqlite3</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">参考: http://ju.outofmemory.cn/entry/97658</span><br></pre></td></tr></table></figure><h4 id="7-第二次尝试启动redis-monitor-py抛错-redis"><a href="#7-第二次尝试启动redis-monitor-py抛错-redis" class="headerlink" title="7.第二次尝试启动redis-monitor.py抛错 redis"></a>7.第二次尝试启动redis-monitor.py抛错 redis</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">ImportError: No module named redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name redis</span><br><span class="line">/etc/rc.d/init.d/redis</span><br><span class="line">/root/learnproject/app/redis</span><br><span class="line">/root/learnproject/app/redis-monitor/src/main/java/sun/redis</span><br><span class="line">/root/learnproject/app/redis-monitor/src/test/java/sun/redis</span><br><span class="line">/usr/local/redis</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r  /usr/local/python27/lib/python2.7/site-packages/redis  /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# python </span><br><span class="line">Python 2.7.5 (default, Sep 17 2016, 15:34:31) </span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import redis</span><br></pre></td></tr></table></figure><h4 id="8-第三次尝试启动redis-monitor-py，成功；按ctrl-c中断掉"><a href="#8-第三次尝试启动redis-monitor-py，成功；按ctrl-c中断掉" class="headerlink" title="8.第三次尝试启动redis-monitor.py，成功；按ctrl+c中断掉"></a>8.第三次尝试启动redis-monitor.py，成功；按ctrl+c中断掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 </span><br><span class="line">^Cshutting down...</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]#</span><br></pre></td></tr></table></figure><h4 id="9-尝试第一次启动redis-live-py-，tornado-ioloop"><a href="#9-尝试第一次启动redis-live-py-，tornado-ioloop" class="headerlink" title="9.尝试第一次启动redis-live.py ，tornado.ioloop"></a>9.尝试第一次启动redis-live.py ，tornado.ioloop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 3, in &lt;module&gt;</span><br><span class="line">    import tornado.ioloop</span><br><span class="line">ImportError: No module named tornado.ioloop</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name  tornado</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/tornado  /usr/local/lib/python2.7/lib-dynload/</span><br></pre></td></tr></table></figure><h4 id="10-尝试第二次启动redis-live-py-，singledispatch"><a href="#10-尝试第二次启动redis-live-py-，singledispatch" class="headerlink" title="10.尝试第二次启动redis-live.py ，singledispatch"></a>10.尝试第二次启动redis-live.py ，singledispatch</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    import tornado.web</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/web.py&quot;, line 84, in &lt;module&gt;</span><br><span class="line">    from tornado import gen</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/gen.py&quot;, line 98, in &lt;module&gt;</span><br><span class="line">    from singledispatch import singledispatch  # backport</span><br><span class="line">ImportError: No module named singledispatch</span><br></pre></td></tr></table></figure><p>这个 singledispatch 错误，其实就是在tornado里的，谷歌和思考过后，怀疑是版本问题，于是果断卸载tornado<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# rm -rf  /usr/local/lib/python2.7/lib-dynload/tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name tornado</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">假如find有的话 ，就要手工删除掉</span><br></pre></td></tr></table></figure><p></p><h4 id="11-于是想想其他也是要卸载掉"><a href="#11-于是想想其他也是要卸载掉" class="headerlink" title="11.于是想想其他也是要卸载掉"></a>11.于是想想其他也是要卸载掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall argparse</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip uninstall python-dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name argparse</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name python-dateutil</span><br><span class="line">假如find有的话 ，就要手工删除掉</span><br></pre></td></tr></table></figure><h4 id="12-关键一步-根据step3的指定版本来安装"><a href="#12-关键一步-根据step3的指定版本来安装" class="headerlink" title="12.关键一步: 根据step3的指定版本来安装"></a>12.关键一步: 根据step3的指定版本来安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v tornado==2.1.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v argparse==1.2.1</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# pip install -v python-dateutil==1.5</span><br></pre></td></tr></table></figure><h4 id="13-再次尝试启动redis-live-py-，抛错dateutil-parser"><a href="#13-再次尝试启动redis-live-py-，抛错dateutil-parser" class="headerlink" title="13.再次尝试启动redis-live.py ，抛错dateutil.parser"></a>13.再次尝试启动redis-live.py ，抛错dateutil.parser</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 10, in &lt;module&gt;</span><br><span class="line">    from api.controller.ServerListController import ServerListController</span><br><span class="line">  File &quot;/root/learnproject/app/RedisLive/src/api/controller/ServerListController.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from BaseController import BaseController</span><br><span class="line">  File &quot;/root/learnproject/app/RedisLive/src/api/controller/BaseController.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    import dateutil.parser</span><br><span class="line">ImportError: No module named dateutil.parser</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# </span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# find / -name dateutil</span><br><span class="line">/usr/local/python27/lib/python2.7/site-packages/dateutil</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/dateutil  /usr/local/lib/python2.7/lib-dynload/</span><br><span class="line">You have mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure><h4 id="14-再在尝试启动redis-live-py-，成功了，然后按ctrl-c中断掉"><a href="#14-再在尝试启动redis-live-py-，成功了，然后按ctrl-c中断掉" class="headerlink" title="14.再在尝试启动redis-live.py ，成功了，然后按ctrl+c中断掉"></a>14.再在尝试启动redis-live.py ，成功了，然后按ctrl+c中断掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py </span><br><span class="line">^CTraceback (most recent call last):</span><br><span class="line">  File &quot;./redis-live.py&quot;, line 36, in &lt;module&gt;</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br><span class="line">  File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/ioloop.py&quot;, line 283, in start</span><br><span class="line">    event_pairs = self._impl.poll(poll_timeout)</span><br><span class="line">KeyboardInterrupt</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]#</span><br></pre></td></tr></table></figure><h4 id="15-启动"><a href="#15-启动" class="headerlink" title="15.启动"></a>15.启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-04 src]# ./redis-live.py  &amp;</span><br></pre></td></tr></table></figure><p>打开web界面</p><p><a href="http://172.16.101.66:8888/index.html" target="_blank" rel="noopener">http://172.16.101.66:8888/index.html</a></p><p><img src="/assets/blogImg/914_1.png" alt="enter description here"></p><h4 id="16-总结"><a href="#16-总结" class="headerlink" title="16.总结"></a>16.总结</h4><p><strong>a.安装 python2.7+pip</strong></p><p><strong>b.pip指定版本去安装那几个组件</strong></p><h4 id="17-说明"><a href="#17-说明" class="headerlink" title="17.说明:"></a>17.说明:</h4><p><strong>redis live 实时redis监控面板</strong></p><p>可以同时监控多个redis实例 , 包括 内存使用 、分db显示的key数、客户端连接数、 命令处理数、 系统运行时间 , 以及各种直观的折线图柱状图.<br>缺点是使用了monitor 命令监控 , 对性能有影响 ,最好不要长时间启动 .</p><p><strong>redis-monitor.py:</strong></p><p>用来调用redis的monitor命令来收集redis的命令来进行统计</p><p><strong>redis-live.py:</strong></p><p>启动web服务</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11生产预警平台项目之redis-3.2.5 install(单节点)</title>
      <link href="/2018/09/12/11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)/"/>
      <url>/2018/09/12/11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="1-安装jdk1-8"><a href="#1-安装jdk1-8" class="headerlink" title="1.安装jdk1.8"></a>1.安装jdk1.8</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 ~]# cd /usr/java/</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot;  http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# tar -zxvf jdk-8u111-linux-x64.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_111</span><br><span class="line">export path=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]# java -version</span><br><span class="line">java version &quot;1.8.0_111&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_111-b14)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)</span><br><span class="line">[root@sht-sgmhadoopdn-04 java]#</span><br></pre></td></tr></table></figure><h4 id="2-安装-redis-3-2-5"><a href="#2-安装-redis-3-2-5" class="headerlink" title="2.安装 redis 3.2.5"></a>2.安装 redis 3.2.5</h4><h5 id="2-1-安装编绎所需包gcc-tcl"><a href="#2-1-安装编绎所需包gcc-tcl" class="headerlink" title="2.1 安装编绎所需包gcc,tcl"></a>2.1 安装编绎所需包gcc,tcl</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# yum install gcc</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# yum install tcl</span><br></pre></td></tr></table></figure><h5 id="2-2-下载redis-3-2-5"><a href="#2-2-下载redis-3-2-5" class="headerlink" title="2.2 下载redis-3.2.5"></a>2.2 下载redis-3.2.5</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# wget http://download.redis.io/releases/redis-3.2.5.tar.gz</span><br><span class="line">--2016-11-12 20:16:40--  http://download.redis.io/releases/redis-3.2.5.tar.gz</span><br><span class="line">Resolving download.redis.io (download.redis.io)... 109.74.203.151</span><br><span class="line">Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1544040 (1.5M) [application/x-gzip]</span><br><span class="line">Saving to: ‘redis-3.2.5.tar.gz’</span><br><span class="line">100%[==========================================================================================================================&gt;] 1,544,040    221KB/s   in 6.8s  </span><br><span class="line">2016-11-12 20:16:47 (221 KB/s) - ‘redis-3.2.5.tar.gz’ saved [1544040/1544040]</span><br></pre></td></tr></table></figure><h5 id="2-3-安装redis"><a href="#2-3-安装redis" class="headerlink" title="2.3 安装redis"></a>2.3 安装redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 local]# mkdir /usr/local/redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# tar xzvf redis-3.2.5.tar.gz</span><br><span class="line">[root@sht-sgmhadoopdn-04 local]# cd redis-3.2.5</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# make PREFIX=/usr/local/redis install</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# cd ../</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis-3.2.5]# ll /usr/local/redis/bin/</span><br><span class="line">total 15056</span><br><span class="line">-rwxr-xr-x 1 root root 2431728 Nov 12 20:45 redis-benchmark</span><br><span class="line">-rwxr-xr-x 1 root root   25165 Nov 12 20:45 redis-check-aof</span><br><span class="line">-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-check-rdb</span><br><span class="line">-rwxr-xr-x 1 root root 2584443 Nov 12 20:45 redis-cli</span><br><span class="line">lrwxrwxrwx 1 root root      12 Nov 12 20:45 redis-sentinel -&gt; redis-server</span><br><span class="line">-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-server</span><br></pre></td></tr></table></figure><h5 id="2-4-配置redis为服务"><a href="#2-4-配置redis为服务" class="headerlink" title="2.4 配置redis为服务"></a>2.4 配置redis为服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@server redis-3.2.5]# cp utils/redis_init_script /etc/rc.d/init.d/redis</span><br><span class="line">[root@server redis-3.2.5]# vi /etc/rc.d/init.d/redis </span><br><span class="line">在第二行添加：#chkconfig: 2345 80 90</span><br><span class="line">EXEC=/usr/local/bin/redis-server  修改成 EXEC=/usr/local/redis/bin/redis-server</span><br><span class="line">CLIEXEC=/usr/local/bin/redis-cli  修改成 CLIEXEC=/usr/local/redis/bin/redis-cli</span><br><span class="line">CONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot; 修改成 CONF=&quot;/usr/local/redis/conf/$&#123;REDISPORT&#125;.conf&quot;</span><br><span class="line">$EXEC $CONF 修改成  $EXEC $CONF &amp;</span><br><span class="line">[root@server redis-3.2.5]# mkdir /usr/local/redis/conf/</span><br><span class="line">[root@server redis-3.2.5]# chkconfig --add redis</span><br><span class="line">[root@server redis-3.2.5]# cp redis.conf /usr/local/redis/conf/6379.conf </span><br><span class="line">[root@server redis-3.2.5]# vi /usr/local/redis/conf/6379.conf </span><br><span class="line">daemonize yes</span><br><span class="line">pidfile /var/run/redis_6379.pid</span><br><span class="line">bind 172.16.101.66</span><br></pre></td></tr></table></figure><h5 id="2-5-启动redis"><a href="#2-5-启动redis" class="headerlink" title="2.5 启动redis"></a>2.5 启动redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@server redis-3.2.5]# cd ../redis</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis start</span><br><span class="line">Starting Redis server...</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# netstat -tnlp|grep redis</span><br><span class="line">tcp        0      0 172.16.100.79:6379      0.0.0.0:*               LISTEN      30032/redis-server  </span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]#</span><br></pre></td></tr></table></figure><h5 id="2-6-添加环境变量"><a href="#2-6-添加环境变量" class="headerlink" title="2.6 添加环境变量"></a>2.6 添加环境变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-04 redis]# vi /etc/profile</span><br><span class="line">export REDIS_HOME=/usr/local/redis</span><br><span class="line">export PATH=$REDIS_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# which redis-cli</span><br><span class="line">/usr/local/redis/bin/redis-cli</span><br></pre></td></tr></table></figure><h5 id="2-7-测试-和-设置密码-本次实验未设置密码"><a href="#2-7-测试-和-设置密码-本次实验未设置密码" class="headerlink" title="2.7 测试 和 设置密码(本次实验未设置密码)"></a>2.7 测试 和 设置密码(本次实验未设置密码)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# red</span><br><span class="line">is-cli -h sht-sgmhadoopdn-04</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; </span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set testkey testvalue </span><br><span class="line">OK</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get test</span><br><span class="line">(nil)</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get testkey</span><br><span class="line">&quot;testvalue&quot;</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt;</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# vi /usr/local/redis/conf/6379.conf </span><br><span class="line">/*添加一个验证密码*/</span><br><span class="line">requirepass 123456</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis stop</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# service redis start</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set key ss</span><br><span class="line">(error) NOAUTH Authentication required.  </span><br><span class="line">[root@server redis-3.2.5]# redis-cli -h sht-sgmhadoopdn-04 -a 123456</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; set a b</span><br><span class="line">OK</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; get a</span><br><span class="line">&quot;b&quot;</span><br><span class="line">sht-sgmhadoopdn-04:6379&gt; exit;</span><br><span class="line">[root@sht-sgmhadoopdn-04 redis]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10生产预警平台项目之基于Spark Streaming开发OnLineLogAanlysis1</title>
      <link href="/2018/09/11/10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1/"/>
      <url>/2018/09/11/10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-GitHub"><a href="#1-GitHub" class="headerlink" title="1.GitHub"></a>1.GitHub</h4><p><a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java</a></p><h4 id="2-使用IDEA-本地运行测试（未打jar包）"><a href="#2-使用IDEA-本地运行测试（未打jar包）" class="headerlink" title="2.使用IDEA 本地运行测试（未打jar包）"></a>2.使用IDEA 本地运行测试（未打jar包）</h4><p><img src="/assets/blogImg/0911.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09生产预警平台项目之基于Spark Streaming Direct方式的WordCount最详细案例(java版)</title>
      <link href="/2018/09/10/09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)/"/>
      <url>/2018/09/10/09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-前提"><a href="#1-前提" class="headerlink" title="1.前提"></a>1.前提</h4><ul><li><p>a. flume 收集–》flume 聚合–》kafka ，启动进程和启动kafka manager监控，参考08【在线日志分析】之Flume Agent(聚合节点) sink to kafka cluster</p></li><li><p>b.window7 安装jdk1.7 或者1.8(本次环境是1.8)</p></li><li><p>c.window7 安装IDEA开发工具(以下仅供参考)<br>使用IntelliJ IDEA 配置Maven（入门）:<br><a href="http://blog.csdn.net/qq_32588349/article/details/51461182" target="_blank" rel="noopener">http://blog.csdn.net/qq_32588349/article/details/51461182</a><br>IDEA Java/Scala混合项目Maven打包:<br><a href="http://blog.csdn.net/rongyongfeikai2/article/details/51404611" target="_blank" rel="noopener">http://blog.csdn.net/rongyongfeikai2/article/details/51404611</a><br>Intellij idea使用java编写并执行spark程序:<br><a href="http://blog.csdn.net/yhao2014/article/details/44239021" target="_blank" rel="noopener">http://blog.csdn.net/yhao2014/article/details/44239021</a></p></li></ul><h4 id="2-源代码"><a href="#2-源代码" class="headerlink" title="2.源代码"></a>2.源代码</h4><p>（可下载单个java文件，加入projet 或者 整个工程下载，IDEA选择open 即可）<br>GitHub: <a href="https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java" target="_blank" rel="noopener">https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java</a></p><h4 id="3-使用IDEA-本地运行测试（未打jar包）"><a href="#3-使用IDEA-本地运行测试（未打jar包）" class="headerlink" title="3.使用IDEA 本地运行测试（未打jar包）"></a>3.使用IDEA 本地运行测试（未打jar包）</h4><p><img src="/assets/blogImg/0910.png" alt="enter description here"></p><h4 id="海康威视校招电话面试："><a href="#海康威视校招电话面试：" class="headerlink" title="海康威视校招电话面试："></a>海康威视校招电话面试：</h4><p>1.数据倾斜的解决，怎么知道哪里倾斜<br>2.自定义类的广播<br>3.cache机制，rdd和df的cache什么区别<br>4.spark动态内存，堆内和堆外<br>5.rdd算子，map,mappartitions,foreach，union<br>6.宽依赖，窄依赖<br>7.spark DAG过程，doOnrecive，eventloop执行过程<br>8.stage和task怎么分类<br>9.spark调优<br>10.概念，executor，worker，job,task和partition的关系<br>11.用没用过spark什么log，没记住<br>12.讲讲sparkSQL数据清洗过程<br>13.捎带一点项目</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08生产预警平台项目之Flume Agent(聚合节点) sink to kafka cluster</title>
      <link href="/2018/09/07/08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster/"/>
      <url>/2018/09/07/08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="1-创建logtopic"><a href="#1-创建logtopic" class="headerlink" title="1.创建logtopic"></a>1.创建logtopic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic logtopic</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="2-创建avro-memory-kafka-properties-kafka-sink"><a href="#2-创建avro-memory-kafka-properties-kafka-sink" class="headerlink" title="2.创建avro_memory_kafka.properties (kafka sink)"></a>2.创建avro_memory_kafka.properties (kafka sink)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/conf</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# cp avro_memory_hdfs.properties avro_memory_kafka.properties</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_kafka.properties </span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 172.16.101.54</span><br><span class="line">a1.sources.r1.port = 4545</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = logtopic</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 6000</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1</span><br><span class="line">a1.sinks.ki.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90</span><br><span class="line">a1.channels.c1.capacity = 2000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h4 id="3-后台启动-flume-ng-agent-聚合节点-和查看nohup-out"><a href="#3-后台启动-flume-ng-agent-聚合节点-和查看nohup-out" class="headerlink" title="3.后台启动 flume-ng agent(聚合节点)和查看nohup.out"></a>3.后台启动 flume-ng agent(聚合节点)和查看nohup.out</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 ~]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# nohup  flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_kafka.properties  -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 4971</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# nohup: ignoring input and appending output to `nohup.out&apos;</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# </span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# </span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# cat nohup.out</span><br></pre></td></tr></table></figure><h4 id="4-检查log收集的三台-收集节点-开启没"><a href="#4-检查log收集的三台-收集节点-开启没" class="headerlink" title="4.检查log收集的三台(收集节点)开启没"></a>4.检查log收集的三台(收集节点)开启没</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br></pre></td></tr></table></figure><h4 id="5-打开kafka-manager监控"><a href="#5-打开kafka-manager监控" class="headerlink" title="5.打开kafka manager监控"></a>5.打开kafka manager监控</h4><p><a href="http://172.16.101.55:9999" target="_blank" rel="noopener">http://172.16.101.55:9999</a><br><img src="/assets/blogImg/0609_1.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07生产预警平台项目之kafka-manager监控工具的搭建(sbt安装与编译)</title>
      <link href="/2018/09/06/07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)/"/>
      <url>/2018/09/06/07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="1-下载sbt"><a href="#1-下载sbt" class="headerlink" title="1.下载sbt"></a>1.下载sbt</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">http://www.scala-sbt.org/download.html</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# rz</span><br><span class="line">rz waiting to receive.</span><br><span class="line">Starting zmodem transfer.  Press Ctrl+C to cancel.</span><br><span class="line">Transferring sbt-0.13.13.tgz...</span><br><span class="line">  100%    1025 KB    1025 KB/sec    00:00:01       0 Errors</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# tar -zxvf sbt-0.13.13.tgz</span><br><span class="line">sbt-launcher-packaging-0.13.13/</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/sbtconfig.txt</span><br><span class="line">sbt-launcher-packaging-0.13.13/conf/sbtopts</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt.bat</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt-launch.jar</span><br><span class="line">sbt-launcher-packaging-0.13.13/bin/sbt-launch-lib.bash</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# mv sbt-launcher-packaging-0.13.13 sbt</span><br></pre></td></tr></table></figure><h4 id="3-添加脚本文件"><a href="#3-添加脚本文件" class="headerlink" title="3.添加脚本文件"></a>3.添加脚本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 bin]# vi sbt</span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">BT_OPTS=&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;</span><br><span class="line">java $SBT_OPTS -jar /root/learnproject/app/sbt/bin/sbt-launch.jar &quot;$@&quot;</span><br></pre></td></tr></table></figure><h4 id="4-修改权限和环境变量"><a href="#4-修改权限和环境变量" class="headerlink" title="4.修改权限和环境变量"></a>4.修改权限和环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 bin]# chmod u+x sbt</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# vi /etc/profile</span><br><span class="line">export SBT_HOME=/root/learnproject/app/sbt</span><br><span class="line">export PATH=$SBT_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">&quot;/etc/profile&quot; 94L, 2265C written</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="5-测试"><a href="#5-测试" class="headerlink" title="5.测试"></a>5.测试</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/*第一次执行时，会下载一些文件包，然后才能正常使用，要确保联网了，安装成功后显示如下*/</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]# sbt sbt-version</span><br><span class="line">[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)</span><br><span class="line">[info] 0.13.13</span><br><span class="line">[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)</span><br><span class="line">[info] 0.13.13</span><br><span class="line">[root@sht-sgmhadoopnn-01 bin]#</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06生产预警平台项目之 KafkaOffsetMonitor监控工具的搭建</title>
      <link href="/2018/09/05/06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/09/05/06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h3 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h3><p>在window7 手工下载好下面的链接<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/quantifind/KafkaOffsetMonitor/releases/tag/v0.2.1</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# mkdir kafkaoffsetmonitor</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# cd kafkaoffsetmonitor</span><br><span class="line">#使用rz命令上传</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# rz</span><br><span class="line">rz waiting to receive.</span><br><span class="line">Starting zmodem transfer.  Press Ctrl+C to cancel.</span><br><span class="line">Transferring KafkaOffsetMonitor-assembly-0.2.1.jar...</span><br><span class="line">  100%   51696 KB    12924 KB/sec    00:00:04       0 Errors </span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="2-新建一个kafkaMonitor-sh文件，文件内容如下："><a href="#2-新建一个kafkaMonitor-sh文件，文件内容如下：" class="headerlink" title="2.新建一个kafkaMonitor.sh文件，文件内容如下："></a>2.新建一个kafkaMonitor.sh文件，文件内容如下：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# vi kafkaoffsetmonitor.sh</span><br><span class="line">! /bin/bash</span><br><span class="line">java -cp KafkaOffsetMonitor-assembly-0.2.1.jar \</span><br><span class="line">com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line">--zk 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka \</span><br><span class="line">--port 8089 \</span><br><span class="line">--refresh 5.seconds \</span><br><span class="line">--retain 7.days</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# chmod +x *.sh</span><br><span class="line">[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#</span><br></pre></td></tr></table></figure><p>参数说明：<br>–zk 这里写的地址和端口，是zookeeper集群的各个地址和端口。应和kafka/bin文件夹中的zookeeper.properties中的host.name和clientPort一致。<br>–port 这个是本软件KafkaOffsetMonitor的端口。注意不要使用那些著名的端口号，例如80,8080等。我采用了8089.<br>–refresh 这个是软件刷新间隔时间，不要太短也不要太长。<br>–retain 这个是数据在数据库中保存的时间。</p><h3 id="3-后台启动"><a href="#3-后台启动" class="headerlink" title="3.后台启动"></a>3.后台启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# nohup ./kafkaoffsetmonitor.sh &amp;</span><br><span class="line"> 2serving resources from: jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp</span><br><span class="line"> 3SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line"> 4SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line"> 5SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br><span class="line"> 6log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection).</span><br><span class="line"> 7log4j:WARN Please initialize the log4j system properly.</span><br><span class="line"> 8log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line"> 9log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).</span><br><span class="line">10log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">11log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">122016-12-25 22:00:24.252:INFO:oejs.Server:jetty-7.x.y-SNAPSHOT</span><br><span class="line">132016-12-25 22:00:24.319:INFO:oejsh.ContextHandler:started o.e.j.s.ServletContextHandler&#123;/,jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp&#125;</span><br><span class="line">142016-12-25 22:00:24.328:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:8089</span><br></pre></td></tr></table></figure><h3 id="4-IE浏览器打开"><a href="#4-IE浏览器打开" class="headerlink" title="4.IE浏览器打开"></a>4.IE浏览器打开</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://172.16.101.55:8089</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05生产预警平台项目之Kafka 0.10.1.0 Cluster的搭建和Topic简单操作实验</title>
      <link href="/2018/09/04/05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C/"/>
      <url>/2018/09/04/05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>【kafka cluster机器】:<br>机器名称 用户名称<br>sht-sgmhadoopdn-01/02/03 root<br>【安装目录】: /root/learnproject/app<br><a id="more"></a></p><h4 id="1-将scala文件夹同步到集群其他机器-scala-2-11版本，可单独下载解压"><a href="#1-将scala文件夹同步到集群其他机器-scala-2-11版本，可单独下载解压" class="headerlink" title="1.将scala文件夹同步到集群其他机器(scala 2.11版本，可单独下载解压)"></a>1.将scala文件夹同步到集群其他机器(scala 2.11版本，可单独下载解压)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-01:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-02:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line"></span><br><span class="line">#环境变量</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# vi /etc/profile</span><br><span class="line">export SCALA_HOME=/root/learnproject/app/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-02 app]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-03 app]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="2-下载基于Scala-2-11的kafka版本为0-10-1-0"><a href="#2-下载基于Scala-2-11的kafka版本为0-10-1-0" class="headerlink" title="2.下载基于Scala 2.11的kafka版本为0.10.1.0"></a>2.下载基于Scala 2.11的kafka版本为0.10.1.0</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# pwd</span><br><span class="line">/root/learnproject/app</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# wget http://www-eu.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# tar xzvf kafka_2.11-0.10.1.0.tgz </span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# mv kafka_2.11-0.10.1.0 kafka</span><br></pre></td></tr></table></figure><h4 id="3-创建logs目录和修改server-properties-前提zookeeper-cluster部署好，见“03【在线日志分析】之hadoop-2-7-3编译和搭建集群环境-HDFS-HA-Yarn-HA-”"><a href="#3-创建logs目录和修改server-properties-前提zookeeper-cluster部署好，见“03【在线日志分析】之hadoop-2-7-3编译和搭建集群环境-HDFS-HA-Yarn-HA-”" class="headerlink" title="3.创建logs目录和修改server.properties(前提zookeeper cluster部署好，见“03【在线日志分析】之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)” )"></a>3.创建logs目录和修改server.properties(前提zookeeper cluster部署好，见“03【在线日志分析】之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)” )</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# cd kafka</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# mkdir logs</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# cd config/</span><br><span class="line">[root@sht-sgmhadoopdn-01 config]# vi server.properties</span><br><span class="line">broker.id=1</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.58</span><br><span class="line">log.dirs=/root/learnproject/app/kafka/logs</span><br><span class="line">zookeeper.connect=172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka</span><br></pre></td></tr></table></figure><h4 id="4-同步到02-03服务器，更改broker-id-及host-name"><a href="#4-同步到02-03服务器，更改broker-id-及host-name" class="headerlink" title="4.同步到02/03服务器，更改broker.id 及host.name"></a>4.同步到02/03服务器，更改broker.id 及host.name</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line">[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-02 config]# vi server.properties </span><br><span class="line">broker.id=2</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.59</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-03 config]# vi server.properties </span><br><span class="line">broker.id=3</span><br><span class="line">port=9092</span><br><span class="line">host.name=172.16.101.60</span><br></pre></td></tr></table></figure><h4 id="5-环境变量"><a href="#5-环境变量" class="headerlink" title="5.环境变量"></a>5.环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# vi /etc/profile</span><br><span class="line">export KAFKA_HOME=/root/learnproject/app/kafka</span><br><span class="line">export PATH=$KAFKA_HOME/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-02:/etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-03:/etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]#</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopdn-03 kafka]# source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="6-启动-停止"><a href="#6-启动-停止" class="headerlink" title="6.启动/停止"></a>6.启动/停止</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopdn-01 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[root@sht-sgmhadoopdn-03 kafka]# nohup kafka-server-start.sh config/server.properties &amp;</span><br><span class="line"></span><br><span class="line">###停止</span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h4 id="7-topic相关的操作"><a href="#7-topic相关的操作" class="headerlink" title="7.topic相关的操作"></a>7.topic相关的操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">a.创建topic，如能成功创建topic则表示集群安装完成，也可以用jps命令查看kafka进程是否存在。</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic test</span><br><span class="line"></span><br><span class="line">b.通过list命令查看创建的topic:</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --list --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka</span><br><span class="line"></span><br><span class="line">c.查看创建的Topic</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:1        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# </span><br><span class="line">第一行列出了这个topic的总体情况，如topic名称，分区数量，副本数量等。</span><br><span class="line">第二行开始，每一行列出了一个分区的信息，如它是第几个分区，这个分区的leader是哪个broker，副本位于哪些broker，有哪些副本处理同步状态。</span><br><span class="line"></span><br><span class="line">Partition： 分区</span><br><span class="line">Leader ：   负责读写指定分区的节点</span><br><span class="line">Replicas ： 复制该分区log的节点列表</span><br><span class="line">Isr ：      “in-sync” replicas，当前活跃的副本列表（是一个子集），并且可能成为Leader</span><br><span class="line">我们可以通过Kafka自带的bin/kafka-console-producer.sh和bin/kafka-console-consumer.sh脚本，来验证演示如果发布消息、消费消息。</span><br><span class="line"></span><br><span class="line">d.删除topic</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh  --delete --zookeeper  172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka  --topic test</span><br><span class="line"></span><br><span class="line">e.修改topic</span><br><span class="line">使用—-alert原则上可以修改任何配置，以下列出了一些常用的修改选项：</span><br><span class="line">（1）改变分区数量</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]#bin/kafka-topics.sh --alter  --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test --partitions 3</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]#</span><br><span class="line"></span><br><span class="line">（2）增加、修改或者删除一个配置参数</span><br><span class="line"> bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka  --topic my_topic_name --config key=value</span><br><span class="line"> bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka  --topic my_topic_name --deleteConfig key</span><br></pre></td></tr></table></figure><h4 id="8-模拟实验1"><a href="#8-模拟实验1" class="headerlink" title="8.模拟实验1"></a>8.模拟实验1</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在一个终端，启动Producer，并向我们上面创建的名称为my-replicated-topic5的Topic中生产消息，执行如下脚本：</span><br><span class="line">[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-console-producer.sh --broker-list 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092 --topic test</span><br><span class="line"></span><br><span class="line">在另一个终端，启动Consumer，并订阅我们上面创建的名称为my-replicated-topic5的Topic中生产的消息，执行如下脚本：</span><br><span class="line">[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --from-beginning --topic test</span><br><span class="line"></span><br><span class="line">可以在Producer终端上输入字符串消息行，就可以在Consumer终端上看到消费者消费的消息内容。</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03生产预警平台项目之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)</title>
      <link href="/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/"/>
      <url>/2018/09/03/03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA,Yarn%20HA)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="1-下载hadoop2-7-3最新源码"><a href="#1-下载hadoop2-7-3最新源码" class="headerlink" title="1.下载hadoop2.7.3最新源码"></a>1.下载hadoop2.7.3最新源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt </span><br><span class="line">Build instructions for Hadoop</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Requirements:</span><br><span class="line">* Unix System</span><br><span class="line">* JDK 1.7+</span><br><span class="line">* Maven 3.0 or later</span><br><span class="line">* Findbugs 1.3.9 (if running findbugs)</span><br><span class="line">* ProtocolBuffer 2.5.0</span><br><span class="line">* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac</span><br><span class="line">* Zlib devel (if compiling native code)</span><br><span class="line">* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )</span><br><span class="line">* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )</span><br><span class="line">* Internet connection for first build (to fetch all Maven and Hadoop dependencies)</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:</span><br><span class="line">* Oracle JDK 1.7 (preferred)</span><br><span class="line">  $ sudo apt-get purge openjdk*</span><br><span class="line">  $ sudo apt-get install software-properties-common</span><br><span class="line">  $ sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">  $ sudo apt-get update</span><br><span class="line">  $ sudo apt-get install oracle-java7-installer</span><br><span class="line">* Maven</span><br><span class="line">  $ sudo apt-get -y install maven</span><br><span class="line">* Native libraries</span><br><span class="line">  $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev</span><br><span class="line">* ProtocolBuffer 2.5.0 (required)</span><br><span class="line">  $ sudo apt-get -y install libprotobuf-dev protobuf-compiler</span><br><span class="line">Optional packages:</span><br><span class="line">* Snappy compression</span><br><span class="line">  $ sudo apt-get install snappy libsnappy-dev</span><br><span class="line">* Bzip2</span><br><span class="line">  $ sudo apt-get install bzip2 libbz2-dev</span><br><span class="line">* Jansson (C Library for JSON)</span><br><span class="line">  $ sudo apt-get install libjansson-dev</span><br><span class="line">* Linux FUSE</span><br><span class="line">  $ sudo apt-get install fuse libfuse-dev</span><br></pre></td></tr></table></figure><h3 id="2-安装依赖包"><a href="#2-安装依赖包" class="headerlink" title="2.安装依赖包"></a>2.安装依赖包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*</span><br></pre></td></tr></table></figure><h3 id="3-安装jdk"><a href="#3-安装jdk" class="headerlink" title="3.安装jdk"></a>3.安装jdk</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# java -version</span><br><span class="line">java version &quot;1.7.0_67&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_67-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#</span><br></pre></td></tr></table></figure><h3 id="4-安装maven"><a href="#4-安装maven" class="headerlink" title="4.安装maven"></a>4.安装maven</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">#在编译过程中为了防止Java内存溢出，需要加入以下环境变量</span><br><span class="line">export MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;</span><br><span class="line">export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# mvn -version</span><br><span class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</span><br><span class="line">Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9</span><br><span class="line">Java version: 1.7.0_67, vendor: Oracle Corporation</span><br><span class="line">Java home: /usr/java/jdk1.7.0_67-cloudera/jre</span><br><span class="line">Default locale: en_US, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#</span><br></pre></td></tr></table></figure><h3 id="5-编译安装protobuf"><a href="#5-编译安装protobuf" class="headerlink" title="5.编译安装protobuf"></a>5.编译安装protobuf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# make install</span><br><span class="line">#查看protobuf版本以测试是否安装成功</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]# protoc --version</span><br><span class="line">libprotoc 2.5.0</span><br><span class="line">[root@hadoop-01 protobuf-2.5.0]#</span><br></pre></td></tr></table></figure><h3 id="6-安装snappy"><a href="#6-安装snappy" class="headerlink" title="6.安装snappy"></a>6.安装snappy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz</span><br><span class="line">#用root用户执行以下命令</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install</span><br><span class="line">#查看snappy库文件</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy</span><br><span class="line">-rw-r--r--  1 root root 229K Jun 21 15:46 libsnappy.a</span><br><span class="line">-rwxr-xr-x  1 root root  953 Jun 21 15:46 libsnappy.la</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0</span><br><span class="line">lrwxrwxrwx  1 root root   18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0</span><br><span class="line">-rwxr-xr-x  1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0</span><br><span class="line">[root@sht-sgmhadoopnn-01 snappy-1.1.1]#</span><br></pre></td></tr></table></figure><h3 id="7-编译"><a href="#7-编译" class="headerlink" title="7.编译"></a>7.编译</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src</span><br><span class="line">mvn clean package -Pdist,native -DskipTests -Dtar</span><br><span class="line">或</span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package –Pdist,native –DskipTests –Dtar</span><br><span class="line">[INFO] Executing tasks</span><br><span class="line">main:</span><br><span class="line">     [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3</span><br><span class="line">     [exec] $ gzip -f hadoop-2.7.3.tar</span><br><span class="line">     [exec] </span><br><span class="line">     [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz</span><br><span class="line">     [exec] </span><br><span class="line">[INFO] Executed tasks</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---</span><br><span class="line">[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s]</span><br><span class="line">[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  6.832 s]</span><br><span class="line">[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s]</span><br><span class="line">[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s]</span><br><span class="line">[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.411 s]</span><br><span class="line">[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  4.814 s]</span><br><span class="line">[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s]</span><br><span class="line">[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min]</span><br><span class="line">[INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s]</span><br><span class="line">[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s]</span><br><span class="line">[INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min]</span><br><span class="line">[INFO] Apache Hadoop NFS .................................. SUCCESS [  9.285 s]</span><br><span class="line">[INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s]</span><br><span class="line">[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.049 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min]</span><br><span class="line">[INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  4.599 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.044 s]</span><br><span class="line">[INFO] hadoop-yarn ........................................ SUCCESS [  0.043 s]</span><br><span class="line">[INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min]</span><br><span class="line">[INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s]</span><br><span class="line">[INFO] hadoop-yarn-server ................................. SUCCESS [  0.041 s]</span><br><span class="line">[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s]</span><br><span class="line">[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s]</span><br><span class="line">[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  6.415 s]</span><br><span class="line">[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s]</span><br><span class="line">[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s]</span><br><span class="line">[INFO] hadoop-yarn-server-tests ........................... SUCCESS [  7.751 s]</span><br><span class="line">[INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s]</span><br><span class="line">[INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [  5.612 s]</span><br><span class="line">[INFO] hadoop-yarn-applications ........................... SUCCESS [  0.038 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  4.029 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  2.611 s]</span><br><span class="line">[INFO] hadoop-yarn-site ................................... SUCCESS [  0.077 s]</span><br><span class="line">[INFO] hadoop-yarn-registry ............................... SUCCESS [  8.045 s]</span><br><span class="line">[INFO] hadoop-yarn-project ................................ SUCCESS [  5.456 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.226 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  6.697 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  9.328 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  2.412 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  8.876 s]</span><br><span class="line">[INFO] hadoop-mapreduce ................................... SUCCESS [  4.237 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s]</span><br><span class="line">[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s]</span><br><span class="line">[INFO] Apache Hadoop Archives ............................. SUCCESS [  3.069 s]</span><br><span class="line">[INFO] Apache Hadoop Rumen ................................ SUCCESS [  7.446 s]</span><br><span class="line">[INFO] Apache Hadoop Gridmix .............................. SUCCESS [  5.765 s]</span><br><span class="line">[INFO] Apache Hadoop Data Join ............................ SUCCESS [  3.752 s]</span><br><span class="line">[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  2.771 s]</span><br><span class="line">[INFO] Apache Hadoop Extras ............................... SUCCESS [  5.612 s]</span><br><span class="line">[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s]</span><br><span class="line">[INFO] Apache Hadoop OpenStack support .................... SUCCESS [  7.131 s]</span><br><span class="line">[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min]</span><br><span class="line">[INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s]</span><br><span class="line">[INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s]</span><br><span class="line">[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.142 s]</span><br><span class="line">[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  7.354 s]</span><br><span class="line">[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s]</span><br><span class="line">[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.035 s]</span><br><span class="line">[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 26:29 min</span><br><span class="line">[INFO] Finished at: 2016-12-24T21:07:09+08:00</span><br><span class="line">[INFO] Final Memory: 214M/740M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# </span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]# ll</span><br><span class="line">total 193152</span><br><span class="line">drwxr-xr-x 5 root root      4096 Dec 24 20:24 compilesoft</span><br><span class="line">-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz</span><br><span class="line">[root@sht-sgmhadoopnn-01 learnproject]#</span><br></pre></td></tr></table></figure><h3 id="8-搭建HDFS-HA-YARN-HA集群（5个节点）"><a href="#8-搭建HDFS-HA-YARN-HA集群（5个节点）" class="headerlink" title="8.搭建HDFS HA,YARN HA集群（5个节点）"></a>8.搭建HDFS HA,YARN HA集群（5个节点）</h3><p>参考:<br><a href="http://blog.itpub.net/30089851/viewspace-1994585/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-1994585/</a><br><a href="https://github.com/Hackeruncle/Hadoop" target="_blank" rel="noopener">https://github.com/Hackeruncle/Hadoop</a></p><h3 id="9-搭建集群-验证版本和支持的压缩信息"><a href="#9-搭建集群-验证版本和支持的压缩信息" class="headerlink" title="9.搭建集群,验证版本和支持的压缩信息"></a>9.搭建集群,验证版本和支持的压缩信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop version</span><br><span class="line">Hadoop 2.7.3</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2016-12-24T12:45Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4</span><br><span class="line">This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# hadoop checknative</span><br><span class="line">16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">zlib:    true /lib64/libz.so.1</span><br><span class="line">snappy:  true /usr/local/lib/libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   true /lib64/libbz2.so.1</span><br><span class="line">openssl: true /usr/lib64/libcrypto.so</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0</span><br><span class="line">/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped</span><br><span class="line">[root@sht-sgmhadoopnn-01 app]#</span><br></pre></td></tr></table></figure><p>[参考]</p><ul><li><a href="http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html" target="_blank" rel="noopener">http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.html</a></li><li><a href="http://blog.csdn.net/haohaixingyun/article/details/52800048" target="_blank" rel="noopener">http://blog.csdn.net/haohaixingyun/article/details/52800048</a></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04生产预警平台项目之Flume Agent的3台收集+1台聚合到hdfs的搭建</title>
      <link href="/2018/09/03/04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86+1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/09/03/04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86+1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><p>【log收集】:<br>机器名称 服务名称 用户<br>flume-agent-01: namenode hdfs<br>flume-agent-02: datanode hdfs<br>flume-agent-03: datanode hdfs</p><p>【log聚合】:<br>机器名称 用户<br>sht-sgmhadoopcm-01(172.16.101.54) root</p><p>【sink到hdfs】:<br>hdfs://172.16.101.56:8020/testwjp/</p><a id="more"></a><h5 id="1-下载apache-flume-1-7-0-bin-tar-gz"><a href="#1-下载apache-flume-1-7-0-bin-tar-gz" class="headerlink" title="1.下载apache-flume-1.7.0-bin.tar.gz"></a>1.下载apache-flume-1.7.0-bin.tar.gz</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ wget http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz</span><br><span class="line">--2017-01-04 20:40:10--  http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz</span><br><span class="line">Resolving www-eu.apache.org... 88.198.26.2, 2a01:4f8:130:2192::2</span><br><span class="line">Connecting to www-eu.apache.org|88.198.26.2|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 55711670 (53M) [application/x-gzip]</span><br><span class="line">Saving to: “apache-flume-1.7.0-bin.tar.gz”</span><br><span class="line"></span><br><span class="line">100%[===============================================================================================================================================================================================&gt;] 55,711,670   473K/s   in 74s    </span><br><span class="line"></span><br><span class="line">2017-01-04 20:41:25 (733 KB/s) - “apache-flume-1.7.0-bin.tar.gz” saved [55711670/55711670]</span><br></pre></td></tr></table></figure><h5 id="2-解压重命名"><a href="#2-解压重命名" class="headerlink" title="2.解压重命名"></a>2.解压重命名</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ </span><br><span class="line">[hdfs@flume-agent-01 tmp]$ tar -xzvf apache-flume-1.7.0-bin.tar.gz </span><br><span class="line">[hdfs@flume-agent-01 tmp]$ mv apache-flume-1.7.0-bin flume-ng</span><br><span class="line">[hdfs@flume-agent-01 tmp]$ cd flume-ng/conf</span><br></pre></td></tr></table></figure><h5 id="3-拷贝flume环境配置和agent配置文件"><a href="#3-拷贝flume环境配置和agent配置文件" class="headerlink" title="3.拷贝flume环境配置和agent配置文件"></a>3.拷贝flume环境配置和agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ cp flume-env.sh.template flume-env.sh</span><br><span class="line">[hdfs@flume-agent-01 tmp]$ cp flume-conf.properties.template exec_memory_avro.properties</span><br></pre></td></tr></table></figure><h5 id="4-添加hdfs用户的环境变量文件"><a href="#4-添加hdfs用户的环境变量文件" class="headerlink" title="4.添加hdfs用户的环境变量文件"></a>4.添加hdfs用户的环境变量文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ cd</span><br><span class="line">[hdfs@flume-agent-01 ~]$ ls -la</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x   3 hdfs hadoop 4096 Jul  8 14:05 .</span><br><span class="line">drwxr-xr-x. 35 root root   4096 Dec 10  2015 ..</span><br><span class="line">-rw-------   1 hdfs hdfs   4471 Jul  8 17:22 .bash_history</span><br><span class="line">drwxrwxrwt   2 hdfs hadoop 4096 Nov 19  2014 cache</span><br><span class="line">-rw-------   1 hdfs hdfs   3131 Jul  8 14:05 .viminfo</span><br><span class="line">[hdfs@flume-agent-01 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-01 ~]$ ls -la</span><br><span class="line">total 36</span><br><span class="line">drwxr-xr-x   3 hdfs hadoop 4096 Jan  4 20:49 .</span><br><span class="line">drwxr-xr-x. 35 root root   4096 Dec 10  2015 ..</span><br><span class="line">-rw-------   1 hdfs hdfs   4471 Jul  8 17:22 .bash_history</span><br><span class="line">-rw-r--r--   1 hdfs hdfs     18 Jan  4 20:49 .bash_logout</span><br><span class="line">-rw-r--r--   1 hdfs hdfs    176 Jan  4 20:49 .bash_profile</span><br><span class="line">-rw-r--r--   1 hdfs hdfs    124 Jan  4 20:49 .bashrc</span><br><span class="line">drwxrwxrwt   2 hdfs hadoop 4096 Nov 19  2014 cache</span><br><span class="line">-rw-------   1 hdfs hdfs   3131 Jul  8 14:05 .viminfo</span><br></pre></td></tr></table></figure><h5 id="5-添加flume的环境变量"><a href="#5-添加flume的环境变量" class="headerlink" title="5.添加flume的环境变量"></a>5.添加flume的环境变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 ~]$ vi .bash_profile</span><br><span class="line"></span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-01 ~]$ . .bash_profile</span><br></pre></td></tr></table></figure><h5 id="6-修改flume环境配置文件"><a href="#6-修改flume环境配置文件" class="headerlink" title="6.修改flume环境配置文件"></a>6.修改flume环境配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_25</span><br></pre></td></tr></table></figure><h5 id="7-将基于Flume-ng-Exec-Source开发自定义插件AdvancedExecSource的AdvancedExecSource-jar包上传到-FLUME-HOME-lib"><a href="#7-将基于Flume-ng-Exec-Source开发自定义插件AdvancedExecSource的AdvancedExecSource-jar包上传到-FLUME-HOME-lib" class="headerlink" title="7.将基于Flume-ng Exec Source开发自定义插件AdvancedExecSource的AdvancedExecSource.jar包上传到$FLUME_HOME/lib/"></a>7.将基于Flume-ng Exec Source开发自定义插件AdvancedExecSource的AdvancedExecSource.jar包上传到$FLUME_HOME/lib/</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://blog.itpub.net/30089851/viewspace-2131995/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$ pwd</span><br><span class="line">/tmp/flume-ng/lib</span><br><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$ ll AdvancedExecSource.jar </span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 10618 Jan  5 23:50 AdvancedExecSource.jar</span><br><span class="line">[hdfs@LogshedNameNodeLogcollector lib]$</span><br></pre></td></tr></table></figure><h5 id="8-修改flume的agent配置文件"><a href="#8-修改flume的agent配置文件" class="headerlink" title="8.修改flume的agent配置文件"></a>8.修改flume的agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 conf]$ vi exec_memory_avro.properties</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#Describe/configure the custom exec source</span><br><span class="line">a1.sources.r1.type = com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-NAMENODE-flume-agent-01.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-01</span><br><span class="line">a1.sources.r1.servicename = namenode</span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = 172.16.101.54</span><br><span class="line">a1.sinks.k1.port = 4545</span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 60 </span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h5 id="9-将flume-agent-01的flume-ng打包-scp到flume-agent-02-03-和-sht-sgmhadoopcm-01-172-16-101-54"><a href="#9-将flume-agent-01的flume-ng打包-scp到flume-agent-02-03-和-sht-sgmhadoopcm-01-172-16-101-54" class="headerlink" title="9.将flume-agent-01的flume-ng打包,scp到flume-agent-02/03 和 sht-sgmhadoopcm-01(172.16.101.54)"></a>9.将flume-agent-01的flume-ng打包,scp到flume-agent-02/03 和 sht-sgmhadoopcm-01(172.16.101.54)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-01 tmp]$ zip -r flume-ng.zip flume-ng/*</span><br><span class="line"></span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-02:/tmp/</span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-03:/tmp/</span><br><span class="line">[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip sht-sgmhadoopcm-01:/tmp/</span><br></pre></td></tr></table></figure><h5 id="10-在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件"><a href="#10-在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件" class="headerlink" title="10.在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件"></a>10.在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-02 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-02 ~]$ vi .bash_profile</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-02 ~]$ . .bash_profile</span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-02 tmp]$ unzip flume-ng.zip</span><br><span class="line">[hdfs@flume-agent-02 tmp]$ cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">##修改以下参数即可</span><br><span class="line">[hdfs@flume-agent-02 conf]$ vi exec_memory_avro.properties</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-02.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-02</span><br><span class="line">a1.sources.r1.servicename = datanode</span><br><span class="line"></span><br><span class="line">###要检查flume-env.sh的JAVA_HOME目录是否存在</span><br></pre></td></tr></table></figure><h5 id="11-在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件"><a href="#11-在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件" class="headerlink" title="11.在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件"></a>11.在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@flume-agent-03 ~]$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory `/etc/skel/.&apos;</span><br><span class="line">cp: omitting directory `/etc/skel/..&apos;</span><br><span class="line">[hdfs@flume-agent-03 ~]$ vi .bash_profile</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[hdfs@flume-agent-03 ~]$ . .bash_profile</span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-03 tmp]$ unzip flume-ng.zip</span><br><span class="line">[hdfs@flume-agent-03 tmp]$ cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">##修改以下参数即可</span><br><span class="line">[hdfs@flume-agent-03 conf]$ vi exec_memory_avro.properties</span><br><span class="line">a1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-03.log.out</span><br><span class="line">a1.sources.r1.hostname = flume-agent-03</span><br><span class="line">a1.sources.r1.servicename = datanode</span><br><span class="line"></span><br><span class="line">###要检查flume-env.sh的JAVA_HOME目录是否存在</span><br></pre></td></tr></table></figure><h5 id="12-聚合端-sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件"><a href="#12-聚合端-sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件" class="headerlink" title="12.聚合端 sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件"></a>12.聚合端 sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 tmp]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line">export FLUME_HOME=/tmp/flume-ng</span><br><span class="line">export FLUME_CONF_DIR=$FLUME_HOME/conf</span><br><span class="line"></span><br><span class="line">export PATH=$FLUME_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# source /etc/profile</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]#</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# unzip flume-ng.zip</span><br><span class="line">[root@sht-sgmhadoopcm-01 tmp]# cd flume-ng/conf</span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">###测试: 先聚合, sink到hdfs端</span><br><span class="line">[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_hdfs.properties</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 172.16.101.54</span><br><span class="line">a1.sources.r1.port = 4545</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://172.16.101.56:8020/testwjp/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = logs</span><br><span class="line">a1.sinks.k1.hdfs.inUsePrefix = .</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">###roll 16 m = 16777216 bytes</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 1048576</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 6000</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = text</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90 </span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h5 id="13-后台启动"><a href="#13-后台启动" class="headerlink" title="13.后台启动"></a>13.后台启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@sht-sgmhadoopnn-01 flume-ng]# nohup  flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_hdfs.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ nohup  flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;</span><br></pre></td></tr></table></figure><h5 id="14-校验：将集群的日志下载到本地，打开查看即可-略"><a href="#14-校验：将集群的日志下载到本地，打开查看即可-略" class="headerlink" title="14.校验：将集群的日志下载到本地，打开查看即可(略)"></a>14.校验：将集群的日志下载到本地，打开查看即可(略)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">【备注】: </span><br><span class="line">1.错误1 flume-ng安装的机器上没有hadoop环境，所以假如sink到hdfs话，需要用到hdfs的jar包</span><br><span class="line">[ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:146)] Failed to start agent </span><br><span class="line">because dependencies were not found in classpath. Error follows.</span><br><span class="line">java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType</span><br><span class="line"></span><br><span class="line">只需在其他安装hadoop机器上搜索以下5个jar包，拷贝到$FLUME_HOME/lib目录即可。</span><br><span class="line">搜索方法: find $HADOOP_HOME/ -name commons-configuration*.jar</span><br><span class="line"></span><br><span class="line">commons-configuration-1.6.jar</span><br><span class="line">hadoop-auth-2.7.3.jar</span><br><span class="line">hadoop-common-2.7.3.jar</span><br><span class="line">hadoop-hdfs-2.7.3.jar</span><br><span class="line">hadoop-mapreduce-client-core-2.7.3.jar</span><br><span class="line">protobuf-java-2.5.0.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.错误2 无法加载自定义插件的类 Unable to load source type: com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line">2017-01-06 21:10:48,278 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:142)] Failed to load configuration data. Exception follows.</span><br><span class="line">org.apache.flume.FlumeException: Unable to load source type: com.onlinelog.analysis.AdvancedExecSource, class: com.onlinelog.analysis.AdvancedExecSource</span><br><span class="line"></span><br><span class="line">执行hdfs或者root用户的环境变量即可</span><br><span class="line">[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile</span><br><span class="line">[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile </span><br><span class="line">[hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02生产预警平台项目之Flume-1.7.0源码编译导入eclipse</title>
      <link href="/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/"/>
      <url>/2018/08/28/02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="【前提】"><a href="#【前提】" class="headerlink" title="【前提】:"></a>【前提】:</h3><p>1.windows 7 安装maven-3.3.9<br>其中在conf/setting.xml文件添加<br>D:\software\apache-maven-3.3.9\repository<br><a href="http://blog.csdn.net/defonds/article/details/41957287" target="_blank" rel="noopener">http://blog.csdn.net/defonds/article/details/41957287</a><br>2.windows 7 安装eclipse 64位(百度下载，解压即可)<br>3.eclipse安装maven插件，选择第二种方式link<br><a href="http://blog.csdn.net/lfsfxy9/article/details/9397937" target="_blank" rel="noopener">http://blog.csdn.net/lfsfxy9/article/details/9397937</a><br>其中 eclipse-maven3-plugin.7z 这个包可以加群258669058找我，分享给你</p><h3 id="【flume-ng-1-7-0源码的编译导入eclipse】"><a href="#【flume-ng-1-7-0源码的编译导入eclipse】" class="headerlink" title="【flume-ng 1.7.0源码的编译导入eclipse】:"></a>【flume-ng 1.7.0源码的编译导入eclipse】:</h3><h4 id="1-下载官网的源码-不要下载GitHub上源码，因为这时pom文件中版本为1-8-0，编译会有问题"><a href="#1-下载官网的源码-不要下载GitHub上源码，因为这时pom文件中版本为1-8-0，编译会有问题" class="headerlink" title="1.下载官网的源码(不要下载GitHub上源码，因为这时pom文件中版本为1.8.0，编译会有问题)"></a>1.下载官网的源码(不要下载GitHub上源码，因为这时pom文件中版本为1.8.0，编译会有问题)</h4><p><a href="http://archive.apache.org/dist/flume/1.7.0/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/1.7.0/</a><br>a.下载apache-flume-1.7.0-src.tar.gz<br>b.解压重命名为flume-1.7.0</p><h4 id="2-修改pom-xml-大概在621行，将自带的repository注释掉，添加以下的"><a href="#2-修改pom-xml-大概在621行，将自带的repository注释掉，添加以下的" class="headerlink" title="2.修改pom.xml (大概在621行，将自带的repository注释掉，添加以下的)"></a>2.修改pom.xml (大概在621行，将自带的repository注释掉，添加以下的)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;repository&gt;</span><br><span class="line">       &lt;id&gt;maven.tempo-db.com&lt;/id&gt;</span><br><span class="line">       &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt;</span><br><span class="line"> &lt;/repository&gt;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/0828_1.png" alt="enter description here"></p><h4 id="3-打开cmd-编译"><a href="#3-打开cmd-编译" class="headerlink" title="3.打开cmd,编译"></a>3.打开cmd,编译</h4><p>cd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0<br>mvn compile<br><img src="/assets/blogImg/0828_2.png" alt="enter description here"></p><h4 id="4-打开eclipse-单击Window–-gt-Perferences–-gt-左侧的Maven–-gt-User-Settings"><a href="#4-打开eclipse-单击Window–-gt-Perferences–-gt-左侧的Maven–-gt-User-Settings" class="headerlink" title="4.打开eclipse,单击Window–&gt;Perferences–&gt;左侧的Maven–&gt;User Settings"></a>4.打开eclipse,单击Window–&gt;Perferences–&gt;左侧的Maven–&gt;User Settings</h4><p>然后设置自己的mvn的setting.xml路径和Local Repository<br>(最好使用Maven3.3.x版本以上，我是3.3.9)<br><img src="/assets/blogImg/0828_3.png" alt="enter description here"></p><h4 id="5-关闭eclipse的-Project–-gt-Buid-Automatically"><a href="#5-关闭eclipse的-Project–-gt-Buid-Automatically" class="headerlink" title="5.关闭eclipse的 Project–&gt;Buid Automatically"></a>5.关闭eclipse的 Project–&gt;Buid Automatically</h4><p><img src="/assets/blogImg/0828_4.png" alt="enter description here"></p><h4 id="6-关闭eclipse的Download-repository-index-updates-on-startup"><a href="#6-关闭eclipse的Download-repository-index-updates-on-startup" class="headerlink" title="6.关闭eclipse的Download repository index updates on startup"></a>6.关闭eclipse的Download repository index updates on startup</h4><p><img src="/assets/blogImg/0828_5.png" alt="enter description here"></p><h4 id="7-导入flume1-7-0源码"><a href="#7-导入flume1-7-0源码" class="headerlink" title="7.导入flume1.7.0源码"></a>7.导入flume1.7.0源码</h4><p>a.File–&gt;Import–&gt;Maven–&gt;Existing Maven Projects–&gt;Next<br>b.选择目录–&gt; Finish</p><h4 id="8-检查源码，没有抛任何错误"><a href="#8-检查源码，没有抛任何错误" class="headerlink" title="8.检查源码，没有抛任何错误"></a>8.检查源码，没有抛任何错误</h4><p><img src="/assets/blogImg/0828_6.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是一篇热腾腾的面经</title>
      <link href="/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/"/>
      <url>/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>伟梦：<br>1.主要还是项目？<br>基本上没问什么技术，我就说了一遍项目流程，<br>然后说几个优化点，比如上次讲的血案，我也顺带提了一下。<br>2.在大数据中，有没有什么是不足的，遇到过什么问题？<br><a id="more"></a></p><p>微盟：<br>1.SparkStreaming处理完一批次的数据，写偏移量之前挂了，数据怎么保证不重？<br>2.Maxwell的底层原理？<br>3.手写Spring？<br>4.遍历二叉树？<br>5.用过什么算法？<br>6.多线程方面，怎么实现一个主线程，等待其他子线程完成后再运行？<br>7.Maxwell和Cannal的比较？<br>8.direct比较receiver的优势？<br>9.原来是把数据传入到Hive，之后改了架构，怎么把Hive的数据导入到Hbase？<br>10.为什么用Kafka自己存储offset来替代checkpoint，怎么防止了数据双份落地，数据双份是指什么？<br>11.单例用过吗？</p><p>平安：<br>1.问项目，流程，业务？<br>2.数据量，增量？<br>3.几个人开发的，代码量多少？<br>4.你主要做什么的？<br>5.什么场景，用SparkSql分析什么东西？</p><p>总结：<br>基本上都是围绕项目来面，第一家问的比较少，而且都是关于项目；微盟的面试官做的项目，<br>跟简历上的项目，架构上基本一样，所以问的比较深，问我Maxwell的底层原理，对比Cannal有什么优势，<br>为什么选择它，这个我没回答上来，后来让手写Spring，算法，后来就让我走了；<br>平安也是基本围绕项目，业务，数据量，没问什么技术，而且我说了关于优化的点(面试官说不要说网上都有的东西)。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01生产预警平台项目之项目概述</title>
      <link href="/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/"/>
      <url>/2018/08/27/01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h3 id="1-前期基本架构图"><a href="#1-前期基本架构图" class="headerlink" title="1.前期基本架构图"></a>1.前期基本架构图</h3><p><img src="/assets/blogImg/0827_1.png" alt="enter description here"></p><h3 id="2-最终基本架构图"><a href="#2-最终基本架构图" class="headerlink" title="2.最终基本架构图"></a>2.最终基本架构图</h3><p><img src="/assets/blogImg/0827_2.png" alt="enter description here"><br><a id="more"></a></p><h3 id="3-版本"><a href="#3-版本" class="headerlink" title="3.版本"></a>3.版本</h3><table><thead><tr><th>组件</th><th>版本</th></tr></thead><tbody><tr><td>Flume:</td><td>1.7</td></tr><tr><td>Hadoop:</td><td>2.7.3</td></tr><tr><td>Scala:</td><td>2.11</td></tr><tr><td>Kafka:</td><td>0.10.1.0</td></tr><tr><td>Spark:</td><td>2.0.2</td></tr><tr><td>InfluxDB:</td><td>1.2.0</td></tr><tr><td>Grafana:</td><td>4.1.1</td></tr><tr><td>maven:</td><td>3.3.9</td></tr></tbody></table><h3 id="4-主要目的"><a href="#4-主要目的" class="headerlink" title="4.主要目的"></a>4.主要目的</h3><p>主要是想基于Exec Source开发自定义插件AdvancedExecSource，将机器名称 和 服务名称 添加到cdh 服务的角色log数据的每一行前面，则格式为：机器名称 服务名称 年月日 时分秒.毫秒 日志级别 日志信息 ；<br>然后在后面的spark streaming 实时计算我们所需求：比如统计每台机器的服务的每秒出现的error次数 、统计每5秒的warn，error次数等等；<br>来实时可视化展示和邮件短信、微信企业号通知。</p><p>其实主要我们现在的很多监控服务基本达不到秒级的通知，都为5分钟等等，为了方便我们自己的维护；<br>其实对一些即将出现的问题可以提前预知；<br>其实最主要可以有效扩展到实时计算数据库级别日志，比如MySQL慢查询日志，nginx，tomcat，linux的系统级别日志等等。</p><h3 id="5-大概流程"><a href="#5-大概流程" class="headerlink" title="5.大概流程"></a>5.大概流程</h3><p>1.搭建hadoop cluster<br>2.eclipse 导入flume源代码（window7 安装maven，eclipse，eclipse与maven集成）<br>3.开发flume-ng 自定义插件<br>4.flume 收集，汇聚到hdfs(主要测试是否汇聚成功，后期也可以做离线处理)<br>5.flume 收集，汇聚到kafka<br>6.搭建kafka monitor<br>7.搭建 spark client<br>8.window7装ieda开发工具<br>9.idea开发 spark streaming 的wc<br>10.读取kafka日志，开发spark streaming的这块日志分析<br>11.写入influxdb<br>12.grafana可视化展示<br>13.集成邮件</p><p>###说明：<br>针对自身情况，自行选择，步骤如上，但不是固定的，有些顺序是可以打乱的，例如开发工具的安装，可以一起操作的，再如这几个组件的下载编译，如果不<br>想编译可以直接下tar包的，自行选择就好，但是建议还是自己编译，遇到坑才能更好的记住这个东西，本身这个项目就是学习提升的过程，要是什么都是现成的，<br>那就没什么意义了</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 生产预警平台项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 生产预警平台项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark中配置启用LZO压缩</title>
      <link href="/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/"/>
      <url>/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Spark中配置启用LZO压缩，步骤如下：</p><h3 id="一、spark-env-sh配置"><a href="#一、spark-env-sh配置" class="headerlink" title="一、spark-env.sh配置"></a>一、spark-env.sh配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h3 id="二、spark-defaults-conf配置"><a href="#二、spark-defaults-conf配置" class="headerlink" title="二、spark-defaults.conf配置"></a>二、spark-defaults.conf配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br></pre></td></tr></table></figure><p><font color="#FF4500">注：指向编译生成lzo的jar包</font><br><a id="more"></a></p><h3 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h3><h4 id="1、读取Lzo文件"><a href="#1、读取Lzo文件" class="headerlink" title="1、读取Lzo文件"></a>1、读取Lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)</span><br></pre></td></tr></table></figure><h4 id="2、写出lzo文件"><a href="#2、写出lzo文件" class="headerlink" title="2、写出lzo文件"></a>2、写出lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val lzoTest = sc.parallelize(1 to 10)</span><br><span class="line">scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])</span><br></pre></td></tr></table></figure><p>结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2018-03-16 23:24 /input/test_lzo/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         61 2018-03-16 23:24 /input/test_lzo/part-00001.lzo</span><br></pre></td></tr></table></figure><p></p><p>至此配置与测试完成。</p><h3 id="四、配置与测试中存问题"><a href="#四、配置与测试中存问题" class="headerlink" title="四、配置与测试中存问题"></a>四、配置与测试中存问题</h3><h4 id="1、引用native，缺少LD-LIBRARY-PATH"><a href="#1、引用native，缺少LD-LIBRARY-PATH" class="headerlink" title="1、引用native，缺少LD_LIBRARY_PATH"></a>1、引用native，缺少LD_LIBRARY_PATH</h4><h5 id="1-1、错误提示："><a href="#1-1、错误提示：" class="headerlink" title="1.1、错误提示："></a>1.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: native-lzo library not available</span><br><span class="line">  at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120)</span><br><span class="line">  at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178)</span><br><span class="line">  at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111)</span><br><span class="line">  at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)</span><br><span class="line">  at org.apache.spark.scheduler.Task.run(Task.scala:108)</span><br><span class="line">  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><h5 id="1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容："><a href="#1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容：" class="headerlink" title="1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容："></a>1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h4 id="2、无法找到LzopCodec类"><a href="#2、无法找到LzopCodec类" class="headerlink" title="2、无法找到LzopCodec类"></a>2、无法找到LzopCodec类</h4><h5 id="2-1、错误提示："><a href="#2-1、错误提示：" class="headerlink" title="2.1、错误提示："></a>2.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found.</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)</span><br><span class="line">    at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found</span><br><span class="line">    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)</span><br></pre></td></tr></table></figure><h5 id="2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容："><a href="#2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容：" class="headerlink" title="2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容："></a>2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS之垃圾回收箱配置及使用</title>
      <link href="/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>HDFS为每个用户创建一个回收站:<br>目录:/user/用户/.Trash/Current, 系统回收站都有一个周期,周期过后hdfs会彻底删除清空,周期内可以恢复。<br><a id="more"></a></p><h4 id="一、HDFS删除文件-无法恢复"><a href="#一、HDFS删除文件-无法恢复" class="headerlink" title="一、HDFS删除文件,无法恢复"></a>一、HDFS删除文件,无法恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">Deleted /123.log</span><br></pre></td></tr></table></figure><h4 id="二、-启用回收站功能"><a href="#二、-启用回收站功能" class="headerlink" title="二、 启用回收站功能"></a>二、 启用回收站功能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vim core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多长时间创建CheckPoint NameNode节点上运行的CheckPointer </span><br><span class="line">从Current文件夹创建CheckPoint; 默认: 0 由fs.trash.interval项指定 --&gt;</span><br><span class="line">&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多少分钟.Trash下的CheckPoint目录会被删除,</span><br><span class="line">该配置服务器设置优先级大于客户端，默认:不启用 --&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;  -- 清除周期分钟(24小时)</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="1、重启hdfs服务"><a href="#1、重启hdfs服务" class="headerlink" title="1、重启hdfs服务"></a>1、重启hdfs服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sbin]$ ./stop-dfs.sh</span><br><span class="line">[hadoop@hadoop001 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure><h5 id="2、测试回收站功能"><a href="#2、测试回收站功能" class="headerlink" title="2、测试回收站功能"></a>2、测试回收站功能</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /123.log</span><br></pre></td></tr></table></figure><h5 id="文件删除成功存放回收站路径下"><a href="#文件删除成功存放回收站路径下" class="headerlink" title="文件删除成功存放回收站路径下"></a>文件删除成功存放回收站路径下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="恢复文件"><a href="#恢复文件" class="headerlink" title="恢复文件"></a>恢复文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /456.log</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="删除文件跳过回收站"><a href="#删除文件跳过回收站" class="headerlink" title="删除文件跳过回收站"></a>删除文件跳过回收站</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.log</span><br><span class="line">Deleted /456.log</span><br></pre></td></tr></table></figure><p>源码参考：<br><a href="https://blog.csdn.net/tracymkgld/article/details/17557655" target="_blank" rel="noopener">https://blog.csdn.net/tracymkgld/article/details/17557655</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark序列化，你了解吗</title>
      <link href="/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h3 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h3><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p><h4 id="测试代码："><a href="#测试代码：" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="测试结果："><a href="#测试结果：" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_1.png" alt="enter description here"></p><h3 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h3><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p><h4 id="测试代码：-1"><a href="#测试代码：-1" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/716_2.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">      .registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="测试结果：-1"><a href="#测试结果：-1" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_3.png" alt="enter description here"></p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line"> * Component which configures serialization, compression and encryption for various Spark</span><br><span class="line"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span><br><span class="line"> */</span><br><span class="line">private[spark] class SerializerManager(</span><br><span class="line">    defaultSerializer: Serializer,</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    encryptionKey: Option[Array[Byte]]) &#123;</span><br><span class="line"></span><br><span class="line">  def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None)</span><br><span class="line"></span><br><span class="line">  private[this] val kryoSerializer = new KryoSerializer(conf)</span><br><span class="line"></span><br><span class="line">  private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]]</span><br><span class="line">  private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123;</span><br><span class="line">    val primitiveClassTags = Set[ClassTag[_]](</span><br><span class="line">      ClassTag.Boolean,</span><br><span class="line">      ClassTag.Byte,</span><br><span class="line">      ClassTag.Char,</span><br><span class="line">      ClassTag.Double,</span><br><span class="line">      ClassTag.Float,</span><br><span class="line">      ClassTag.Int,</span><br><span class="line">      ClassTag.Long,</span><br><span class="line">      ClassTag.Null,</span><br><span class="line">      ClassTag.Short</span><br><span class="line">    )</span><br><span class="line">    val arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">    primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 状态管理函数，你了解吗</title>
      <link href="/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
      <url>/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><p>Spark Streaming 状态管理函数包括updateStateByKey和mapWithState</p><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p><p>统计全局的key的状态，但是就算没有数据输入，他也会在每一个批次的时候返回之前的key的状态。</p><p>这样的缺点：如果数据量太大的话，我们需要checkpoint数据会占用较大的存储。而且效率也不高<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//[root@bda3 ~]# nc -lk 9999  </span><br><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start()  // 一定要写  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">  /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">  def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="二、mapWithState-效率更高，生产中建议使用"><a href="#二、mapWithState-效率更高，生产中建议使用" class="headerlink" title="二、mapWithState  (效率更高，生产中建议使用)"></a>二、mapWithState (效率更高，生产中建议使用)</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。</p><p>这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储。</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/**  </span><br><span class="line"> * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every  </span><br><span class="line"> * second starting with initial value of word count.  </span><br><span class="line"> * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;  </span><br><span class="line"> *   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive  </span><br><span class="line"> *   data.  </span><br><span class="line"> *  </span><br><span class="line"> * To run this on your local machine, you need to first run a Netcat server  </span><br><span class="line"> *    `$ nc -lk 9999`  </span><br><span class="line"> * and then run the example  </span><br><span class="line"> *    `$ bin/run-example  </span><br><span class="line"> *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`  </span><br><span class="line"> */  </span><br><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;)  </span><br><span class="line">    // Create the context with a 1 second batch size  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)  </span><br><span class="line"></span><br><span class="line">    // Initial state RDD for mapWithState operation  </span><br><span class="line">    val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1)))  </span><br><span class="line"></span><br><span class="line">    // Create a ReceiverInputDStream on target ip:port and count the  </span><br><span class="line">    // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;)  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    // Update the cumulative count using mapWithState  </span><br><span class="line">    // This will give a DStream made of state (which is the cumulative count of the words)  </span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">      StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark和DL/AI结合，谁与争锋? 期待Spark3.0的到来！</title>
      <link href="/2018/06/22/AI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/"/>
      <url>/2018/06/22/AI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p><img src="/assets/blogImg/0622_1.png" alt="enter description here"><br><a id="more"></a></p><p>不知各位，是否关注社区的发展？关注Spark呢？</p><p>官网的Spark图标和解释语已经发生变化了。</p><p>然而在6-18号，社区提出Spark and DL/AI相结合，这无比再一次说明，Spark在大数据的地位是无法撼动的！期待Spark3.0的到来！</p><p>接下来对SPARK-24579的翻译:</p><p>在大数据和人工智能的十字路口，我们看到了Apache Spark作为一个统一的分析引擎以及AI框架如TensorFlow和Apache MXNet (正在孵化中)的兴起及这两大块的巨大成功 。</p><p>大数据和人工智能都是推动企业创新的不可或缺的组成部分， 两个社区的多次尝试，使他们结合在一起。</p><p>我们看到AI社区的努力，为AI框架实现数据解决方案，如TF.DATA和TF.Tror。然而，50+个数据源和内置SQL、数据流和流特征，Spark仍然是对于大数据社区选择。</p><p>这就是为什么我们看到许多努力,将DL/AI框架与Spark结合起来，以利用它的力量，例如，Spark数据源TFRecords、TensorFlowOnSpark, TensorFrames等。作为项目Hydrogen的一部分，这个SPIP将Spark+AI从不同的角度统一起来。</p><p>没有在Spark和外部DL/AI框架之间交换数据，这些集成都是不可能的,也有性能问题。然而，目前还没有一种标准的方式来交换数据，因此实现和性能优化就陷入了困境。例如，在Python中，TensorFlowOnSpark使用Hadoop InputFormat/OutputFormat作为TensorFlow的TFRecords，来加载和保存数据，并将RDD数据传递给TensorFlow。TensorFrames使用TensorFlow的Java API，转换为 Spark DataFrames Rows to/from TensorFlow Tensors 。我们怎样才能降低复杂性呢?</p><p>这里的建议是标准化Spark和DL/AI框架之间的数据交换接口(或格式)，并优化从/到这个接口的数据转换。因此，DL/AI框架可以利用Spark从任何地方加载数据，而无需花费额外的精力构建复杂的数据解决方案，比如从生产数据仓库读取特性或流模型推断。Spark用户可以使用DL/AI框架，而无需学习那里实现的特定数据api。而且双方的开发人员都可以独立地进行性能优化，因为接口本身不会带来很大的开销。</p><p>ISSUE: <a href="https://issues.apache.org/jira/browse/SPARK-24579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24579</a><br>若泽数据，星星本人水平有限，翻译多多包涵。</p><p>对了忘记说了，本ISSUE有个PDF文档，赶快去下载吧。<br><a href="https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf" target="_blank" rel="noopener">https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最前沿！带你读Structured Streaming重量级论文！</title>
      <link href="/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/"/>
      <url>/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-论文下载地址"><a href="#1-论文下载地址" class="headerlink" title="1.论文下载地址"></a>1.论文下载地址</h4><p><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf</a></p><h4 id="2-前言"><a href="#2-前言" class="headerlink" title="2.前言"></a>2.前言</h4><p>建议首先阅读Structured Streaming官网：<a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a><br>以及这两篇Databricks在2016年关于Structured Streaming的文章：</p><p><a href="https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html</a></p><p><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</a></p><p>言归正传<br>该论文收录自2018年ACM SIGMOD会议，是由美国计算机协会（ACM）发起的、在数据库领域具有最高学术地位的国际性学术会议。论文的作者为Databricks的工程师及Spark的开发者，其权威性、重要程度不言而喻。文章开头为该论文的下载地址，供读者阅读交流。本文对该论文进行简要的总结，希望大家能够下载原文细细品读，了解最前沿的大数据技术。</p><h4 id="3-论文简要总结"><a href="#3-论文简要总结" class="headerlink" title="3.论文简要总结"></a>3.论文简要总结</h4><p>题目：Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark</p><h5 id="3-1-摘要"><a href="#3-1-摘要" class="headerlink" title="3.1 摘要"></a>3.1 摘要</h5><p>摘要是一篇论文的精髓，这里给出摘要完整的翻译。<br>随着实时数据的普遍存在，我们需要可扩展的、易用的、易于集成的流式处理系统。结构化流是基于我们对Spark Streaming的经验开发出来的高级别的Spark流式API。结构化流与其他现有的流式API，如谷歌的Dataflow，主要有两点不同。第一，它是一个基于自动增量化的关系型查询API，无需用户自己构建DAG；第二，结构化流旨在于支持端到端的实时应用并整合流与批处理的交互分析。在实践中，我们发现这种整合是一个关键的挑战。结构化流通过Spark SQL的代码生成引擎实现了很高的性能，是Apache Flink的两倍以及Apache Kafka的90倍。它还提供了丰富的运行特性，如回滚、代码更新以及流/批混合执行。最后我们描述了系统的设计以及部署在Databricks几百个生产节点的一个用例。</p><h5 id="3-2-流式处理面临的挑战"><a href="#3-2-流式处理面临的挑战" class="headerlink" title="3.2 流式处理面临的挑战"></a>3.2 流式处理面临的挑战</h5><p>(1) 复杂、低级别的API<br>(2) 端到端应用的集成<br>(3) 运行时挑战：容灾，代码更新，监控等<br>(4) 成本和性能挑战</p><h5 id="3-3-结构化流基本概念"><a href="#3-3-结构化流基本概念" class="headerlink" title="3.3 结构化流基本概念"></a>3.3 结构化流基本概念</h5><p><img src="/assets/blogImg/614_1.png" alt="enter description here"><br>图1 结构化流的组成部分</p><p>(1) Input and Output<br>Input sources 必须是 replayable 的，支持节点宕机后从当前输入继续读取。例如：Apache Kinesis和Apache Kafka。<br>Output sinks 必须支持 idempotent （幂等），确保在节点宕机时可靠的恢复。<br>(2) APIs<br>编写结构化流程序时，可以使用Spark SQL的APIs：DataFrame和SQL来查询streams和tables，该查询定义了一个output table（输出表），用来接收来自steam的数据。engine决定如何计算并将输出表 incrementally（增量地）写入sink。不同的sinks支持不同的output modes（输出模式，后面会提到）。<br>为了处理流式数据，结构化流还增加了一些APIs与已有的Spark SQL API相配合：</p><ul><li>a. Triggers 控制engine多久执行一次计算</li><li>b. event time 是数据源的时间戳；watermark 策略，与event time 相差一段时间后不再接收数据。</li><li>c.Stateful operator（状态算子），类似于Spark Streaming 的updateStateByKey。</li></ul><p>(3) 执行<br>一旦接收到了查询，结构化流就会进行优化递增，并开始执行。结构化流使用两种持久化存储的方式实现容错：</p><ul><li>a.write-ahead log （WAL：预写日志）持续追踪哪些数据已被执行，确保数据的可靠写入。</li><li>b.系统采用大规模的 state store（状态存储）来保存长时间运行的聚合算子的算子状态快照。</li></ul><h5 id="3-4-编程模型"><a href="#3-4-编程模型" class="headerlink" title="3.4 编程模型"></a>3.4 编程模型</h5><p>结构化流将谷歌的Dataflow、增量查询和Spark Streaming 结合起来，以便在Spark SQL下实现流式处理。</p><ul><li>a. A Short Example<pre><code>首先从一个批处理作业开始，统计一个web应用在不同国家的点击数。假设输入数据是一个JSON文件，输出一个Parquet文件，该作业可以通过DataFrame来完成：</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read from static data</span><br><span class="line">2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a static data sink</span><br><span class="line">6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure></li></ul><p>把该作业变成使用结构化流仅仅需要改变输入和输出源，例如，如果新的JSON文件continually（持续地）上传，我们只需要改变第一行和最后一行。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read streaming data</span><br><span class="line">2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a streaming data sink</span><br><span class="line">6counts . writeStream . format (&quot; parquet &quot;)</span><br><span class="line">7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure><p></p><p>结构化流也支持 windowing（窗口）和通过Spark SQL已存在的聚合算子处理event time。例如：我们可以通过修改中间的代码，计算1小时的滑动窗口，每五分钟前进一次：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1// Count events by windows on the &quot; time &quot; field</span><br><span class="line">2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()</span><br></pre></td></tr></table></figure><p></p><ul><li>b. 编程模型语义<br><img src="/assets/blogImg/614_2.png" alt="enter description here"></li></ul><p>图 2 两种输出模式</p><ul><li>i. 每一个输入源提供了一个基于时间的部分有序的记录集（set of records），例如，Kafka将流式数据分为各自有序的partitions。</li><li>ii. 用户提供跨输入数据执行的查询，该输入数据可以在任意给定的处理时间点输出一个 result table（结果表）。结构化流总会产生与所有输入源的数据的前缀上（prefix of the data in all input sources）查询相一致的结果。</li><li>iii. Triggers 告诉系统何时去运行一个新的增量计算，何时更新result table。例如，在微批处理模式，用户希望会每分钟触发一次增量计算。</li><li>iiii. engine支持三种output mode：<pre><code>  Complete：engine一次写所有result table。  Append：engine仅仅向sink增加记录。  Update：engine基于key更新每一个record，更新值改变的keys。该模型有两个特性：第一，结果表的内容独立于输出模式。第二，该模型具有很强的语义一致性，被称为prefix consistency。</code></pre>c.流式算子<pre><code>加入了两种类型的算子：watermarking算子告诉系统何时关闭event time window和输出结果；结构化流允许用户通过withWatermark算子来设置一个watermark，该算子给系统设置一个给定时间戳C的延迟阈值tC，在任意时间点，C的watermark是max（C）-tC。stateful operators允许用户编写自定义逻辑来实现复杂的功能。</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1// Define an update function that simply tracks the</span><br><span class="line"> 2// number of events for each key as its state , returns</span><br><span class="line"> 3// that as its result , and times out keys after 30 min.</span><br><span class="line"> 4def updateFunc (key: UserId , newValues : Iterator [ Event ],</span><br><span class="line"> 5state : GroupState [Int ]): Int = &#123;</span><br><span class="line"> 6val totalEvents = state .get () + newValues . size ()</span><br><span class="line"> 7state . update ( totalEvents )</span><br><span class="line"> 8state . setTimeoutDuration (&quot;30 min&quot;)</span><br><span class="line"> 9return totalEvents</span><br><span class="line">10&#125;</span><br><span class="line">11// Use this update function on a stream , returning a</span><br><span class="line">12// new table lens that contains the session lengths .</span><br><span class="line">13lens = events . groupByKey ( event =&gt; event . userId )</span><br><span class="line">14. mapGroupsWithState ( updateFunc )</span><br></pre></td></tr></table></figure></li></ul><p>用mapGroupWithState算子来追踪每个会话的事件数量，30分钟后关闭会话。</p><h5 id="3-5-运行特性"><a href="#3-5-运行特性" class="headerlink" title="3.5 运行特性"></a>3.5 运行特性</h5><p>(1) 代码更新（code update）<br>开发者能够在编程过程中更新UDF，并且可以简单的重启以使用新版本的代码。<br>(2) 手动回滚（manual rollback）<br>有时在用户发现之前，程序会输出错误的结果，因此回滚至关重要。结构化流很容易定位问题所在。同时手动回滚与前面提到的prefix consistency有很好的交互。<br>(3) 流式和批次混合处理<br>这是结构化流最显而易见的好处，用户能够共用流式处理和批处理作业的代码。<br>(4) 监控<br>结构化流使用Spark已有的API和结构化日志来报告信息，例如处理过的记录数量，跨网络的字节数等。这些接口被Spark开发者所熟知，并易于连接到不同的UI工具。</p><h5 id="3-6-生产用例与总结"><a href="#3-6-生产用例与总结" class="headerlink" title="3.6 生产用例与总结"></a>3.6 生产用例与总结</h5><p>给出简要架构图，篇幅原因不再赘述，希望详细了解的下载论文自行阅读。本文只挑选了部分关键点进行了浅层次的叙述，希望读者能够将论文下载下来认真品读，搞懂开发者的开发思路，跟上大数据的前沿步伐。<br><img src="/assets/blogImg/614_3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产开发必用-Spark RDD转DataFrame的两种方法</title>
      <link href="/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
      <url>/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:<br><a href="https://blog.csdn.net/lemonzhaotao/article/details/80211231" target="_blank" rel="noopener">https://blog.csdn.net/lemonzhaotao/article/details/80211231</a></p><p>在本篇文章中，将介绍RDD转换为DataFrame的2种方式</p><p>官网之RDD转DF:<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds</a><br><a id="more"></a><br>DataFrame 与 RDD 的交互</p><p>Spark SQL它支持两种不同的方式转换已经存在的RDD到DataFrame</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>第一种方式是使用反射的方式，用反射去推倒出来RDD里面的schema。这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。<br>对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常非常多的，是不可能只有20来个字段的。<br>示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 1</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runInferSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  val df = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">              .map(x =&gt; People(x(0), x(1).trim.toInt))  //将rdd的每一行都转换成了一个people</span><br><span class="line">              .toDF         //必须先导入import spark.implicits._  不然这个方法会报错</span><br><span class="line">  df.show()</span><br><span class="line">  df.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">  // 这个DF包含了两个字段name和age</span><br><span class="line">  val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">  // teenager(0)代表第一个字段</span><br><span class="line">  // 取值的第一种方式：index from zero</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line">  // 取值的第二种方式：byName</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()</span><br><span class="line">&#125;</span><br><span class="line">// 注意：case class必须定义在main方法之外；否则会报错</span><br><span class="line">case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p></p><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>创建一个DataFrame，使用编程的方式 这个方式用的非常多。通过编程方式指定schema ，对于第一种方式的schema其实定义在了case class里面了。<br>官网解读：<br>当我们的case class不能提前定义(因为业务处理的过程当中，你的字段可能是在变化的),因此使用case class很难去提前定义。<br>使用该方式创建DF的三大步骤：</p><ul><li>Create an RDD of Rows from the original RDD;</li><li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li><li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 2</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  // 1.转成RDD</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  // 2.定义schema，带有StructType的</span><br><span class="line">  // 定义schema信息</span><br><span class="line">  val schemaString = &quot;name age&quot;</span><br><span class="line">  // 对schema信息按空格进行分割</span><br><span class="line">  // 最终fileds里包含了2个StructField</span><br><span class="line">  val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">                            // 字段类型，字段名称判断是不是为空</span><br><span class="line">                           .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">  val schema = StructType(fields)</span><br><span class="line">  // 3.把我们的schema信息作用到RDD上</span><br><span class="line">  //   这个RDD里面包含了一些行</span><br><span class="line">  // 形成Row类型的RDD</span><br><span class="line">  val rowRDD = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">                  .map(x =&gt; Row(x(0), x(1).trim))</span><br><span class="line">  // 通过SparkSession创建一个DataFrame</span><br><span class="line">  // 传进来一个rowRDD和schema，将schema作用到rowRDD上</span><br><span class="line">  val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="扩展-生产上创建DataFrame的代码举例"><a href="#扩展-生产上创建DataFrame的代码举例" class="headerlink" title="[扩展]生产上创建DataFrame的代码举例"></a>[扩展]生产上创建DataFrame的代码举例</h4><p>在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，这里将展示部分代码：</p><h4 id="Schema的定义"><a href="#Schema的定义" class="headerlink" title="Schema的定义"></a>Schema的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">object AccessConvertUtil &#123;</span><br><span class="line">  val struct = StructType(</span><br><span class="line">    Array(</span><br><span class="line">      StructField(&quot;url&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsType&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsId&quot;,LongType),</span><br><span class="line">      StructField(&quot;traffic&quot;,LongType),</span><br><span class="line">      StructField(&quot;ip&quot;,StringType),</span><br><span class="line">      StructField(&quot;city&quot;,StringType),</span><br><span class="line">      StructField(&quot;time&quot;,StringType),</span><br><span class="line">      StructField(&quot;day&quot;,StringType)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  /**</span><br><span class="line">    * 根据输入的每一行信息转换成输出的样式</span><br><span class="line">    */</span><br><span class="line">  def parseLog(log:String) = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val splits = log.split(&quot;\t&quot;)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      val traffic = splits(2).toLong</span><br><span class="line">      val ip = splits(3)</span><br><span class="line">      val domain = &quot;http://www.imooc.com/&quot;</span><br><span class="line">      val cms = url.substring(url.indexOf(domain) + domain.length)</span><br><span class="line">      val cmsTypeId = cms.split(&quot;/&quot;)</span><br><span class="line">      var cmsType = &quot;&quot;</span><br><span class="line">      var cmsId = 0l</span><br><span class="line">      if (cmsTypeId.length &gt; 1) &#123;</span><br><span class="line">        cmsType = cmsTypeId(0)</span><br><span class="line">        cmsId = cmsTypeId(1).toLong</span><br><span class="line">      &#125;</span><br><span class="line">      val city = IpUtils.getCity(ip)</span><br><span class="line">      val time = splits(0)</span><br><span class="line">      val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;)</span><br><span class="line">      //这个Row里面的字段要和struct中的字段对应上</span><br><span class="line">      Row(url, cmsType, cmsId, traffic, ip, city, time, day)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt; Row(0)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object SparkStatCleanJob &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;)</span><br><span class="line">    //accessRDD.take(10).foreach(println)</span><br><span class="line">    //RDD ==&gt; DF，创建生成DataFrame</span><br><span class="line">    val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)),</span><br><span class="line">      AccessConvertUtil.struct)</span><br><span class="line">    accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite)</span><br><span class="line">            .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java可扩展线程池之ThreadPoolExecutor</title>
      <link href="/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/"/>
      <url>/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="1、ThreadPoolExecutor"><a href="#1、ThreadPoolExecutor" class="headerlink" title="1、ThreadPoolExecutor"></a>1、ThreadPoolExecutor</h4><p>我们知道ThreadPoolExecutor是可扩展的,它提供了几个可以在子类中改写的空方法如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;</span><br><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;  </span><br><span class="line">protected void terminated() &#123; &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="2、为什么要进行扩展？"><a href="#2、为什么要进行扩展？" class="headerlink" title="2、为什么要进行扩展？"></a>2、为什么要进行扩展？</h4><p>因为在实际应用中，可以对线程池运行状态进行跟踪，输出一些有用的调试信息，以帮助故障诊断。</p><h4 id="3、ThreadPoolExecutor-Worker的run方法实现"><a href="#3、ThreadPoolExecutor-Worker的run方法实现" class="headerlink" title="3、ThreadPoolExecutor.Worker的run方法实现"></a>3、ThreadPoolExecutor.Worker的run方法实现</h4><p>通过看源码我们发现 ThreadPoolExecutor的工作线程其实就是Worker实例，Worker.runTask()会被线程池以多线程模式异步调用，则以上三个方法也将被多线程同时访问。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">1// 基于jdk1.8.0_161final void runWorker(Worker w) &#123;</span><br><span class="line"> 2         Thread wt = Thread.currentThread();</span><br><span class="line"> 3         Runnable task = w.firstTask;</span><br><span class="line"> 4         w.firstTask = null;</span><br><span class="line"> 5         w.unlock(); // allow interrupts</span><br><span class="line"> 6         boolean completedAbruptly = true;        </span><br><span class="line"> 7             try &#123;            </span><br><span class="line"> 8             while (task != null || (task = getTask()) != null) &#123;</span><br><span class="line"> 9                  w.lock();                </span><br><span class="line">10              if ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class="line">11                     (Thread.interrupted() &amp;&amp;</span><br><span class="line">12                      runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class="line">13                    !wt.isInterrupted())</span><br><span class="line">14                    wt.interrupt();               </span><br><span class="line">15              try &#123;</span><br><span class="line">16                    beforeExecute(wt, task);</span><br><span class="line">17                    Throwable thrown = null;                   </span><br><span class="line">18              try &#123;</span><br><span class="line">19                        task.run();</span><br><span class="line">20                    &#125; catch (RuntimeException x) &#123;</span><br><span class="line">21                        thrown = x; throw x;</span><br><span class="line">22                    &#125; catch (Error x) &#123;</span><br><span class="line">23                        thrown = x; throw x;</span><br><span class="line">24                    &#125; catch (Throwable x) &#123;</span><br><span class="line">25                        thrown = x; throw new Error(x);</span><br><span class="line">26                    &#125; finally &#123;</span><br><span class="line">27                        afterExecute(task, thrown);</span><br><span class="line">28                    &#125;</span><br><span class="line">29                &#125; finally &#123;</span><br><span class="line">30                    task = null;</span><br><span class="line">31                    w.completedTasks++;</span><br><span class="line">32                    w.unlock();</span><br><span class="line">33                &#125;</span><br><span class="line">34            &#125;</span><br><span class="line">35            completedAbruptly = false;</span><br><span class="line">36        &#125; finally &#123;</span><br><span class="line">37            processWorkerExit(w, completedAbruptly);</span><br><span class="line">38        &#125;</span><br><span class="line">39    &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="4、扩展线程池实现"><a href="#4、扩展线程池实现" class="headerlink" title="4、扩展线程池实现"></a>4、扩展线程池实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1public class ExtThreadPool &#123;</span><br><span class="line"> 2    public static class MyTask implements Runnable &#123;</span><br><span class="line"> 3        public String name;        </span><br><span class="line"> 4        public MyTask(String name) &#123;            </span><br><span class="line"> 5          this.name = name;</span><br><span class="line"> 6        &#125;       </span><br><span class="line"> 7        public void run() &#123;</span><br><span class="line"> 8            System.out.println(&quot;正在执行:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name);            try &#123;</span><br><span class="line"> 9                Thread.sleep(100);</span><br><span class="line">10            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">11                e.printStackTrace();</span><br><span class="line">12            &#125;</span><br><span class="line">13        &#125;</span><br><span class="line">14    &#125;    </span><br><span class="line">15public static void main(String args[]) throws InterruptedException &#123;</span><br><span class="line">16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,</span><br><span class="line">17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123;            </span><br><span class="line">18protected void beforeExecute(Thread t, Runnable r) &#123;</span><br><span class="line">19 System.out.println(&quot;准备执行：&quot; + ((MyTask) r).name);</span><br><span class="line">20&#125;            </span><br><span class="line">21protected void afterExecute(Thread t, Runnable r) &#123;</span><br><span class="line">22  System.out.println(&quot;执行完成&quot; + ((MyTask) r).name);</span><br><span class="line">23&#125;            </span><br><span class="line">24protected void terminated() &#123;</span><br><span class="line">25  System.out.println(&quot;线程池退出！&quot;);</span><br><span class="line">26&#125;</span><br><span class="line">27&#125;;        </span><br><span class="line">28for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">29 MyTask task = new MyTask(&quot;TASK--&quot; + i);</span><br><span class="line">30            executorService.execute(task);</span><br><span class="line">31            Thread.sleep(10);</span><br><span class="line">32        &#125;</span><br><span class="line">33 executorService.shutdown();</span><br><span class="line">34    &#125;</span><br><span class="line">35&#125;</span><br></pre></td></tr></table></figure><p>输出结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">准备执行：TASK–0 </span><br><span class="line">正在执行:Thread ID:10,Task Name:TASK–0 </span><br><span class="line">准备执行：TASK–1 </span><br><span class="line">正在执行:Thread ID:11,Task Name:TASK–1 </span><br><span class="line">准备执行：TASK–2 </span><br><span class="line">正在执行:Thread ID:12,Task Name:TASK–2 </span><br><span class="line">准备执行：TASK–3 </span><br><span class="line">正在执行:Thread ID:13,Task Name:TASK–3 </span><br><span class="line">准备执行：TASK–4 </span><br><span class="line">正在执行:Thread ID:14,Task Name:TASK–4 </span><br><span class="line">线程池退出！</span><br></pre></td></tr></table></figure><p></p><p>这样就实现了在执行前后进行的一些控制，除此之外我们还可以输出每个线程的执行时间，或者一些其他增强操作。</p><h4 id="5、思考？"><a href="#5、思考？" class="headerlink" title="5、思考？"></a>5、思考？</h4><p>请读者思考shutdownNow和shutdown方法的区别？<br>如何优雅的关闭线程池？</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你大爷永远是你大爷，RDD血缘关系源码详解！</title>
      <link href="/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/"/>
      <url>/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="一、RDD的依赖关系"><a href="#一、RDD的依赖关系" class="headerlink" title="一、RDD的依赖关系"></a>一、RDD的依赖关系</h4><p>RDD的依赖关系分为两类：宽依赖和窄依赖。我们可以这样认为：</p><ul><li><p>（1）窄依赖：每个parent RDD 的 partition 最多被 child RDD 的一个partition 使用。</p></li><li><p>（2）宽依赖：每个parent RDD partition 被多个 child RDD 的partition 使用。</p></li></ul><p>窄依赖每个 child RDD 的 partition 的生成操作都是可以并行的，而宽依赖则需要所有的 parent RDD partition shuffle 结果得到后再进行。<br><a id="more"></a></p><h4 id="二、org-apache-spark-Dependency-scala-源码解析"><a href="#二、org-apache-spark-Dependency-scala-源码解析" class="headerlink" title="二、org.apache.spark.Dependency.scala 源码解析"></a>二、org.apache.spark.Dependency.scala 源码解析</h4><p>Dependency是一个抽象类：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">  def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。</p><h5 id="（1）NarrowDependency也是一个抽象类"><a href="#（1）NarrowDependency也是一个抽象类" class="headerlink" title="（1）NarrowDependency也是一个抽象类"></a>（1）NarrowDependency也是一个抽象类</h5><p>定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;  </span><br><span class="line">/**</span><br><span class="line">   * Get the parent partitions for a child partition.</span><br><span class="line">   * @param partitionId a partition of the child RDD</span><br><span class="line">   * @return the partitions of the parent RDD that the child partition depends upon</span><br><span class="line">   */</span><br><span class="line">  def getParents(partitionId: Int): Seq[Int]</span><br><span class="line"></span><br><span class="line">  override def rdd: RDD[T] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>窄依赖又有两个具体的实现：OneToOneDependency和RangeDependency。<br>（a）OneToOneDependency指child RDD的partition只依赖于parent RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = List(partitionId)</span><br><span class="line">&#125;</span><br><span class="line">        （b）RangeDependency指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。</span><br><span class="line"></span><br><span class="line">// Denpendency.scala</span><br><span class="line">class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)  </span><br><span class="line">  extends NarrowDependency[T](rdd) &#123;//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = &#123;    </span><br><span class="line">    if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      List(partitionId - outStart + inStart)//表示于当前索引的相对位置</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      Nil</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="（2）ShuffleDependency指宽依赖"><a href="#（2）ShuffleDependency指宽依赖" class="headerlink" title="（2）ShuffleDependency指宽依赖"></a>（2）ShuffleDependency指宽依赖</h5><p>表示一个parent RDD的partition会被child RDD的partition使用多次。需要经过shuffle才能形成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</span><br><span class="line">    @transient private val _rdd: RDD[_ &lt;: Product2[K, V]],    </span><br><span class="line">    val partitioner: Partitioner,    </span><br><span class="line">    val serializer: Serializer = SparkEnv.get.serializer,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = false)</span><br><span class="line">  extends Dependency[Product2[K, V]] &#123;  //shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的</span><br><span class="line">  override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"></span><br><span class="line">  private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName</span><br><span class="line">  private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName</span><br><span class="line">  private[spark] val combinerClassName: Option[String] =</span><br><span class="line">    Option(reflect.classTag[C]).map(_.runtimeClass.getName)  //获取shuffleId</span><br><span class="line">  val shuffleId: Int = _rdd.context.newShuffleId()  //向shuffleManager注册shuffle信息</span><br><span class="line">  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, this)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。</p><h4 id="三、两种依赖的区分"><a href="#三、两种依赖的区分" class="headerlink" title="三、两种依赖的区分"></a>三、两种依赖的区分</h4><p>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark 技术团队开源机器学习平台 MLflow</title>
      <link href="/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/"/>
      <url>/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。<br><a id="more"></a><br><img src="/assets/blogImg/612_1.png" alt="enter description here"><br>Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。<br>MLflow 正是受现有的 ML 平台启发，主打开放性：</p><ul><li>开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。</li><li>开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。<br>MLflow 目前的 alpha 版本包含三个组件：<br><img src="/assets/blogImg/612_2.png" alt="enter description here"><br>其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow</span><br><span class="line"></span><br><span class="line"># Log parameters (key-value pairs)</span><br><span class="line">mlflow.log_param(&quot;num_dimensions&quot;, 8)</span><br><span class="line">mlflow.log_param(&quot;regularization&quot;, 0.1)</span><br><span class="line"></span><br><span class="line"># Log a metric; metrics can be updated throughout the run</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.1)</span><br><span class="line">...</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.45)</span><br><span class="line"></span><br><span class="line"># Log artifacts (output files)</span><br><span class="line">mlflow.log_artifact(&quot;roc.png&quot;)</span><br><span class="line">mlflow.log_artifact(&quot;model.pkl&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name: My Project</span><br><span class="line">conda_env: conda.yaml</span><br><span class="line">entry_points:</span><br><span class="line">  main:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">      regularization: &#123;type: float, default: 0.1&#125;</span><br><span class="line">    command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot;</span><br><span class="line">  validate:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">    command: &quot;python validate.py &#123;data_file&#125;&quot;</span><br></pre></td></tr></table></figure><p></p><p>MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_created: 2018-02-21T13:21:34.12</span><br><span class="line">flavors:</span><br><span class="line">  sklearn:</span><br><span class="line">    sklearn_version: 0.19.1</span><br><span class="line">    pickled_model: model.pkl</span><br><span class="line">  python_function:</span><br><span class="line">    loader_module: mlflow.sklearn</span><br><span class="line">    pickled_model: model.pkl</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark MLlib </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStreaming 状态管理函数的选择比较</title>
      <link href="/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/"/>
      <url>/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</span><br></pre></td></tr></table></figure><p></p><p>也即是说它会统计全局的key的状态，就算没有数据输入，它也会在每一个批次的时候返回之前的key的状态。</p><p>缺点：若数据量太大的话，需要checkpoint的数据会占用较大的存储，效率低下。</p><p>程序示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：要使用updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start() </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;   </span><br><span class="line"> /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">def updateFunction(currentValues: Seq[Int], preValues: Option[Int]):                                Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="二、mapWithState"><a href="#二、mapWithState" class="headerlink" title="二、mapWithState"></a>二、mapWithState</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。效率更高，生产中建议使用</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount </span><br><span class="line">      &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;StatefulNetworkWordCount&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)   </span><br><span class="line">    val initialRDD = ssc.sparkContext</span><br><span class="line">      .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1)))  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], </span><br><span class="line">     state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">    StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="三、源码分析"><a href="#三、源码分析" class="headerlink" title="三、源码分析"></a>三、源码分析</h4><h5 id="upateStateByKey："><a href="#upateStateByKey：" class="headerlink" title="upateStateByKey："></a>upateStateByKey：</h5><ul><li>map返回的是MappedDStream，而MappedDStream并没有updateStateByKey方法，并且它的父类DStream中也没有该方法。但是DStream的伴生对象中有一个隐式转换函数：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">object DStream &#123;</span><br><span class="line">  implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)])</span><br><span class="line">      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null):</span><br><span class="line">    PairDStreamFunctions[K, V] = &#123;</span><br><span class="line">    new PairDStreamFunctions[K, V](stream)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>跟进去 PairDStreamFunctions ，发现最终调用的是自己的updateStateByKey。<br>其中updateFunc就要传入的参数，他是一个函数，Seq[V]表示当前key对应的所有值，<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Option[S] 是当前key的历史状态，返回的是新的状态。</span><br><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Seq[V], Option[S]) =&gt; Option[S]</span><br><span class="line">  ): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>最终调用：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],</span><br><span class="line">    partitioner: Partitioner,</span><br><span class="line">    rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  val cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">  val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123;</span><br><span class="line">    cleanedFunc(it)</span><br><span class="line">  &#125;</span><br><span class="line">  new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>再跟进去 new StateDStream:<br>在这里面new出了一个StateDStream对象。在其compute方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中StateDStream的父类计算出的RDD（本次batch的单词计数）分别是prevStateRDD和parentRDD，然后在调用 computeUsingPreviousRDD 方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private [this] def computeUsingPreviousRDD(</span><br><span class="line">    batchTime: Time,</span><br><span class="line">    parentRDD: RDD[(K, V)],</span><br><span class="line">    prevStateRDD: RDD[(K, S)]) = &#123;</span><br><span class="line">  // Define the function for the mapPartition operation on cogrouped RDD;</span><br><span class="line">  // first map the cogrouped tuple to tuples of required type,</span><br><span class="line">  // and then apply the update function</span><br><span class="line">  val updateFuncLocal = updateFunc</span><br><span class="line">  val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123;</span><br><span class="line">    val i = iterator.map &#123; t =&gt;</span><br><span class="line">      val itr = t._2._2.iterator</span><br><span class="line">      val headOption = if (itr.hasNext) Some(itr.next()) else None</span><br><span class="line">      (t._1, t._2._1.toSeq, headOption)</span><br><span class="line">    &#125;</span><br><span class="line">    updateFuncLocal(batchTime, i)</span><br><span class="line">  &#125;</span><br><span class="line">  val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</span><br><span class="line">  val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</span><br><span class="line">  Some(stateRDD)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在这里两个RDD进行cogroup然后应用updateStateByKey传入的函数。我们知道cogroup的性能是比较低下，参考<a href="http://lxw1234.com/archives/2015/07/384.htm。" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/07/384.htm。</a></p><h5 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState:"></a>mapWithState:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Experimental</span><br><span class="line">def mapWithState[StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    spec: StateSpec[K, V, StateType, MappedType]</span><br><span class="line">  ): MapWithStateDStream[K, V, StateType, MappedType] = &#123;</span><br><span class="line">  new MapWithStateDStreamImpl[K, V, StateType, MappedType](</span><br><span class="line">    self,</span><br><span class="line">    spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>说明：StateSpec 封装了状态管理函数，并在该方法中创建了MapWithStateDStreamImpl对象。</p><p>MapWithStateDStreamImpl 中创建了一个InternalMapWithStateDStream类型对象internalStream，在MapWithStateDStreamImpl的compute方法中调用了internalStream的getOrCompute方法。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private[streaming] class MapWithStateDStreamImpl[</span><br><span class="line">    KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    dataStream: DStream[(KeyType, ValueType)],</span><br><span class="line">    spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType])</span><br><span class="line">  extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123;</span><br><span class="line"></span><br><span class="line">  private val internalStream =</span><br><span class="line">    new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec)</span><br><span class="line"></span><br><span class="line">  override def slideDuration: Duration = internalStream.slideDuration</span><br><span class="line"></span><br><span class="line">  override def dependencies: List[DStream[_]] = List(internalStream)</span><br><span class="line"></span><br><span class="line">  override def compute(validTime: Time): Option[RDD[MappedType]] = &#123;</span><br><span class="line">    internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>InternalMapWithStateDStream中没有getOrCompute方法，这里调用的是其父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream的Compute方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">/** Method that generates an RDD for the given time */</span><br><span class="line">override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123;</span><br><span class="line">  // Get the previous state or create a new empty state RDD</span><br><span class="line">  val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123;</span><br><span class="line">    case Some(rdd) =&gt;</span><br><span class="line">      if (rdd.partitioner != Some(partitioner)) &#123;</span><br><span class="line">        // If the RDD is not partitioned the right way, let us repartition it using the</span><br><span class="line">        // partition index as the key. This is to ensure that state RDD is always partitioned</span><br><span class="line">        // before creating another state RDD using it</span><br><span class="line">        MapWithStateRDD.createFromRDD[K, V, S, E](</span><br><span class="line">          rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line">    case None =&gt;</span><br><span class="line">      MapWithStateRDD.createFromPairRDD[K, V, S, E](</span><br><span class="line">        spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)),</span><br><span class="line">        partitioner,</span><br><span class="line">        validTime</span><br><span class="line">      )</span><br><span class="line">  &#125;</span><br><span class="line">  // Compute the new state RDD with previous state RDD and partitioned data RDD</span><br><span class="line">  // Even if there is no data RDD, use an empty one to create a new state RDD</span><br><span class="line">  val dataRDD = parent.getOrCompute(validTime).getOrElse &#123;</span><br><span class="line">    context.sparkContext.emptyRDD[(K, V)]</span><br><span class="line">  &#125;</span><br><span class="line">  val partitionedDataRDD = dataRDD.partitionBy(partitioner)</span><br><span class="line">  val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt;</span><br><span class="line">    (validTime - interval).milliseconds</span><br><span class="line">  &#125;</span><br><span class="line">  Some(new MapWithStateRDD(</span><br><span class="line">    prevStateRDD, partitionedDataRDD, mappingFunction, </span><br><span class="line">    validTime, timeoutThresholdTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>根据给定的时间生成一个MapWithStateRDD，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD:dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取partitionedDataRDD。最后将preStateRDD，partitionedDataRDD和用户定义的函数mappingFunction传给新生成的MapWithStateRDD对象返回。</p><p>后续若有兴趣可以继续跟进MapWithStateRDD的compute方法，限于篇幅不再展示。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL 之外部数据源如何成为在企业开发中的一把利器</title>
      <link href="/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/"/>
      <url>/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h4><p>1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。<br>2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。<br>3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。<br>4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。<br><a id="more"></a></p><h4 id="2-读取json文件"><a href="#2-读取json文件" class="headerlink" title="2.读取json文件"></a>2.读取json文件</h4><p>启动shell进行测试<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//标准写法</span><br><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)</span><br><span class="line">//另外一种写法</span><br><span class="line">spark.read.json(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">看看源码这两者之间到底有啥不同呢？</span><br><span class="line">/**</span><br><span class="line">   * Loads a JSON file and returns the results as a `DataFrame`.</span><br><span class="line">   *</span><br><span class="line">   * See the documentation on the overloaded `json()` method with varargs for more details.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def json(path: String): DataFrame = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    json(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line">我们调用josn() 方法其实进行了 overloaded ，我们继续查看</span><br><span class="line"> def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*)</span><br><span class="line"> 这句话是不是很熟悉，其实就是我们的标准写法</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"> df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h4 id="3-读取parquet数据"><a href="#3-读取parquet数据" class="headerlink" title="3 读取parquet数据"></a>3 读取parquet数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h4 id="4-读取hive中的数据"><a href="#4-读取hive中的数据" class="headerlink" title="4 读取hive中的数据"></a>4 读取hive中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">| default|states_raw|      false|</span><br><span class="line">| default|states_seq|      false|</span><br><span class="line">| default|        t1|      false|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line"></span><br><span class="line">spark.table(&quot;states_raw&quot;).show</span><br><span class="line">+-----+------+</span><br><span class="line">| code|  name|</span><br><span class="line">+-----+------+</span><br><span class="line">|hello|  java|</span><br><span class="line">|hello|hadoop|</span><br><span class="line">|hello|  hive|</span><br><span class="line">|hello| sqoop|</span><br><span class="line">|hello|  hdfs|</span><br><span class="line">|hello| spark|</span><br><span class="line">+-----+------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select name from states_raw &quot;).show</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|  java|</span><br><span class="line">|hadoop|</span><br><span class="line">|  hive|</span><br><span class="line">| sqoop|</span><br><span class="line">|  hdfs|</span><br><span class="line">| spark|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure><h4 id="5-保存数据"><a href="#5-保存数据" class="headerlink" title="5 保存数据"></a>5 保存数据</h4><p>注意：</p><ol><li>保存的文件夹不能存在，否则报错(默认情况下，可以选择不同的模式)：org.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;</li><li>保存成文本格式，只能保存一列，否则报错：org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line">//保存</span><br><span class="line">df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[hadoop@hadoop out]$ pwd</span><br><span class="line">/home/hadoop/data/out</span><br><span class="line">[hadoop@hadoop out]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:34 _SUCCESS</span><br><span class="line">[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt </span><br><span class="line">Michael</span><br><span class="line">Andy</span><br><span class="line">Justin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//保存为json格式</span><br><span class="line">df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)</span><br><span class="line"></span><br><span class="line">结果</span><br><span class="line">[hadoop@hadoop data]$ cd out1</span><br><span class="line">[hadoop@hadoop out1]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:35 _SUCCESS</span><br><span class="line">[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;</span><br></pre></td></tr></table></figure></li></ol><p>上面说了在保存数据时如果目录已经存在，在默认模式下会报错，那我们下面讲解保存的几种模式：<br><img src="/assets/blogImg/606_1.png" alt="enter description here"></p><h4 id="6-读取mysql中的数据"><a href="#6-读取mysql中的数据" class="headerlink" title="6 读取mysql中的数据"></a>6 读取mysql中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">val jdbcDF = spark.read</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;123456&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- TBL_ID: long (nullable = false)</span><br><span class="line"> |-- CREATE_TIME: integer (nullable = false)</span><br><span class="line"> |-- DB_ID: long (nullable = true)</span><br><span class="line"> |-- LAST_ACCESS_TIME: integer (nullable = false)</span><br><span class="line"> |-- OWNER: string (nullable = true)</span><br><span class="line"> |-- RETENTION: integer (nullable = false)</span><br><span class="line"> |-- SD_ID: long (nullable = true)</span><br><span class="line"> |-- TBL_NAME: string (nullable = true)</span><br><span class="line"> |-- TBL_TYPE: string (nullable = true)</span><br><span class="line"> |-- VIEW_EXPANDED_TEXT: string (nullable = true)</span><br><span class="line"> |-- VIEW_ORIGINAL_TEXT: string (nullable = true)</span><br><span class="line"></span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="7-spark-SQL操作mysql表数据"><a href="#7-spark-SQL操作mysql表数据" class="headerlink" title="7 spark SQL操作mysql表数据"></a>7 spark SQL操作mysql表数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW jdbcTable</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost:3306&quot;,</span><br><span class="line">  dbtable &quot;basic01.tbls&quot;,</span><br><span class="line">  user &apos;root&apos;,</span><br><span class="line">  password &apos;123456&apos;,</span><br><span class="line">  driver &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">查看：</span><br><span class="line">show tables;</span><br><span class="line">default states_raw      false</span><br><span class="line">default states_seq      false</span><br><span class="line">default t1      false</span><br><span class="line">jdbctable       true</span><br><span class="line"></span><br><span class="line">select * from jdbctable;</span><br><span class="line">1       1519944170      6       0       hadoop  0       1       page_views      MANAGED_TABLE   NULL    NULL</span><br><span class="line">2       1519944313      6       0       hadoop  0       2       page_views_bzip2        MANAGED_TABLE   NULL    NULL</span><br><span class="line">3       1519944819      6       0       hadoop  0       3       page_views_snappy       MANAGED_TABLE   NULL    NULL</span><br><span class="line">21      1520067771      6       0       hadoop  0       21      tt      MANAGED_TABLE   NULL    NULL</span><br><span class="line">22      1520069148      6       0       hadoop  0       22      page_views_seq  MANAGED_TABLE   NULL    NULL</span><br><span class="line">23      1520071381      6       0       hadoop  0       23      page_views_rcfile       MANAGED_TABLE   NULL    NULL</span><br><span class="line">24      1520074675      6       0       hadoop  0       24      page_views_orc_zlib     MANAGED_TABLE   NULL    NULL</span><br><span class="line">27      1520078184      6       0       hadoop  0       27      page_views_lzo_index    MANAGED_TABLE   NULL    NULL</span><br><span class="line">30      1520083461      6       0       hadoop  0       30      page_views_lzo_index1   MANAGED_TABLE   NULL    NULL</span><br><span class="line">31      1524370014      1       0       hadoop  0       31      t1      EXTERNAL_TABLE  NULL    NULL</span><br><span class="line">37      1524468636      1       0       hadoop  0       37      states_raw      MANAGED_TABLE   NULL    NULL</span><br><span class="line">38      1524468678      1       0       hadoop  0       38      states_seq      MANAGED_TABLE   NULL    NULL</span><br><span class="line"></span><br><span class="line">mysql中的tbls的数据已经存在jdbctable表中了。</span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="8-分区推测（Partition-Discovery）"><a href="#8-分区推测（Partition-Discovery）" class="headerlink" title="8 分区推测（Partition Discovery）"></a>8 分区推测（Partition Discovery）</h4><p>表分区是在像Hive这样的系统中使用的常见优化方法。 在分区表中，数据通常存储在不同的目录中，分区列值在每个分区目录的路径中编码。 所有内置的文件源（包括Text / CSV / JSON / ORC / Parquet）都能够自动发现和推断分区信息。 例如，我们创建如下的目录结构;<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN</span><br><span class="line"></span><br><span class="line">添加json文件：</span><br><span class="line">people.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line"></span><br><span class="line"> hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN</span><br></pre></td></tr></table></figure><p></p><p>我们使用spark sql读取外部数据源：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p></p><p>我们改变读取的目录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)</span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- country: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+-------+</span><br><span class="line">| age|   name|country|</span><br><span class="line">+----+-------+-------+</span><br><span class="line">|null|Michael|     CN|</span><br><span class="line">|  30|   Andy|     CN|</span><br><span class="line">|  19| Justin|     CN|</span><br><span class="line">+----+-------+-------+</span><br></pre></td></tr></table></figure><p></p><p>大家有没有发现什么呢？Spark SQL将自动从路径中提取分区信息。<br>注意，分区列的数据类型是自动推断的。目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以通过</p><p><font color="#FF4500">spark.sql.sources.partitionColumnTypeInference.enabled</font>进行配置，默认为true。当禁用类型推断时，字符串类型将用于分区列。<br>从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下找到分区。对于上面的示例，如果用户将路径/table/gender=male传递给</p><p><font color="#FF4500">SparkSession.read.parquet或SparkSession.read.load</font>，则不会将性别视为分区列。如果用户需要指定启动分区发现的基本路径，则可以basePath在数据源选项中进行设置。例如，当path/to/table/gender=male是数据路径并且用户将basePath设置为path/to/table/时，性别将是分区列。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统重要参数调优，你知道吗</title>
      <link href="/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/"/>
      <url>/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="当前会话生效"><a href="#当前会话生效" class="headerlink" title="当前会话生效"></a>当前会话生效</h4><p>ulimit -u -&gt; 查看当前最大进程数<br>ulimit -n -&gt;查看当前最大文件数<br>ulimit -u xxx -&gt; 修改当前最大进程数为xxx<br>ulimit -n xxx -&gt; 修改当前最大文件数为xxx</p><h4 id="永久生效"><a href="#永久生效" class="headerlink" title="永久生效"></a>永久生效</h4><p>1.vi /etc/security/limits.conf，添加如下的行</p><ul><li>soft noproc 11000</li><li>hard noproc 11000</li><li>soft nofile 4100</li><li>hard nofile 4100<a id="more"></a> 说明：</li><li>代表针对所有用户<br>noproc 是代表最大进程数<br>nofile 是代表最大文件打开数</li></ul><h4 id="2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制："><a href="#2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制：" class="headerlink" title="2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制："></a>2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制：</h4><ul><li>1)、vi /etc/ssh/sshd_config<br>把 UserLogin 的值改为 yes，并把 # 注释去掉</li><li>2)、重启 sshd 服务<br>/etc/init.d/sshd restart</li><li>3)、修改所有 linux 用户的环境变量文件：<br>vi /etc/profile<br>ulimit -u 10000<br>ulimit -n 4096<br>ulimit -d unlimited<br>ulimit -m unlimited<br>ulimit -s unlimited<br>ulimit -t unlimited<br>ulimit -v unlimited</li><li>4)、生效<br>source /etc/profile</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark动态内存管理源码解析！</title>
      <link href="/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/"/>
      <url>/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h4 id="一、Spark内存管理模式"><a href="#一、Spark内存管理模式" class="headerlink" title="一、Spark内存管理模式"></a>一、Spark内存管理模式</h4><p>Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// SparkEnv.scala</span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="二、Spark动态内存管理空间分配"><a href="#二、Spark动态内存管理空间分配" class="headerlink" title="二、Spark动态内存管理空间分配"></a>二、Spark动态内存管理空间分配</h4><p><img src="/assets/blogImg/603_1.png" alt="enter description here"><br>相比于Static MemoryManager模式，Unified MemoryManager模型打破了存储内存和运行内存的界限，使每一个内存区能够动态伸缩，降低OOM的概率。由上图可知，executor JVM内存主要由以下几个区域组成：</p><ul><li>（1）Reserved Memory（预留内存）：这部分内存预留给系统使用，默认为300MB，可通过spark.testing.reservedMemory进行设置。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024</span><br></pre></td></tr></table></figure></li></ul><p>另外，JVM内存的最小值也与reserved Memory有关，即minSystemMemory = reserved Memory<em>1.5，即默认情况下JVM内存最小值为300MB</em>1.5=450MB。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong</span><br></pre></td></tr></table></figure><p></p><ul><li>（2）Spark Memeoy:分为execution Memory和storage Memory。去除掉reserved Memory，剩下usableMemory的一部分用于execution和storage这两类堆内存，默认是0.6，可通过spark.memory.fraction进行设置。例如：JVM内存是1G，那么用于execution和storage的默认内存为（1024-300）*0.6=434MB。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val usableMemory = systemMemory - reservedMemory</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    (usableMemory * memoryFraction).toLong</span><br></pre></td></tr></table></figure></li></ul><p>他们的边界由spark.memory.storageFraction设定，默认为0.5。即默认状态下storage Memory和execution Memory为1：1.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">     onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</span><br><span class="line">      numCores = numCores)</span><br></pre></td></tr></table></figure><p></p><ul><li>（3）user Memory:剩余内存，用户根据需要使用，默认占usableMemory的（1-0.6）=0.4.</li></ul><h5 id="三、内存控制详解"><a href="#三、内存控制详解" class="headerlink" title="三、内存控制详解"></a>三、内存控制详解</h5><p>首先我们先来了解一下Spark内存管理实现类之前的关系。<br><img src="/assets/blogImg/603_2.png" alt="enter description here"></p><h5 id="1-MemoryManager主要功能是："><a href="#1-MemoryManager主要功能是：" class="headerlink" title="1.MemoryManager主要功能是："></a>1.MemoryManager主要功能是：</h5><ul><li>（1）记录用了多少StorageMemory和ExecutionMemory；</li><li>（2）申请Storage、Execution和Unroll Memory；</li><li>（3）释放Stroage和Execution Memory。</li></ul><p>Execution内存用来执行shuffle、joins、sorts和aggegations操作，Storage内存用于缓存和广播数据，每一个JVM中都存在着一个MemoryManager。构造MemoryManager需要指定onHeapStorageMemory和onHeapExecutionMemory参数。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> // MemoryManager.scala</span><br><span class="line">private[spark] abstract class MemoryManager(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    numCores: Int,</span><br><span class="line">    onHeapStorageMemory: Long,</span><br><span class="line">    onHeapExecutionMemory: Long) extends Logging &#123;</span><br></pre></td></tr></table></figure><p></p><p>创建StorageMemoryPool和ExecutionMemoryPool对象，用来创建堆内或堆外的Storage和Execution内存池，管理Storage和Execution的内存分配。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)</span><br></pre></td></tr></table></figure><p></p><p>默认情况下，不使用堆外内存，可通过saprk.memory.offHeap.enabled设置，默认堆外内存为0，可使用spark.memory.offHeap.size参数设置。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// All the code you will ever need</span><br><span class="line"> final val tungstenMemoryMode: MemoryMode = &#123;</span><br><span class="line">    if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123;</span><br><span class="line">      require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0,</span><br><span class="line">        &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;)</span><br><span class="line">      require(Platform.unaligned(),</span><br><span class="line">        &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;)</span><br><span class="line">      MemoryMode.OFF_HEAP</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MemoryMode.ON_HEAP</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)</span><br></pre></td></tr></table></figure><p>释放numBytes字节的Execution内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放指定task的所有Execution内存并将该task标记为inactive。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123;</span><br><span class="line">    onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) +</span><br><span class="line">      offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放numBytes字节的Stoarge内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放所有Storage内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">final def releaseAllStorageMemory(): Unit = synchronized &#123;</span><br><span class="line">    onHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">    offHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"><a href="#2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？" class="headerlink" title="2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"></a>2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？</h5><p>UnifiedMemoryManage继承了MemoryManager<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">private[spark] class UnifiedMemoryManager private[memory] (</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    val maxHeapMemory: Long,</span><br><span class="line">    onHeapStorageRegionSize: Long,</span><br><span class="line">    numCores: Int)</span><br><span class="line">  extends MemoryManager(</span><br><span class="line">    conf,</span><br><span class="line">    numCores,</span><br><span class="line">    onHeapStorageRegionSize,</span><br><span class="line">    maxHeapMemory - onHeapStorageRegionSize) &#123;</span><br></pre></td></tr></table></figure><p></p><p>重写了maxOnHeapStorageMemory方法，最大Storage内存=最大内存-最大Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line"> override def maxOnHeapStorageMemory: Long = synchronized &#123;</span><br><span class="line">    maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireStorageMemory：申请Storage内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override def acquireStorageMemory(</span><br><span class="line">      blockId: BlockId,</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      memoryMode: MemoryMode): Boolean = synchronized &#123;</span><br><span class="line">    assertInvariants()</span><br><span class="line">    assert(numBytes &gt;= 0)</span><br><span class="line">    val (executionPool, storagePool, maxMemory) = memoryMode match &#123;</span><br><span class="line">      //根据不同的内存模式去创建StorageMemoryPool和ExecutionMemoryPool</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; (</span><br><span class="line">        onHeapExecutionMemoryPool,</span><br><span class="line">        onHeapStorageMemoryPool,</span><br><span class="line">        maxOnHeapStorageMemory)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; (</span><br><span class="line">        offHeapExecutionMemoryPool,</span><br><span class="line">        offHeapStorageMemoryPool,</span><br><span class="line">        maxOffHeapMemory)</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; maxMemory) &#123;</span><br><span class="line">      // 若申请内存大于最大内存，则申请失败</span><br><span class="line">      logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; +</span><br><span class="line">        s&quot;memory limit ($maxMemory bytes)&quot;)</span><br><span class="line">      return false</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">      // 如果Storage内存池没有足够的内存，则向Execution内存池借用</span><br><span class="line">      val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//当Execution内存有空闲时，Storage才能借到内存</span><br><span class="line">      executionPool.decrementPoolSize(memoryBorrowedFromExecution)//缩小Execution内存</span><br><span class="line">      storagePool.incrementPoolSize(memoryBorrowedFromExecution)//增加Storage内存</span><br><span class="line">    &#125;</span><br><span class="line">    storagePool.acquireMemory(blockId, numBytes)</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireExecutionMemory：申请Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override private[memory] def acquireExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Long = synchronized &#123;//使用了synchronized关键字，调用acquireExecutionMemory方法可能会阻塞，直到Execution内存池有足够的内存。</span><br><span class="line">   ...</span><br><span class="line">    executionPool.acquireMemory(</span><br><span class="line">      numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>方法最后调用了ExecutionMemoryPool的acquireMemory方法，该方法的参数需要两个函数：maybeGrowExecutionPool()和computeMaxExecutionPoolSize()。<br>每个Task能够使用的内存被限制在pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks。其中maxPoolSize代表了execution pool的最大内存，poolSize表示当前这个pool的大小。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// ExecutionMemoryPool.scala</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">      val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br></pre></td></tr></table></figure><p></p><p>maybeGrowExecutionPool()方法实现了如何动态增加Execution内存区的大小。<br>在每次申请execution内存的同时，execution内存池会进行多次尝试，每次尝试都可能会回收一些存储内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">     def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">      if (extraMemoryNeeded &gt; 0) &#123;//如果申请的内存大于0</span><br><span class="line">        //计算execution可借到的storage内存，是storage剩余内存和可借出内存的最大值</span><br><span class="line">        val memoryReclaimableFromStorage = math.max(</span><br><span class="line">          storagePool.memoryFree,</span><br><span class="line">          storagePool.poolSize - storageRegionSize)</span><br><span class="line">        if (memoryReclaimableFromStorage &gt; 0) &#123;//如果可以申请到内存</span><br><span class="line">          val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">            math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//实际需要的内存，取实际需要的内存和storage内存区域全部可用内存大小的最小值</span><br><span class="line">          storagePool.decrementPoolSize(spaceToReclaim)//storage内存区域减少</span><br><span class="line">          executionPool.incrementPoolSize(spaceToReclaim)//execution内存区域增加</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽大数据-零基础学员深圳某司高薪面试题</title>
      <link href="/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><a id="more"></a><p>啥也不说！直接上题</p><p>面试时间：20180531</p><ul><li>简单说下hdfs读文件和写文件的流程</li><li>每天数据量有多大？生产集群规模有多大？</li><li>说几个spark开发中遇到的问题，和解决的方案</li><li>阐述一下最近开发的项目，以及担任的角色位置</li><li>kafka有做过哪些调优</li><li>我们项目中数据倾斜的场景和解决方案</li></ul><p>零基础➕四个月紧跟若泽大数据学习之后是这样</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从Hive中的stored as file_foramt看hive调优</title>
      <link href="/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/"/>
      <url>/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="一、行式数据库和列式数据库的对比"><a href="#一、行式数据库和列式数据库的对比" class="headerlink" title="一、行式数据库和列式数据库的对比"></a>一、行式数据库和列式数据库的对比</h4><h5 id="1、存储比较"><a href="#1、存储比较" class="headerlink" title="1、存储比较"></a>1、存储比较</h5><p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p><h5 id="2、压缩比较"><a href="#2、压缩比较" class="headerlink" title="2、压缩比较"></a>2、压缩比较</h5><a id="more"></a><p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p><p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p><h5 id="3、查询比较"><a href="#3、查询比较" class="headerlink" title="3、查询比较"></a>3、查询比较</h5><p>假设执行的查询操作是：select id,name from table_emp;</p><p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p><p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p><p>假设执行的查询操作是：select * from table_emp;</p><p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p><p><strong><font color="#FF4500">但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</font></strong></p><h4 id="二、stored-as-file-format-详解"><a href="#二、stored-as-file-format-详解" class="headerlink" title="二、stored as file_format 详解"></a>二、stored as file_format 详解</h4><h5 id="1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"><a href="#1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。" class="headerlink" title="1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"></a>1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tt (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CREATE TABLE tt2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;</span><br><span class="line"></span><br><span class="line">CREATE TABLE tt3 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS </span><br><span class="line">INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;</span><br><span class="line">OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;</span><br><span class="line"></span><br><span class="line">#以上三种方式存储的格式都是TEXTFILE。</span><br></pre></td></tr></table></figure><h5 id="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"><a href="#2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较" class="headerlink" title="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"></a>2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">file_format:</span><br><span class="line">  | SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p><strong>TEXTFILE:</strong> 只是hive中表数据默认的存储格式，它将所有类型的数据都存储为String类型，不便于数据的解析，但它却比较通用。不具备随机读写的能力。支持压缩。</p><p><strong>SEQUENCEFILE:</strong> 这种储存格式比TEXTFILE格式多了头部、标识、信息长度等信息，这些信息使得其具备随机读写的能力。支持压缩，但压缩的是value。（存储相同的数据，SEQUENCEFILE比TEXTFILE略大）</p><p><strong>RCFILE（Record Columnar File）:</strong> 现在水平上划分为很多个Row Group,每个Row Group默认大小4MB，Row Group内部再按列存储信息。由facebook开源，比标准行式存储节约10%的空间。</p><p><strong>ORC:</strong> 优化过后的RCFile,现在水平上划分为多个Stripes,再在Stripe中按列存储。每个Stripe由一个Index Data、一个Row Data、一个Stripe Footer组成。每个Stripes的大小为250MB，每个Index Data记录的是整型数据最大值最小值、字符串数据前后缀信息，每个列的位置等等诸如此类的信息。这就使得查询十分得高效，默认每一万行数据建立一个Index Data。ORC存储大小为TEXTFILE的40%左右，使用压缩则可以进一步将这个数字降到10%~20%。</p><p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure><p></p><p>The parameters are all placed in the TBLPROPERTIES (see Create Table). They are:</p><p>Key|Default|Notes<br>|-|-|-|<br>orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)<br>|orc.compress.size|262,144|number of bytes in each compression chunk<br>|orc.stripe.size|67,108,864|number of bytes in each stripe<br>|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)<br>|orc.create.index|true|whether to create row indexes<br>|orc.bloom.filter.columns |””| comma separated list of column names for which bloom filter should be created<br>|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</p><p>示例：创建带压缩的ORC存储表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table Addresses (</span><br><span class="line">  name string,</span><br><span class="line">  street string,</span><br><span class="line">  city string,</span><br><span class="line">  state string,</span><br><span class="line">  zip int</span><br><span class="line">) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure><p></p><p>PARQUET: 存储大小为TEXTFILE的60%~70%，压缩后在20%~30%之间。</p><hr><p>注意：</p><ol><li><p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p></li><li><p>如果要使用TEXTFILE作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“insert into table table_stored_file_ORC select <em>from table_t0;”创建。或者使用”create table as select </em>from table_t0;”创建。</p></li></ol><hr><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之序列化在生产中的应用</title>
      <link href="/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h4 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h4><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_1.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_2.png" alt="enter description here"></p><h4 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h4><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_3.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_4.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次。<br><img src="/assets/blogImg/529_5.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_6.png" alt="enter description here"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br><img src="/assets/blogImg/529_7.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>若泽数据带你随时了解业界面试题，随时跳高薪</title>
      <link href="/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/"/>
      <url>/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="链家-一面，二面"><a href="#链家-一面，二面" class="headerlink" title="链家(一面，二面)"></a>链家(一面，二面)</h4><p>0.自我介绍</p><p>1.封装继承多态概念</p><p>2.mvc设计思想</p><p>3.线程池,看过源码吗<br><a id="more"></a><br>4.ssh框架中分别对应mvc中那一层</p><p>5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）</p><p>6.spring ioc aop 原理</p><p>7.单利模式</p><p>8.SQL题，想不起来了。。</p><p>9.jvm 运行时数据区域</p><p>10.spring mvc知道吗。。</p><p>11.工厂模式</p><p>12.mr 计算流程</p><p>13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费 查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）</p><p>14.git的使用</p><p>15.hadoop的理解</p><p>16.hive内部表和外部表的区别</p><p>17.hive存储格式和压缩格式</p><p>18.对spark了解吗？ 当时高级班还没学。。</p><p>19.hive于关系型数据库的区别</p><p>20.各种排序 手写堆排序,说说原理</p><p>21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。</p><p>22中间也穿插了项目。</p><p>无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。</p><p>若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一次跳槽经历（阿里/美团/头条/网易/有赞...)</title>
      <link href="/2018/05/24/%E6%9C%89%E8%B5%9E...)/"/>
      <url>/2018/05/24/%E6%9C%89%E8%B5%9E...)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><h6 id="为啥跳槽"><a href="#为啥跳槽" class="headerlink" title="为啥跳槽"></a>为啥跳槽</h6><p>每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。<br><img src="//yoursite.com/2018/05/24/有赞...)/blogImg/tiaocao524.png" alt="enter description here"><br><a id="more"></a></p><h6 id="面试过程"><a href="#面试过程" class="headerlink" title="面试过程"></a>面试过程</h6><p>（先打个广告，有兴趣加入阿里的欢迎发简历至 <a href="mailto:zhangzb2007@gmail.com" target="_blank" rel="noopener">zhangzb2007@gmail.com</a>，或简书上给我发信息）<br>面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。</p><h6 id="上半场"><a href="#上半场" class="headerlink" title="上半场"></a>上半场</h6><ul><li><p>曹操专车<br>这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。</p></li><li><p>美亚柏科<br>估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。</p></li><li><p>有赞<br>绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。<br>这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。</p></li><li><p>字节跳动(今日头条)<br>HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。<br>一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。<br>二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。</p></li></ul><h6 id="下半场"><a href="#下半场" class="headerlink" title="下半场"></a>下半场</h6><p>一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。</p><ul><li><p>美团<br>这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。<br>两点半进去。<br>一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。<br>二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。<br>三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。<br>四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。<br>出来的时候已经是六点半。</p></li><li><p>网易<br>面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。<br>一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。<br>然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。</p></li><li><p>阿里<br>这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。<br>一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。<br>二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。<br>三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。<br>不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。</p></li></ul><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。<strong>整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。</strong> 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中自定义UDAF函数生产小案例</title>
      <link href="/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="一、UDAF-回顾"><a href="#一、UDAF-回顾" class="headerlink" title="一、UDAF 回顾"></a>一、UDAF 回顾</h4><ul><li>1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。</li><li>2.Hive有两种UDAF：简单和通用<br>简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。<br>通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。</li><li>3.一个计算函数必须实现的5个方法的具体含义如下：<br>init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。<br>iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。<br>terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。<br>merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。<br>terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。<h4 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h4>使用UDAF简单方式实现统计区域产品用户访问排名<a id="more"></a><h4 id="三、自定义UDAF函数代码实现"><a href="#三、自定义UDAF函数代码实现" class="headerlink" title="三、自定义UDAF函数代码实现"></a>三、自定义UDAF函数代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">package hive.org.ruozedata;</span><br><span class="line">import java.util.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">public class UserClickUDAF extends UDAF &#123;</span><br><span class="line">    // 日志对象初始化</span><br><span class="line">    public static Logger logger = Logger.getLogger(UserClickUDAF.class);</span><br><span class="line">    // 静态类实现UDAFEvaluator</span><br><span class="line">    public static class Evaluator implements UDAFEvaluator &#123;</span><br><span class="line">        // 设置成员变量，存储每个统计范围内的总记录数</span><br><span class="line">        private static Map&lt;String, String&gt; courseScoreMap;</span><br><span class="line">        private static Map&lt;String, String&gt; city_info;</span><br><span class="line">        private static Map&lt;String, String&gt; product_info;</span><br><span class="line">        private static Map&lt;String, String&gt; user_click;</span><br><span class="line">        //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span><br><span class="line">        public Evaluator() &#123;</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line">        // 初始化函数间传递的中间变量</span><br><span class="line">        public void init() &#123;</span><br><span class="line">            courseScoreMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">            city_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">            product_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">        //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出</span><br><span class="line">        public boolean iterate(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (pcid == null || pcname == null || pccount == null) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (pccount.equals(&quot;-1&quot;)) &#123;</span><br><span class="line">                // 城市表</span><br><span class="line">                city_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (pccount.equals(&quot;-2&quot;)) &#123;</span><br><span class="line">                // 产品表</span><br><span class="line">                product_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // 处理用户点击关联</span><br><span class="line">                unionCity_Prod_UserClic1(pcid, pcname, pccount);</span><br><span class="line">           &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 处理用户点击关联</span><br><span class="line">        private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (product_info.containsKey(pcid)) &#123;</span><br><span class="line">                if (city_info.containsKey(pcname)) &#123;</span><br><span class="line">                    String city_name = city_info.get(pcname);</span><br><span class="line">                    String prod_name = product_info.get(pcid);</span><br><span class="line">                    String cp_name = city_name + prod_name;</span><br><span class="line">                    // 如果之前已经Put过Key值为区域信息，则把记录相加处理</span><br><span class="line">                    if (courseScoreMap.containsKey(cp_name)) &#123;</span><br><span class="line">                        int pcrn = 0;</span><br><span class="line">                        String strTemp = courseScoreMap.get(cp_name);</span><br><span class="line">                        String courseScoreMap_pn </span><br><span class="line">                         = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim();</span><br><span class="line">                        pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn);</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn));</span><br><span class="line">                    &#125;</span><br><span class="line">                    else &#123;</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput</span><br><span class="line">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span><br><span class="line">         */</span><br><span class="line">        public Map&lt;String, String&gt; terminatePartial() &#123;</span><br><span class="line">            return courseScoreMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span><br><span class="line">        public boolean merge(Map&lt;String, String&gt; mapOutput) &#123;</span><br><span class="line">            this.courseScoreMap.putAll(mapOutput);</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span><br><span class="line">        public String terminate() &#123;</span><br><span class="line">            return courseScoreMap.toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="四、创建hive中的临时函数"><a href="#四、创建hive中的临时函数" class="headerlink" title="四、创建hive中的临时函数"></a>四、创建hive中的临时函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION user_click;</span><br><span class="line">add jar /data/hive_udf-1.0.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;</span><br></pre></td></tr></table></figure><h4 id="五、调用自定义UDAF函数处理数据"><a href="#五、调用自定义UDAF函数处理数据" class="headerlink" title="五、调用自定义UDAF函数处理数据"></a>五、调用自定义UDAF函数处理数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from (</span><br><span class="line">  select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from (</span><br><span class="line">    select * from (</span><br><span class="line">      select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info</span><br><span class="line">      union all</span><br><span class="line">      select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info</span><br><span class="line">      union all</span><br><span class="line">      select count(1) as type,</span><br><span class="line">             product_id as pcid,</span><br><span class="line">             city_id as pcname</span><br><span class="line">        from user_click</span><br><span class="line">       where action_time=&apos;2016-05-05&apos;</span><br><span class="line">      group by product_id,city_id</span><br><span class="line">    ) a</span><br><span class="line">  order by type) b</span><br><span class="line">) c ;</span><br></pre></td></tr></table></figure><h4 id="六、创建Hive临时外部表"><a href="#六、创建Hive临时外部表" class="headerlink" title="六、创建Hive临时外部表"></a>六、创建Hive临时外部表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table tmp1(</span><br><span class="line">city_name string,</span><br><span class="line">product_name string,</span><br><span class="line">rn string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">location &apos;/works/tmp1&apos;;</span><br></pre></td></tr></table></figure><h4 id="七、统计最终区域前3产品排名"><a href="#七、统计最终区域前3产品排名" class="headerlink" title="七、统计最终区域前3产品排名"></a>七、统计最终区域前3产品排名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">select city_name,</span><br><span class="line">       product_name,</span><br><span class="line">       floor(sum(rn)) visit_num,</span><br><span class="line">       row_number()over(partition by city_name order by sum(rn) desc) rn,</span><br><span class="line">       &apos;2016-05-05&apos; action_time</span><br><span class="line">  from tmp1 </span><br><span class="line"> group by city_name,product_name</span><br><span class="line">) a where rn &lt;=3 ;</span><br></pre></td></tr></table></figure><h4 id="八、最终结果"><a href="#八、最终结果" class="headerlink" title="八、最终结果"></a>八、最终结果</h4><p><img src="/assets/blogImg/hive523.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 基本概念</title>
      <link href="/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><strong>基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）</strong><br><a id="more"></a></p><h5 id="spark名词解释"><a href="#spark名词解释" class="headerlink" title="spark名词解释"></a>spark名词解释</h5><ul><li><p>Application jar：应用程序jar包<br>包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；</p></li><li><p>Driver Program：<br>这个进程运行应用程序的 main 方法并且新建 SparkContext ；</p></li><li><p>Cluster Manager：集群管理者<br>在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）</p></li><li><p>Deploy mode：部署模式<br>告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；</p></li><li><p>Worker Node：工作节点<br>集群中任何可以运行应用代码的节点；（yarn上就是node manager）</p></li><li><p>Executor：<br>在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；</p></li><li><p>Task：任务<br>被送到某个 executor 上执行的工作单元；</p></li><li><p>Job：<br>包含很多并行计算的task。一个 action 就会产生一个job；</p></li><li><p>Stage：<br>一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。</p></li></ul><h5 id="spark工作流程"><a href="#spark工作流程" class="headerlink" title="spark工作流程"></a>spark工作流程</h5><p>1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。</p><p>spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。<br>如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行</p><ul><li>1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。</li><li>2、spark并不关心底层的集群管理。</li><li>3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。</li><li>4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark History Server Web UI配置</title>
      <link href="/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h5 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd /opt/app/spark/conf</span><br><span class="line">[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /Found 3 items</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line">drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 items</span><br><span class="line">drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog</span><br></pre></td></tr></table></figure><p>在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息</p><h5 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir             hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br></pre></td></tr></table></figure><p>spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建<br>spark.yarn.historyServer.address : Spark history server的地址(不加http://).<br>这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</p><h5 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 conf]# vi spark-env.sh</span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h5 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01  ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer</span><br><span class="line">[root@hadoop01  ~]# ps -ef|grep sparkroot     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop01  ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java</span><br></pre></td></tr></table></figure><p>以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark不得不理解的重要概念——从源码角度看RDD</title>
      <link href="/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
      <url>/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1.RDD是什么"></a>1.RDD是什么</h4><p>Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合</p><h4 id="2-RDD五大特性"><a href="#2-RDD五大特性" class="headerlink" title="2.RDD五大特性"></a>2.RDD五大特性</h4><a id="more"></a><ol><li><p>A list of partitions<br>每个rdd有多个分区<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>计算作用到每个分区<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rdd之间存在依赖（RDD的血缘关系）如：<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选，默认哈希的分区<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>源码来自github。</p><h4 id="3-如何创建RDD"><a href="#3-如何创建RDD" class="headerlink" title="3.如何创建RDD"></a>3.如何创建RDD</h4><p>创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfile（）</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><p><strong>源码总结：<br>1）.取_2是因为数据为（key（偏移量），value（数据））</strong></p><h4 id="4-常见的transformation和action"><a href="#4-常见的transformation和action" class="headerlink" title="4.常见的transformation和action"></a>4.常见的transformation和action</h4><p>由于比较简单，大概说一下常用的用处，不做代码测试</p><p>transformation</p><ul><li>Map：对数据集的每一个元素进行操作</li><li>FlatMap：先对数据集进行扁平化处理，然后再Map</li><li>Filter：对数据进行过滤，为true则通过</li><li>destinct：去重操作</li></ul><p>action</p><ul><li>reduce：对数据进行聚集</li><li>reduceBykey：对key值相同的进行操作</li><li>collect：没有效果的action，但是很有用</li><li>saveAstextFile：数据存入文件系统</li><li>foreach：对每个元素进行func的操作</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>美味不用等大数据面试题(201804月)</title>
      <link href="/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/"/>
      <url>/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h6 id="1-若泽大数据线下班，某某某的小伙伴现场面试题截图"><a href="#1-若泽大数据线下班，某某某的小伙伴现场面试题截图" class="headerlink" title="1.若泽大数据线下班，某某某的小伙伴现场面试题截图:"></a><strong>1.若泽大数据线下班，某某某的小伙伴现场面试题截图:</strong></h6><a id="more"></a><p><img src="/assets/blogImg/520_1.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_2.png" alt="enter description here"></p><h6 id="2-分享另外1家的忘记名字公司的大数据面试题："><a href="#2-分享另外1家的忘记名字公司的大数据面试题：" class="headerlink" title="2.分享另外1家的忘记名字公司的大数据面试题："></a><strong>2.分享另外1家的忘记名字公司的大数据面试题：</strong></h6><p><img src="/assets/blogImg/520_3.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 面试真题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD、DataFrame和DataSet的区别</title>
      <link href="/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br>在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！</font><h5 id="一-、共性"><a href="#一-、共性" class="headerlink" title="一 、共性"></a>一 、共性</h5><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p><p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</p><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>4、三者都有partition的概念。<br><a id="more"></a></p><h5 id="二、RDD优缺点"><a href="#二、RDD优缺点" class="headerlink" title="二、RDD优缺点"></a>二、RDD优缺点</h5><p><strong>优点：</strong></p><ul><li><p>1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</p></li><li><p>2、面向对象的编程风格</p></li><li><p>3、编译时类型安全，编译时就能检查出类型错误</p></li></ul><p><strong>缺点：</strong></p><ul><li><p>1、序列化和反序列化的性能开销</p></li><li><p>2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p></li></ul><h5 id="三、DataFrame"><a href="#三、DataFrame" class="headerlink" title="三、DataFrame"></a>三、DataFrame</h5><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreach&#123;</span><br><span class="line">  x =&gt;</span><br><span class="line">    val v1=x.getAs[String](&quot;v1&quot;)</span><br><span class="line">    val v2=x.getAs[String](&quot;v2&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>2、DataFrame引入了schema和off-heap</p><ul><li><p>schema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.</p></li><li><p>off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.</p></li><li><p>off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.</p></li></ul><p>3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表</p><p>4、兼容Hive，支持Hql、UDF</p><p><strong>有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.</strong></p><h5 id="四、DataSet"><a href="#四、DataSet" class="headerlink" title="四、DataSet"></a>四、DataSet</h5><p>1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。</p><p>2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。</p><p>3、Dataset<row>等同于DataFrame（Spark 2.X）</row></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决</title>
      <link href="/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
      <url>/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h5 id="一-数据源同步中间件："><a href="#一-数据源同步中间件：" class="headerlink" title="一.数据源同步中间件："></a>一.数据源同步中间件：</h5><p>Canal<br><a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">https://github.com/alibaba/canal</a><br><a href="https://github.com/Hackeruncle/syncClient" target="_blank" rel="noopener">https://github.com/Hackeruncle/syncClient</a></p><p>Maxwell<br><a href="https://github.com/zendesk/maxwell" target="_blank" rel="noopener">https://github.com/zendesk/maxwell</a><br><img src="/assets/blogImg/514_1.png" alt="maxwell"><br><a id="more"></a></p><h5 id="二-架构使用"><a href="#二-架构使用" class="headerlink" title="二.架构使用"></a>二.架构使用</h5><p>MySQL —- 中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra 增量的<br>a.全量 bootstrap<br>b.增量</p><h6 id="1-对比"><a href="#1-对比" class="headerlink" title="1.对比"></a>1.对比</h6><table><thead><tr><th></th><th></th><th>Canal(服务端)</th><th>Maxwell(服务端+客户端)</th></tr></thead><tbody><tr><td>语言</td><td>Java</td><td>Java</td><td></td></tr><tr><td>活跃度</td><td>活跃</td><td>活跃</td><td></td></tr><tr><td>HA</td><td>支持</td><td>定制 但是支持断点还原功能</td></tr><tr><td>数据落地</td><td>定制</td><td>落地到kafka</td></tr><tr><td>分区</td><td>支持</td><td>支持</td></tr><tr><td>bootstrap(引导)</td><td>不支持</td><td>支持</td></tr><tr><td>数据格式</td><td>格式自由</td><td>json(格式固定) spark json–&gt;DF</td></tr><tr><td>文档</td><td>较详细</td><td>较详细</td><td></td></tr><tr><td>随机读</td><td>支持</td><td>支持</td><td></td></tr></tbody></table><p><strong>个人选择Maxwell</strong></p><p>a.服务端+客户端一体，轻量级的<br>b.支持断点还原功能+bootstrap+json<br>Can do SELECT * from table (bootstrapping) initial loads of a table.<br>supports automatic position recover on master promotion<br>flexible partitioning schemes for Kakfa - by database, table, primary key, or column<br>Maxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).</p><h6 id="2-官网解读"><a href="#2-官网解读" class="headerlink" title="2.官网解读"></a>2.官网解读</h6><p><a href="https://www.bilibili.com/video/av34778187?from=search&amp;seid=18393822973469412185" target="_blank" rel="noopener">B站视频</a></p><h6 id="3-部署"><a href="#3-部署" class="headerlink" title="3.部署"></a>3.部署</h6><p><strong>3.1 MySQL Install</strong><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a><br><a href="https://ke.qq.com/course/262452?tuin=11cffd50" target="_blank" rel="noopener">https://ke.qq.com/course/262452?tuin=11cffd50</a></p><p><strong>3.2 修改</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">binlog_format=row</span><br><span class="line"></span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line">3.3 创建Maxwell的db和用户</span><br><span class="line">mysql&gt; create database maxwell;</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4解压</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz</span><br></pre></td></tr></table></figure><p></p><p><strong>3.5测试STDOUT:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --user=&apos;maxwell&apos; \</span><br><span class="line">--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \</span><br><span class="line">--producer=stdout</span><br></pre></td></tr></table></figure><p></p><p>测试1：insert sql：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br></pre></td></tr></table></figure><p></p><p>maxwell输出：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;insert&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959044,</span><br><span class="line">    &quot;xid&quot;: 201,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>测试1：update sql:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update ruozedata set age=29 where id=999;</span><br></pre></td></tr></table></figure><p></p><p><strong>问题: ROW，你觉得binlog更新几个字段？</strong></p><p>maxwell输出：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;update&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959208,</span><br><span class="line">    &quot;xid&quot;: 255,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 29,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;old&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="4-其他注意点和新特性"><a href="#4-其他注意点和新特性" class="headerlink" title="4.其他注意点和新特性"></a>4.其他注意点和新特性</h6><p><strong>4.1 kafka_version 版本</strong><br>Using kafka version: 0.11.0.1 0.10<br>jar:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 kafka-clients]# ll</span><br><span class="line">total 4000</span><br><span class="line">-rw-r--r--. 1 ruoze games  746207 May  8 06:34 kafka-clients-0.10.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  951041 May  8 06:35 kafka-clients-0.10.2.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games 1419544 May  8 06:35 kafka-clients-0.11.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  324016 May  8 06:34 kafka-clients-0.8.2.2.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  641408 May  8 06:34 kafka-clients-0.9.0.1.jar</span><br><span class="line">[root@hadoop000 kafka-clients]#</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 其他组件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级 </tag>
            
            <tag> maxwell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on YARN-Cluster和YARN-Client的区别</title>
      <link href="/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h5 id="一-YARN-Cluster和YARN-Client的区别"><a href="#一-YARN-Cluster和YARN-Client的区别" class="headerlink" title="一. YARN-Cluster和YARN-Client的区别"></a>一. YARN-Cluster和YARN-Client的区别</h5><p><img src="/assets/blogImg/512_1.png" alt><br>（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；<br>（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；<br>（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。<br><a id="more"></a></p><h5 id="二-yarn-client-模式"><a href="#二-yarn-client-模式" class="headerlink" title="二. yarn client 模式"></a>二. yarn client 模式</h5><p><img src="/assets/blogImg/512_2.png" alt></p><p><font color="#FF4200">yarn-client 模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。<br></font></p><h5 id="三-yarn-cluster-模式"><a href="#三-yarn-cluster-模式" class="headerlink" title="三.yarn  cluster 模式"></a>三.yarn cluster 模式</h5><p><img src="/assets/blogImg/512_3.png" alt></p><p><font color="#FF4200">yarn-cluster 模式的话， client 关闭是可以提交任务的 ，<br></font></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p><strong>1.spark-shell/spark-sql 只支持 yarn-client模式；<br>2.spark-submit对于两种模式都支持。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产改造Spark1.6源代码，create table语法支持Oracle列表分区</title>
      <link href="/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/"/>
      <url>/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h5 id="1-需求"><a href="#1-需求" class="headerlink" title="1.需求"></a>1.需求</h5><p>通过Spark SQL JDBC 方法，抽取Oracle表数据。</p><h5 id="2-问题"><a href="#2-问题" class="headerlink" title="2.问题"></a>2.问题</h5><p>大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。<br>参考 <a href="http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases" target="_blank" rel="noopener">http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases</a><br><a id="more"></a></p><h5 id="3-Oracle的分区"><a href="#3-Oracle的分区" class="headerlink" title="3.Oracle的分区"></a>3.Oracle的分区</h5><h6 id="3-1列表分区"><a href="#3-1列表分区" class="headerlink" title="3.1列表分区:"></a>3.1列表分区:</h6><p>该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。<br>例一:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PROBLEM_TICKETS</span><br><span class="line">(</span><br><span class="line">PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),</span><br><span class="line">CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,</span><br><span class="line">STATUS VARCHAR2(20)</span><br><span class="line">)</span><br><span class="line">PARTITION BY LIST (STATUS)</span><br><span class="line">(</span><br><span class="line">PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,</span><br><span class="line">PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h6 id="3-2散列分区"><a href="#3-2散列分区" class="headerlink" title="3.2散列分区:"></a>3.2散列分区:</h6><p>这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。<br>例一:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE HASH_TABLE</span><br><span class="line">(</span><br><span class="line">COL NUMBER(8),</span><br><span class="line">INF VARCHAR2(100) </span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (COL)</span><br><span class="line">(</span><br><span class="line">PARTITION PART01 TABLESPACE HASH_TS01, </span><br><span class="line">PARTITION PART02 TABLESPACE HASH_TS02, </span><br><span class="line">PARTITION PART03 TABLESPACE HASH_TS03</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><h5 id="4-改造"><a href="#4-改造" class="headerlink" title="4.改造"></a>4.改造</h5><p>蓝色代码是改造Spark源代码,加课程顾问领取PDF。</p><h6 id="1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。"><a href="#1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。" class="headerlink" title="1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。"></a>1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE TBLS_IN</span><br><span class="line">USING org.apache.spark.sql.jdbc OPTIONS (</span><br><span class="line">driver &quot;com.mysql.jdbc.Driver&quot;,</span><br><span class="line">url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,</span><br><span class="line">fetchSize &quot;1000&quot;,</span><br><span class="line">partitionColumn &quot;TBL_ID&quot;,</span><br><span class="line">numPartitions &quot;null&quot;,</span><br><span class="line">lowerBound &quot;null&quot;,</span><br><span class="line">upperBound &quot;null&quot;,</span><br><span class="line">user &quot;hive2user&quot;,</span><br><span class="line">password &quot;hive2user&quot;,</span><br><span class="line">partitionInRule &quot;1|15,16,18,19|20,21&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation"><a href="#2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation" class="headerlink" title="2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation"></a>2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">sqlContext: SQLContext,</span><br><span class="line">parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))</span><br><span class="line">val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)</span><br><span class="line">var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)</span><br><span class="line">var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)</span><br><span class="line"></span><br><span class="line">// add partition in rule</span><br><span class="line">val partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)</span><br><span class="line">// validind all the partition in rule </span><br><span class="line">if (partitionColumn != null</span><br><span class="line">&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)</span><br><span class="line">&amp;&amp; partitionInRule == null </span><br><span class="line">)&#123;</span><br><span class="line">   sys.error(&quot;Partitioning incompletely specified&quot;) </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val partitionInfo = </span><br><span class="line">if (partitionColumn == null) &#123; </span><br><span class="line">    null</span><br><span class="line">&#125; else &#123;</span><br><span class="line"></span><br><span class="line">val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123;</span><br><span class="line">val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot;</span><br><span class="line">upperBound = &quot;0&quot;</span><br><span class="line">inGroups &#125;</span><br><span class="line">else&#123;</span><br><span class="line">Array[String]() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JDBCPartitioningInfo( partitionColumn, </span><br><span class="line">lowerBound.toLong, </span><br><span class="line">upperBound.toLong, </span><br><span class="line">numPartitions.toInt, </span><br><span class="line">inPartitions)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val parts = JDBCRelation.columnPartition(partitionInfo)</span><br><span class="line">val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))</span><br><span class="line">// parameters is immutable</span><br><span class="line">if(numPartitions != null)&#123;</span><br><span class="line">properties.put(&quot;numPartitions&quot; , numPartitions) &#125;</span><br><span class="line">JDBCRelation(url, table, parts, properties)(sqlContext)</span><br><span class="line"></span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition"><a href="#3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition" class="headerlink" title="3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition"></a>3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;</span><br><span class="line">if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))</span><br><span class="line">val column = partitioning.column</span><br><span class="line">var i: Int = 0</span><br><span class="line">var ans = new ArrayBuffer[Partition]()</span><br><span class="line"></span><br><span class="line">// partition by long if(partitioning.inPartitions.length == 0)&#123;</span><br><span class="line"></span><br><span class="line">val numPartitions = partitioning.numPartitions</span><br><span class="line">if (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.</span><br><span class="line">// Here we get a little roundoff, but that&apos;s (hopefully) OK.</span><br><span class="line">val stride: Long = (partitioning.upperBound / numPartitions</span><br><span class="line"></span><br><span class="line">- partitioning.lowerBound / numPartitions)</span><br><span class="line">var currentValue: Long = partitioning.lowerBound</span><br><span class="line">while (i &lt; numPartitions) &#123;</span><br><span class="line">val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else null</span><br><span class="line">currentValue += stride</span><br><span class="line">val upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =</span><br><span class="line"></span><br><span class="line">if (upperBound == null) &#123; </span><br><span class="line">  lowerBound</span><br><span class="line"></span><br><span class="line">&#125; else if (lowerBound == null) &#123; </span><br><span class="line">  upperBound</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">  s&quot;$lowerBound AND $upperBound&quot; </span><br><span class="line">&#125;</span><br><span class="line">  ans += JDBCPartition(whereClause, i)</span><br><span class="line">   i= i+ 1 &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// partition by in </span><br><span class="line">else&#123;</span><br><span class="line">    while(i &lt; partitioning.inPartitions.length)&#123;</span><br><span class="line">           val inContent = partitioning.inPartitions(i)</span><br><span class="line">           val whereClause = s&quot;$column in ($inContent)&quot; </span><br><span class="line">           ans += JDBCPartition(whereClause, i)</span><br><span class="line">           i= i+ 1</span><br><span class="line">     &#125; </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ans.toArray </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="4-对外方法org-apache-spark-sql-SQLContext-方法jdbc"><a href="#4-对外方法org-apache-spark-sql-SQLContext-方法jdbc" class="headerlink" title="4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc"></a>4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">url: String,</span><br><span class="line">table: String,</span><br><span class="line">columnName: String,</span><br><span class="line">lowerBound: Long,</span><br><span class="line">upperBound: Long,</span><br><span class="line">numPartitions: Int,</span><br><span class="line">inPartitions: Array[String] = Array[String]()</span><br><span class="line"></span><br><span class="line">): DataFrame = &#123;</span><br><span class="line">read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产中Hive静态和动态分区表，该怎样抉择呢？</title>
      <link href="/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/"/>
      <url>/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h6 id="一-需求"><a href="#一-需求" class="headerlink" title="一.需求"></a>一.需求</h6><p>按照不同部门作为分区，导数据到目标表</p><h6 id="二-使用静态分区表来完成"><a href="#二-使用静态分区表来完成" class="headerlink" title="二.使用静态分区表来完成"></a>二.使用静态分区表来完成</h6><p>71.创建静态分区表：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table emp_static_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><p>2.插入数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_static_partition partition(deptno=10)</span><br><span class="line">     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><p>3.查询数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_static_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_1.png" alt></p><h6 id="三-使用动态分区表来完成"><a href="#三-使用动态分区表来完成" class="headerlink" title="三.使用动态分区表来完成"></a>三.使用动态分区表来完成</h6><p>1.创建动态分区表：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table emp_dynamic_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的</font><p>2.插入数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_dynamic_partition partition(deptno)     </span><br><span class="line">select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br></pre></td></tr></table></figure><p></p><font color="#FF4500">【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where</font><p>需要设置属性的值：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；</span><br></pre></td></tr></table></figure><p></p><p>假如不设置，报错如下:<br><img src="/assets/blogImg/0506_2.png" alt><br>3.查询数据：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_dynamic_partition;</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/0506_3.png" alt></p><p><font color="#FF4500">分区列为deptno，实现了动态分区</font></p><h6 id="四-总结"><a href="#四-总结" class="headerlink" title="四.总结"></a>四.总结</h6><p>在生产上我们更倾向是选择<strong>动态分区</strong>，<br>无需手工指定数据导入的具体分区，<br>而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用</title>
      <link href="/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><img src="/assets/blogImg/504_1.png" alt><br><a id="more"></a></p><h6 id="1-介绍："><a href="#1-介绍：" class="headerlink" title="1. 介绍："></a>1. 介绍：</h6><p>两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，<br>客户端可以在不启动CLI的情况下对Hive中的数据进行操作，<br>两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果<br>（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。</p><p>HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，<br>而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？<br>这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，<br>不能通过修改HiveServer的代码修正。</p><p>因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。<br>HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。</p><h6 id="2-配置参数"><a href="#2-配置参数" class="headerlink" title="2.配置参数"></a>2.配置参数</h6><p>Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：<br>参数 | 含义 |<br>-|-|<br>hive.server2.thrift.min.worker.threads| 最小工作线程数，默认为5。<br>hive.server2.thrift.max.worker.threads| 最小工作线程数，默认为500。<br>hive.server2.thrift.port| TCP 的监听端口，默认为10000。<br>hive.server2.thrift.bind.host| TCP绑定的主机，默认为localhost</p><p>配置监听端口和路径<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;192.168.48.130&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="3-启动hiveserver2"><a href="#3-启动hiveserver2" class="headerlink" title="3. 启动hiveserver2"></a>3. 启动hiveserver2</h6><p>使用hadoop用户启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/</span><br><span class="line">[hadoop@hadoop001 bin]$ hiveserver2 </span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br></pre></td></tr></table></figure><p></p><h6 id="4-重新开个窗口，使用beeline方式连接"><a href="#4-重新开个窗口，使用beeline方式连接" class="headerlink" title="4. 重新开个窗口，使用beeline方式连接"></a>4. 重新开个窗口，使用beeline方式连接</h6><ul><li>-n 指定机器登陆的名字，当前机器的登陆用户名</li><li>-u 指定一个连接串</li><li>每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK</li><li>如果命令错误，hiveserver2那个窗口就会抛出异常</li></ul><p>使用hadoop用户启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoop</span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br><span class="line">scan complete in 4ms</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.7.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.7.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.7.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt;</span><br></pre></td></tr></table></figure><p></p><p>使用SQL<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show databases;</span><br><span class="line">INFO  : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</span><br><span class="line">INFO  : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">1 row selected</span><br></pre></td></tr></table></figure><p></p><h6 id="5-使用编写java代码方式连接"><a href="#5-使用编写java代码方式连接" class="headerlink" title="5.使用编写java代码方式连接"></a>5.使用编写java代码方式连接</h6><p><strong>5.1</strong>使用maven构建项目，pom.xml文件如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-train&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive-train&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;    </span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>5.2</strong>JdbcApp.java文件代码:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class JdbcApp &#123;</span><br><span class="line">     private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">         &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">             // TODO Auto-generated catch block</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">             System.exit(1);</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;);</span><br><span class="line">         Statement stmt = con.createStatement();</span><br><span class="line">         //select table:ename</span><br><span class="line">         String tableName = &quot;emp&quot;;</span><br><span class="line">         String sql = &quot;select ename from &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">          while(res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1));</span><br><span class="line">         &#125;</span><br><span class="line">         // describe table</span><br><span class="line">         sql = &quot;describe &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         res = stmt.executeQuery(sql);</span><br><span class="line">         while (res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2min快速了解，Hive内部表和外部表</title>
      <link href="/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
      <url>/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p><font color="#FF4500"><br></font><br>在了解内部表和外部表区别前，<br>我们需要先了解一下<strong>Hive架构</strong> ：</p><p><img src="/assets/blogImg/501_1.png" alt="Hive架构"><br><a id="more"></a><br>大家可以简单看一下这个架构图，我介绍其中要点：<br>Hive的数据分为两种，<strong>一种为普通数据，一种为元数据。</strong></p><ol><li>元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。</li><li>Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。</li></ol><p>下面我们来介绍表的两种类型：内部表和外部表</p><ol><li><p>内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。</p></li><li><p>外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表<br>而内部表和外部表的主要区别就是</p><ul><li>内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；</li><li>外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；</li></ul></li></ol><h6 id="1-准备数据-按tab键制表符作为字段分割符"><a href="#1-准备数据-按tab键制表符作为字段分割符" class="headerlink" title="1.准备数据:  按tab键制表符作为字段分割符"></a>1.准备数据: 按tab键制表符作为字段分割符</h6><pre><code>cat /tmp/ruozedata.txt1   jepson  32  1102   ruoze   22  1123   www.ruozedata.com   18  120</code></pre><h6 id="2-内部表测试："><a href="#2-内部表测试：" class="headerlink" title="2.内部表测试："></a>2.内部表测试：</h6><ol><li><p>在Hive里面创建一个表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table ruozedata(id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tele string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.759 seconds</span><br></pre></td></tr></table></figure></li><li><p>这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>内部表删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table ruozedata;</span><br></pre></td></tr></table></figure></li></ol><h6 id="3-外部表测试"><a href="#3-外部表测试" class="headerlink" title="3.外部表测试:"></a>3.外部表测试:</h6><ol><li>创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table exter_ruozedata(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tel string)</span><br><span class="line">    &gt; location &apos;/hive/external&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.098 seconds</span><br></pre></td></tr></table></figure></li></ol><p>创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径<br>（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）</p><ol start="2"><li><p>外部表导入数据和内部表一样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table exter_ruozedata;</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谈谈我和大数据的情缘及入门</title>
      <link href="/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>&#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。</p><p>&#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。<br><a id="more"></a><br>&#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。</p><p>&#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。</p><p>&#8195;<strong>后来这样的进度太慢了</strong>，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。</p><p>&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。</p><p>&#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。<strong>然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。</strong></p><p>&#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！</p><p><strong>以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。</strong><br><strong>1. 心态要端正。</strong><br>既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。<br>后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。</p><p><strong>2. 心目中要有计划。</strong><br>先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，<br>然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。</p><p><strong>3. 各种方式学习。</strong><br>QQ群，博客，上下班看技术文章，选择好的老师和课程培训，</p><p><font color="#FF4500"><br>(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)</font><br>可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。</p><p><strong>4. 项目经验。</strong><br>很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，<br>这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。<br>而面试，就看看其他人面试分享，学习他人。</p><p><strong>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 有缘大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义函数(UDF)的部署使用，你会吗？</title>
      <link href="/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Hive自定义函数(UDF)的部署使用，你会吗，三种方式！<br><a id="more"></a></p><font color="#FF4500"><br></font><h6 id="一-临时函数"><a href="#一-临时函数" class="headerlink" title="一.临时函数"></a>一.临时函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>添加jar包<br>hive&gt;add xxx.jar jar_filepath;</li><li>查看jar包<br>hive&gt;list jars;</li><li>创建临时函数<br>hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;</li></ol><h6 id="二-持久函数"><a href="#二-持久函数" class="headerlink" title="二.持久函数"></a>二.持久函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>上传到HDFS<br>$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jar</li><li>创建持久函数<br>hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;</li></ol><p><strong>注意点：</strong></p><ul><li><ol><li>此方法在show functions时是看不到的，但是可以使用</li></ol></li><li><ol start="2"><li>需要上传至hdfs</li></ol></li></ul><h6 id="三-持久函数，并注册"><a href="#三-持久函数，并注册" class="headerlink" title="三.持久函数，并注册"></a>三.持久函数，并注册</h6><p>环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>下载源码<br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a></p></li><li><p>解压源码<br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>将HelloUDF.java文件增加到HIVE源码中<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>修改FunctionRegistry.java 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>重新编译<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>编译结果全部为：BUILD SUCCESS<br>文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>配置hive环境<br>配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：<br>7.1. 全部配置：参照之前文档 <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hive全网最详细的编译及部署</a></p><p>7.2. 将编译后带UDF函数的包复制到旧hive环境<br>到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉<br>命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>最终启动hive</p></li><li><p>测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- 能查看到有 helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudf函数生效</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义函数(UDF)的编程开发，你会吗？</title>
      <link href="/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>本地开发环境：IntelliJ IDEA+Maven3.3.9<br><a id="more"></a></p><h6 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h6><p>打开IntelliJ IDEA<br>File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h6><p>在工程中找到pom.xml文件，添加hadoop、hive依赖<br><img src="/assets/blogImg/425hive1.png" alt="Hive图1"></p><h6 id="3-创建类、并编写一个HelloUDF-java，代码如下："><a href="#3-创建类、并编写一个HelloUDF-java，代码如下：" class="headerlink" title="3. 创建类、并编写一个HelloUDF.java，代码如下："></a>3. 创建类、并编写一个HelloUDF.java，代码如下：</h6><p><img src="/assets/blogImg/425hive2.png" alt="Hive图2"></p><p><strong>首先一个UDF必须满足下面两个条件:</strong></p><ul><li><ol><li>一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）</li></ol></li><li><ol start="2"><li>一个UDF必须至少实现了evaluate()方法</li></ol></li></ul><h6 id="4-测试，右击运行run-‘HelloUDF-main-’"><a href="#4-测试，右击运行run-‘HelloUDF-main-’" class="headerlink" title="4. 测试，右击运行run ‘HelloUDF.main()’"></a>4. 测试，右击运行run ‘HelloUDF.main()’</h6><h6 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h6><p>在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包<br>执行成功后在日志中找：<br><font color="#FF4500">[INFO] Building jar: (路径)/hive-1.0.jar</font></p><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive DDL，你真的了解吗？</title>
      <link href="/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:23 GMT+0800 (GMT+08:00) --><p>若泽大数据，带你全面剖析Hive DDL！</p><font color="#FF4500"><br></font><p><img src="/assets/blogImg/hive424.png" alt="Hive架构图"><br><a id="more"></a></p><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p><strong>Database</strong><br>Hive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）</p><p><strong>Table</strong><br>Hive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table</p><p><strong>Partition</strong><br>分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：<br>/user/hadoop/hive/warehouse/[databasename.db]/table</p><h5 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h5><p><strong>Create Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><p></p><p>IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。<br>COMMENT：数据库的描述<br>LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下<br>WITH DBPROPERTIES：数据库的属性</p><p><strong>Drop Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name </span><br><span class="line">[RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><p></p><p>RESTRICT：默认是restrict，如果该数据库还有表存在则报错；<br>CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。</p><p><strong>Alter Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>Use Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br></pre></td></tr></table></figure><p></p><p><strong>Show Databases</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;</span><br><span class="line">“ | ”：可以选择其中一种</span><br><span class="line"></span><br><span class="line">“[ ]”：可选项</span><br><span class="line"></span><br><span class="line">LIKE ‘identifier_with_wildcards’：模糊查询数据库</span><br></pre></td></tr></table></figure><p></p><p><strong>Describe Database</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE DATABASE [EXTENDED] db_name;</span><br><span class="line">DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；</span><br><span class="line">EXTENDED：加上数据库键值对的属性信息。</span><br><span class="line">hive&gt; describe database default;</span><br><span class="line">OK</span><br><span class="line">default    Default Hive database    hdfs://hadoop1:9000/user/hive/warehouse    public    ROLE    </span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line"></span><br><span class="line">hive&gt; describe database extended hive2;</span><br><span class="line">OK</span><br><span class="line">hive2   it is my database       hdfs://hadoop1:9000/user/hive/warehouse/hive2.db        hadoop      USER    &#123;date=2018-08-08, creator=zhangsan&#125;</span><br><span class="line">Time taken: 0.135 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p></p><p><strong>Create Table</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line"> ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line"> [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line"> | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"></span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br></pre></td></tr></table></figure><p></p><p><strong>data_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">: primitive_type</span><br><span class="line">| array_type</span><br><span class="line">| map_type</span><br><span class="line">| struct_type</span><br><span class="line">| union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>primitive_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> : TINYINT</span><br><span class="line"> | SMALLINT</span><br><span class="line"> | INT</span><br><span class="line"> | BIGINT</span><br><span class="line"> | BOOLEAN</span><br><span class="line">| FLOAT</span><br><span class="line"> | DOUBLE</span><br><span class="line"> | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line"> | STRING</span><br><span class="line"> | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line"> | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>array_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: ARRAY &lt; data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>map_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: MAP &lt; primitive_type, data_type &gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>struct_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br></pre></td></tr></table></figure><p></p><p><strong>union_type</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note:     Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure><p></p><p><strong>row_format</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p><strong>file_format:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p></p><p><strong>constraint_specification:</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">TEMPORARY（临时表）</span><br><span class="line">Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。</span><br><span class="line">语法：CREATE TEMPORARY TABLE …</span><br></pre></td></tr></table></figure><p></p><h6 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h6><ol><li>如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表</li><li>临时表限制：不支持分区字段和创建索引</li></ol><p>EXTERNAL（外部表）<br>Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table external_table(</span><br><span class="line">  &gt; id int,</span><br><span class="line">&gt;  name string </span><br><span class="line">&gt; );</span><br></pre></td></tr></table></figure><p></p><p>PARTITIONED BY（分区表）<br>产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。</p><p>可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；</p><p>分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。</p><p>分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。</p><p>单分区：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE order_partition (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">    &gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (event_month string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>多分区：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE TABLE order_partition2 (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">&gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt;  PARTITIONED BY (event_month string,every_day string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db</span><br><span class="line">18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2</span><br><span class="line">[hadoop@hadoop000 ~]$</span><br><span class="line">ROW FORMAT</span><br></pre></td></tr></table></figure><p>官网解释：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char [ESCAPED BY char]]       [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   </span><br><span class="line">-- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p></p><p>DELIMITED：分隔符（可以自定义分隔符）；</p><p>FIELDS TERMINATED BY char:每个字段之间使用的分割；</p><p>例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;</p><p>COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；</p><p>MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；</p><p>LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）</p><p>一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。</p><p>创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo1(</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>创建demo2表，并指定其他字段：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo2 (</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string,</span><br><span class="line">&gt; hobbies ARRAY &lt;string&gt;,</span><br><span class="line">&gt; address MAP &lt;string, string&gt;</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;</span><br><span class="line">&gt; MAP KEYS TERMINATED BY &apos;:&apos;;</span><br><span class="line">OK</span><br><span class="line">STORED AS（存储格式）</span><br><span class="line">Create Table As Select</span><br></pre></td></tr></table></figure><p></p><p>创建表（拷贝表结构及数据，并且会运行MapReduce作业）<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp (</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">salary double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure><p></p><p>加载数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;</span><br></pre></td></tr></table></figure><p></p><p>复制整张表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp2 as select * from emp;</span><br><span class="line">Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/</span><br><span class="line">Kill Command = /opt/software/hadoop/bin/hadoop job  -kill job_1514116522188_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2018-01-08 05:21:07,707 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2018-01-08 05:21:19,605 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 810 msec</span><br><span class="line">Ended Job = job_1514116522188_0003</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2</span><br><span class="line">Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 3927 HDFS Write: 730 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 810 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 33.322 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp2</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.071 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>复制表中的一些字段<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table emp3 as select empno,ename from emp;</span><br></pre></td></tr></table></figure><p></p><p>LIKE<br>使用like创建表时，只会复制表的结构，不会复制表的数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp4 like emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.149 seconds</span><br><span class="line">hive&gt; select * from emp4;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.151 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>并没有查询到数据</p><p>desc formatted table_name<br>查询表的详细信息<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted emp;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p></p><p>col_name data_type comment<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">empno                   int                                         </span><br><span class="line">ename                   string                                      </span><br><span class="line">job                     string                                      </span><br><span class="line">mgr                     int                                         </span><br><span class="line">hiredate                string                                      </span><br><span class="line">salary                  double                                      </span><br><span class="line">comm                    double                                      </span><br><span class="line">deptno                  int</span><br></pre></td></tr></table></figure><p></p><p>Detailed Table Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Database:               hive                     </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Mon Jan 08 05:17:54 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Protect Mode:           None                     </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">COLUMN_STATS_ACCURATE    true                </span><br><span class="line">numFiles                1                   </span><br><span class="line">numRows                 0                   </span><br><span class="line">rawDataSize             0                   </span><br><span class="line">totalSize               668                 </span><br><span class="line">transient_lastDdlTime    1515359982</span><br></pre></td></tr></table></figure><p></p><p>Storage Information<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">field.delim             \t                  </span><br><span class="line">serialization.format    \t                  </span><br><span class="line">Time taken: 0.228 seconds, Fetched: 39 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;</p><p>查询数据库下的所有表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp1</span><br><span class="line">emp2</span><br><span class="line">emp3</span><br><span class="line">emp4</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.047 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p></p><p>查询创建表的语法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show create table emp;</span><br><span class="line">OK</span><br><span class="line">CREATE TABLE `emp`(</span><br><span class="line">  `empno` int, </span><br><span class="line">  `ename` string, </span><br><span class="line">  `job` string, </span><br><span class="line">  `mgr` int, </span><br><span class="line">  `hiredate` string, </span><br><span class="line">  `salary` double, </span><br><span class="line">  `comm` double, </span><br><span class="line">  `deptno` int)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;\t&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;numRows&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;rawDataSize&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;668&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)</span><br><span class="line">Time taken: 0.192 seconds, Fetched: 24 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">Drop Table</span><br><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];     -- (Note: PURGE available in Hive 0.14.0 and later)</span><br></pre></td></tr></table></figure><p></p><p>指定PURGE后，数据不会放到回收箱，会直接删除</p><p>DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失</p><p>删除EXTERNAL表时，表中的数据不会从文件系统中删除<br>Alter Table</p><p>重命名<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table demo2 rename to new_demo2;</span><br><span class="line">OK</span><br><span class="line">Add Partitions</span><br><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line">  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)</span><br><span class="line">用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。</span><br><span class="line">原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。</span><br><span class="line"></span><br><span class="line">hive&gt;  create table dept(</span><br><span class="line">&gt;  deptno int,</span><br><span class="line">&gt; dname string,</span><br><span class="line">&gt; loc string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (dt string)</span><br><span class="line">&gt;  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.953 seconds </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);</span><br><span class="line">Loading data to table default.dept partition (dt=2018-08-08)</span><br><span class="line">Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.147 seconds</span><br></pre></td></tr></table></figure><p></p><p>查询结果<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 0.481 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);</span><br><span class="line">OK</span><br><span class="line">Drop Partitions</span><br><span class="line">ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);</span><br></pre></td></tr></table></figure><p></p><p>查看分区语句<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept;</span><br><span class="line">OK</span><br><span class="line">dt=2018-08-08</span><br><span class="line">dt=2018-09-09</span><br><span class="line">Time taken: 0.385 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><p></p><p>按分区查询<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 2.323 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive生产上，压缩和存储结合使用案例</title>
      <link href="/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>你们Hive生产上，压缩和存储，结合使用了吗？</p><p>案例：<br>原文件大小：19M<br><img src="/assets/blogImg/423_1.png" alt="enter description here"><br><a id="more"></a></p><h6 id="1-ORC-Zlip结合"><a href="#1-ORC-Zlip结合" class="headerlink" title="1. ORC+Zlip结合"></a>1. ORC+Zlip结合</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc_zlib</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><font color="#FF4500">用ORC+Zlip之后的文件为2.8M<br><br></font><br>用ORC+Zlip之后的文件为2.8M<br><img src="/assets/blogImg/423_2.png" alt="enter description here"><br><br><br>###### 2. Parquet+gzip结合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       set parquet.compression=gzip;</span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><br><br><font color="#FF4500"><br>用Parquet+gzip之后的文件为3.9M<br></font><p><img src="/assets/blogImg/423_3.png" alt="enter description here"></p><h6 id="3-Parquet-Lzo结合"><a href="#3-Parquet-Lzo结合" class="headerlink" title="3. Parquet+Lzo结合"></a>3. Parquet+Lzo结合</h6><p><strong>3.1 安装Lzo</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gz</span><br><span class="line">tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">cd lzo-2.06</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzo/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib/</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib64/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export PATH=/usr/local//hadoop/lzo/:$PATH</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.2 安装Lzop</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.lzop.org/download/lzop-1.03.tar.gz</span><br><span class="line">tar -zxvf lzop-1.03.tar.gz</span><br><span class="line">cd lzop-1.03</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzop</span><br><span class="line">make  &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p></p><p><strong>3.3 软连接</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop</span><br></pre></td></tr></table></figure><p></p><p><strong>3.4 测试lzop</strong><br>lzop xxx.log<br>若生成xxx.log.lzo文件，则说明成功<br><strong>3.5 安装Hadoop-LZO</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   git或svn 下载https://github.com/twitter/hadoop-lzo</span><br><span class="line">cd hadoop-lzo</span><br><span class="line">mvn clean package -Dmaven.test.skip=true </span><br><span class="line">tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/</span><br></pre></td></tr></table></figure><p></p><p><strong>3.6 配置</strong><br>在core-site.xml配置<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">     org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">           &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在mapred-site.xml中配置</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">           &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在hadoop-env.sh中配置</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure><p></p><p><strong>3.7 测试</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.compress.output=true;  </span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line">SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line">create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">TBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p></p><p><font color="#FF4500">用Parquet+Lzo(未建立索引)之后的文件为5.9M<br></font><br><img src="/assets/blogImg/423_4.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 压缩格式 </tag>
            
            <tag> 案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>又又又是源码！RDD 作业的DAG是如何切分的？</title>
      <link href="/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/"/>
      <url>/2018/04/23/%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>我们都知道，RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。那么这整个过程在源码中是如何体现的呢？<br><a id="more"></a></p><h4 id="1-作业的提交"><a href="#1-作业的提交" class="headerlink" title="1.作业的提交"></a>1.作业的提交</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// SparkContext.scala</span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// DAGScheduler.scala</span><br><span class="line">   def runJob[T, U](</span><br><span class="line">    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure><p>可以看到，SparkContext的runjob方法调用了DAGScheduler的runjob方法正式向集群提交任务，最终调用了submitJob方法。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2   def submitJob[T, U](</span><br><span class="line"> 3      rdd: RDD[T],</span><br><span class="line"> 4      func: (TaskContext, Iterator[T]) =&gt; U,</span><br><span class="line"> 5      partitions: Seq[Int],</span><br><span class="line"> 6      callSite: CallSite,</span><br><span class="line"> 7      resultHandler: (Int, U) =&gt; Unit,</span><br><span class="line"> 8      properties: Properties): JobWaiter[U] = &#123;</span><br><span class="line"> 9    // Check to make sure we are not launching a task on a partition that does not exist.</span><br><span class="line">10    val maxPartitions = rdd.partitions.length</span><br><span class="line">11    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;</span><br><span class="line">12      throw new IllegalArgumentException(</span><br><span class="line">13        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +</span><br><span class="line">14          &quot;Total number of partitions: &quot; + maxPartitions)</span><br><span class="line">15    &#125;</span><br><span class="line">16</span><br><span class="line">17    val jobId = nextJobId.getAndIncrement()</span><br><span class="line">18    if (partitions.size == 0) &#123;</span><br><span class="line">19      // Return immediately if the job is running 0 tasks</span><br><span class="line">20      return new JobWaiter[U](this, jobId, 0, resultHandler)</span><br><span class="line">21    &#125;</span><br><span class="line">22</span><br><span class="line">23    assert(partitions.size &gt; 0)</span><br><span class="line">24    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</span><br><span class="line">25    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</span><br><span class="line">26    //给eventProcessLoop发送JobSubmitted消息</span><br><span class="line">27    eventProcessLoop.post(JobSubmitted(</span><br><span class="line">28      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">29      SerializationUtils.clone(properties)))</span><br><span class="line">30    waiter</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><p>这里向eventProcessLoop对象发送了JobSubmitted消息。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">1// DAGScheduler.scala</span><br><span class="line">2   private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)</span><br><span class="line">    eventProcessLoop是DAGSchedulerEventProcessLoop类的一个对象。</span><br><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123;</span><br><span class="line"> 3    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line"> 4      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"> 5</span><br><span class="line"> 6    case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line"> 7      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"> 8</span><br><span class="line"> 9    case StageCancelled(stageId) =&gt;</span><br><span class="line">10      dagScheduler.handleStageCancellation(stageId)</span><br><span class="line">11</span><br><span class="line">12    case JobCancelled(jobId) =&gt;</span><br><span class="line">13      dagScheduler.handleJobCancellation(jobId)</span><br><span class="line">14</span><br><span class="line">15    case JobGroupCancelled(groupId) =&gt;</span><br><span class="line">16      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line">17</span><br><span class="line">18    case AllJobsCancelled =&gt;</span><br><span class="line">19      dagScheduler.doCancelAllJobs()</span><br><span class="line">20</span><br><span class="line">21    case ExecutorAdded(execId, host) =&gt;</span><br><span class="line">22      dagScheduler.handleExecutorAdded(execId, host)</span><br><span class="line">23</span><br><span class="line">24    case ExecutorLost(execId, reason) =&gt;</span><br><span class="line">25      val filesLost = reason match &#123;</span><br><span class="line">26        case SlaveLost(_, true) =&gt; true</span><br><span class="line">27        case _ =&gt; false</span><br><span class="line">28      &#125;</span><br><span class="line">29      dagScheduler.handleExecutorLost(execId, filesLost)</span><br><span class="line">30</span><br><span class="line">31    case BeginEvent(task, taskInfo) =&gt;</span><br><span class="line">32      dagScheduler.handleBeginEvent(task, taskInfo)</span><br><span class="line">33</span><br><span class="line">34    case GettingResultEvent(taskInfo) =&gt;</span><br><span class="line">35      dagScheduler.handleGetTaskResult(taskInfo)</span><br><span class="line">36</span><br><span class="line">37    case completion: CompletionEvent =&gt;</span><br><span class="line">38      dagScheduler.handleTaskCompletion(completion)</span><br><span class="line">39</span><br><span class="line">40    case TaskSetFailed(taskSet, reason, exception) =&gt;</span><br><span class="line">41      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span><br><span class="line">42</span><br><span class="line">43    case ResubmitFailedStages =&gt;</span><br><span class="line">44      dagScheduler.resubmitFailedStages()</span><br><span class="line">45  &#125;</span><br></pre></td></tr></table></figure><p></p><p>DAGSchedulerEventProcessLoop对接收到的消息进行处理，在doOnReceive方法中形成一个event loop。<br>接下来将调用submitStage()方法进行stage的划分。</p><h4 id="2-stage的划分"><a href="#2-stage的划分" class="headerlink" title="2.stage的划分"></a>2.stage的划分</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2 private def submitStage(stage: Stage) &#123;</span><br><span class="line"> 3    val jobId = activeJobForStage(stage)//查找该Stage的所有激活的job</span><br><span class="line"> 4    if (jobId.isDefined) &#123;</span><br><span class="line"> 5      logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)</span><br><span class="line"> 6      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line"> 7        val missing = getMissingParentStages(stage).sortBy(_.id)//得到Stage的父Stage，并排序</span><br><span class="line"> 8        logDebug(&quot;missing: &quot; + missing)</span><br><span class="line"> 9        if (missing.isEmpty) &#123;</span><br><span class="line">10          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)</span><br><span class="line">11          submitMissingTasks(stage, jobId.get)//如果Stage没有父Stage，则提交任务集</span><br><span class="line">12        &#125; else &#123;</span><br><span class="line">13          for (parent &lt;- missing) &#123;//如果有父Stage，递归调用submiStage</span><br><span class="line">14            submitStage(parent)</span><br><span class="line">15          &#125;</span><br><span class="line">16          waitingStages += stage//将其标记为等待状态，等待下次提交</span><br><span class="line">17        &#125;</span><br><span class="line">18      &#125;</span><br><span class="line">19    &#125; else &#123;</span><br><span class="line">20      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//如果该Stage没有激活的job，则丢弃该Stage</span><br><span class="line">21    &#125;</span><br><span class="line">22  &#125;</span><br></pre></td></tr></table></figure><p>在submitStage方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 1// DAGScheduler.scala</span><br><span class="line"> 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123;</span><br><span class="line"> 3    val missing = new HashSet[Stage] //用于存放父Stage</span><br><span class="line"> 4    val visited = new HashSet[RDD[_]] //用于存放已访问过的RDD</span><br><span class="line"> 5</span><br><span class="line"> 6    val waitingForVisit = new Stack[RDD[_]]</span><br><span class="line"> 7    def visit(rdd: RDD[_]) &#123;</span><br><span class="line"> 8      if (!visited(rdd)) &#123; //如果RDD没有被访问过，则进行访问</span><br><span class="line"> 9        visited += rdd //添加到已访问RDD的HashSet中</span><br><span class="line">10        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)</span><br><span class="line">11        if (rddHasUncachedPartitions) &#123;</span><br><span class="line">12          for (dep &lt;- rdd.dependencies) &#123; //获取该RDD的依赖</span><br><span class="line">13            dep match &#123;</span><br><span class="line">14              case shufDep: ShuffleDependency[_, _, _] =&gt;//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage</span><br><span class="line">15                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//生成父Stage</span><br><span class="line">16                if (!mapStage.isAvailable) &#123;//若父Stage不存在，则添加到父Stage的HashSET中</span><br><span class="line">17                  missing += mapStage</span><br><span class="line">18                &#125;</span><br><span class="line">19              case narrowDep: NarrowDependency[_] =&gt;//若为窄依赖，则继续访问父RDD</span><br><span class="line">20                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">21            &#125;</span><br><span class="line">22          &#125;</span><br><span class="line">23        &#125;</span><br><span class="line">24      &#125;</span><br><span class="line">25    &#125;</span><br><span class="line">26    waitingForVisit.push(stage.rdd)</span><br><span class="line">27    while (waitingForVisit.nonEmpty) &#123;//循环遍历所有RDD</span><br><span class="line">28      visit(waitingForVisit.pop())</span><br><span class="line">29    &#125;</span><br><span class="line">30    missing.toList</span><br><span class="line">31  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="getmissingParentStages-方法为核心方法。"><a href="#getmissingParentStages-方法为核心方法。" class="headerlink" title="getmissingParentStages()方法为核心方法。"></a>getmissingParentStages()方法为核心方法。</h4><font color="#FF4500"><br><br>这里我们要懂得这样一个逻辑：我们都知道，Stage是通过shuffle划分的，所以，每一Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。<br></font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Core </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive存储格式的生产应用</title>
      <link href="/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/"/>
      <url>/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p><strong>相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。</strong></p><p>原始大小: 19M</p><p><img src="/assets/blogImg/420_1.png" alt="enter description here"><br><a id="more"></a></p><h5 id="1-TextFile-默认-文件大小为18-1M"><a href="#1-TextFile-默认-文件大小为18-1M" class="headerlink" title="1. TextFile(默认) 文件大小为18.1M"></a>1. TextFile(默认) 文件大小为18.1M</h5><p><img src="/assets/blogImg/420_2.png" alt="enter description here"></p><h5 id="2-SequenceFile"><a href="#2-SequenceFile" class="headerlink" title="2. SequenceFile"></a>2. SequenceFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_seq( </span><br><span class="line">track_time string, </span><br><span class="line">url string, </span><br><span class="line">session_id string, </span><br><span class="line">referer string, </span><br><span class="line">ip string, </span><br><span class="line">end_user_id string, </span><br><span class="line">city_id string </span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” </span><br><span class="line">STORED AS SEQUENCEFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_seq select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用SequenceFile存储后的文件为19.6M</strong><br><img src="/assets/blogImg/420_3.png" alt="enter description here"></p><h5 id="3-RcFile"><a href="#3-RcFile" class="headerlink" title="3. RcFile"></a>3. RcFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_rcfile(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS RCFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_rcfile select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用RcFile存储后的文件为17.9M</strong><br><img src="/assets/blogImg/420_4.png" alt="enter description here"></p><h5 id="4-ORCFile"><a href="#4-ORCFile" class="headerlink" title="4. ORCFile"></a>4. ORCFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用ORCFile存储后的文件为7.7M</strong><br><img src="/assets/blogImg/420_5.png" alt="enter description here"></p><h5 id="5-Parquet"><a href="#5-Parquet" class="headerlink" title="5. Parquet"></a>5. Parquet</h5><pre><code>create table page_views_parquetROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;</code></pre><p><strong>用ORCFile存储后的文件为13.1M</strong><br><img src="/assets/blogImg/420_6.png" alt="enter description here"></p><p><strong>总结：磁盘空间占用大小比较</strong></p><font color="#FF4500">ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)</font><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 压缩格式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据压缩，你们真的了解吗？</title>
      <link href="/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，带你们剖析大数据之压缩！<br><a id="more"></a></p><h6 id="1-压缩的好处和坏处"><a href="#1-压缩的好处和坏处" class="headerlink" title="1. 压缩的好处和坏处"></a>1. 压缩的好处和坏处</h6><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h6 id="2-压缩格式"><a href="#2-压缩格式" class="headerlink" title="2. 压缩格式"></a>2. 压缩格式</h6><p><img src="/assets/blogImg/压缩1.png" alt="enter description here"><br>压缩比<br><img src="/assets/blogImg/压缩2.png" alt="enter description here"><br>压缩时间<br><img src="/assets/blogImg/yasuo3.png" alt="enter description here"></p><font color="#FF0000">可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</font><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>gzip</strong></td><td>压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td>不支持split</td></tr><tr><td><strong>lzo</strong></td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td><strong>snappy</strong></td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td><strong>bzip2</strong></td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h5><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h5 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a><strong>应用场景：</strong></h5><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 压缩格式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark2.2.0 全网最详细的源码编译</title>
      <link href="/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，Spark2.2.0 全网最详细的源码编译<br><a id="more"></a></p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><hr><p>JDK： Spark 2.2.0及以上版本只支持JDK1.8</p><hr><p>Maven：3.3.9<br>设置maven环境变量时，需设置maven内存：<br>export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”</p><hr><p>Scala：2.11.8</p><hr><p>Git</p><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>下载spark的tar包，并解压<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</span><br><span class="line">[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz</span><br></pre></td></tr></table></figure><p></p><p>编辑dev/make-distribution.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh</span><br><span class="line">注释以下内容：</span><br><span class="line">#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use &quot;set -o pipefail&quot;</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure><p></p><p>添加以下内容：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure><p></p><p>编辑pom.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml</span><br><span class="line">添加在repositorys内</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">      &lt;id&gt;clouders&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;clouders Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br></pre></td></tr></table></figure><p></p><p>安装<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn</span><br></pre></td></tr></table></figure><p></p><p>稍微等待几小时，网络较好的话，非常快。<br>也可以参考J哥博客：<br>基于CentOS6.4环境编译Spark-2.1.0源码 <a href="http://blog.itpub.net/30089851/viewspace-2140779/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2140779/</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop常用命令大全</title>
      <link href="/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
      <url>/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hadoop常用命令大全<br><a id="more"></a></p><h6 id="1-单独启动和关闭hadoop服务"><a href="#1-单独启动和关闭hadoop服务" class="headerlink" title="1. 单独启动和关闭hadoop服务"></a><strong>1. 单独启动和关闭hadoop服务</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>启动名称节点</strong></td><td style="text-align:center">hadoop-daemon.sh start namenode</td></tr><tr><td><strong>启动数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh start datanode slave</td></tr><tr><td><strong>启动secondarynamenode</strong></td><td style="text-align:center">hadoop-daemon.sh start secondarynamenode</td></tr><tr><td><strong>启动resourcemanager</strong></td><td style="text-align:center">yarn-daemon.sh start resourcemanager</td></tr><tr><td><strong>启动nodemanager</strong></td><td style="text-align:center">bin/yarn-daemons.sh start nodemanager</td></tr><tr><td><strong>停止数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh stop datanode</td></tr></tbody></table><h6 id="2-常用的命令"><a href="#2-常用的命令" class="headerlink" title="2. 常用的命令"></a><strong>2. 常用的命令</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>创建目录</strong></td><td style="text-align:center">hdfs dfs -mkdir /input</td></tr><tr><td><strong>查看</strong></td><td style="text-align:center">hdfs dfs -ls</td></tr><tr><td><strong>递归查看</strong></td><td style="text-align:center">hdfs dfs ls -R</td></tr><tr><td><strong>上传</strong></td><td style="text-align:center">hdfs dfs -put</td></tr><tr><td><strong>下载</strong></td><td style="text-align:center">hdfs dfs -get</td></tr><tr><td><strong>删除</strong></td><td style="text-align:center">hdfs dfs -rm</td></tr><tr><td><strong>从本地剪切粘贴到hdfs</strong></td><td style="text-align:center">hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>从hdfs剪切粘贴到本地</strong></td><td style="text-align:center">hdfs fs -moveToLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>追加一个文件到另一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -appedToFile ./hello.txt /input/hello.txt</td></tr><tr><td><strong>查看文件内容</strong></td><td style="text-align:center">hdfs fs -cat /input/hello.txt</td></tr><tr><td><strong>显示一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -tail /input/hello.txt</td></tr><tr><td><strong>以字符串的形式打印文件的内容</strong></td><td style="text-align:center">hdfs fs -text /input/hello.txt</td></tr><tr><td><strong>修改文件权限</strong></td><td style="text-align:center">hdfs fs -chmod 666 /input/hello.txt</td></tr><tr><td><strong>修改文件所属</strong></td><td style="text-align:center">hdfs fs -chown ruoze.ruoze /input/hello.txt</td></tr><tr><td><strong>从本地文件系统拷贝到hdfs里</strong></td><td style="text-align:center">hdfs fs -copyFromLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs拷贝到本地</strong></td><td style="text-align:center">hdfs fs -copyToLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs到一个路径拷贝到另一个路径</strong></td><td style="text-align:center">hdfs fs -cp /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>从hdfs到一个路径移动到另一个路径</strong></td><td style="text-align:center">hdfs fs -mv /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>统计文件系统的可用空间信息</strong></td><td style="text-align:center">hdfs fs -df -h /</td></tr><tr><td><strong>统计文件夹的大小信息</strong></td><td style="text-align:center">hdfs fs -du -s -h /</td></tr><tr><td><strong>统计一个指定目录下的文件节点数量</strong></td><td style="text-align:center">hadoop fs -count /aaa</td></tr><tr><td><strong>设置hdfs的文件副本数量</strong></td><td style="text-align:center">hadoop fs -setrep 3 /input/xx.txt</td></tr></tbody></table><h5 id="总结：一定要学会查看命令帮助"><a href="#总结：一定要学会查看命令帮助" class="headerlink" title="总结：一定要学会查看命令帮助"></a>总结：一定要学会查看命令帮助</h5><p><strong>1.hadoop命令直接回车查看命令帮助<br>2.hdfs命令、hdfs dfs命令直接回车查看命令帮助<br>3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么我们生产上要选择Spark On Yarn模式？</title>
      <link href="/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/"/>
      <url>/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，为什么我们生产上要选择Spark On Yarn？<br><a id="more"></a><br>开发上我们选择local[2]模式<br>生产上跑任务Job，我们选择Spark On Yarn模式 ，</p><p>将Spark Application部署到yarn中，有如下优点：</p><p>1.部署Application和服务更加方便</p><ul><li>只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。</li></ul><p>2.资源隔离机制</p><ul><li>yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。</li></ul><p>3.资源弹性管理</p><ul><li>Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。</li></ul><p>Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。</p><p><strong>运行client模式：</strong></p><ul><li><p>“./spark-shell –master yarn”</p></li><li><p>“./spark-shell –master yarn-client”</p></li><li><p>“./spark-shell –master yarn –deploy-mode client”</p></li></ul><p><strong>运行的是cluster模式</strong></p><ul><li><p>“./spark-shell –master yarn-cluster”</p></li><li><p>“./spark-shell –master yarn –deploy-mode cluster”</p></li></ul><p><strong>client和cluster模式的主要区别：<br>a. client的driver是运行在客户端进程中<br>b. cluster的driver是运行在Application Master之中</strong></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Spark Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 高级 </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive全网最详细的编译及部署</title>
      <link href="/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hive全网最详细的编译及部署<br><a id="more"></a></p><h6 id="一、需要安装的软件"><a href="#一、需要安装的软件" class="headerlink" title="一、需要安装的软件"></a>一、需要安装的软件</h6><ul><li><p>相关环境：</p><ul><li><p>jdk-7u80</p><ul><li>hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7</li></ul></li><li><p>apache-maven-3.3.9</p></li><li><p>mysql5.1</p></li><li><p>hadoop伪分布集群已启动</p></li></ul></li></ul><h6 id="二、安装jdk"><a href="#二、安装jdk" class="headerlink" title="二、安装jdk"></a>二、安装jdk</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /usr/java &amp;&amp; cd  /usr/java/    </span><br><span class="line"></span><br><span class="line">tar -zxvf  /tmp/server-jre-7u80-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line">chown -R root:root  /usr/java/jdk1.7.0_80/ </span><br><span class="line"></span><br><span class="line">echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="三、安装maven"><a href="#三、安装maven" class="headerlink" title="三、安装maven"></a>三、安装maven</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">unzip /tmp/apache-maven-3.3.9-bin.zip</span><br><span class="line"></span><br><span class="line">chown root: /usr/local/apache-maven-3.3.9 -R</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="四、安装mysql"><a href="#四、安装mysql" class="headerlink" title="四、安装mysql"></a>四、安装mysql</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server mysql</span><br><span class="line"></span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">chkconfig mysqld on</span><br><span class="line"></span><br><span class="line">mysqladmin -u root password 123456</span><br><span class="line"></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;</span><br><span class="line"></span><br><span class="line">delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line"></span><br><span class="line">delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line"></span><br><span class="line">drop database test;</span><br><span class="line"></span><br><span class="line">DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h6 id="五、下载hive源码包："><a href="#五、下载hive源码包：" class="headerlink" title="五、下载hive源码包："></a>五、下载hive源码包：</h6><p>输入：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>根据cdh版本选择对应hive软件包：<br>hive-1.1.0-cdh5.7.1-src.tar.gz<br>解压后使用maven命令编译成安装包</p><h6 id="六、编译"><a href="#六、编译" class="headerlink" title="六、编译:"></a>六、编译:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line"></span><br><span class="line">tar -xf hive-1.1.0-cdh5.7.1-src.tar.gz</span><br><span class="line"></span><br><span class="line">cd /tmp/hive-1.1.0-cdh5.7.1</span><br><span class="line"></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br><span class="line"></span><br><span class="line"># 编译生成的包在以下位置：</span><br><span class="line"></span><br><span class="line"># packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br></pre></td></tr></table></figure><h6 id="七、安装编译生成的Hive包，然后测试"><a href="#七、安装编译生成的Hive包，然后测试" class="headerlink" title="七、安装编译生成的Hive包，然后测试"></a>七、安装编译生成的Hive包，然后测试</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-1.1.0-cdh5.7.1-bin hive</span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin </span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop hive </span><br><span class="line"></span><br><span class="line">echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h6 id="八、更改环境变量"><a href="#八、更改环境变量" class="headerlink" title="八、更改环境变量"></a>八、更改环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">cd /usr/local/hive</span><br><span class="line"></span><br><span class="line">cd conf</span><br></pre></td></tr></table></figure><p>1、hive-env.sh<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh&amp;&amp;vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure><p></p><p>2、hive-site.xml<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt; </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;vincent&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p></p><h6 id="九、拷贝mysql驱动包到-HIVE-HOME-lib"><a href="#九、拷贝mysql驱动包到-HIVE-HOME-lib" class="headerlink" title="九、拷贝mysql驱动包到$HIVE_HOME/lib"></a>九、拷贝mysql驱动包到$HIVE_HOME/lib</h6><p>上方的hive-site.xml使用了java的mysql驱动包<br>需要将这个包上传到hive的lib目录之下<br>解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">unzip mysql-connector-java-5.1.45.zip</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure><p></p><p>未拷贝有相关报错：</p><p>The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH.</p><p>Please check your CLASSPATH specification,</p><p>and the name of the driver.</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)</title>
      <link href="/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/"/>
      <url>/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)<br><a id="more"></a></p><ol><li><p>修改mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template  mapred-site.xml</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# cd ../../</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop全网最详细的伪分布式部署(HDFS)</title>
      <link href="/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/"/>
      <url>/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Hadoop全网最详细的伪分布式部署(HDFS)<br><a id="more"></a></p><h6 id="1-添加hadoop用户"><a href="#1-添加hadoop用户" class="headerlink" title="1.添加hadoop用户"></a>1.添加hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 ~]# useradd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# vi /etc/sudoers</span><br><span class="line"># 找到root ALL=(ALL) ALL，添加</span><br><span class="line"></span><br><span class="line">hadoop ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure><h6 id="2-上传并解压"><a href="#2-上传并解压" class="headerlink" title="2.上传并解压"></a>2.上传并解压</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz</span><br></pre></td></tr></table></figure><h6 id="3-软连接"><a href="#3-软连接" class="headerlink" title="3.软连接"></a>3.软连接</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span><br></pre></td></tr></table></figure><h6 id="4-设置环境变量"><a href="#4-设置环境变量" class="headerlink" title="4.设置环境变量"></a>4.设置环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="5-设置用户、用户组"><a href="#5-设置用户、用户组" class="headerlink" title="5.设置用户、用户组"></a>5.设置用户、用户组</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# cd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# rm -f *.txt</span><br></pre></td></tr></table></figure><h6 id="6-切换hadoop用户"><a href="#6-切换hadoop用户" class="headerlink" title="6.切换hadoop用户"></a>6.切换hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# ll</span><br><span class="line"></span><br><span class="line">total 32</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 bin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 etc</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 include</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 lib</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexec</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logs</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 sbin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop 4096 Jun  2 14:24 share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin:可执行文件</span><br><span class="line"></span><br><span class="line"># etc: 配置文件</span><br><span class="line"></span><br><span class="line"># sbin:shell脚本，启动关闭hdfs,yarn等</span><br></pre></td></tr></table></figure><h6 id="7-配置文件"><a href="#7-配置文件" class="headerlink" title="7.配置文件"></a>7.配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt;    # 配置自己机器的IP</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="8-配置hadoop用户的ssh信任关系"><a href="#8-配置hadoop用户的ssh信任关系" class="headerlink" title="8.配置hadoop用户的ssh信任关系"></a>8.配置hadoop用户的ssh信任关系</h6><p>8.1公钥/密钥 配置无密码登录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>8.2 查看日期，看是否配置成功<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br></pre></td></tr></table></figure><p></p><h6 id="9-格式化和启动"><a href="#9-格式化和启动" class="headerlink" title="9.格式化和启动"></a>9.格式化和启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">hadoop-01: Error: JAVA_HOME is not set and could not be found.</span><br><span class="line"></span><br><span class="line">localhost: Error: JAVA_HOME is not set and could not be found.</span><br></pre></td></tr></table></figure><p>9.1解决方法:添加环境变量<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]#  vi etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line"># 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied</span><br></pre></td></tr></table></figure><p>9.2解决方法:添加权限<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# exit</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# cd ../</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# cd /opt/software/hadoop</span><br></pre></td></tr></table></figure><p></p><p>9.3 继续启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><p>9.4检查是否成功<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# jps</span><br><span class="line"></span><br><span class="line">19536 DataNode</span><br><span class="line"></span><br><span class="line">19440 NameNode</span><br><span class="line"></span><br><span class="line">19876 Jps</span><br><span class="line"></span><br><span class="line">19740 SecondaryNameNode</span><br></pre></td></tr></table></figure><p></p><p>9.5访问： <a href="http://192.168.137.130:50070" target="_blank" rel="noopener">http://192.168.137.130:50070</a></p><p>9.6修改dfs启动的进程，以hadoop-01启动</p><p>启动的三个进程：</p><p>namenode: hadoop-01 bin/hdfs getconf -namenodes</p><p>datanode: localhost datanodes (using default slaves file) etc/hadoop/slaves</p><p>secondarynamenode: 0.0.0.0<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# echo  &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">hadoop-01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50090&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50091&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p></p><p>9.7重启<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常用命令（一）</title>
      <link href="/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/"/>
      <url>/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（一）<br><a id="more"></a></p><ol><li><p>查看当前目录 pwd</p></li><li><p>查看IP</p></li></ol><ul><li><p>ifconfig 查看虚拟机ip</p></li><li><p>hostname 主机名字</p><ul><li>i 查看主机名映射的IP</li></ul></li></ul><ol start="3"><li>切换目录 cd</li></ol><ul><li><p>cd ~ 切换家目录（root为/root，普通用户为/home/用户名）</p></li><li><p>cd /filename 以绝对路径切换目录</p></li><li><p>cd - 返回上一次操作路径，并输出路径</p></li><li><p>cd ../ 返回上一层目录</p></li></ul><ol start="4"><li><p>清理桌面 clear</p></li><li><p>显示当前目录文件和文件夹 ls</p></li></ol><ul><li><p>ls -l(ll) 显示详细信息</p></li><li><p>ls -la 显示详细信息+隐藏文件（以 . 开头，例：.ssh）</p></li><li><p>ls -lh 显示详细信息+文件大小</p></li><li><p>ls -lrt 显示详细信息+按时间排序</p></li></ul><ol start="6"><li><p>查看文件夹大小 du -sh</p></li><li><p>命令帮助</p></li></ol><ul><li><p>man 命令</p></li><li><p>命令 –help</p></li></ul><ol start="8"><li>创建文件夹 mkdir</li></ol><ul><li>mkdir -p filename1/filename2 递归创建文件夹</li></ul><ol start="9"><li><p>创建文件 touch/vi/echo xx&gt;filename</p></li><li><p>查看文件内容</p></li></ol><ul><li><p>cat filename 直接打印所有内容</p></li><li><p>more filename 根据窗口大小进行分页显示</p></li></ul><ol start="11"><li>文件编辑 vi</li></ol><ul><li><p>vi分为命令行模式，插入模式，尾行模式</p></li><li><p>命令行模式—&gt;插入模式：按i或a键</p></li><li><p>插入模式—&gt;命令行模式：按Esc键</p></li><li><p>命令行模式—&gt;尾行模式：按Shift和:键</p><p>插入模式</p><ul><li><p>dd 删除光标所在行</p></li><li><p>n+dd 删除光标以下的n行</p></li><li><p>dG 删除光标以下行</p></li><li><p>gg 第一行第一个字母</p></li><li><p>G 最后一行第一个字母</p></li><li><p>shift+$ 该行最后一个字母</p><p>尾行模式</p></li><li><p>q! 强制退出</p></li><li><p>qw 写入并退出</p></li><li><p>qw! 强制写入退出</p></li><li><p>x 退出，如果存在改动，则保存再退出</p></li></ul></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常用命令（二）</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（二）<br><a id="more"></a></p><ol><li>实时查看文件内容 tail filename</li></ol><ul><li><p>tail -f filename 当文件(名)被修改后，不能监视文件内容</p></li><li><p>tail -F filename 当文件(名)被修改后，依然可以监视文件内容</p></li></ul><ol start="2"><li>复制、移动文件</li></ol><ul><li><p>cp oldfilename newfilename 复制</p></li><li><p>mv oldfilename newfilename 移动/重命名</p></li></ul><ol start="3"><li>echo</li></ol><ul><li><p>echo “xxx” 输出</p></li><li><p>echo “xxx” &gt; filename 覆盖</p></li><li><p>echo “xxx” &gt;&gt; filename 追加</p></li></ul><ol start="4"><li>删除 rm</li></ol><ul><li><p>rm -f 强制删除</p></li><li><p>rm -rf 强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件</p></li></ul><ol start="5"><li>别名 alias</li></ol><ul><li><p>alias x=”xxxxxx” 临时引用别名</p></li><li><p>alias x=”xxxxxx” 配置到环境变量中即为永久生效</p></li></ul><ol start="6"><li>查看历史命令 history</li></ol><ul><li><p>history 显示出所有历史记录</p></li><li><p>history n 显示出n条记录</p></li><li><p>!n 执行第n条记录</p></li></ul><ol start="7"><li>管道命令 （ | ）</li></ol><ul><li>管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入</li></ul><ol start="8"><li>查看进程、查看id、端口</li></ol><ul><li><p>ps -ef ｜grep 进程名 查看进程基本信息</p></li><li><p>netstat -npl｜grep 进程名或进程id 查看服务id和端口</p></li></ul><ol start="9"><li>杀死进程 kill</li></ol><ul><li><p>kill -9 进程名/pid 强制删除</p></li><li><p>kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程</p></li></ul><ol start="10"><li>rpm 搜索、卸载</li></ol><ul><li><p>rpm -qa | grep xxx 搜索xxx</p></li><li><p>rpm –nodeps -e xxx 删除xxx</p></li><li><p>–nodeps 不验证包的依赖性</p></li></ul><ol start="11"><li>查询</li></ol><ul><li><p>find 路径 -name xxx (推荐)</p></li><li><p>which xxx</p></li><li><p>local xxx</p></li></ul><ol start="12"><li>查看磁盘、内存、系统的情况</li></ol><ul><li><p>df -h 查看磁盘大小及其使用情况</p></li><li><p>free -m 查看内存大小及其使用情况</p></li><li><p>top 查看系统情况</p></li></ul><ol start="13"><li>软连接</li></ol><ul><li>ln -s 原始目录 目标目录</li></ul><ol start="14"><li>压缩、解压</li></ol><ul><li><p>tar -czf 压缩 tar -xzvf 解压</p></li><li><p>zip 压缩 unzip 解压</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令（三）</title>
      <link href="/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/"/>
      <url>/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>Linux最常用实战命令（三）<br><a id="more"></a></p><ol><li><p>用户、用户组</p><p>用户</p><ul><li><p>useradd 用户名 添加用户</p></li><li><p>userdel 用户名 删除用户</p></li><li><p>id 用户名 查看用户信息</p></li><li><p>passwd 用户名 修改用户密码</p></li><li><p>su - 用户名 切换用户</p></li><li><p>ll /home/ 查看已有的用户</p><p>用户组</p></li><li><p>groupadd 用户组 添加用户组</p></li><li><p>cat /etc/group 用户组的文件</p></li><li><p>usermod -a -G 用户组 用户 将用户添加到用户组中</p><p>给一个普通用户添加sudo权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span><br><span class="line">    #在root     ALL=(ALL)    ALL    下面添加一行</span><br><span class="line">    用户    ALL=(ALL)    NOPASSWD:ALL</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改文件权限</p><p>chown 修改文件或文件夹的所属用户和用户组</p><ul><li><p>chown -R 用户:用户组 文件夹名 -R 为递归参数，指针对文件夹</p></li><li><p>chown 用户:用户组 文件名</p><p>chmod: 修改文件夹或者文件的权限</p></li><li><p>chmod -R 700 文件夹名</p></li><li><p>chmod 700 文件夹名</p></li></ul></li></ol><pre><code>r  =&gt;    4w  =&gt;    2x  =&gt;    1</code></pre><ol start="3"><li>后台执行命令</li></ol><ul><li><p>&amp;</p></li><li><p>nohup</p></li><li><p>screen</p></li></ul><ol start="4"><li>多人合作 screen</li></ol><ul><li><p>screen -list 查看会话</p></li><li><p>screen -S 建立一个后台的会话</p></li><li><p>screen -r 进入会话</p></li><li><p>ctrl+a+d 退出会话</p></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构设计及副本放置策略</title>
      <link href="/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"/>
      <url>/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><p>HDFS架构设计及副本放置策略<br><a id="more"></a><br>HDFS主要由3个组件构成，分别是<strong>NameNode、SecondaryNameNode和DataNode</strong>，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。</p><h6 id="NameNode和DataNode架构图"><a href="#NameNode和DataNode架构图" class="headerlink" title="NameNode和DataNode架构图"></a>NameNode和DataNode架构图</h6><p><img src="/assets/blogImg/1.png" alt="1"><br>NameNode(名称节点)<br>存储：元信息的种类，包含:</p><ul><li>文件名称</li><li>文件目录结构</li><li>文件的属性[权限,创建时间,副本数]</li><li>文件对应哪些数据块–&gt;数据块对应哪些datanode节点</li><li>作用：</li><li>管理着文件系统命名空间</li><li>维护这文件系统树及树中的所有文件和目录</li><li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li></ul><p>DataNode(数据节点)<br>存储：数据块、数据块校验、与NameNode通信<br>作用：</p><ul><li>读写文件的数据块</li><li>NameNode的指示来进行创建、删除、和复制等操作</li><li>通过心跳定期向NameNode发送所存储文件块列表信息</li><li>Scondary NameNode(第二名称节点)<br>存储: 命名空间镜像文件fsimage+编辑日志editlog<br>作用: 定期合并fsimage+editlog文件为新的fsimage推送给NamenNode<h6 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h6><img src="/assets/blogImg/2.png" alt="2"><br><strong>第一副本</strong>：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上<br><strong>第二副本</strong>：放置在与第一个副本不同的机架的节点上<br><strong>第三副本</strong>：与第二个副本相同机架的不同节点上<br>如果还有更多的副本：随机放在节点中</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hdfs </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置多台虚拟机之间的SSH信任</title>
      <link href="/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/"/>
      <url>/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Fri Jun 14 2019 15:42:22 GMT+0800 (GMT+08:00) --><h4 id="本机环境"><a href="#本机环境" class="headerlink" title="本机环境"></a>本机环境</h4><a id="more"></a><p><img src="/assets/blogImg/640.png" alt="1"></p><p>3台机器执行命令ssh-keygen<br><img src="/assets/blogImg/641.png" alt="2"></p><p>选取第一台,生成authorized_keys文件<br><img src="/assets/blogImg/642.png" alt="3"></p><p>hadoop002 hadoop003传输id_rsa.pub文件到hadoop001<br><img src="/assets/blogImg/643.png" alt="4"><br><img src="/assets/blogImg/644.png" alt="5"></p><p>hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys<br><img src="/assets/blogImg/645.png" alt="6"></p><p>设置每台机器的权限<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 -R ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p></p><p>将authorized_keys分发到hadoop002、hadoop003机器<br><img src="/assets/blogImg/646.png" alt="7"></p><p><img src="/assets/blogImg/647.png" alt="8"></p><p>验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ssh root@hadoop002 date</span><br><span class="line">[root@hadoop002 ~]# ssh root@hadoop001 date</span><br><span class="line">[root@hadoop003 ~]# ssh root@hadoop001 date</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
            <tag> 基础 </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
