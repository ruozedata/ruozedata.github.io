<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ç”Ÿäº§SparkStreamingæ•°æ®é›¶ä¸¢å¤±æœ€ä½³å®è·µ(å«ä»£ç )]]></title>
    <url>%2F2019%2F06%2F14%2F%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)%2F</url>
    <content type="text"><![CDATA[MySQLåˆ›å»ºå­˜å‚¨offsetçš„è¡¨æ ¼123456789mysql&gt; use testmysql&gt; create table hlw_offset( topic varchar(32), groupid varchar(50), partitions int, fromoffset bigint, untiloffset bigint, primary key(topic,groupid,partitions) );Mavenä¾èµ–åŒ…12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;--------------------------------------------------&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe&lt;/groupId&gt; &lt;artifactId&gt;config&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt;å®ç°æ€è·¯123451ï¼‰StreamingContext2ï¼‰ä»kafkaä¸­è·å–æ•°æ®(ä»å¤–éƒ¨å­˜å‚¨è·å–offset--&gt;æ ¹æ®offsetè·å–kafkaä¸­çš„æ•°æ®)3ï¼‰æ ¹æ®ä¸šåŠ¡è¿›è¡Œé€»è¾‘å¤„ç†4ï¼‰å°†å¤„ç†ç»“æœå­˜åˆ°å¤–éƒ¨å­˜å‚¨ä¸­--ä¿å­˜offset5ï¼‰å¯åŠ¨ç¨‹åºï¼Œç­‰å¾…ç¨‹åºç»“æŸä»£ç å®ç°SparkStreamingä¸»ä½“ä»£ç å¦‚ä¸‹1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scalikejdbc._import scalikejdbc.config._object JDBCOffsetApp &#123; def main(args: Array[String]): Unit = &#123; //åˆ›å»ºSparkStreamingå…¥å£ val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;) val ssc = new StreamingContext(conf,Seconds(5)) //kafkaæ¶ˆè´¹ä¸»é¢˜ val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet //kafkaå‚æ•° //è¿™é‡Œåº”ç”¨äº†è‡ªå®šä¹‰çš„ValueUtilså·¥å…·ç±»ï¼Œæ¥è·å–application.confé‡Œçš„å‚æ•°ï¼Œæ–¹ä¾¿åæœŸä¿®æ”¹ val kafkaParams = Map[String,String]( &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;), &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;), &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;) ) //å…ˆä½¿ç”¨scalikejdbcä»MySQLæ•°æ®åº“ä¸­è¯»å–offsetä¿¡æ¯ //+------------+------------------+------------+------------+-------------+ //| topic | groupid | partitions | fromoffset | untiloffset | //+------------+------------------+------------+------------+-------------+ //MySQLè¡¨ç»“æ„å¦‚ä¸Šï¼Œå°†â€œtopicâ€ï¼Œâ€œpartitionsâ€ï¼Œâ€œuntiloffsetâ€åˆ—è¯»å–å‡ºæ¥ //ç»„æˆ fromOffsets: Map[TopicAndPartition, Long]ï¼Œåé¢createDirectStreamç”¨åˆ° DBs.setup() val fromOffset = DB.readOnly( implicit session =&gt; &#123; SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123; (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;)) &#125;).list().apply() &#125;).toMap //å¦‚æœMySQLè¡¨ä¸­æ²¡æœ‰offsetä¿¡æ¯ï¼Œå°±ä»0å¼€å§‹æ¶ˆè´¹ï¼›å¦‚æœæœ‰ï¼Œå°±ä»å·²ç»å­˜åœ¨çš„offsetå¼€å§‹æ¶ˆè´¹ val messages = if (fromOffset.isEmpty) &#123; println(&quot;ä»å¤´å¼€å§‹æ¶ˆè´¹...&quot;) KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics) &#125; else &#123; println(&quot;ä»å·²å­˜åœ¨è®°å½•å¼€å§‹æ¶ˆè´¹...&quot;) val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message()) KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler) &#125; messages.foreachRDD(rdd=&gt;&#123; if(!rdd.isEmpty())&#123; //è¾“å‡ºrddçš„æ•°æ®é‡ println(&quot;æ•°æ®ç»Ÿè®¡è®°å½•ä¸ºï¼š&quot;+rdd.count()) //å®˜æ–¹æ¡ˆä¾‹ç»™å‡ºçš„è·å¾—rdd offsetä¿¡æ¯çš„æ–¹æ³•ï¼ŒoffsetRangesæ˜¯ç”±ä¸€ç³»åˆ—offsetRangeç»„æˆçš„æ•°ç»„// trait HasOffsetRanges &#123;// def offsetRanges: Array[OffsetRange]// &#125; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges offsetRanges.foreach(x =&gt; &#123; //è¾“å‡ºæ¯æ¬¡æ¶ˆè´¹çš„ä¸»é¢˜ï¼Œåˆ†åŒºï¼Œå¼€å§‹åç§»é‡å’Œç»“æŸåç§»é‡ println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;) //å°†æœ€æ–°çš„åç§»é‡ä¿¡æ¯ä¿å­˜åˆ°MySQLè¡¨ä¸­ DB.autoCommit( implicit session =&gt; &#123; SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;) .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset) .update().apply() &#125;) &#125;) &#125; &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;è‡ªå®šä¹‰çš„ValueUtilså·¥å…·ç±»å¦‚ä¸‹12345678910111213import com.typesafe.config.ConfigFactoryimport org.apache.commons.lang3.StringUtilsobject ValueUtils &#123;val load = ConfigFactory.load() def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;val value = load.getString(key) if(StringUtils.isNotEmpty(value)) &#123; value &#125; else &#123; defaultValue &#125; &#125;&#125;application.confå†…å®¹å¦‚ä¸‹1234567891011metadata.broker.list = &quot;192.168.137.251:9092&quot;auto.offset.reset = &quot;smallest&quot;group.id = &quot;hlw_offset_group&quot;kafka.topics = &quot;hlw_offset&quot;serializer.class = &quot;kafka.serializer.StringEncoder&quot;request.required.acks = &quot;1&quot;# JDBC settingsdb.default.driver = &quot;com.mysql.jdbc.Driver&quot;db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;db.default.user=&quot;root&quot;db.default.password=&quot;123456&quot;è‡ªå®šä¹‰kafka producer123456789101112131415161718192021import java.util.&#123;Date, Properties&#125;import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;object KafkaProducer &#123; def main(args: Array[String]): Unit = &#123; val properties = new Properties() properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;)) properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;)) properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;)) val producerConfig = new ProducerConfig(properties) val producer = new Producer[String,String](producerConfig) val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;) //æ¯æ¬¡äº§ç”Ÿ100æ¡æ•°æ® var i = 0 for (i &lt;- 1 to 100) &#123; val runtimes = new Date().toString val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes) producer.send(messages) &#125; println(&quot;æ•°æ®å‘é€å®Œæ¯•...&quot;) &#125;&#125;æµ‹è¯•å¯åŠ¨kafkaæœåŠ¡ï¼Œå¹¶åˆ›å»ºä¸»é¢˜123[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offsetæµ‹è¯•å‰æŸ¥çœ‹MySQLä¸­offsetè¡¨ï¼Œåˆšå¼€å§‹æ˜¯ä¸ªç©ºè¡¨12mysql&gt; select * from hlw_offset;Empty set (0.00 sec)é€šè¿‡kafka produceräº§ç”Ÿ500æ¡æ•°æ®å¯åŠ¨SparkStreamingç¨‹åº1234//æ§åˆ¶å°è¾“å‡ºç»“æœï¼šä»å¤´å¼€å§‹æ¶ˆè´¹...æ•°æ®ç»Ÿè®¡è®°å½•ä¸ºï¼š500---hlw_offset,0,0,500---æŸ¥çœ‹MySQLè¡¨ï¼Œoffsetè®°å½•æˆåŠŸ 123456mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic | groupid | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group | 0 | 0 | 500 |+------------+------------------+------------+------------+-------------+ å…³é—­SparkStreamingç¨‹åºï¼Œå†ä½¿ç”¨kafka producerç”Ÿäº§300æ¡æ•°æ®,å†æ¬¡å¯åŠ¨sparkç¨‹åºï¼ˆå¦‚æœsparkä»500å¼€å§‹æ¶ˆè´¹ï¼Œè¯´æ˜æˆåŠŸè¯»å–äº†offsetï¼Œåšåˆ°äº†åªè¯»å–ä¸€æ¬¡è¯­ä¹‰ï¼‰1234//æ§åˆ¶å°ç»“æœè¾“å‡ºï¼šä»å·²å­˜åœ¨è®°å½•å¼€å§‹æ¶ˆè´¹...æ•°æ®ç»Ÿè®¡è®°å½•ä¸ºï¼š300---hlw_offset,0,500,800---æŸ¥çœ‹æ›´æ–°åçš„offset MySQLæ•°æ®123456mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic | groupid | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group | 0 | 500 | 800 |+------------+------------------+------------+------------+-------------+]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>spark streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019ç«¯åˆ-çº¿ä¸‹é¡¹ç›®ç¬¬14æœŸåœ†æ»¡ç»“æŸ]]></title>
    <url>%2F2019%2F06%2F11%2F2019%E7%AB%AF%E5%8D%88-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC14%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019å¹´ç«¯åˆèŠ‚4å¤©3å¤œä¸Šæµ·çº¿ä¸‹ç­åœ†æ»¡ç»“æŸä¸€å¥è¯ï¼Œä¸Šæµ·æ¸©åº¦æœ‰ç‚¹ç‡¥çƒ­ä½†ï¼Œä¼šè®®å®¤ç©ºè°ƒå¾ˆç»™åŠ›å°ä¼™ä¼´ä»¬æ¥è‡ª11ä¸ªåŸå¸‚åŒ—äº¬ã€ä¸Šæµ·ã€æ·±åœ³å¹¿å·ã€æ­å·ã€åˆè‚¥ã€å¾å·çŸ³å®¶åº„ã€å¤§åº†ã€å¤©æ´¥ã€å¦é—¨å¤§å®¶ä¸ºäº†ä¸€ä¸ªçœŸå®ç›®æ ‡å­¦ä¹ çœŸæ­£ä¼ä¸šçº§å¤§æ•°æ®ç”Ÿäº§é¡¹ç›®3ä¸ªç”Ÿäº§é¡¹ç›®+2ä¸ªTopicåˆ†äº«ä¸€å¹´æˆ‘ä»¬åªåœ¨èŠ‚å‡æ—¥&amp;å‘¨æœ«ä¸¾åŠé”™è¿‡äº†å°±æ˜¯é”™è¿‡äº†æœŸå¾…8æœˆä¸‹æ—¬çº¿ä¸‹é¡¹ç›®ç­ç¬¬15æœŸ]]></content>
      <categories>
        <category>çº¿ä¸‹å®æˆ˜ç­</category>
      </categories>
      <tags>
        <tag>çº¿ä¸‹å®æˆ˜ç­</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¸¸ç”¨Sparkç´¯åŠ å™¨å‰–æä¹‹å››]]></title>
    <url>%2F2019%2F05%2F31%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[ç°è±¡æè¿°1234567891011val acc = sc.accumulator(0, â€œError Accumulatorâ€)val data = sc.parallelize(1 to 10)val newData = data.map(x =&gt; &#123; if (x % 2 == 0) &#123; accum += 1&#125;&#125;)newData.countacc.valuenewData.foreach(println)acc.valueä¸Šè¿°ç°è±¡ï¼Œä¼šé€ æˆacc.valueçš„æœ€ç»ˆå€¼å˜ä¸º10åŸå› åˆ†æSparkä¸­çš„ä¸€ç³»åˆ—transformæ“ä½œéƒ½ä¼šæ„é€ æˆä¸€é•¿ä¸²çš„ä»»åŠ¡é“¾ï¼Œæ­¤æ—¶å°±éœ€è¦é€šè¿‡ä¸€ä¸ªactionæ“ä½œæ¥è§¦å‘ï¼ˆlazyçš„ç‰¹æ€§ï¼‰ï¼Œaccumulatorä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤åœ¨ä¸€ä¸ªactionæ“ä½œä¹‹åï¼Œè°ƒç”¨valueæ–¹æ³•æŸ¥çœ‹ï¼Œæ˜¯æ²¡æœ‰ä»»ä½•å˜åŒ–ç¬¬ä¸€æ¬¡actionæ“ä½œä¹‹åï¼Œè°ƒç”¨valueæ–¹æ³•æŸ¥çœ‹ï¼Œå˜æˆäº†5ç¬¬äºŒæ¬¡actionæ“ä½œä¹‹åï¼Œè°ƒç”¨valueæ–¹æ³•æŸ¥çœ‹ï¼Œå˜æˆäº†10åŸå› å°±åœ¨äºç¬¬äºŒæ¬¡actionæ“ä½œçš„æ—¶å€™ï¼Œåˆæ‰§è¡Œäº†ä¸€æ¬¡ç´¯åŠ å™¨çš„æ“ä½œï¼ŒåŒä¸ªç´¯åŠ å™¨ï¼Œåœ¨åŸæœ‰çš„åŸºç¡€ä¸ŠåˆåŠ äº†5ï¼Œä»è€Œå˜æˆäº†10è§£å†³æ–¹æ¡ˆé€šè¿‡ä¸Šè¿°çš„ç°è±¡æè¿°ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¿«çŸ¥é“è§£å†³çš„æ–¹æ³•ï¼šåªè¿›è¡Œä¸€æ¬¡actionæ“ä½œã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬åªè¦åˆ‡æ–­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»å°±å¯ä»¥äº†ï¼Œå³ä½¿ç”¨cacheã€persistã€‚è¿™æ ·æ“ä½œä¹‹åï¼Œé‚£ä¹ˆåç»­çš„ç´¯åŠ å™¨æ“ä½œå°±ä¸ä¼šå—å‰é¢çš„transformæ“ä½œå½±å“äº†æ¡ˆä¾‹åœ°å€ç›¸å…³çš„å·¥ç¨‹æ¡ˆä¾‹åœ°å€åœ¨Githubä¸Šï¼šhttps://github.com/lemonahit/spark-train/tree/master/01-Accumulator123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * ä½¿ç”¨Spark Accumulatorså®ŒæˆJobçš„æ•°æ®é‡å¤„ç† * ç»Ÿè®¡empè¡¨ä¸­NULLå‡ºç°çš„æ¬¡æ•°ä»¥åŠæ­£å¸¸æ•°æ®çš„æ¡æ•° &amp; æ‰“å°æ­£å¸¸æ•°æ®çš„ä¿¡æ¯ * * è‹¥æ³½æ•°æ®å­¦å‘˜-å‘¼å‘¼å‘¼ on 2017/11/9. */object AccumulatorsApp &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;AccumulatorsApp&quot;) val sc = new SparkContext(conf) val lines = sc.textFile(&quot;E:/emp.txt&quot;) // longç±»å‹çš„ç´¯åŠ å™¨å€¼ val nullNum = sc.longAccumulator(&quot;NullNumber&quot;) val normalData = lines.filter(line =&gt; &#123; var flag = true val splitLines = line.split(&quot;\t&quot;) for (splitLine &lt;- splitLines)&#123; if (&quot;&quot;.equals(splitLine))&#123; flag = false nullNum.add(1) &#125; &#125; flag &#125;) // ä½¿ç”¨cacheæ–¹æ³•ï¼Œå°†RDDçš„ç¬¬ä¸€æ¬¡è®¡ç®—ç»“æœè¿›è¡Œç¼“å­˜ï¼›é˜²æ­¢åé¢RDDè¿›è¡Œé‡å¤è®¡ç®—ï¼Œå¯¼è‡´ç´¯åŠ å™¨çš„å€¼ä¸å‡†ç¡® normalData.cache() // æ‰“å°æ¯ä¸€æ¡æ­£å¸¸æ•°æ® normalData.foreach(println) // æ‰“å°æ­£å¸¸æ•°æ®çš„æ¡æ•° println(&quot;NORMAL DATA NUMBER: &quot; + normalData.count()) // æ‰“å°empè¡¨ä¸­NULLå‡ºç°çš„æ¬¡æ•° println(&quot;NULL: &quot; + nullNum.value) sc.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç´¯åŠ å™¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸Šæµ·æŸå…¬å¸çš„ç”Ÿäº§MySQLç¾éš¾æ€§æŒ½æ•‘]]></title>
    <url>%2F2019%2F05%2F30%2F%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91%2F</url>
    <content type="text"><![CDATA[1.èƒŒæ™¯æœ¬äºº(è‹¥æ³½æ•°æ®Jå“¥)çš„åª³å¦‡ï¼Œæ˜¯ä¸ªæ¼‚äº®çš„å¦¹å­ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€æšçˆ¬è™«&amp;Sparkå¼€å‘å·¥ç¨‹å¸ˆã€‚å‰å¤©ï¼Œå¥¹çš„å…¬å¸MySQL(é˜¿é‡Œäº‘ECSæœåŠ¡å™¨)ï¼Œç”±äºç£ç›˜çˆ†äº†åŠ ä¸Šäººä¸ºçš„ä¿®å¤ï¼Œå¯¼è‡´å„ç§é—®é¢˜ï¼Œç„¶åç»è¿‡2å¤©çš„æŠ˜è…¾ï¼Œç»ˆäºå…¬å¸çš„å¤§ç¥ä¿®å¤ä¸äº†äº†ã€‚äºæ˜¯å°±ä¸¢ç»™å¥¹äº†ï¼Œé¡ºç†æˆç« çš„å°±ä¸¢ç»™æˆ‘äº†ã€‚æˆ‘æƒ³è¯´ï¼Œéš¾é“Jå“¥è¿™ä¹ˆå‡ºåå—ï¼Ÿé‚£ä¸ºäº†åœ¨å¦¹å­é¢å‰ä¸èƒ½ä¸¢æˆ‘ä»¬çœŸæ­£å¤§ä½¬çš„ç¥æŠ€ï¼Œäºæ˜¯ä¹æˆ‘å°±å¾ˆçˆ½å¿«æ¥äº†è¿™ä¸ªMySQLæ•…éšœæ¢å¤ï¼Œæ­¤æ¬¡æ•…éšœçš„æ˜¯ä¸€ä¸ªæ•°æ®ç›˜ï¼Œ1Tã€‚è¿™æ—¶çš„æˆ‘ï¼Œè¯´çœŸçš„å¹¶æ²¡æœ‰æ„è¯†åˆ°ï¼Œæ­¤äº‹æ˜¯å¦‚æ­¤çš„ç¹æ‚ï¼Œç‰¹æ­¤å†™æ­¤åšæ–‡è®°å½•ä¸€ä¸‹ï¼Œæ¯•ç«ŸJå“¥æˆ‘å¹´çºªä¹Ÿå¤§äº†ã€‚PS:è¿™é‡Œåæ§½ä¸€ä¸‹ï¼Œå¹¶æ²¡æœ‰å‘¨æ—¥å…¨å¤‡+å‘¨1~å‘¨6å¢é‡å¤‡ä»½æœºåˆ¶å“Ÿï¼Œä¸ç„¶æ¢å¤å°±çˆ½æ­ªæ­ªäº†ã€‚2.æ•…éšœç°è±¡12æŸ¥çœ‹è¡¨ç»“æ„ã€æŸ¥è¯¢è¡¨æ•°æ®éƒ½å¦‚ä¸‹æŠ›é”™:ERROR 1030 (HY000): Got error 122 from storage engine3.å°è¯•ä¿®å¤ç¬¬ä¸€æ¬¡ï¼Œå¤±è´¥3.1 ä½¿ç”¨repairå‘½ä»¤ä¿®å¤è¡¨123mysql&gt; repair table wenshu.wenshu2018; é”™è¯¯ä¾æ—§:ERROR 1030 (HY000): Got error 122 from storage engine3.2 è°·æ­Œä¸€ç¯‡æœ‰æŒ‡å¯¼æ„ä¹‰çš„https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine3.2.1 è®©å…¶æ‰©å®¹æ•°æ®ç£ç›˜ä¸º1.5Tï¼Œè¯•è¯•ï¼Œä¾æ—§è¿™ä¸ªé”™è¯¯ï¼›3.2.2 ä¸´æ—¶ç›®å½•ä¿®æ”¹ä¸ºå¤§çš„ç£ç›˜ç©ºé—´ï¼Œè¯•è¯•ï¼Œä¾æ—§è¿™ä¸ªé”™è¯¯ï¼›3.2.3 å–æ¶ˆç£ç›˜é™é¢ï¼Œè¯•è¯•ï¼Œä¾æ—§è¿™ä¸ªé”™è¯¯ï¼›3.2.4 å°±æ˜¯ä¸€å¼€å§‹çš„repairå‘½ä»¤ä¿®å¤ï¼Œè¯•è¯•ï¼Œä¾æ—§è¿™ä¸ªé”™è¯¯ï¼›è¿™æ—¶çš„æˆ‘ï¼Œä¹Ÿæ— è¯­äº†ï¼Œä»€ä¹ˆé¬¼ï¼è°·æ­Œä¸€é¡µé¡µæœç´¢éªŒè¯ï¼Œæ²¡æœ‰ç”¨ï¼4.å…ˆéƒ¨ç½²ç›¸åŒç³»ç»Ÿçš„ç›¸åŒç‰ˆæœ¬çš„æœºå™¨å’ŒMySQLäºæ˜¯Jå“¥ï¼Œå¿«é€Ÿåœ¨ã€è‹¥æ³½æ•°æ®ã€‘çš„é˜¿é‡Œäº‘è´¦å·ä¸Šä¹°äº†1å°Ubuntu 16.04.6çš„æŒ‰é‡ä»˜è´¹æœºå™¨è¿…é€Ÿéƒ¨ç½²MySQL5.7.26ã€‚4.1 è´­ä¹°æŒ‰é‡ä»˜è´¹æœºå™¨(å‡å¦‚ä¸ä¼šè´­ä¹°ï¼Œæ‰¾Jå“¥)4.2 éƒ¨ç½²MySQL123456789101112131415161718a.æ›´æ–°apt-get$ apt-get updateb.å®‰è£…MySQL-Server$ apt-get install mysql-serverä¹‹åä¼šé—®ä½ ï¼Œæ˜¯å¦è¦ä¸‹è½½æ–‡ä»¶ï¼Œ è¾“å…¥ y å°±å¥½äº†ç„¶åä¼šå‡ºç°è®©ä½ è®¾ç½® root å¯†ç çš„ç•Œé¢è¾“å…¥å¯†ç : ruozedata123ç„¶åå†é‡å¤ä¸€ä¸‹ï¼Œå†æ¬¡è¾“å…¥å¯†ç : ruozedata123c.å®‰è£…MySQL-Client$ apt install mysql-clientd.æˆ‘ä»¬å¯ä»¥ä½¿ç”¨$ mysql -uroot -pruozedata123æ¥è¿æ¥æœåŠ¡å™¨æœ¬åœ°çš„ MySQL5.å°è¯•å…ˆé€šè¿‡frmæ–‡ä»¶æ¢å¤è¡¨ç»“æ„ï¼Œå¤±è´¥123456789101112131415161718192021222324252627a. å»ºç«‹ä¸€ä¸ªæ•°æ®åº“ï¼Œæ¯”å¦‚wenshu.b. åœ¨ruozedataæ•°æ®åº“ä¸‹å»ºç«‹åŒåçš„æ•°æ®è¡¨wenshu2018ï¼Œè¡¨ç»“æ„éšæ„ï¼Œè¿™é‡Œåªæœ‰ä¸€ä¸ªidå­—æ®µï¼Œæ“ä½œè¿‡ç¨‹ç‰‡æ®µå¦‚ä¸‹ï¼šmysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;mysql&gt; show tables;+--------------+| Tables_in_aa |+--------------+| wenshu2018 |+--------------+1 rows in set (0.00 sec)mysql&gt; desc wenshu2018;+-------+------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+------------+------+-----+---------+-------+| id | bigint(20) | NO | | NULL | |+-------+------------+------+-----+---------+-------+1 row in set (0.00 sec)c.åœæ­¢mysqlæœåŠ¡å™¨ï¼Œå°†wenshu2018.frmæ–‡ä»¶scpè¿œç¨‹æ‹·è´åˆ°æ–°çš„æ­£å¸¸æ•°æ®åº“çš„æ•°æ®ç›®å½•wenshuä¸‹ï¼Œè¦†ç›–æ‰ä¸‹è¾¹åŒåçš„frmæ–‡ä»¶ï¼šd.é‡æ–°å¯åŠ¨MYSQLæœåŠ¡e.æµ‹è¯•ä¸‹æ˜¯å¦æ¢å¤æˆåŠŸï¼Œè¿›å…¥wenshuæ•°æ®åº“ï¼Œç”¨descå‘½ä»¤æµ‹è¯•ä¸‹ï¼Œé”™è¯¯ä¸º:mysql Tablespace is missing for table `wenshu`.`wenshu2018`.6.å°è¯•æœ‰æ²¡æœ‰å¤‡ä»½çš„è¡¨ç»“æ„æ¢å¤æ•°æ®ï¼Œå¤±è´¥åª³å¦‡å…¬å¸ç»™å‡ºä¸€ä¸ªè¡¨ç»“æ„,å¦‚ä¸‹ï¼Œç»è¿‡æµ‹è¯•æ— æ³•æ¢å¤ï¼ŒåŸå› å°±æ˜¯æ— æ³•å’Œibdæ–‡ä»¶åŒ¹é…ã€‚123456789101112DROP TABLE IF EXISTS cpws_batch;CREATE TABLE cpws_batch ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, crawl_time datetime NULL DEFAULT NULL, status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 æˆåŠŸ/å¤±è´¥&apos;, PRIMARY KEY (id) USING BTREE, INDEX ix_status(status) USING BTREE, INDEX ix_doc_id(doc_id) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;7.å¦‚ä½•è·å–æ­£ç¡®çš„è¡¨ç»“æ„ï¼Œè¿™æ˜¯ã€æˆåŠŸçš„ç¬¬ä¸€æ­¥ã€‘1234567891011121314151617181920212223242526272829303132$ curl -s get.dbsake.net &gt; /tmp/dbsake$ chmod u+x /tmp/dbsake$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm ---- Table structure for table wenshu_0_1000-- Created with MySQL Version 5.7.25--CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 æˆåŠŸ/å¤±è´¥&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;å¯¹æ¯”Step6çš„è¡¨ç»“æ„ï¼Œæ„Ÿè§‰å°±å·®åˆ†åŒºè®¾ç½®è€Œå·²ï¼Œå‘ï¼è¿™æ—¶ï¼ŒJå“¥æœ‰ç§ä¿¡å¿ƒï¼Œæ¢å¤åº”è¯¥å°èœäº†ã€‚8.ç”±äºæ¢å¤ECSæœºå™¨æ˜¯è‹¥æ³½æ•°æ®è´¦å·è´­ä¹°ï¼Œè¿™æ—¶éœ€è¦ä»åª³å¦‡å…¬å¸è´¦å·çš„æœºå™¨ä¼ è¾“è¿™å¼ è¡¨ibdæ–‡ä»¶ï¼Œå·®ä¸å¤š300Gï¼Œå°½ç®¡æˆ‘ä»¬æ˜¯é˜¿é‡Œäº‘çš„åŒä¸€ä¸ªåŒºåŸŸåŒä¸€ä¸ªå¯ç”¨åŒºï¼ŒåŠ ä¸Šè°ƒå¤§å¤–ç½‘å¸¦å®½ä¼ è¾“ï¼Œä¾ç„¶ä¸èƒ½ç­‰å¾…è¿™ä¹ˆä¹…ä¼ è¾“ï¼9.è¦æ±‚åª³å¦‡å…¬å¸è´­ä¹°åŒè´¦æˆ·ä¸‹åŒåŒºåŸŸçš„å¯ç”¨åŒºåŸŸçš„äº‘ä¸»æœºï¼Œç³»ç»Ÿç›˜300Gï¼Œæ²¡æœ‰ä¹°æ•°æ®ç›˜ï¼Œå…ˆå°è¯•åšæ¢å¤çœ‹çœ‹ï¼Œèƒ½ä¸èƒ½æˆåŠŸæ¢å¤ç¬¬ä¸€ä¸ªè¡¨å“Ÿï¼Ÿã€æˆåŠŸçš„ç¬¬äºŒæ­¥ã€‘1234567891011121314151617181920212223242526272829303132333435363738394041429.1é¦–å…ˆéœ€è¦ä¸€ä¸ªè·Ÿè¦æ¢å¤çš„è¡¨ç»“æ„å®Œå…¨ä¸€è‡´çš„è¡¨ï¼Œè‡³å…³é‡è¦mysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;USE wenshu;CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 æˆåŠŸ/å¤±è´¥&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;9.2ç„¶åDISCARD TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;9.3æŠŠè¦æ¢å¤çš„ibdæ–‡ä»¶å¤åˆ¶åˆ°mysqlçš„dataæ–‡ä»¶å¤¹ä¸‹ï¼Œä¿®æ”¹ç”¨æˆ·å’Œç”¨æˆ·ç»„ä¸ºmysql$ scp wenshu2018#P#p*.ibd æ–°å»ºæœºå™¨IP:/mnt/mysql_data/wenshu/$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd9.4ç„¶åæ‰§è¡ŒIMPORT TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;9.5ç­‰å¾…ï¼Œæœ‰æˆï¼Œè€—æ—¶3hï¼Œè¿™æ—¶æˆ‘ç›¸ä¿¡åº”è¯¥ä¹ˆé—®é¢˜çš„9.6æŸ¥è¯¢æ•°æ®ï¼Œæœç„¶æ¢å¤æœ‰ç»“æœï¼Œå¿ƒé‡Œæš—æš—è‡ªå–œmysql&gt; select * from wenshu.wenshu2018 limit 1\G;10.ç»™åª³å¦‡å…¬å¸ä¸¤ä¸ªé€‰æ‹©ï¼Œè¿™ä¸ªå¾ˆé‡è¦ï¼Œåœ¨è‡ªå·±å…¬å¸ç»™é¢†å¯¼åšé€‰æ‹©æ—¶ï¼Œä¹Ÿè¦åº”è¯¥è¿™æ ·ï¼Œå¤šé¡¹é€‰æ‹©ï¼Œåˆ©å¼Šè¯´æ˜ï¼Œä¾›å¯¹æ–¹é€‰æ‹©10.1 é‡æ–°è´­ä¹°ä¸€å°æ–°çš„æœåŠ¡å™¨ï¼Œåœ¨åˆå§‹åŒ–é…ç½®æ—¶ï¼Œå°±åŠ ä¸Š1å—1.5Tçš„å¤§ç£ç›˜ã€‚å¥½å¤„æ˜¯æ— éœ€æŒ‚ç›˜æ“ä½œï¼Œåå¤„æ˜¯éœ€è¦é‡æ–°åšç¬¬ä¸€ä¸ªè¡¨ï¼Œæµªè´¹3hï¼›10.2 è´­ä¹°1.5Tçš„å¤§ç£ç›˜ï¼ŒæŒ‚è½½è¿™ä¸ªæœºå™¨ä¸Šã€‚å¥½å¤„æ˜¯æ— éœ€å†åšä¸€æ¬¡ç¬¬ä¸€ä¸ªè¡¨ï¼Œåå¤„æ˜¯éœ€è¦ä¿®æ”¹mysqlçš„æ•°æ®ç›®å½•æŒ‡å‘ä¸ºè¿™ä¸ªå¤§ç£ç›˜ã€‚ç³»ç»Ÿç›˜æ‰©å®¹æœ€å¤§ä¹Ÿå°±500Gï¼Œæ‰€ä»¥å¿…é¡»å¤–åŠ ä¸€ä¸ªæ•°æ®ç›˜1.5Tå®¹é‡ã€‚æ‰€ä»¥Jå“¥æ˜¯èŒåœºè€æ‰‹äº†ï¼è´¼ç¬‘ï¼11.æœåŠ¡å™¨åŠ æ•°æ®ç£ç›˜ï¼Œ1.5Tï¼Œè´­ä¹°ã€æŒ‚è½½ã€æ ¼å¼åŒ–æ¥ä¸‹æ¥çš„æ“ä½œæ˜¯æˆ‘åª³å¦‡ç‹¬ç«‹å®Œæˆçš„ï¼Œè¿™é‡Œè¡¨æ‰¬ä¸€ä¸‹:11.1 å…ˆä¹°äº‘ç›˜ https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n811.2 å†æŒ‚è½½äº‘ç›˜ åˆ°å¯¹åº”æœºå™¨ https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB11.3 æœ€åLinuxæ ¼å¼åŒ–æ•°æ®ç›˜ https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lrå›¾2æ‰€ç¤ºï¼Œdf -hå‘½ä»¤æŸ¥çœ‹ï¼Œå¤§ç£ç›˜/dev/vdb112.MySQLä¿®æ”¹æ•°æ®ç›®å½•ä¸ºå¤§ç£ç›˜ï¼Œé‡æ–°å¯åŠ¨å¤±è´¥ï¼Œè§£å†³12345678910111213141516171819202122232425262728293031323312.1 ä¿®æ”¹æ•°æ®ç›®å½•ä¸ºå¤§ç£ç›˜$ mkdir -p /mnt/mysql_data$ chown mysql:mysql /mnt/mysql_data$ vi /etc/mysql/mysql.conf.d/mysqld.cnfdatadir = /mnt/mysql_data12.2 æ— æ³•å¯åŠ¨mysql$ service mysql restartæ— æ³•å¯åŠ¨æˆåŠŸï¼ŒæŸ¥çœ‹æ—¥å¿—2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error12.3 ç™¾æ€ä¸å¾—å…¶è§£ï¼ŒCentOSä¹Ÿæ²¡æœ‰è¿™ä¹ˆéº»çƒ¦ï¼ŒUbuntuéš¾é“è¿™ä¹ˆæäº‹å—ï¼Ÿ12.4 æ–°å¢mysqldå†…å®¹$ vi /etc/apparmor.d/local/usr.sbin.mysqld# Site-specific additions and overrides for usr.sbin.mysqld.# For more details, please see /etc/apparmor.d/local/README./mnt/mysql_data/ r,/mnt/mysql_data/** rwk,12.5 reload apparmorçš„é…ç½®å¹¶é‡å¯$ service apparmor reload $ service apparmor restart 12.6 é‡å¯mysql$ service mysql restartå¦‚æœå¯åŠ¨ä¸äº†ï¼ŒæŸ¥çœ‹/var/log/mysql/error.logå¦‚æœå‡ºç°ï¼šInnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable ä»”ç»†æ ¸å¯¹ç›®å½•æƒé™12.7 è¿›mysqlæŸ¥è¯¢æ•°æ®éªŒè¯ï¼ŒæˆåŠŸselect * from wenshu.wenshu2018 limit 1\G;13.å¼€å§‹æŒ‡å¯¼æˆ‘åª³å¦‡åšç¬¬äºŒä¸ªã€ç¬¬ä¸‰ä¸ªè¡¨ï¼Œæ‰¹é‡æ¢å¤ï¼Œè€—æ—¶å…±è®¡16å°æ—¶ï¼Œå…¨éƒ¨æ¢å¤å®Œæˆã€‚æœ€å@è‹¥æ³½æ•°æ®Jå“¥æ€»ç»“ä¸€ä¸‹:è¡¨ç»“æ„æ­£ç¡®çš„è·å–ï¼›æœºå™¨ç£ç›˜è§„åˆ’æå‰æ€è€ƒï¼›ibdæ•°æ®æ–‡ä»¶æ¢å¤ï¼›æœ€ååŠ ä¸Šä¸€ä¸ªèªæ˜çš„åª³å¦‡ï¼(PS:è€æ¿ä¼šç»™åª³å¦‡æ¶¨è–ªæ°´ä¸ğŸ™…â€â™‚ï¸)]]></content>
      <categories>
        <category>å…¶ä»–ç»„ä»¶</category>
        <category>æ•…éšœæ¡ˆä¾‹</category>
      </categories>
      <tags>
        <tag>æ¶æ„</tag>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>æ¡ˆä¾‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…¥é—¨Impalaåªéœ€æ­¤ç¯‡]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)%2F</url>
    <content type="text"><![CDATA[å­¦ä¹ è·¯å¾„å®˜ç½‘ï¼šhttp://impala.apache.org/ä½¿ç”¨æ‰‹å†Œï¼šhttp://impala.apache.org/docs/build/html/index.htmlSqlï¼šhttp://impala.apache.org/docs/build/html/topics/impala_langref_sql.htmlçª—å£å‡½æ•°ï¼šhttp://impala.apache.org/docs/build/html/topics/impala_functions.htmlåŸºæœ¬æ“ä½œï¼šhttp://impala.apache.org/docs/build/html/topics/impala_tutorial.htmlimpala-shellï¼šhttp://impala.apache.org/docs/build/html/topics/impala_impala_shell.htmlæ¦‚è¿°Apache Impalaæ˜¯Apache Hadoopçš„å¼€æºåŸç”Ÿåˆ†ææ•°æ®åº“;Impalaäº2017å¹´11æœˆ15æ—¥ä»Apacheå­µåŒ–æˆé¡¶çº§é¡¹ç›®ã€‚åœ¨ä»¥å‰ç§°ä¸ºâ€œCloudera Impalaâ€çš„æ–‡æ¡£ä¸­ï¼Œç°åœ¨çš„å®˜æ–¹åç§°æ˜¯â€œApache Impalaâ€ã€‚Impalaä¸ºHadoopä¸Šçš„BI /åˆ†ææŸ¥è¯¢æä¾›ä½å»¶è¿Ÿå’Œé«˜å¹¶å‘æ€§ï¼ˆä¸æ˜¯ç”±Apache Hiveç­‰æ‰¹å¤„ç†æ¡†æ¶æä¾›ï¼‰ã€‚å³ä½¿åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼ŒImpalaä¹Ÿå¯ä»¥çº¿æ€§æ‰©å±•ã€‚åˆ©ç”¨ä¸Hadoopéƒ¨ç½²ç›¸åŒçš„æ–‡ä»¶å’Œæ•°æ®æ ¼å¼ä»¥åŠå…ƒæ•°æ®ï¼Œå®‰å…¨æ€§å’Œèµ„æºç®¡ç†æ¡†æ¶ - æ— å†—ä½™åŸºç¡€æ¶æ„æˆ–æ•°æ®è½¬æ¢/å¤åˆ¶ã€‚å¯¹äºApache Hiveç”¨æˆ·ï¼ŒImpalaä½¿ç”¨ç›¸åŒçš„å…ƒæ•°æ®å’ŒODBCé©±åŠ¨ç¨‹åºã€‚ä¸Hiveä¸€æ ·ï¼ŒImpalaæ”¯æŒSQLImpalaä¸æœ¬æœºHadoopå®‰å…¨æ€§å’ŒKerberosé›†æˆä»¥è¿›è¡Œèº«ä»½éªŒè¯ï¼Œé€šè¿‡Sentryæ¨¡å—ï¼Œæ‚¨å¯ä»¥ç¡®ä¿ä¸ºæ­£ç¡®çš„ç”¨æˆ·å’Œåº”ç”¨ç¨‹åºæˆæƒä½¿ç”¨æ­£ç¡®çš„æ•°æ®ã€‚ä½¿ç”¨Impalaï¼Œæ— è®ºæ˜¯ä½¿ç”¨SQLæŸ¥è¯¢è¿˜æ˜¯BIåº”ç”¨ç¨‹åºï¼Œæ›´å¤šç”¨æˆ·éƒ½å¯ä»¥é€šè¿‡å•ä¸ªå­˜å‚¨åº“å’Œå…ƒæ•°æ®å­˜å‚¨è¿›è¡Œäº¤äº’ä»€ä¹ˆæ˜¯ImpalaImpalaæ˜¯ä¸€ç§é¢å‘å®æ—¶æˆ–è€…é¢å‘æ‰¹å¤„ç†çš„æ¡†æ¶;Impalaçš„æ•°æ®å¯ä»¥å­˜å‚¨åœ¨HDFS,HBaseå’ŒAmazon Simple Storage Servive(S3)ä¸­;Impalaå’ŒHiveä½¿ç”¨äº†ç›¸åŒçš„å…ƒæ•°æ®å­˜å‚¨;å¯ä»¥é€šè¿‡SQLçš„è¯­æ³•,JDBC,ODBCå’Œç”¨æˆ·ç•Œé¢(Hueä¸­çš„Impalaè¿›è¡ŒæŸ¥è¯¢);æˆ‘ä»¬çŸ¥é“Hiveåº•å±‚æ˜¯MapReduce,åœ¨è¿™é‡Œå°±å¯ä»¥çœ‹å‡ºåŒºåˆ«äº†,Impalaå¹¶ä¸æ˜¯ä¸ºäº†æ›¿æ¢æ„å»ºåœ¨MapReduceä¸Šçš„æ‰¹å¤„ç†æ¡†æ¶,å°±åƒæˆ‘ä»¬è¯´çš„Hive,Hiveé€‚ç”¨äºé•¿æ—¶é—´è¿è¡Œçš„æ‰¹å¤„ç†ä½œä¸š,ä¾‹å¦‚æ¶‰åŠåˆ°Extract,Transformå’ŒLoad(ETL)ç±»å‹çš„ä½œä¸š.è€ŒImpalaæ˜¯è¿›è¡Œå®æ—¶å¤„ç†çš„.ä¼˜åŠ¿é€šè¿‡sqlè¿›è¡Œå¤§é‡æ•°æ®å¤„ç†;å¯ä»¥è¿›è¡Œåˆ†å¸ƒå¼éƒ¨ç½²,è¿›è¡Œåˆ†å¸ƒå¼æŸ¥è¯¢;å¯ä»¥å’Œä¸åŒç»„ä»¶ä¹‹é—´è¿›è¡Œæ•°æ®å…±äº«,ä¸éœ€è¦å¤åˆ¶æˆ–è€…å¯¼å…¥,å¯¼å‡ºç­‰æ­¥éª¤,ä¾‹å¦‚:å¯ä»¥å…ˆä½¿ç”¨hiveå¯¹æ•°æ®è¿›è¡ŒETLæ“ä½œç„¶åä½¿ç”¨Impalaè¿›è¡ŒæŸ¥è¯¢.å› ä¸ºImpalaå’Œhiveå…¬ç”¨åŒä¸€ä¸ªå…ƒæ•°æ®,è¿™æ ·å°±å¯ä»¥æ–¹ä¾¿çš„å¯¹hiveç”Ÿæˆçš„æ•°æ®è¿›è¡Œåˆ†æ.Impalaå¦‚ä½•ä¸Apache Hadoopä¸€èµ·ä½¿ç”¨Impalaè§£å†³æ–¹æ¡ˆç”±ä»¥ä¸‹ç»„ä»¶ç»„æˆï¼šå®¢æˆ·ç«¯ - åŒ…æ‹¬Hueï¼ŒODBCå®¢æˆ·ç«¯ï¼ŒJDBCå®¢æˆ·ç«¯å’ŒImpala Shellçš„å®ä½“éƒ½å¯ä»¥ä¸Impalaè¿›è¡Œäº¤äº’ã€‚è¿™äº›æ¥å£é€šå¸¸ç”¨äºå‘å‡ºæŸ¥è¯¢æˆ–å®Œæˆç®¡ç†ä»»åŠ¡ï¼Œä¾‹å¦‚è¿æ¥åˆ°Impalaã€‚Hive Metastore - å­˜å‚¨æœ‰å…³Impalaå¯ç”¨æ•°æ®çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼ŒMetastoreè®©ImpalaçŸ¥é“å“ªäº›æ•°æ®åº“å¯ç”¨ï¼Œä»¥åŠè¿™äº›æ•°æ®åº“çš„ç»“æ„æ˜¯ä»€ä¹ˆã€‚åœ¨åˆ›å»ºï¼Œåˆ é™¤å’Œæ›´æ”¹æ¨¡å¼å¯¹è±¡ï¼Œå°†æ•°æ®åŠ è½½åˆ°è¡¨ä¸­ç­‰ç­‰æ—¶ï¼Œé€šè¿‡Impala SQLè¯­å¥ï¼Œç›¸å…³çš„å…ƒæ•°æ®æ›´æ”¹å°†é€šè¿‡Impala 1.2ä¸­å¼•å…¥çš„ä¸“ç”¨ç›®å½•æœåŠ¡è‡ªåŠ¨å¹¿æ’­åˆ°æ‰€æœ‰ImpalaèŠ‚ç‚¹ã€‚Impala - æ­¤è¿‡ç¨‹åœ¨DataNodesä¸Šè¿è¡Œï¼Œåè°ƒå¹¶æ‰§è¡ŒæŸ¥è¯¢ã€‚Impalaçš„æ¯ä¸ªå®ä¾‹éƒ½å¯ä»¥æ¥æ”¶ï¼Œè®¡åˆ’å’Œåè°ƒæ¥è‡ªImpalaå®¢æˆ·ç«¯çš„æŸ¥è¯¢ã€‚HBaseå’ŒHDFS -æ•°æ®çš„å­˜å‚¨ã€‚ä¸‹é¢è¿™å¹…å›¾åº”è¯¥è¯´çš„å¾ˆæ¸…æ¥šäº†:ä½¿ç”¨Impalaæ‰§è¡Œçš„æŸ¥è¯¢æµç¨‹å¦‚ä¸‹ï¼šç”¨æˆ·åº”ç”¨ç¨‹åºé€šè¿‡ODBCæˆ–JDBCå‘Impalaå‘é€SQLæŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢æä¾›æ ‡å‡†åŒ–çš„æŸ¥è¯¢æ¥å£ã€‚ç”¨æˆ·åº”ç”¨ç¨‹åºå¯ä»¥è¿æ¥åˆ°impaladç¾¤é›†ä¸­çš„ä»»ä½•åº”ç”¨ç¨‹åºã€‚è¿™impaladå°†æˆä¸ºæŸ¥è¯¢çš„åè°ƒè€…ã€‚Impalaä¼šè§£ææŸ¥è¯¢å¹¶å¯¹å…¶è¿›è¡Œåˆ†æï¼Œä»¥ç¡®å®šimpaladæ•´ä¸ªç¾¤é›†ä¸­çš„å®ä¾‹éœ€è¦æ‰§è¡Œå“ªäº›ä»»åŠ¡ ã€‚è®¡åˆ’æ‰§è¡Œä»¥å®ç°æœ€ä½³æ•ˆç‡ã€‚æœ¬åœ°impaladå®ä¾‹è®¿é—®HDFSå’ŒHBaseç­‰æœåŠ¡ä»¥æä¾›æ•°æ®ã€‚æ¯ä¸ªéƒ½impaladå°†æ•°æ®è¿”å›ç»™åè°ƒimpaladï¼Œåè°ƒå°†è¿™äº›ç»“æœå‘é€ç»™å®¢æˆ·ç«¯ã€‚impala-shellä½¿ç”¨Impala shellå·¥å…·ï¼ˆimpala-shellï¼‰æ¥è®¾ç½®æ•°æ®åº“å’Œè¡¨ï¼Œæ’å…¥æ•°æ®å’Œå‘å‡ºæŸ¥è¯¢é€‰é¡¹æè¿°-B or â€“delimitedå¯¼è‡´ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²çš„æ™®é€šæ–‡æœ¬æ ¼å¼æ‰“å°æŸ¥è¯¢ç»“æœã€‚å½“ä¸ºå…¶ä»– Hadoop ç»„ä»¶ç”Ÿæˆæ•°æ®æ—¶æœ‰ç”¨ã€‚å¯¹äºé¿å…æ•´é½æ‰“å°æ‰€æœ‰è¾“å‡ºçš„æ€§èƒ½å¼€é”€æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æŸ¥è¯¢è¿”å›å¤§é‡çš„ç»“æœé›†è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ—¶å€™ã€‚ä½¿ç”¨ â€“output_delimiter é€‰é¡¹æŒ‡å®šåˆ†éš”ç¬¦ã€‚ä½¿ç”¨ -B é€‰é¡¹å¸¸ç”¨äºä¿å­˜æ‰€æœ‰æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶é‡Œè€Œä¸æ˜¯æ‰“å°åˆ°å±å¹•ä¸Šã€‚åœ¨ Impala 1.0.1 ä¸­æ·»åŠ â€“print_headeræ˜¯å¦æ‰“å°åˆ—åã€‚æ•´é½æ‰“å°æ—¶æ˜¯é»˜è®¤å¯ç”¨ã€‚åŒæ—¶ä½¿ç”¨ -B é€‰é¡¹æ—¶ï¼Œåœ¨é¦–è¡Œæ‰“å°åˆ—å-o filename or â€“output_file filenameä¿å­˜æ‰€æœ‰æŸ¥è¯¢ç»“æœåˆ°æŒ‡å®šçš„æ–‡ä»¶ã€‚é€šå¸¸ç”¨äºä¿å­˜åœ¨å‘½ä»¤è¡Œä½¿ç”¨ -q é€‰é¡¹æ‰§è¡Œå•ä¸ªæŸ¥è¯¢æ—¶çš„æŸ¥è¯¢ç»“æœã€‚å¯¹äº¤äº’å¼ä¼šè¯åŒæ ·ç”Ÿæ•ˆï¼›æ­¤æ—¶ä½ åªä¼šçœ‹åˆ°è·å–äº†å¤šå°‘è¡Œæ•°æ®ï¼Œä½†çœ‹ä¸åˆ°å®é™…çš„æ•°æ®é›†ã€‚å½“ç»“åˆä½¿ç”¨ -q å’Œ -o é€‰é¡¹æ—¶ï¼Œä¼šè‡ªåŠ¨å°†é”™è¯¯ä¿¡æ¯è¾“å‡ºåˆ° /dev/null(To suppress these incidental messages when combining the -q and -o options, redirect stderr to /dev/null)ã€‚åœ¨ Impala 1.0.1 ä¸­æ·»åŠ â€“output_delimiter=characterå½“ä½¿ç”¨ -B é€‰é¡¹ä»¥æ™®é€šæ–‡ä»¶æ ¼å¼æ‰“å°æŸ¥è¯¢ç»“æœæ—¶ï¼Œç”¨äºæŒ‡å®šå­—æ®µä¹‹é—´çš„åˆ†éš”ç¬¦(Specifies the character to use as a delimiter between fields when query results are printed in plain format by the -B option)ã€‚é»˜è®¤æ˜¯åˆ¶è¡¨ç¬¦ tab (â€™\tâ€™)ã€‚å‡å¦‚è¾“å‡ºç»“æœä¸­åŒ…å«äº†åˆ†éš”ç¬¦ï¼Œè¯¥åˆ—ä¼šè¢«å¼•èµ·ä¸”/æˆ–è½¬ä¹‰( If an output value contains the delimiter character, that field is quoted and/or escaped)ã€‚åœ¨ Impala 1.0.1 ä¸­æ·»åŠ -p or â€“show_profileså¯¹ shell ä¸­æ‰§è¡Œçš„æ¯ä¸€ä¸ªæŸ¥è¯¢ï¼Œæ˜¾ç¤ºå…¶æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ (ä¸ EXPLAIN è¯­å¥è¾“å‡ºç›¸åŒ) å’Œå‘ç”Ÿä½çº§æ•…éšœ(low-level breakdown)çš„æ‰§è¡Œæ­¥éª¤çš„æ›´è¯¦ç»†çš„ä¿¡æ¯-h or â€“helpæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯-i hostname or â€“impalad=hostnameæŒ‡å®šè¿æ¥è¿è¡Œ impalad å®ˆæŠ¤è¿›ç¨‹çš„ä¸»æœºã€‚é»˜è®¤ç«¯å£æ˜¯ 21000ã€‚ä½ å¯ä»¥è¿æ¥åˆ°é›†ç¾¤ä¸­è¿è¡Œ impalad çš„ä»»æ„ä¸»æœºã€‚å‡å¦‚ä½ è¿æ¥åˆ° impalad å®ä¾‹é€šè¿‡ â€“fe_port æ ‡å¿—ä½¿ç”¨äº†å…¶ä»–ç«¯å£ï¼Œåˆ™åº”å½“åŒæ—¶æä¾›ç«¯å£å·ï¼Œæ ¼å¼ä¸º hostname:port-q query or â€“query=queryä»å‘½ä»¤è¡Œä¸­ä¼ é€’ä¸€ä¸ªæŸ¥è¯¢æˆ–å…¶ä»– shell å‘½ä»¤ã€‚æ‰§è¡Œå®Œè¿™ä¸€è¯­å¥å shell ä¼šç«‹å³é€€å‡ºã€‚é™åˆ¶ä¸ºå•æ¡è¯­å¥ï¼Œå¯ä»¥æ˜¯ SELECT, CREATE TABLE, SHOW TABLES, æˆ–å…¶ä»– impala-shell è®¤å¯çš„è¯­å¥ã€‚å› ä¸ºæ— æ³•ä¼ é€’ USE è¯­å¥å†åŠ ä¸Šå…¶ä»–æŸ¥è¯¢ï¼Œå¯¹äº default æ•°æ®åº“ä¹‹å¤–çš„è¡¨ï¼Œåº”åœ¨è¡¨åå‰åŠ ä¸Šæ•°æ®åº“æ ‡è¯†ç¬¦(æˆ–è€…ä½¿ç”¨ -f é€‰é¡¹ä¼ é€’ä¸€ä¸ªåŒ…å« USE è¯­å¥å’Œå…¶ä»–æŸ¥è¯¢çš„æ–‡ä»¶)-f query_file or â€“query_file=query_fileä¼ é€’ä¸€ä¸ªæ–‡ä»¶ä¸­çš„ SQL æŸ¥è¯¢ã€‚æ–‡ä»¶å†…å®¹å¿…é¡»ä»¥åˆ†å·åˆ†éš”-k or â€“kerberoså½“è¿æ¥åˆ° impalad æ—¶ä½¿ç”¨ Kerberos è®¤è¯ã€‚å¦‚æœè¦è¿æ¥çš„ impalad å®ä¾‹ä¸æ”¯æŒ Kerberosï¼Œå°†æ˜¾ç¤ºä¸€ä¸ªé”™è¯¯-s kerberos_service_name or â€“kerberos_service_name=nameInstructs impala-shell to authenticate to a particular impalad service principal. å¦‚ä½•æ²¡æœ‰è®¾ç½® kerberos_service_name ï¼Œé»˜è®¤ä½¿ç”¨ impalaã€‚å¦‚ä½•å¯ç”¨äº†æœ¬é€‰é¡¹ï¼Œè€Œè¯•å›¾å»ºç«‹ä¸æ”¯æŒKerberos çš„è¿æ¥æ—¶ï¼Œè¿”å›ä¸€ä¸ªé”™è¯¯(If this option is used in conjunction with a connection in which Kerberos is not supported, errors are returned)-V or â€“verboseå¯ç”¨è¯¦ç»†è¾“å‡ºâ€“quietå…³é—­è¯¦ç»†è¾“å‡º-v or â€“versionæ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯-cæŸ¥è¯¢æ‰§è¡Œå¤±è´¥æ—¶ç»§ç»­æ‰§è¡Œ-r or â€“refresh_after_connectå»ºç«‹è¿æ¥ååˆ·æ–° Impala å…ƒæ•°æ®ï¼Œä¸å»ºç«‹è¿æ¥åæ‰§è¡Œ REFRESH è¯­å¥æ•ˆæœç›¸åŒ-d default_db or â€“database=default_dbæŒ‡å®šå¯åŠ¨åä½¿ç”¨çš„æ•°æ®åº“ï¼Œä¸å»ºç«‹è¿æ¥åä½¿ç”¨ USE è¯­å¥é€‰æ‹©æ•°æ®åº“ä½œç”¨ç›¸åŒï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œé‚£ä¹ˆä½¿ç”¨ default æ•°æ®åº“-lå¯ç”¨ LDAP è®¤è¯-uå½“ä½¿ç”¨ -l é€‰é¡¹å¯ç”¨ LDAP è®¤è¯æ—¶ï¼Œæä¾›ç”¨æˆ·å(ä½¿ç”¨çŸ­ç”¨æˆ·åï¼Œè€Œä¸æ˜¯å®Œæ•´çš„ LDAP ä¸“æœ‰åç§°(distinguished name)) ï¼Œshell ä¼šæç¤ºè¾“å…¥å¯†ç æ¦‚å¿µä¸æ¶æ„Impala Serverçš„ç»„ä»¶ImpalaæœåŠ¡å™¨æ˜¯åˆ†å¸ƒå¼ï¼Œå¤§è§„æ¨¡å¹¶è¡Œå¤„ç†ï¼ˆMPPï¼‰æ•°æ®åº“å¼•æ“ã€‚å®ƒç”±åœ¨ç¾¤é›†ä¸­çš„ç‰¹å®šä¸»æœºä¸Šè¿è¡Œçš„ä¸åŒå®ˆæŠ¤ç¨‹åºè¿›ç¨‹ç»„æˆã€‚The Impala DaemonImpalaçš„æ ¸å¿ƒç»„ä»¶æ˜¯Impala daemonã€‚Impala daemonæ‰§è¡Œçš„ä¸€äº›å…³é”®åŠŸèƒ½æ˜¯ï¼šè¯»å–å’Œå†™å…¥æ•°æ®æ–‡ä»¶ã€‚æ¥å—ä»impala-shellå‘½ä»¤ï¼ŒHueï¼ŒJDBCæˆ–ODBCä¼ è¾“çš„æŸ¥è¯¢ã€‚å¹¶è¡ŒåŒ–æŸ¥è¯¢å¹¶åœ¨ç¾¤é›†ä¸­åˆ†é…å·¥ä½œã€‚å°†ä¸­é—´æŸ¥è¯¢ç»“æœå‘é€å›ä¸­å¤®åè°ƒå™¨ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€éƒ¨ç½²Impalaå®ˆæŠ¤ç¨‹åºï¼šHDFSå’ŒImpalaä½äºåŒä¸€ä½ç½®ï¼Œæ¯ä¸ªImpalaå®ˆæŠ¤ç¨‹åºä¸DataNodeåœ¨åŒä¸€ä¸»æœºä¸Šè¿è¡Œã€‚Impalaå•ç‹¬éƒ¨ç½²åœ¨è®¡ç®—ç¾¤é›†ä¸­ï¼Œå¯ä»HDFSï¼ŒS3ï¼ŒADLSç­‰è¿œç¨‹è¯»å–ã€‚Impalaå®ˆæŠ¤è¿›ç¨‹ä¸StateStoreä¿æŒæŒç»­é€šä¿¡ï¼Œä»¥ç¡®è®¤å“ªäº›å®ˆæŠ¤è¿›ç¨‹æ˜¯å¥åº·çš„å¹¶ä¸”å¯ä»¥æ¥å—æ–°å·¥ä½œã€‚åœ¨Impala 2.9åŠæ›´é«˜ç‰ˆæœ¬ä¸­ï¼Œæ‚¨å¯ä»¥æ§åˆ¶å“ªäº›ä¸»æœºå……å½“æŸ¥è¯¢åè°ƒå™¨ï¼Œå“ªäº›ä¸»æœºå……å½“æŸ¥è¯¢æ‰§è¡Œç¨‹åºï¼Œä»¥æé«˜å¤§å‹ç¾¤é›†ä¸Šé«˜åº¦å¹¶å‘å·¥ä½œè´Ÿè½½çš„å¯ä¼¸ç¼©æ€§ã€‚Impala StatestoreImpala Statestoreè¿›ç¨‹æ£€æŸ¥é›†ç¾¤ä¸­æ‰€æœ‰Impala daemonçš„è¿è¡ŒçŠ¶å†µï¼Œå¹¶æŠŠä¿¡æ¯åé¦ˆç»™Impala daemonè¿›ç¨‹ã€‚æ‚¨åªéœ€è¦åœ¨ç¾¤é›†ä¸­çš„ä¸€å°ä¸»æœºä¸Šæ‰§è¡Œæ­¤ç±»è¿‡ç¨‹ã€‚å¦‚æœImpalaå®ˆæŠ¤ç¨‹åºç”±äºç¡¬ä»¶æ•…éšœï¼Œç½‘ç»œé”™è¯¯ï¼Œè½¯ä»¶é—®é¢˜æˆ–å…¶ä»–åŸå› è€Œè„±æœºï¼Œåˆ™StateStoreä¼šé€šçŸ¥æ‰€æœ‰å…¶ä»–Impala daemonç¨‹åºï¼Œä»¥ä¾¿å°†æ¥çš„æŸ¥è¯¢å¯ä»¥é¿å…å‘æ— æ³•è®¿é—®çš„Impalaå®ˆæŠ¤ç¨‹åºå‘å‡ºè¯·æ±‚ã€‚å› ä¸ºStateStoreçš„ç›®çš„æ˜¯åœ¨å‡ºç°é—®é¢˜æ—¶æä¾›å¸®åŠ©å¹¶å‘åè°ƒå™¨å¹¿æ’­å…ƒæ•°æ®ï¼Œå› æ­¤å¯¹Impalaé›†ç¾¤çš„æ­£å¸¸æ“ä½œå¹¶ä¸æ€»æ˜¯è‡³å…³é‡è¦çš„ã€‚å¦‚æœStateStoreæœªè¿è¡Œæˆ–æ— æ³•è®¿é—®ï¼Œåˆ™åœ¨å¤„ç†Impalaå·²çŸ¥çš„æ•°æ®æ—¶ï¼ŒImpalaå®ˆæŠ¤ç¨‹åºä¼šåƒå¾€å¸¸ä¸€æ ·ç»§ç»­è¿è¡Œå’Œåˆ†é…å·¥ä½œã€‚å¦‚æœå…¶ä»–Impalaå®ˆæŠ¤ç¨‹åºå¤±è´¥ï¼Œåˆ™ç¾¤é›†å˜å¾—ä¸é‚£ä¹ˆå¥å£®ï¼Œå¹¶ä¸”å½“StateStoreè„±æœºæ—¶ï¼Œå…ƒæ•°æ®å˜å¾—ä¸é‚£ä¹ˆä¸€è‡´ã€‚å½“StateStoreé‡æ–°è”æœºæ—¶ï¼Œå®ƒä¼šé‡æ–°å»ºç«‹ä¸Impalaå®ˆæŠ¤ç¨‹åºçš„é€šä¿¡å¹¶æ¢å¤å…¶ç›‘è§†å’Œå¹¿æ’­åŠŸèƒ½ã€‚The Impala Catalog ServiceImpala Catalog Serviceè¿›ç¨‹å¯ä»¥æŠŠImpala SQLè¯­å¥ä¸­çš„å…ƒæ•°æ®æ›´æ”¹ä¿¡æ¯åé¦ˆåˆ°é›†ç¾¤ä¸­çš„æ‰€æœ‰Impalaå®ˆæŠ¤ç¨‹åºã€‚åªéœ€è¦åœ¨ç¾¤é›†ä¸­çš„ä¸€å°ä¸»æœºä¸Šæ‰§è¡Œæ­¤ç±»è¿‡ç¨‹ã€‚å› ä¸ºè¯·æ±‚æ˜¯é€šè¿‡StateStoreå®ˆæŠ¤ç¨‹åºä¼ é€’çš„ï¼Œæ‰€ä»¥è¦åœ¨åŒä¸€ä¸»æœºä¸Šè¿è¡Œstatestoredå’ŒcatalogdæœåŠ¡ã€‚å½“é€šè¿‡Impalaå‘å‡ºçš„è¯­å¥æ‰§è¡Œå…ƒæ•°æ®æ›´æ”¹æ—¶ï¼ŒImpala Catalog Serviceè¿›ç¨‹é¿å…äº†REFRESHå’ŒINVALIDATE METADATAè¯­å¥çš„ä½¿ç”¨,è¯¥è¿›ç¨‹å¯ä»¥ä¸ºæˆ‘ä»¬æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯ã€‚ä½¿ç”¨â€“load_catalog_in_backgroundé€‰é¡¹æ§åˆ¶ä½•æ—¶åŠ è½½è¡¨çš„å…ƒæ•°æ®ã€‚å¦‚æœè®¾ç½®ä¸ºfalseï¼Œåˆ™åœ¨ç¬¬ä¸€æ¬¡å¼•ç”¨è¡¨æ—¶ä¼šåŠ è½½è¡¨çš„å…ƒæ•°æ®ã€‚è¿™æ„å‘³ç€ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½æ¯”åç»­è¿è¡Œæ…¢ã€‚åœ¨impala2.2å¼€å§‹ï¼Œé»˜è®¤load_catalog_in_backgroundæ˜¯ falseã€‚å¦‚æœè®¾ç½®ä¸ºtrueï¼Œå³ä½¿æ²¡æœ‰æŸ¥è¯¢éœ€è¦è¯¥å…ƒæ•°æ®ï¼Œç›®å½•æœåŠ¡ä¹Ÿä¼šå°è¯•åŠ è½½è¡¨çš„å…ƒæ•°æ®ã€‚å› æ­¤ï¼Œå½“è¿è¡Œéœ€è¦å®ƒçš„ç¬¬ä¸€ä¸ªæŸ¥è¯¢æ—¶ï¼Œå¯èƒ½å·²ç»åŠ è½½äº†å…ƒæ•°æ®ã€‚ä½†æ˜¯ï¼Œç”±äºä»¥ä¸‹åŸå› ï¼Œæˆ‘ä»¬å»ºè®®ä¸è¦å°†é€‰é¡¹è®¾ç½®ä¸ºtrueã€‚åå°åŠ è½½å¯èƒ½ä¼šå¹²æ‰°æŸ¥è¯¢ç‰¹å®šçš„å…ƒæ•°æ®åŠ è½½ã€‚è¿™å¯èƒ½åœ¨å¯åŠ¨æ—¶æˆ–åœ¨ä½¿å…ƒæ•°æ®æ— æ•ˆä¹‹åå‘ç”Ÿï¼ŒæŒç»­æ—¶é—´å–å†³äºå…ƒæ•°æ®çš„æ•°é‡ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´çœ‹ä¼¼éšæœºçš„é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢éš¾ä»¥è¯Šæ–­ã€‚Impalaå¯èƒ½ä¼šåŠ è½½ä»æœªä½¿ç”¨è¿‡çš„è¡¨çš„å…ƒæ•°æ®ï¼Œè¿™ä¼šå¢åŠ ç›®å½•æœåŠ¡å’ŒImpalaå®ˆæŠ¤ç¨‹åºçš„ç›®å½•å¤§å°ï¼Œä»è€Œå¢åŠ å†…å­˜ä½¿ç”¨é‡ã€‚è´Ÿè½½å‡è¡¡å’Œé«˜å¯ç”¨æ€§çš„å¤§å¤šæ•°æ³¨æ„äº‹é¡¹é€‚ç”¨äºimpaladå®ˆæŠ¤ç¨‹åºã€‚è¯¥statestoredå’Œcatalogdå®ˆæŠ¤è¿›ç¨‹ä¸å…·å¤‡é«˜å¯ç”¨æ€§çš„ç‰¹æ®Šè¦æ±‚ï¼Œå› ä¸ºè¿™äº›å®ˆæŠ¤è¿›ç¨‹çš„é—®é¢˜ä¸ä¼šé€ æˆæ•°æ®ä¸¢å¤±ã€‚å¦‚æœè¿™äº›å®ˆæŠ¤ç¨‹åºç”±äºç‰¹å®šä¸»æœºä¸Šçš„ä¸­æ–­è€Œå˜å¾—ä¸å¯ç”¨ï¼Œåˆ™å¯ä»¥åœæ­¢ImpalaæœåŠ¡ï¼Œåˆ é™¤Impala StateStoreå’ŒImpalaç›®å½•æœåŠ¡å™¨è§’è‰²ï¼Œåœ¨å…¶ä»–ä¸»æœºä¸Šæ·»åŠ è§’è‰²ï¼Œç„¶åé‡æ–°å¯åŠ¨ImpalaæœåŠ¡ã€‚æ•°æ®ç±»å‹Impalaæ”¯æŒä¸€ç»„æ•°æ®ç±»å‹ï¼Œå¯ç”¨äºè¡¨åˆ—ï¼Œè¡¨è¾¾å¼å€¼ï¼Œå‡½æ•°å‚æ•°å’Œè¿”å›å€¼ã€‚æ³¨æ„ï¼š ç›®å‰ï¼ŒImpalaä»…æ”¯æŒæ ‡é‡ç±»å‹ï¼Œè€Œä¸æ”¯æŒå¤åˆç±»å‹æˆ–åµŒå¥—ç±»å‹ã€‚è®¿é—®åŒ…å«ä»»ä½•å…·æœ‰ä¸å—æ”¯æŒç±»å‹çš„åˆ—çš„è¡¨ä¼šå¯¼è‡´é”™è¯¯ã€‚æœ‰å…³Impalaå’ŒHiveæ•°æ®ç±»å‹ä¹‹é—´çš„å·®å¼‚ï¼Œè¯·å‚é˜…:http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_deltaARRAYå¤æ‚ç±»å‹ï¼ˆä»…é™Impala 2.3æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰BIGINTæ•°æ®ç±»å‹BOOLEANæ•°æ®ç±»å‹CHARæ•°æ®ç±»å‹ï¼ˆä»…é™Impala 2.0æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰DECIMALæ•°æ®ç±»å‹ï¼ˆä»…é™Impala 3.0æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰åŒæ•°æ®ç±»å‹FLOATæ•°æ®ç±»å‹INTæ•°æ®ç±»å‹MAPå¤æ‚ç±»å‹ï¼ˆä»…é™Impala 2.3æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰REALæ•°æ®ç±»å‹SMALLINTæ•°æ®ç±»å‹STRINGæ•°æ®ç±»å‹STRUCTå¤æ‚ç±»å‹ï¼ˆä»…é™Impala 2.3æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰TIMESTAMPæ•°æ®ç±»å‹TINYINTæ•°æ®ç±»å‹VARCHARæ•°æ®ç±»å‹ï¼ˆä»…é™Impala 2.0æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰å¤æ‚ç±»å‹ï¼ˆä»…é™Impala 2.3æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰]]></content>
      <categories>
        <category>Impala</category>
      </categories>
      <tags>
        <tag>Impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearchå¸¸ç”¨æ“ä½œè§£æ]]></title>
    <url>%2F2019%2F05%2F13%2FElasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[åˆ›å»ºMavenç®¡ç†çš„Javaé¡¹ç›®åœ¨pom.xmlä¸­æ·»åŠ ä¾èµ–ï¼š1234567&lt;es.version&gt;6.1.1&lt;/es.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;$&#123;es.version&#125;&lt;/version&gt;&lt;/dependency&gt;ç„¶ååˆ›å»ºä¸€ä¸ªå•å…ƒæµ‹è¯•ç±»ESAppï¼š1234567891011121314private TransportClient client; @Before public void setUp() throws Exception &#123; Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;mycluster&quot;) .put(&quot;client.transport.sniff&quot;, &quot;true&quot;)//å¢åŠ è‡ªåŠ¨å—…æ¢é…ç½® .build(); client = new PreBuiltTransportClient(settings); client.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;10.8.24.94&quot;), 9300)); System.out.println(client.toString()); &#125;è¿è¡ŒåæŠ¥é”™1java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonFactory123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;è¿è¡ŒåæˆåŠŸæ‹¿åˆ°ESçš„clientï¼šåˆ›å»ºä¸€ä¸ªIndex123456@Test public void createIndex() &#123; client.admin().indices().prepareCreate(INDEX).get(); System.out.println(&quot;åˆ›å»ºIndexæˆåŠŸ&quot;); &#125;åˆ é™¤ä¸€ä¸ªIndex12345@Test public void deleteIndex() &#123; client.admin().indices().prepareDelete(INDEX).get(); System.out.println(&quot;åˆ é™¤IndexæˆåŠŸ&quot;); &#125;æ”¾å…¥æ•°æ®çš„ä¸‰ç§æ–¹å¼1234567891011121314151617181920212223242526272829303132333435//ä¸æ¨èä½¿ç”¨ï¼Œå¤ªç¹çæ‹¼jsonæ ¼å¼ @Test public void createDoc() &#123; String json = &quot;&#123;\&quot;name\&quot;:\&quot;è‹¥æ³½æ•°æ®\&quot;&#125;&quot;; IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;100&quot;) .setSource(json, XContentType.JSON) .get(); &#125; //æ¨èä½¿ç”¨ @Test public void test01() throws Exception &#123; Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;(); json.put(&quot;name&quot;, &quot;ruozedata&quot;); json.put(&quot;message&quot;, &quot;trying out Elasticsearch&quot;); IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;101&quot;).setSource(json).get(); System.out.println(response.getVersion()); &#125; //æ¨èä½¿ç”¨ @Test public void test02() throws Exception &#123; XContentBuilder builder = jsonBuilder() .startObject() .field(&quot;user&quot;, &quot;ruoze&quot;) .field(&quot;postDate&quot;, new Date()) .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;) .endObject(); IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;102&quot;).setSource(builder).get(); System.out.println(response.getVersion()); &#125;æ‹¿åˆ°ä¸€æ¡æ•°æ®123456@Test public void getDoc() &#123; GetResponse response = client.prepareGet(INDEX, TYPE, &quot;100&quot;).get(); System.out.println(response.getSourceAsString()); &#125;æ‹¿åˆ°å¤šæ¡æ•°æ®123456789101112131415161718@Test public void getDocsByIds() &#123; MultiGetResponse responses = client.prepareMultiGet() .add(INDEX, TYPE,&quot;100&quot;) .add(INDEX, TYPE, &quot;101&quot;, &quot;102&quot;, &quot;1000&quot;) .get(); for (MultiGetItemResponse response : responses) &#123; GetResponse res = response.getResponse(); if (res.isExists()) &#123; System.out.println(res); &#125; else &#123; System.out.println(&quot;æ²¡æœ‰è¿™æ¡æ•°æ®&quot;); &#125; &#125; &#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®-CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ)]]></title>
    <url>%2F2019%2F05%2F13%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½æ•°æ®CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®è·µ)è§†é¢‘:https://www.bilibili.com/video/av52167219PS:å»ºè®®å…ˆçœ‹è¯¾ç¨‹è§†é¢‘1-2ç¯‡ï¼Œå†æ ¹æ®è§†é¢‘æˆ–æ–‡æ¡£éƒ¨ç½²ï¼Œå¦‚æœ‰é—®é¢˜ï¼ŒåŠæ—¶ä¸@è‹¥æ³½æ•°æ®Jå“¥è”ç³»ã€‚ä¸€.å‡†å¤‡å·¥ä½œ1.ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—:a.MySQLç¦»çº¿éƒ¨ç½²b.CMç¦»çº¿éƒ¨ç½²c.Parcelæ–‡ä»¶ç¦»çº¿æºéƒ¨ç½²2.è§„åˆ’:èŠ‚ç‚¹MySQLéƒ¨ç½²ç»„ä»¶Parcelæ–‡ä»¶ç¦»çº¿æºCMæœåŠ¡è¿›ç¨‹å¤§æ•°æ®ç»„ä»¶hadoop001MySQLParcelActivity MonitorNN RM DN NMhadoop002Alert PublisherEvent ServerDN NMhadoop003Host MonitorService MonitorDN NM3.ä¸‹è½½æº:CMcloudera-manager-centos7-cm5.16.1_x86_64.tar.gzParcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1manifest.jsonJDKhttps://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.htmlä¸‹è½½jdk-8u202-linux-x64.tar.gzMySQLhttps://dev.mysql.com/downloads/mysql/5.7.html#downloadsä¸‹è½½mysql-5.7.26-el7-x86_64.tar.gzMySQL jdbc jarmysql-connector-java-5.1.47.jarä¸‹è½½å®Œæˆåè¦é‡å‘½åå»æ‰ç‰ˆæœ¬å·ï¼Œmv mysql-connector-java-5.1.47.jar mysql-connector-java.jar###å‡†å¤‡å¥½ç™¾åº¦äº‘,ä¸‹è½½å®‰è£…åŒ…:é“¾æ¥:https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA å¯†ç :viqpäºŒ.é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–1.é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹ŸæœºCentOS7.2æ“ä½œç³»ç»Ÿï¼Œ2æ ¸8Gæœ€ä½é…ç½®2.å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶MAC: /etc/hostsWindow: C:\windows\system32\drivers\etc\hosts1234å…¬ç½‘åœ°å€: 106.15.234.222 hadoop001 106.15.235.200 hadoop002 106.15.234.239 hadoop0033.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶1234ç§æœ‰åœ°é“ã€å†…ç½‘åœ°å€:echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts4.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™123systemctl stop firewalld systemctl disable firewalldiptables -F5.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux123vi /etc/selinux/configå°†SELINUX=enforcingæ”¹ä¸ºSELINUX=disabled è®¾ç½®åéœ€è¦é‡å¯æ‰èƒ½ç”Ÿæ•ˆ6.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥1234567891011121314151617181920212223242526272829303132333435363738394041424344454647486.1.æ—¶åŒº[root@hadoop001 ~]# dateSat May 11 10:07:53 CST 2019[root@hadoop001 ~]# timedatectl Local time: Sat 2019-05-11 10:10:31 CST Universal time: Sat 2019-05-11 02:10:31 UTC RTC time: Sat 2019-05-11 10:10:29 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: yes DST active: n/a#æŸ¥çœ‹å‘½ä»¤å¸®åŠ©ï¼Œå­¦ä¹ è‡³å…³é‡è¦ï¼Œæ— éœ€ç™¾åº¦ï¼Œå¤ªğŸ‘[root@hadoop001 ~]# timedatectl --helptimedatectl [OPTIONS...] COMMAND ...Query or change system time and date settings. -h --help Show this help message --version Show package version --no-pager Do not pipe output into a pager --no-ask-password Do not prompt for password -H --host=[USER@]HOST Operate on remote host -M --machine=CONTAINER Operate on local container --adjust-system-clock Adjust system clock when changing local RTC modeCommands: status Show current time settings set-time TIME Set system time set-timezone ZONE Set system time zone list-timezones Show known time zones set-local-rtc BOOL Control whether RTC is in local time set-ntp BOOL Control whether NTP is enabled#æŸ¥çœ‹å“ªäº›æ—¶åŒº[root@hadoop001 ~]# timedatectl list-timezonesAfrica/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/AlgiersAfrica/AsmaraAfrica/Bamako#æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®äºšæ´²ä¸Šæµ·æ—¶åŒº [root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455566.2.æ—¶é—´#æ‰€æœ‰èŠ‚ç‚¹å®‰è£…ntp[root@hadoop001 ~]# yum install -y ntp#é€‰å–hadoop001ä¸ºntpçš„ä¸»èŠ‚ç‚¹[root@hadoop001 ~]# vi /etc/ntp.conf #timeserver 0.asia.pool.ntp.orgserver 1.asia.pool.ntp.orgserver 2.asia.pool.ntp.orgserver 3.asia.pool.ntp.org#å½“å¤–éƒ¨æ—¶é—´ä¸å¯ç”¨æ—¶ï¼Œå¯ä½¿ç”¨æœ¬åœ°ç¡¬ä»¶æ—¶é—´server 127.127.1.0 iburst local clock #å…è®¸å“ªäº›ç½‘æ®µçš„æœºå™¨æ¥åŒæ­¥æ—¶é—´restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap#å¼€å¯ntpdåŠæŸ¥çœ‹çŠ¶æ€[root@hadoop001 ~]# systemctl start ntpd[root@hadoop001 ~]# systemctl status ntpd ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago Main PID: 18518 (ntpd) CGroup: /system.slice/ntpd.service â””â”€18518 /usr/sbin/ntpd -u ntp:ntp -gMay 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usecMay 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabledMay 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.#éªŒè¯[root@hadoop001 ~]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== LOCAL(0) .LOCL. 10 l 726 64 0 0.000 0.000 0.000#å…¶ä»–ä»èŠ‚ç‚¹åœæ­¢ç¦ç”¨ntpdæœåŠ¡ [root@hadoop002 ~]# systemctl stop ntpd[root@hadoop002 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´[root@hadoop002 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop001 [root@hadoop003 ~]# systemctl stop ntpd[root@hadoop004 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´[root@hadoop003 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop0017.éƒ¨ç½²é›†ç¾¤çš„JDK123456789mkdir /usr/javatar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/#åˆ‡è®°å¿…é¡»ä¿®æ­£æ‰€å±ç”¨æˆ·åŠç”¨æˆ·ç»„chown -R root:root /usr/java/jdk1.8.0_45echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profileecho &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profilesource /etc/profilewhich java8.hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5.7(å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£)æ–‡æ¡£é“¾æ¥:https://github.com/Hackeruncle/MySQLè§†é¢‘é“¾æ¥:https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ å¯†ç :whic9.åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·12345create database cmf DEFAULT CHARACTER SET utf8;create database amon DEFAULT CHARACTER SET utf8;grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;flush privileges;10.hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql jdbc jar12mkdir -p /usr/share/java/cp mysql-connector-java.jar /usr/share/java/ä¸‰.CDHéƒ¨ç½²1.ç¦»çº¿éƒ¨ç½²cm serveråŠagent1234567891011121314151617181920211.1.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç›®å½•åŠè§£å‹mkdir /opt/cloudera-managertar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/1.2.æ‰€æœ‰èŠ‚ç‚¹ä¿®æ”¹agentçš„é…ç½®ï¼ŒæŒ‡å‘serverçš„èŠ‚ç‚¹hadoop001sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini1.3.ä¸»èŠ‚ç‚¹ä¿®æ”¹serverçš„é…ç½®:vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=hadoop001com.cloudera.cmf.db.name=cmfcom.cloudera.cmf.db.user=cmfcom.cloudera.cmf.db.password=Ruozedata123456!com.cloudera.cmf.db.setupType=EXTERNAL1.4.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç”¨æˆ·useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm1.5.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager2.hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº1234567891011121314151617182.1.éƒ¨ç½²ç¦»çº¿parcelæº$ mkdir -p /opt/cloudera/parcel-repo$ lltotal 3081664-rw-r--r-- 1 root root 2127506677 May 9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel-rw-r--r-- 1 root root 41 May 9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1-rw-r--r-- 1 root root 841524318 May 9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz-rw-r--r-- 1 root root 185515842 Aug 10 2017 jdk-8u144-linux-x64.tar.gz-rw-r--r-- 1 root root 66538 May 9 18:03 manifest.json-rw-r--r-- 1 root root 989495 May 25 2017 mysql-connector-java.jar$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/#åˆ‡è®°cpæ—¶ï¼Œé‡å‘½åå»æ‰1ï¼Œä¸ç„¶åœ¨éƒ¨ç½²è¿‡ç¨‹CMè®¤ä¸ºå¦‚ä¸Šæ–‡ä»¶ä¸‹è½½æœªå®Œæ•´ï¼Œä¼šæŒç»­ä¸‹è½½$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha$ cp manifest.json /opt/cloudera/parcel-repo/2.2.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/3.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™mkdir -p /opt/cloudera/parcelschown -R cloudera-scm:cloudera-scm /opt/cloudera/4.hadoop001èŠ‚ç‚¹å¯åŠ¨Server1234564.1.å¯åŠ¨server/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start4.2.é˜¿é‡Œäº‘webç•Œé¢ï¼Œè®¾ç½®è¯¥hadoop001èŠ‚ç‚¹é˜²ç«å¢™æ”¾å¼€7180ç«¯å£4.3.ç­‰å¾…1minï¼Œæ‰“å¼€ http://hadoop001:7180 è´¦å·å¯†ç :admin/admin4.4.å‡å¦‚æ‰“ä¸å¼€ï¼Œå»çœ‹serverçš„logï¼Œæ ¹æ®é”™è¯¯ä»”ç»†æ’æŸ¥é”™è¯¯5.æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent1/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start6.æ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œhttp://hadoop001:7180/è´¦å·å¯†ç :admin/admin7.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸æ¡ä»¶ã€‚å‹¾é€‰8.æ¬¢è¿ä½¿ç”¨Cloudera Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera Expresså…è´¹ç‰ˆæœ¬9.æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera Managerå’ŒCDH10.ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©[å½“å‰ç®¡ç†çš„ä¸»æœº]ï¼Œå…¨éƒ¨å‹¾é€‰11.é€‰æ‹©å­˜å‚¨åº“12.é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚æœ¬åœ°parcelç¦»çº¿æºé…ç½®æ­£ç¡®ï¼Œåˆ™â€ä¸‹è½½â€é˜¶æ®µç¬é—´å®Œæˆï¼Œå…¶ä½™é˜¶æ®µè§†èŠ‚ç‚¹æ•°ä¸å†…éƒ¨ç½‘ç»œæƒ…å†µå†³å®šã€‚13.æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§123456789101112131415161718192021222324252627282913.1.å»ºè®®å°†/proc/sys/vm/swappinessè®¾ç½®ä¸ºæœ€å¤§å€¼10ã€‚swappinesså€¼æ§åˆ¶æ“ä½œç³»ç»Ÿå°è¯•äº¤æ¢å†…å­˜çš„ç§¯æï¼›swappiness=0ï¼šè¡¨ç¤ºæœ€å¤§é™åº¦ä½¿ç”¨ç‰©ç†å†…å­˜ï¼Œä¹‹åæ‰æ˜¯swapç©ºé—´ï¼›swappiness=100ï¼šè¡¨ç¤ºç§¯æä½¿ç”¨swapåˆ†åŒºï¼Œå¹¶ä¸”æŠŠå†…å­˜ä¸Šçš„æ•°æ®åŠæ—¶æ¬è¿åˆ°swapç©ºé—´ï¼›å¦‚æœæ˜¯æ··åˆæœåŠ¡å™¨ï¼Œä¸å»ºè®®å®Œå…¨ç¦ç”¨swapï¼Œå¯ä»¥å°è¯•é™ä½swappinessã€‚ä¸´æ—¶è°ƒæ•´ï¼šsysctl vm.swappiness=10æ°¸ä¹…è°ƒæ•´ï¼šcat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf# Adjust swappiness valuevm.swappiness=10EOF13.2.å·²å¯ç”¨é€æ˜å¤§é¡µé¢å‹ç¼©ï¼Œå¯èƒ½ä¼šå¯¼è‡´é‡å¤§æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ç¦ç”¨æ­¤è®¾ç½®ã€‚ä¸´æ—¶è°ƒæ•´ï¼šecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledæ°¸ä¹…è°ƒæ•´ï¼šcat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local# Disable transparent_hugepageecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledEOF# centos7.xç³»ç»Ÿï¼Œéœ€è¦ä¸º&quot;/etc/rc.d/rc.local&quot;æ–‡ä»¶èµ‹äºˆæ‰§è¡Œæƒé™chmod +x /etc/rc.d/rc.local14.è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡15.è‡ªå®šä¹‰è§’è‰²åˆ†é…16.æ•°æ®åº“è®¾ç½®17.å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯18.é¦–æ¬¡è¿è¡Œ19.æ­å–œæ‚¨!20.ä¸»é¡µCDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡(ruoze_star)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061620.é’äº‘ç¯å¢ƒä»‹ç»å’Œä½¿ç”¨ 1.Preparation è°ˆè°ˆæ€æ ·å…¥é—¨å¤§æ•°æ® è°ˆè°ˆæ€æ ·åšå¥½ä¸€ä¸ªå¤§æ•°æ®å¹³å°çš„è¿è¥å·¥ä½œ Linuxæœºå™¨,å„è½¯ä»¶ç‰ˆæœ¬ä»‹ç»åŠå®‰è£…(å½•æ’­) 2.Introduction Clouderaã€CMåŠCDHä»‹ç» CDHç‰ˆæœ¬é€‰æ‹© CDHå®‰è£…å‡ ç§æ–¹å¼è§£è¯» 3.Install&amp;UnInstall é›†ç¾¤èŠ‚ç‚¹è§„åˆ’,ç¯å¢ƒå‡†å¤‡(NTP,Jdk and etc) MySQLç¼–è¯‘å®‰è£…åŠå¸¸ç”¨å‘½ä»¤ æ¨è:CDHç¦»çº¿å®‰è£…(è¸©å‘å¿ƒå¾—,å…¨é¢å‰–æ) è§£è¯»æš´åŠ›å¸è½½è„šæœ¬ 4.CDH Management CDHä½“ç³»æ¶æ„å‰–æ CDHé…ç½®æ–‡ä»¶æ·±åº¦è§£æ CMçš„å¸¸ç”¨å‘½ä»¤ CDHé›†ç¾¤æ­£ç¡®å¯åŠ¨å’Œåœæ­¢é¡ºåº CDH Tsquery Language CDHå¸¸è§„ç®¡ç†(ç›‘æ§/é¢„è­¦/é…ç½®/èµ„æº/æ—¥å¿—/å®‰å…¨) 5.Maintenance Experiment HDFS HA é…ç½® åŠhadoop/hdfså¸¸è§„å‘½ä»¤ Yarn HA é…ç½® åŠyarnå¸¸è§„å‘½ä»¤ Other CDH Components HA é…ç½® CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœåŠ¡(hive/spark/hbase) CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœºå™¨ CDHåŠ¨æ€æ·»åŠ åˆ é™¤åŠè¿ç§»DataNodeè¿›ç¨‹ç­‰ CDHå‡çº§(5.10.0--&gt;5.12.0) 6.Resource Management Linux Cgroups é™æ€èµ„æºæ±  åŠ¨æ€èµ„æºæ±  å¤šç§Ÿæˆ·æ¡ˆä¾‹ 7.Performance Tunning Memory/CPU/Network/DiskåŠé›†ç¾¤è§„åˆ’ Linuxå‚æ•° HDFSå‚æ•° MapReduceåŠYarnå‚æ•° å…¶ä»–æœåŠ¡å‚æ•° 8.Cases Share CDH4&amp;5ä¹‹Alternativeså‘½ä»¤ çš„ç ”ç©¶ CDH5.8.2å®‰è£…ä¹‹Hash verification failed è®°å½•ä¸€æ¬¡CDH4.8.6 é…ç½®HDFS HA å‘ CDH5.0é›†ç¾¤IPæ›´æ”¹ CDHçš„active namenode exit(GC)å’Œå½©è›‹åˆ†äº« 9. Kerberos Kerberosç®€ä»‹ Kerberosä½“ç³»ç»“æ„ Kerberoså·¥ä½œæœºåˆ¶ Kerberoså®‰è£…éƒ¨ç½² CDHå¯ç”¨kerberos Kerberoså¼€å‘ä½¿ç”¨(çœŸå®ä»£ç )10.Summary æ€»ç»“Join us if you have a dream.è‹¥æ³½æ•°æ®å®˜ç½‘: http://ruozedata.comè…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®: http://ruoze.ke.qq.comBilibiliç½‘ç«™,æœè‹¥æ³½æ•°æ®: https://space.bilibili.com/356836323è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆè‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦:]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¸¸ç”¨Sparkç´¯åŠ å™¨å‰–æä¹‹ä¸‰(è‡ªå®šä¹‰ç´¯åŠ å™¨)]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[æ€è·¯ &amp; éœ€æ±‚å‚è€ƒIntAccumulatorParamçš„å®ç°æ€è·¯ï¼ˆä¸Šè¿°æ–‡ç« ä¸­æœ‰è®²ï¼‰ï¼š1234567trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; // addInPlaceæœ‰å¾ˆå¤šå…·ä½“çš„å®ç°ç±» // å¦‚æœæƒ³è¦å®ç°è‡ªå®šä¹‰çš„è¯ï¼Œå°±å¾—å®ç°è¿™ä¸ªæ–¹æ³• addInPlace(t1, t2) &#125;&#125;è‡ªå®šä¹‰ä¹Ÿå¯ä»¥é€šè¿‡è¿™ä¸ªæ–¹æ³•å»å®ç°ï¼Œä»è€Œå…¼å®¹æˆ‘ä»¬è‡ªå®šä¹‰çš„ç´¯åŠ å™¨éœ€æ±‚ï¼šè¿™é‡Œå®ç°ä¸€ä¸ªç®€å•çš„æ¡ˆä¾‹ï¼Œç”¨åˆ†å¸ƒå¼çš„æ–¹æ³•å»å®ç°éšæœºæ•°1234567891011121314151617181920212223242526272829303132333435363738** * è‡ªå®šä¹‰çš„AccumulatorParam * * Created by lemon on 2018/7/28. */object UniqueKeyAccumulator extends AccumulatorParam[Map[Int, Int]] &#123; override def addInPlace(r1: Map[Int, Int], r2: Map[Int, Int]): Map[Int, Int] = &#123; // ++ç”¨äºä¸¤ä¸ªé›†åˆç›¸åŠ  r1++r2 &#125; override def zero(initialValue: Map[Int, Int]): Map[Int, Int] = &#123; var data: Map[Int, Int] = Map() data &#125;&#125;/** * ä½¿ç”¨è‡ªå®šä¹‰çš„ç´¯åŠ å™¨ï¼Œå®ç°éšæœºæ•° * * Created by lemon on 2018/7/28. */object CustomAccumulator &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(&quot;CustomAccumulator&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(sparkConf) val uniqueKeyAccumulator = sc.accumulable(Map[Int, Int]())(UniqueKeyAccumulator) val distData = sc.parallelize(1 to 10) val mapCount = distData.map(x =&gt; &#123; val randomNum = new Random().nextInt(20) // æ„é€ ä¸€ä¸ªk-vå¯¹ val map: Map[Int, Int] = Map[Int, Int](randomNum -&gt; randomNum) uniqueKeyAccumulator += map &#125;) println(mapCount.count()) // è·å–åˆ°ç´¯åŠ å™¨çš„å€¼ ä¸­çš„keyå€¼ï¼Œå¹¶è¿›è¡Œæ‰“å° uniqueKeyAccumulator.value.keys.foreach(println) sc.stop() &#125;&#125;è¿è¡Œç»“æœå¦‚ä¸‹å›¾ï¼š]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç´¯åŠ å™¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerå¸¸ç”¨å‘½ä»¤ä»¥åŠå®‰è£…mysql]]></title>
    <url>%2F2019%2F05%2F08%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.ç®€ä»‹Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„åº”ç”¨å®¹å™¨å¼•æ“ï¼›æ˜¯ä¸€ä¸ªè½»é‡çº§å®¹å™¨æŠ€æœ¯ï¼›Dockeræ”¯æŒå°†è½¯ä»¶ç¼–è¯‘æˆä¸€ä¸ªé•œåƒï¼›ç„¶ååœ¨é•œåƒä¸­å„ç§è½¯ä»¶åšå¥½é…ç½®ï¼Œå°†é•œåƒå‘å¸ƒå‡ºå»ï¼Œå…¶ä»–ä½¿ç”¨è€…å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªé•œåƒï¼›è¿è¡Œä¸­çš„è¿™ä¸ªé•œåƒç§°ä¸ºå®¹å™¨ï¼Œå®¹å™¨å¯åŠ¨æ˜¯éå¸¸å¿«é€Ÿçš„ã€‚2.æ ¸å¿ƒæ¦‚å¿µdockerä¸»æœº(Host)ï¼šå®‰è£…äº†Dockerç¨‹åºçš„æœºå™¨ï¼ˆDockerç›´æ¥å®‰è£…åœ¨æ“ä½œç³»ç»Ÿä¹‹ä¸Šï¼‰ï¼›dockerå®¢æˆ·ç«¯(Client)ï¼šè¿æ¥dockerä¸»æœºè¿›è¡Œæ“ä½œï¼›dockerä»“åº“(Registry)ï¼šç”¨æ¥ä¿å­˜å„ç§æ‰“åŒ…å¥½çš„è½¯ä»¶é•œåƒï¼›dockeré•œåƒ(Images)ï¼šè½¯ä»¶æ‰“åŒ…å¥½çš„é•œåƒï¼›æ”¾åœ¨dockerä»“åº“ä¸­ï¼›dockerå®¹å™¨(Container)ï¼šé•œåƒå¯åŠ¨åçš„å®ä¾‹ç§°ä¸ºä¸€ä¸ªå®¹å™¨ï¼›å®¹å™¨æ˜¯ç‹¬ç«‹è¿è¡Œçš„ä¸€ä¸ªæˆ–ä¸€ç»„åº”ç”¨3.å®‰è£…ç¯å¢ƒ1234VM ware Workstation10CentOS-7-x86_64-DVD-1804.isouname -r3.10.0-862.el7.x86_64æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Š æŸ¥çœ‹å‘½ä»¤ï¼šuname -r4.åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…dockeræ­¥éª¤ï¼š1ã€æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Š1uname -r2ã€å®‰è£…docker1yum install docker3ã€è¾“å…¥yç¡®è®¤å®‰è£…12345Dependency Updated: audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1 Complete!(æˆåŠŸæ ‡å¿—)4ã€å¯åŠ¨docker123[root@hadoop000 ~]# systemctl start docker[root@hadoop000 ~]# docker -vDocker version 1.13.1, build 8633870/1.13.15ã€å¼€æœºå¯åŠ¨docker12[root@hadoop000 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6ã€åœæ­¢docker1[root@hadoop000 ~]# systemctl stop docker5.å¸¸ç”¨å‘½ä»¤é•œåƒæ“ä½œæ“ä½œå‘½ä»¤è¯´æ˜æ£€ç´¢docker search å…³é”®å­— egï¼šdocker search redisæˆ‘ä»¬ç»å¸¸å»docker hubä¸Šæ£€ç´¢é•œåƒçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚é•œåƒçš„TAGæ‹‰å–docker pull é•œåƒå:tag:tagæ˜¯å¯é€‰çš„ï¼Œtagè¡¨ç¤ºæ ‡ç­¾ï¼Œå¤šä¸ºè½¯ä»¶çš„ç‰ˆæœ¬ï¼Œé»˜è®¤æ˜¯lateståˆ—è¡¨docker imagesæŸ¥çœ‹æ‰€æœ‰æœ¬åœ°é•œåƒåˆ é™¤docker rmi image-idåˆ é™¤æŒ‡å®šçš„æœ¬åœ°é•œåƒå½“ç„¶å¤§å®¶ä¹Ÿå¯ä»¥åœ¨å®˜ç½‘æŸ¥æ‰¾ï¼šhttps://hub.docker.com/å®¹å™¨æ“ä½œè½¯ä»¶é•œåƒï¼ˆQQå®‰è£…ç¨‹åºï¼‰â€”-è¿è¡Œé•œåƒâ€”-äº§ç”Ÿä¸€ä¸ªå®¹å™¨ï¼ˆæ­£åœ¨è¿è¡Œçš„è½¯ä»¶ï¼Œè¿è¡Œçš„QQï¼‰ï¼›æ­¥éª¤ï¼š1ã€æœç´¢é•œåƒ[root@localhost ~]# docker search tomcat2ã€æ‹‰å–é•œåƒ[root@localhost ~]# docker pull tomcat3ã€æ ¹æ®é•œåƒå¯åŠ¨å®¹å™¨docker run â€“name mytomcat -d tomcat:latest4ã€docker psæŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨5ã€ åœæ­¢è¿è¡Œä¸­çš„å®¹å™¨docker stop å®¹å™¨çš„id6ã€æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨docker ps -a7ã€å¯åŠ¨å®¹å™¨docker start å®¹å™¨id8ã€åˆ é™¤ä¸€ä¸ªå®¹å™¨docker rm å®¹å™¨id9ã€å¯åŠ¨ä¸€ä¸ªåšäº†ç«¯å£æ˜ å°„çš„tomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-dï¼šåå°è¿è¡Œ-p: å°†ä¸»æœºçš„ç«¯å£æ˜ å°„åˆ°å®¹å™¨çš„ä¸€ä¸ªç«¯å£ ä¸»æœºç«¯å£:å®¹å™¨å†…éƒ¨çš„ç«¯å£10ã€ä¸ºäº†æ¼”ç¤ºç®€å•å…³é—­äº†linuxçš„é˜²ç«å¢™service firewalld status ï¼›æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€service firewalld stopï¼šå…³é—­é˜²ç«å¢™systemctl disable firewalld.service #ç¦æ­¢firewallå¼€æœºå¯åŠ¨11ã€æŸ¥çœ‹å®¹å™¨çš„æ—¥å¿—docker logs container-name/container-idæ›´å¤šå‘½ä»¤å‚çœ‹https://docs.docker.com/engine/reference/commandline/docker/å¯ä»¥å‚è€ƒé•œåƒæ–‡æ¡£6.ä½¿ç”¨dockerå®‰è£…mysqldocker pull mysql123456789101112131415161718192021docker pull mysql Using default tag: latestTrying to pull repository docker.io/library/mysql ... latest: Pulling from docker.io/library/mysqla5a6f2f73cd8: Pull complete 936836019e67: Pull complete 283fa4c95fb4: Pull complete 1f212fb371f9: Pull complete e2ae0d063e89: Pull complete 5ed0ae805b65: Pull complete 0283dc49ef4e: Pull complete a7e1170b4fdb: Pull complete 88918a9e4742: Pull complete 241282fa67c2: Pull complete b0fecf619210: Pull complete bebf9f901dcc: Pull complete Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8Status: Downloaded newer image for docker.io/mysql:latest[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MBå¯åŠ¨1234567891011[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB[root@hadoop000 ~]# docker run --name mysql01 -d mysql756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a[root@hadoop000 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES756620c8e583 mysql &quot;docker-entrypoint...&quot; 9 seconds ago Exited (1) 7 seconds ago mysql01[root@hadoop000 ~]# docker logs 756620c8e583error: database is uninitialized and password option is not specified You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORDå¯ä»¥çœ‹åˆ°ä¸Šé¢å¯åŠ¨çš„æ–¹å¼æ˜¯é”™è¯¯çš„ï¼Œæç¤ºæˆ‘ä»¬è¦å¸¦ä¸Šå…·ä½“çš„å¯†ç 12[root@hadoop000 ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqleae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc061234â€“nameï¼šç»™æ–°åˆ›å»ºçš„å®¹å™¨å‘½åï¼Œæ­¤å¤„å‘½åä¸ºpwc-mysql-eï¼šé…ç½®ä¿¡æ¯ï¼Œæ­¤å¤„é…ç½®mysqlçš„rootç”¨æˆ·çš„ç™»é™†å¯†ç -pï¼šç«¯å£æ˜ å°„ï¼Œæ­¤å¤„æ˜ å°„ä¸»æœº3306ç«¯å£åˆ°å®¹å™¨pwc-mysqlçš„3306ç«¯å£-dï¼šæˆåŠŸå¯åŠ¨å®¹å™¨åè¾“å‡ºå®¹å™¨çš„å®Œæ•´IDï¼Œä¾‹å¦‚ä¸Šå›¾ 73f8811f669ee...æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ123[root@hadoop000 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeae86796e132 mysql &quot;docker-entrypoint...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql02ç™»é™†MySQL12345678910111213141516docker exec -it mysql04 /bin/bashroot@e34aba02c0c3:/# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 80Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;å…¶ä»–çš„é«˜çº§æ“ä½œ123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tagæŠŠä¸»æœºçš„/conf/mysqlæ–‡ä»¶å¤¹æŒ‚è½½åˆ° mysqldockerå®¹å™¨çš„/etc/mysql/conf.dæ–‡ä»¶å¤¹é‡Œé¢æ”¹mysqlçš„é…ç½®æ–‡ä»¶å°±åªéœ€è¦æŠŠmysqlé…ç½®æ–‡ä»¶æ”¾åœ¨è‡ªå®šä¹‰çš„æ–‡ä»¶å¤¹ä¸‹ï¼ˆ/conf/mysqlï¼‰docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ciæŒ‡å®šmysqlçš„ä¸€äº›é…ç½®å‚æ•°]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®è¯¾ç¨‹ä¸€è§ˆ]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—åŸºç¡€ç­LiunxVMè™šæ‹Ÿæœºå®‰è£…Liunxå¸¸ç”¨å‘½ä»¤ï¼ˆé‡ç‚¹ï¼‰å¼€å‘ç¯å¢ƒæ­MySQLæºç å®‰è£…&amp;yumå®‰è£…CRUDç¼–å†™æƒé™æ§åˆ¶Hadoopæ¶æ„ä»‹ç»&amp;&amp;æºç ç¼–è¯‘ä¼ªåˆ†å¸ƒå¼å®‰è£…&amp;&amp;ä¼ä¸šåº”ç”¨HDFSï¼ˆé‡ç‚¹ï¼‰æ¶æ„è®¾è®¡å‰¯æœ¬æ”¾ç½®ç­–ç•¥è¯»å†™æµç¨‹YARNï¼ˆé‡ç‚¹ï¼‰æ¶æ„è®¾è®¡å·¥ä½œæµç¨‹è°ƒåº¦ç®¡ç†&amp;&amp;å¸¸è§å‚æ•°é…ç½®ï¼ˆè°ƒä¼˜ï¼‰MapReduceæ¶æ„è®¾è®¡wordcountåŸç†&amp;&amp;joinåŸç†å’Œæ¡ˆä¾‹Hiveæ¶æ„è®¾è®¡Hive DDL&amp;DMLjoinåœ¨å¤§æ•°æ®ä¸­çš„ä½¿ç”¨ä½¿ç”¨è‡ªå¸¦UDFå’Œå¼€å‘è‡ªå®šä¹‰UDFSqoopæ¶æ„è®¾è®¡RDBMSå¯¼å…¥å¯¼å‡ºæ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚äººå·¥æ™ºèƒ½åŸºç¡€pythonåŸºç¡€å¸¸ç”¨åº“â€”â€”pandasã€numpyã€sklearnã€kerasé«˜çº§ç­scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰Hadoopé«˜çº§Hiveé«˜çº§FlumeKafkaHBaseFlinkCDHå®¹å™¨è°ƒåº¦å¹³å°çº¿ä¸‹ç­]]></content>
      <categories>
        <category>ç”Ÿäº§è¯¾ç¨‹</category>
      </categories>
      <tags>
        <tag>è¯¾ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuduä¸Spark ç”Ÿäº§æœ€ä½³å®è·µ]]></title>
    <url>%2F2019%2F05%2F07%2FKudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[ç¯å¢ƒ12345 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;&lt;/properties&gt;æµ‹è¯•ä»£ç 123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import org.apache.kudu.client._import collection.JavaConverters._object KuduApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate() //Read a table from Kudu val df = spark.read .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;)) .format(&quot;kudu&quot;).load df.schema.printTreeString()// // Use KuduContext to create, delete, or write to Kudu tables// val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)////// // The schema is encoded in a string// val schemalString=&quot;id,age,name&quot;//// // Generate the schema based on the string of schema// val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))// val schema=StructType(fields)////// val KuduTable = kuduContext.createTable(// &quot;test_table&quot;, schema, Seq(&quot;id&quot;),// new CreateTableOptions()// .setNumReplicas(1)// .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema//// val id = KuduTable.getColumn(&quot;id&quot;)// print(id)//// kuduContext.tableExists(&quot;test_table&quot;) &#125;&#125;ç°è±¡:é€šè¿‡spark sql æ“ä½œæŠ¥å¦‚ä¸‹é”™è¯¯:12345678910111213141516171819202122Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549) at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86) at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146) at cn.zhangyu.KuduApp$.main(KuduApp.scala:18) at cn.zhangyu.KuduApp.main(KuduApp.scala)Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533) at scala.util.Try$.apply(Try.scala:192) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533) at scala.util.Try.orElse(Try.scala:84) at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533) ... 7 moreè€Œé€šè¿‡KuduContextæ˜¯å¯ä»¥æ“ä½œçš„æ²¡æœ‰æŠ¥é”™,ä»£ç ä¸ºä¸Šé¢æ³¨è§£éƒ¨åˆ†è§£å†³æ€è·¯æŸ¥è¯¢kuduå®˜ç½‘:https://kudu.apache.org/docs/developing.htmlå®˜ç½‘ä¸­è¯´å‡ºäº†ç‰ˆæœ¬çš„é—®é¢˜:1234å¦‚æœå°†Spark 2ä¸Scala 2.11ä¸€èµ·ä½¿ç”¨ï¼Œè¯·ä½¿ç”¨kudu-spark2_2.11å·¥ä»¶ã€‚kudu-sparkç‰ˆæœ¬1.8.0åŠæ›´ä½ç‰ˆæœ¬çš„è¯­æ³•ç•¥æœ‰ä¸åŒã€‚æœ‰å…³æœ‰æ•ˆç¤ºä¾‹ï¼Œè¯·å‚é˜…æ‚¨çš„ç‰ˆæœ¬çš„æ–‡æ¡£ã€‚å¯ä»¥åœ¨å‘å¸ƒé¡µé¢ä¸Šæ‰¾åˆ°ç‰ˆæœ¬åŒ–æ–‡æ¡£ã€‚spark-shell --packages org.apache.kudu:kudu-spark2_2.11:1.9.0çœ‹åˆ°äº† å®˜ç½‘ä½¿ç”¨çš„æ˜¯1.9.0çš„ç‰ˆæœ¬.ä½†æ˜¯ä½†æ˜¯ä½†æ˜¯å®˜ç½‘ä¸‹é¢è¯´åˆ°äº†ä¸‹é¢å‡ ä¸ªé›†æˆé—®é¢˜:Spark 2.2+åœ¨è¿è¡Œæ—¶éœ€è¦Java 8ï¼Œå³ä½¿Kudu Spark 2.xé›†æˆä¸Java 7å…¼å®¹ã€‚Spark 2.2æ˜¯Kudu 1.5.0çš„é»˜è®¤ä¾èµ–ç‰ˆæœ¬ã€‚å½“æ³¨å†Œä¸ºä¸´æ—¶è¡¨æ—¶ï¼Œå¿…é¡»ä¸ºåç§°åŒ…å«å¤§å†™æˆ–éasciiå­—ç¬¦çš„Kuduè¡¨åˆ†é…å¤‡ç”¨åç§°ã€‚åŒ…å«å¤§å†™æˆ–éasciiå­—ç¬¦çš„åˆ—åçš„Kuduè¡¨ä¸èƒ½ä¸SparkSQLä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥åœ¨Kuduä¸­é‡å‘½ååˆ—ä»¥è§£å†³æ­¤é—®é¢˜ã€‚&lt;&gt;å¹¶ä¸”ORè°“è¯ä¸ä¼šè¢«æ¨é€åˆ°Kuduï¼Œè€Œæ˜¯ç”±Sparkä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚åªæœ‰LIKEå¸¦æœ‰åç¼€é€šé…ç¬¦çš„è°“è¯æ‰ä¼šè¢«æ¨é€åˆ°Kuduï¼Œè¿™æ„å‘³ç€å®ƒLIKE â€œFOO%â€è¢«æ¨ä¸‹ä½†LIKE â€œFOO%BARâ€ä¸æ˜¯ã€‚Kuduä¸æ”¯æŒSpark SQLæ”¯æŒçš„æ¯ç§ç±»å‹ã€‚ä¾‹å¦‚ï¼Œ Dateä¸æ”¯æŒå¤æ‚ç±»å‹ã€‚Kuduè¡¨åªèƒ½åœ¨SparkSQLä¸­æ³¨å†Œä¸ºä¸´æ—¶è¡¨ã€‚ä½¿ç”¨HiveContextå¯èƒ½æ— æ³•æŸ¥è¯¢Kuduè¡¨ã€‚é‚£å°±å¾ˆå¥‡æ€ªäº†æˆ‘ç”¨çš„1.5.0ç‰ˆæœ¬æŠ¥é”™ä¸º:æ‰¾ä¸åˆ°ç±»,æ•°æ®æºæœ‰é—®é¢˜ä½†æ˜¯æŠŠkuduæ”¹æˆ1.9.0 é—®é¢˜è§£å†³è¿è¡Œç»“æœ:1234root |-- id: string (nullable = false) |-- age: string (nullable = true) |-- name: string (nullable = true)Sparké›†æˆæœ€ä½³å®è·µæ¯ä¸ªç¾¤é›†é¿å…å¤šä¸ªKuduå®¢æˆ·ç«¯ã€‚ä¸€ä¸ªå¸¸è§çš„Kudu-Sparkç¼–ç é”™è¯¯æ˜¯å®ä¾‹åŒ–é¢å¤–çš„KuduClientå¯¹è±¡ã€‚åœ¨kudu-sparkä¸­ï¼Œa KuduClientå±äºKuduContextã€‚Sparkåº”ç”¨ç¨‹åºä»£ç ä¸åº”åˆ›å»ºå¦ä¸€ä¸ªKuduClientè¿æ¥åˆ°åŒä¸€ç¾¤é›†ã€‚ç›¸åï¼Œåº”ç”¨ç¨‹åºä»£ç åº”ä½¿ç”¨KuduContextè®¿é—®KuduClientä½¿ç”¨1234567KuduContext#syncClientã€‚ // Use KuduContext to create, delete, or write to Kudu tables val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext) val list = kuduContext.syncClient.getTablesList.getTablesList if (list.iterator().hasNext)&#123; print(list.iterator().next()) &#125;è¦è¯Šæ–­KuduClientSparkä½œä¸šä¸­çš„å¤šä¸ªå®ä¾‹ï¼Œè¯·æŸ¥çœ‹ä¸»æœåŠ¡å™¨çš„æ—¥å¿—ä¸­çš„ç¬¦å·ï¼Œè¿™äº›ç¬¦å·ä¼šè¢«æ¥è‡ªä¸åŒå®¢æˆ·ç«¯çš„è®¸å¤šGetTableLocationsæˆ– GetTabletLocationsè¯·æ±‚è¿‡è½½ï¼Œé€šå¸¸å¤§çº¦åœ¨åŒä¸€æ—¶é—´ã€‚è¿™ç§ç—‡çŠ¶ç‰¹åˆ«é€‚ç”¨äºSpark Streamingä»£ç ï¼Œå…¶ä¸­åˆ›å»ºKuduClientæ¯ä¸ªä»»åŠ¡å°†å¯¼è‡´æ¥è‡ªæ–°å®¢æˆ·ç«¯çš„ä¸»è¯·æ±‚çš„å‘¨æœŸæ€§æ³¢ã€‚Sparkæ“ä½œkudu(Scala demo)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.zhangyuimport org.apache.kudu.spark.kudu._import org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.slf4j.LoggerFactoryimport org.apache.kudu.client._import collection.JavaConverters._object SparkTest &#123; //kuduMasters and tableName val kuduMasters = &quot;192.168.13.130:7051&quot; val tableName = &quot;kudu_spark_table&quot; //table column val idCol = &quot;id&quot; val ageCol = &quot;age&quot; val nameCol = &quot;name&quot; //replication val tableNumReplicas = Integer.getInteger(&quot;tableNumReplicas&quot;, 1) val logger = LoggerFactory.getLogger(SparkTest.getClass) def main(args: Array[String]): Unit = &#123; //create SparkSession val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate() //create kuduContext val kuduContext = new KuduContext(kuduMasters,spark.sparkContext) //schema val schema = StructType( List( StructField(idCol, IntegerType, false), StructField(nameCol, StringType, false), StructField(ageCol,StringType,false) ) ) var tableIsCreated = false try&#123; // Make sure the table does not exist if (kuduContext.tableExists(tableName)) &#123; throw new RuntimeException(tableName + &quot;: table already exists&quot;) &#125; //create kuduContext.createTable(tableName, schema, Seq(idCol), new CreateTableOptions() .addHashPartitions(List(idCol).asJava, 3) .setNumReplicas(tableNumReplicas)) tableIsCreated = true import spark.implicits._ //write logger.info(s&quot;writing to table &apos;$tableName&apos;&quot;) val data = Array(Person(1,&quot;12&quot;,&quot;zhangsan&quot;),Person(2,&quot;20&quot;,&quot;lisi&quot;),Person(3,&quot;30&quot;,&quot;wangwu&quot;)) val personRDD = spark.sparkContext.parallelize(data) val personDF = personRDD.toDF() kuduContext.insertRows(personDF,tableName) //useing SparkSQL read table val sqlDF = spark.sqlContext.read .options(Map(&quot;kudu.master&quot; -&gt; kuduMasters, &quot;kudu.table&quot; -&gt; tableName)) .format(&quot;kudu&quot;).kudu sqlDF.createOrReplaceTempView(tableName) spark.sqlContext.sql(s&quot;SELECT * FROM $tableName &quot;).show //upsert some rows val upsertPerson = Array(Person(1,&quot;10&quot;,&quot;jack&quot;)) val upsertPersonRDD = spark.sparkContext.parallelize(upsertPerson) val upsertPersonDF = upsertPersonRDD.toDF() kuduContext.updateRows(upsertPersonDF,tableName) //useing RDD read table val readCols = Seq(idCol,ageCol,nameCol) val readRDD = kuduContext.kuduRDD(spark.sparkContext, tableName, readCols) val userTuple = readRDD.map &#123; case Row( id: Int,age: String,name: String) =&gt; (id,age,name) &#125; println(&quot;count:&quot;+userTuple.count()) userTuple.collect().foreach(println(_)) //delete table kuduContext.deleteTable(tableName) &#125;catch &#123; // Catch, log and re-throw. Not the best practice, but this is a very // simplistic example. case unknown : Throwable =&gt; logger.error(s&quot;got an exception: &quot; + unknown) throw unknown &#125; finally &#123; // Clean up. if (tableIsCreated) &#123; logger.info(s&quot;deleting table &apos;$tableName&apos;&quot;) kuduContext.deleteTable(tableName) &#125; logger.info(s&quot;closing down the session&quot;) spark.close() &#125; &#125;&#125;case class Person(id: Int,age: String,name: String)]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>Kudu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019äº”ä¸€-çº¿ä¸‹é¡¹ç›®ç¬¬13æœŸåœ†æ»¡ç»“æŸ]]></title>
    <url>%2F2019%2F05%2F05%2F2019%E4%BA%94%E4%B8%80-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC13%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019å¹´äº”ä¸€ï¼Œ3å¤©2å¤œåŒ—äº¬çº¿ä¸‹ç­åœ†æ»¡ç»“æŸä¸€å¥è¯ï¼ŒåŒ—äº¬æ¸©åº¦é€‚å®œå°ä¼™ä¼´ä»¬æ¥è‡ªåŒ—äº¬ã€ä¸Šæµ·ã€æ·±åœ³å¤§å®¶ä¸ºäº†ä¸€ä¸ªçœŸå®ç›®æ ‡å­¦ä¹ çœŸæ­£ä¼ä¸šçº§å¤§æ•°æ®ç”Ÿäº§é¡¹ç›®2ä¸ªç”Ÿäº§é¡¹ç›®+3ä¸ªTopicåˆ†äº«ä¸€å¹´æˆ‘ä»¬åªåœ¨èŠ‚å‡æ—¥ä¸¾åŠé”™è¿‡äº†å°±æ˜¯é”™è¿‡äº†æœŸå¾…@ç«¯åˆèŠ‚çº¿ä¸‹é¡¹ç›®ç­ç¬¬14æœŸ]]></content>
      <categories>
        <category>çº¿ä¸‹å®æˆ˜ç­</category>
      </categories>
      <tags>
        <tag>çº¿ä¸‹å®æˆ˜ç­</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¸¸ç”¨Sparkç´¯åŠ å™¨å‰–æä¹‹äºŒ]]></title>
    <url>%2F2019%2F04%2F26%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Driverç«¯Driverç«¯åˆå§‹åŒ–æ„å»ºAccumulatorå¹¶åˆå§‹åŒ–ï¼ŒåŒæ—¶å®Œæˆäº†Accumulatoræ³¨å†Œï¼ŒAccumulators.register(this)æ—¶Accumulatorä¼šåœ¨åºåˆ—åŒ–åå‘é€åˆ°Executorç«¯Driveræ¥æ”¶åˆ°ResultTaskå®Œæˆçš„çŠ¶æ€æ›´æ–°åï¼Œä¼šå»æ›´æ–°Valueçš„å€¼ ç„¶ååœ¨Actionæ“ä½œæ‰§è¡Œåå°±å¯ä»¥è·å–åˆ°Accumulatorçš„å€¼äº†Executorç«¯Executorç«¯æ¥æ”¶åˆ°Taskä¹‹åä¼šè¿›è¡Œååºåˆ—åŒ–æ“ä½œï¼Œååºåˆ—åŒ–å¾—åˆ°RDDå’Œfunctionã€‚åŒæ—¶åœ¨ååºåˆ—åŒ–çš„åŒæ—¶ä¹Ÿå»ååºåˆ—åŒ–Accumulator(åœ¨readObjectæ–¹æ³•ä¸­å®Œæˆ)ï¼ŒåŒæ—¶ä¹Ÿä¼šå‘TaskContextå®Œæˆæ³¨å†Œå®Œæˆä»»åŠ¡è®¡ç®—ä¹‹åï¼Œéšç€Taskç»“æœä¸€èµ·è¿”å›ç»™Driverç»“åˆæºç åˆ†æDriverç«¯åˆå§‹åŒ–&ensp;&ensp;Driverç«¯ä¸»è¦ç»è¿‡ä»¥ä¸‹æ­¥éª¤ï¼Œå®Œæˆåˆå§‹åŒ–æ“ä½œï¼š123val accum = sparkContext.accumulator(0, â€œAccumulatorTestâ€)val acc = new Accumulator(initialValue, param, Some(name))Accumulators.register(this)Executorç«¯ååºåˆ—åŒ–å¾—åˆ°Accumulator&ensp;&ensp;ååºåˆ—åŒ–æ˜¯åœ¨è°ƒç”¨ResultTaskçš„runTaskæ–¹å¼æ—¶å€™åšçš„æ“ä½œï¼š123// ä¼šååºåˆ—åŒ–å‡ºæ¥RDDå’Œè‡ªå·±å®šä¹‰çš„functionval (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)&ensp;&ensp;åœ¨ååºåˆ—åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œä¼šè°ƒç”¨Accumulableä¸­çš„readObjectæ–¹æ³•ï¼š123456789101112131415161718private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123; in.defaultReadObject() // valueçš„åˆå§‹å€¼ä¸ºzeroï¼›è¯¥å€¼æ˜¯ä¼šè¢«åºåˆ—åŒ–çš„ value_ = zero deserialized = true // Automatically register the accumulator when it is deserialized with the task closure. // // Note internal accumulators sent with task are deserialized before the TaskContext is created // and are registered in the TaskContext constructor. Other internal accumulators, such SQL // metrics, still need to register here. val taskContext = TaskContext.get() if (taskContext != null) &#123; // å½“å‰ååºåˆ—åŒ–æ‰€å¾—åˆ°çš„å¯¹è±¡ä¼šè¢«æ³¨å†Œåˆ°TaskContextä¸­ // è¿™æ ·TaskContextå°±å¯ä»¥è·å–åˆ°ç´¯åŠ å™¨ // ä»»åŠ¡è¿è¡Œç»“æŸä¹‹åï¼Œå°±å¯ä»¥é€šè¿‡context.collectAccumulators()è¿”å›ç»™executor taskContext.registerAccumulator(this) &#125; &#125;æ³¨æ„Accumulable.scalaä¸­çš„value_ï¼Œæ˜¯ä¸ä¼šè¢«åºåˆ—åŒ–çš„ï¼Œ@transientå…³é”®è¯ä¿®é¥°äº†1@volatile @transient private var value_ : R = initialValue // Current value on masterç´¯åŠ å™¨åœ¨å„ä¸ªèŠ‚ç‚¹çš„ç´¯åŠ æ“ä½œé’ˆå¯¹ä¼ å…¥functionä¸­ä¸åŒçš„æ“ä½œï¼Œå¯¹åº”æœ‰ä¸åŒçš„è°ƒç”¨æ–¹æ³•ï¼Œä»¥ä¸‹åˆ—ä¸¾å‡ ç§ï¼ˆåœ¨Accumulator.scalaä¸­ï¼‰ï¼š123def += (term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;def add(term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;æ ¹æ®ä¸åŒçš„ç´¯åŠ å™¨å‚æ•°ï¼Œæœ‰ä¸åŒå®ç°çš„AccumulableParamï¼ˆåœ¨Accumulator.scalaä¸­ï¼‰ï¼š123456trait AccumulableParam[R, T] extends Serializable &#123; /** def addAccumulator(r: R, t: T): R def addInPlace(r1: R, r2: R): R def zero(initialValue: R): R&#125;ä¸åŒçš„å®ç°å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šä»¥IntAccumulatorParamä¸ºä¾‹ï¼š1234implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123; def addInPlace(t1: Int, t2: Int): Int = t1 + t2 def zero(initialValue: Int): Int = 0&#125;æˆ‘ä»¬å‘ç°IntAccumulatorParamå®ç°çš„æ˜¯trait AccumulatorParam[T]ï¼š12345trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; addInPlace(t1, t2) &#125;&#125;åœ¨å„ä¸ªèŠ‚ç‚¹ä¸Šçš„ç´¯åŠ æ“ä½œå®Œæˆä¹‹åï¼Œå°±ä¼šç´§è·Ÿç€è¿”å›æ›´æ–°ä¹‹åçš„Accumulatorsçš„value_å€¼èšåˆæ“ä½œåœ¨Task.scalaä¸­çš„runæ–¹æ³•ï¼Œä¼šæ‰§è¡Œå¦‚ä¸‹ï¼š123// è¿”å›ç´¯åŠ å™¨ï¼Œå¹¶è¿è¡Œtask// è°ƒç”¨TaskContextImplçš„collectAccumulatorsï¼Œè¿”å›å€¼çš„ç±»å‹ä¸ºä¸€ä¸ªMap(runTask(context), context.collectAccumulators())åœ¨Executorç«¯å·²ç»å®Œæˆäº†ä¸€ç³»åˆ—æ“ä½œï¼Œéœ€è¦å°†å®ƒä»¬çš„å€¼è¿”å›åˆ°Driverç«¯è¿›è¡Œèšåˆæ±‡æ€»ï¼Œæ•´ä¸ªé¡ºåºå¦‚å›¾ç´¯åŠ å™¨æ‰§è¡Œæµç¨‹ï¼šæ ¹æ®æ‰§è¡Œæµç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œåœ¨æ‰§è¡Œå®ŒcollectAccumulatorsæ–¹æ³•ä¹‹åï¼Œæœ€ç»ˆä¼šåœ¨DAGSchedulerä¸­è°ƒç”¨updateAccumulators(event)ï¼Œè€Œåœ¨è¯¥æ–¹æ³•ä¸­ä¼šè°ƒç”¨Accumulatorsçš„addæ–¹æ³•ï¼Œä»è€Œå®Œæˆèšåˆæ“ä½œï¼š12345678910111213141516171819def add(values: Map[Long, Any]): Unit = synchronized &#123; // éå†ä¼ è¿›æ¥çš„å€¼ for ((id, value) &lt;- values) &#123; if (originals.contains(id)) &#123; // Since we are now storing weak references, we must check whether the underlying data // is valid. // æ ¹æ®idä»æ³¨å†Œçš„Mapä¸­å–å‡ºå¯¹åº”çš„ç´¯åŠ å™¨ originals(id).get match &#123; // å°†å€¼ç»™ç´¯åŠ èµ·æ¥ï¼Œæœ€ç»ˆå°†ç»“æœåŠ åˆ°valueé‡Œé¢ // ++=æ˜¯è¢«é‡è½½äº† case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value case None =&gt; throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;) &#125; &#125; else &#123; logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;) &#125; &#125;&#125;è·å–ç´¯åŠ å™¨çš„å€¼é€šè¿‡accum.valueæ–¹æ³•å¯ä»¥è·å–åˆ°ç´¯åŠ å™¨çš„å€¼è‡³æ­¤ï¼Œç´¯åŠ å™¨æ‰§è¡Œå®Œæ¯•ã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç´¯åŠ å™¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2è¯¦ç»†ä»‹ç»]]></title>
    <url>%2F2019%2F04%2F23%2Fspark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Sparkå‘å¸ƒäº†æœ€æ–°çš„ç‰ˆæœ¬spark-2.4.2æ ¹æ®å®˜ç½‘ä»‹ç»ï¼Œæ­¤ç‰ˆæœ¬å¯¹äºä½¿ç”¨spark2.4çš„ç”¨æˆ·æ¥è¯´å¸®åŠ©æ˜¯å·¨å¤§çš„ç‰ˆæœ¬ä»‹ç»Spark2.4.2æ˜¯ä¸€ä¸ªåŒ…å«ç¨³å®šæ€§ä¿®å¤çš„ç»´æŠ¤ç‰ˆæœ¬ã€‚ æ­¤ç‰ˆæœ¬åŸºäºSpark2.4ç»´æŠ¤åˆ†æ”¯ã€‚ æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‰€æœ‰2.4ç”¨æˆ·å‡çº§åˆ°æ­¤ç¨³å®šç‰ˆæœ¬ã€‚æ˜¾è‘—çš„å˜åŒ–SPARK-27419ï¼šåœ¨spark2.4ä¸­å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äº1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥ã€‚ å› ä¸ºè¯¥å€¼å°†è½¬æ¢ä¸º0ï¼Œå¿ƒè·³å°†å§‹ç»ˆè¶…æ—¶ï¼Œå¹¶æœ€ç»ˆç»ˆæ­¢æ‰§è¡Œç¨‹åºã€‚è¿˜åŸSPARK-25250ï¼šå¯èƒ½å¯¼è‡´ä½œä¸šæ°¸ä¹…æŒ‚èµ·ï¼Œåœ¨2.4.2ä¸­è¿˜åŸã€‚è¯¦ç»†æ›´æ”¹BUGissueså†…å®¹æ‘˜è¦[ SPARK-26961 ]åœ¨Spark Driverä¸­å‘ç°Javaæ­»é”[ SPARK-26998 ]åœ¨Standaloneæ¨¡å¼ä¸‹æ‰§è¡Œâ€™ps -efâ€™ç¨‹åºè¿›ç¨‹,è¾“å‡ºspark.ssl.keyStorePasswordçš„æ˜æ–‡[ SPARK-27216 ]å°†RoaringBitmapå‡çº§åˆ°0.7.45ä»¥ä¿®å¤Kryoä¸å®‰å…¨çš„ser / dseré—®é¢˜[ SPARK-27244 ]ä½¿ç”¨é€‰é¡¹logConf = trueæ—¶å¯†ç å°†ä»¥confçš„æ˜æ–‡å½¢å¼è®°å½•[ SPARK-27267 ]ç”¨Snappy 1.1.7.1è§£å‹ã€å‹ç¼©ç©ºåºåˆ—åŒ–æ•°æ®æ—¶å¤±è´¥[ SPARK-27275 ]EncryptedMessage.transferToä¸­çš„æ½œåœ¨æŸå[ SPARK-27301 ]DStreamCheckpointDataå› æ–‡ä»¶ç³»ç»Ÿå·²ç¼“å­˜è€Œæ— æ³•æ¸…ç†[ SPARK-27338 ]TaskMemoryManagerå’ŒUnsafeExternalSorter $ SpillableIteratorä¹‹é—´çš„æ­»é”[ SPARK-27351 ]åœ¨ä»…ä½¿ç”¨ç©ºå€¼åˆ—çš„AggregateEstimationä¹‹åçš„é”™è¯¯outputRowsä¼°è®¡[ SPARK-27390 ]ä¿®å¤åŒ…åç§°ä¸åŒ¹é…[ SPARK-27394 ]å½“æ²¡æœ‰ä»»åŠ¡å¼€å§‹æˆ–ç»“æŸæ—¶ï¼ŒUI çš„é™ˆæ—§æ€§å¯èƒ½æŒç»­æ•°åˆ†é’Ÿæˆ–æ•°å°æ—¶[ SPARK-27403 ]ä¿®å¤updateTableStatsä»¥ä½¿ç”¨æ–°ç»Ÿè®¡ä¿¡æ¯æˆ–æ— æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯[ SPARK-27406 ]å½“ä¸¤å°æœºå™¨å…·æœ‰ä¸åŒçš„Oopså¤§å°æ—¶ï¼ŒUnsafeArrayDataåºåˆ—åŒ–ä¼šä¸­æ–­[ SPARK-27419 ]å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äº1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥[ SPARK-27453 ]DSV1é™é»˜åˆ é™¤DataFrameWriter.partitionByæ”¹è¿›issueså†…å®¹æ‘˜è¦[ SPARK-27346 ]æ¾å¼€åœ¨ExpressionInfoçš„â€™examplesâ€™å­—æ®µä¸­æ¢è¡Œæ–­è¨€æ¡ä»¶[ SPARK-27358 ]å°†jqueryæ›´æ–°ä¸º1.12.xä»¥è·å–å®‰å…¨ä¿®å¤ç¨‹åº[ SPARK-27479 ]éšè—â€œorg.apache.spark.util.kvstoreâ€çš„APIæ–‡æ¡£å·¥ä½œissueså†…å®¹æ‘˜è¦[ SPARK-27382 ]åœ¨HiveExternalCatalogVersionsSuiteä¸­æ›´æ–°Spark 2.4.xæµ‹è¯•]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¸¸ç”¨Sparkç´¯åŠ å™¨å‰–æä¹‹ä¸€]]></title>
    <url>%2F2019%2F04%2F19%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[ç”±äºæœ€è¿‘åœ¨é¡¹ç›®ä¸­éœ€è¦ç”¨åˆ°Sparkçš„ç´¯åŠ å™¨ï¼ŒåŒæ—¶éœ€è¦è‡ªå·±å»è‡ªå®šä¹‰å®ç°Sparkçš„ç´¯åŠ å™¨ï¼Œä»è€Œæ»¡è¶³ç”Ÿäº§ä¸Šçš„éœ€æ±‚ã€‚å¯¹æ­¤ï¼Œå¯¹Sparkçš„ç´¯åŠ å™¨å®ç°æœºåˆ¶è¿›è¡Œäº†è¿½è¸ªå­¦ä¹ ã€‚æœ¬ç³»åˆ—æ–‡ç« ï¼Œå°†ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å…¥æ‰‹ï¼Œå¯¹Sparkç´¯åŠ å™¨è¿›è¡Œå‰–æï¼šSparkç´¯åŠ å™¨çš„åŸºæœ¬æ¦‚å¿µç´¯åŠ å™¨çš„é‡ç‚¹ç±»æ„æˆç´¯åŠ å™¨çš„æºç è§£æç´¯åŠ å™¨çš„æ‰§è¡Œè¿‡ç¨‹ç´¯åŠ å™¨ä½¿ç”¨ä¸­çš„å‘è‡ªå®šä¹‰ç´¯åŠ å™¨çš„å®ç°Sparkç´¯åŠ å™¨åŸºæœ¬æ¦‚å¿µSparkæä¾›çš„Accumulatorï¼Œä¸»è¦ç”¨äºå¤šä¸ªèŠ‚ç‚¹å¯¹ä¸€ä¸ªå˜é‡è¿›è¡Œå…±äº«æ€§çš„æ“ä½œã€‚Accumulatoråªæä¾›äº†ç´¯åŠ çš„åŠŸèƒ½ï¼Œåªèƒ½ç´¯åŠ ï¼Œä¸èƒ½å‡å°‘ç´¯åŠ å™¨åªèƒ½åœ¨Driverç«¯æ„å»ºï¼Œå¹¶åªèƒ½ä»Driverç«¯è¯»å–ç»“æœï¼Œåœ¨Taskç«¯åªèƒ½è¿›è¡Œç´¯åŠ ã€‚è‡³äºè¿™é‡Œä¸ºä»€ä¹ˆåªèƒ½åœ¨Taskç´¯åŠ å‘¢ï¼Ÿä¸‹é¢çš„å†…å®¹å°†ä¼šè¿›è¡Œè¯¦ç»†çš„ä»‹ç»ï¼Œå…ˆç®€å•ä»‹ç»ä¸‹ï¼š123åœ¨TaskèŠ‚ç‚¹ï¼Œå‡†ç¡®çš„å°±æ˜¯è¯´åœ¨executorä¸Šï¼›æ¯ä¸ªTaskéƒ½ä¼šæœ‰ä¸€ä¸ªç´¯åŠ å™¨çš„å˜é‡ï¼Œè¢«åºåˆ—åŒ–ä¼ è¾“åˆ°executorç«¯è¿è¡Œä¹‹åå†è¿”å›è¿‡æ¥éƒ½æ˜¯ç‹¬ç«‹è¿è¡Œçš„ï¼›å¦‚æœåœ¨Taskç«¯å»è·å–å€¼çš„è¯ï¼Œåªèƒ½è·å–åˆ°å½“å‰Taskçš„ï¼ŒTaskä¸Taskä¹‹é—´ä¸ä¼šæœ‰å½±å“ç´¯åŠ å™¨ä¸ä¼šæ”¹å˜Spark lazyè®¡ç®—çš„ç‰¹ç‚¹ï¼Œåªä¼šåœ¨Jobè§¦å‘çš„æ—¶å€™è¿›è¡Œç›¸å…³çš„ç´¯åŠ æ“ä½œç°æœ‰ç´¯åŠ å™¨ç±»å‹:ç´¯åŠ å™¨çš„é‡ç‚¹ç±»ä»‹ç»class Accumulator extends Accumulableæºç ï¼ˆæºç ä¸­å·²ç»å¯¹è¿™ä¸ªç±»çš„ä½œç”¨åšäº†ååˆ†è¯¦ç»†çš„è§£é‡Šï¼‰ï¼š1234567891011121314151617181920212223242526272829/** * A simpler value of [[Accumulable]] where the result type being accumulated is the same * as the types of elements being merged, i.e. variables that are only &quot;added&quot; to through an * associative operation and can therefore be efficiently supported in parallel. They can be used * to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric * value types, and programmers can add support for new types. * * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]]. * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator. * However, they cannot read its value. Only the driver program can read the accumulator&apos;s value, * using its value method. * * @param initialValue initial value of accumulator * @param param helper object defining how to add elements of type `T` * @tparam T result type */class Accumulator[T] private[spark] ( @transient private[spark] val initialValue: T, param: AccumulatorParam[T], name: Option[String], internal: Boolean) extends Accumulable[T, T](initialValue, param, name, internal) &#123; def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123; this(initialValue, param, name, false) &#125; def this(initialValue: T, param: AccumulatorParam[T]) = &#123; this(initialValue, param, None, false) &#125;&#125;ä¸»è¦å®ç°äº†ç´¯åŠ å™¨çš„åˆå§‹åŒ–åŠå°è£…äº†ç›¸å…³çš„ç´¯åŠ å™¨æ“ä½œæ–¹æ³• åŒæ—¶åœ¨ç±»å¯¹è±¡æ„å»ºçš„æ—¶å€™å‘Accumulatorsæ³¨å†Œç´¯åŠ å™¨ ç´¯åŠ å™¨çš„addæ“ä½œçš„è¿”å›å€¼ç±»å‹å’Œä¼ å…¥è¿›å»çš„å€¼ç±»å‹å¯ä»¥ä¸ä¸€æ · æ‰€ä»¥ä¸€å®šè¦å®šä¹‰å¥½ä¸¤æ­¥æ“ä½œï¼ˆå³addæ–¹æ³•ï¼‰ï¼šç´¯åŠ æ“ä½œ/åˆå¹¶æ“ä½œ object Accumulatorsè¯¥æ–¹æ³•åœ¨Driverç«¯ç®¡ç†ç€ç´¯åŠ å™¨ï¼Œä¹ŸåŒ…å«äº†ç´¯åŠ å™¨çš„èšåˆæ“ä½œ trait AccumulatorParam[T] extends AccumulableParam[T, T]æºç ï¼š123456789101112/** * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be * available when you create Accumulators of a specific type. * * @tparam T type of value to accumulate */trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; addInPlace(t1, t2) &#125;&#125;AccumulatorParamçš„addAccumulatoræ“ä½œçš„æ³›å‹å°è£… å…·ä½“çš„å®ç°è¿˜æ˜¯éœ€è¦åœ¨å…·ä½“å®ç°ç±»é‡Œé¢å®ç°addInPlaceæ–¹æ³• è‡ªå®šä¹‰å®ç°ç´¯åŠ å™¨çš„å…³é”® object AccumulatorParamæºç ï¼š1234567891011121314151617181920212223object AccumulatorParam &#123; // The following implicit objects were in SparkContext before 1.2 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, as there are duplicate codes in SparkContext for backward // compatibility, please update them accordingly if you modify the following implicit objects. implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] &#123; def addInPlace(t1: Double, t2: Double): Double = t1 + t2 def zero(initialValue: Double): Double = 0.0 &#125; implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123; def addInPlace(t1: Int, t2: Int): Int = t1 + t2 def zero(initialValue: Int): Int = 0 &#125; implicit object LongAccumulatorParam extends AccumulatorParam[Long] &#123; def addInPlace(t1: Long, t2: Long): Long = t1 + t2 def zero(initialValue: Long): Long = 0L &#125; implicit object FloatAccumulatorParam extends AccumulatorParam[Float] &#123; def addInPlace(t1: Float, t2: Float): Float = t1 + t2 def zero(initialValue: Float): Float = 0f &#125; // TODO: Add AccumulatorParams for other types, e.g. lists and strings&#125;ä»æºç ä¸­å¤§é‡çš„implicitå…³é”®è¯ï¼Œå¯ä»¥å‘ç°è¯¥ç±»ä¸»è¦è¿›è¡Œéšå¼ç±»å‹è½¬æ¢çš„æ“ä½œ TaskContextImplåœ¨Executorç«¯ç®¡ç†ç€æˆ‘ä»¬çš„ç´¯åŠ å™¨ï¼Œç´¯åŠ å™¨æ˜¯é€šè¿‡è¯¥ç±»è¿›è¡Œè¿”å›çš„ ç´¯åŠ å™¨çš„æºç è§£æDriverç«¯&ensp;&ensp;accumulatoræ–¹æ³•ä»¥ä¸‹åˆ—è¿™æ®µä»£ç ä¸­çš„accumulatoræ–¹æ³•ä¸ºå…¥å£ç‚¹ï¼Œè¿›å…¥åˆ°ç›¸åº”çš„æºç ä¸­å»val acc = new Accumulator(initialValue, param, Some(name))æºç ï¼š12345678910111213class Accumulator[T] private[spark] ( @transient private[spark] val initialValue: T, param: AccumulatorParam[T], name: Option[String], internal: Boolean) extends Accumulable[T, T](initialValue, param, name, internal) &#123; def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123; this(initialValue, param, name, false) &#125; def this(initialValue: T, param: AccumulatorParam[T]) = &#123; this(initialValue, param, None, false) &#125;&#125;&ensp;&ensp;ç»§æ‰¿çš„Accumulable[T, T]æºç ï¼š123456789101112131415class Accumulable[R, T] private[spark] ( initialValue: R, param: AccumulableParam[R, T], val name: Option[String], internal: Boolean) extends Serializable &#123;â€¦// è¿™é‡Œçš„_valueå¹¶ä¸æ”¯æŒåºåˆ—åŒ–// æ³¨ï¼šæœ‰@transientçš„éƒ½ä¸ä¼šè¢«åºåˆ—åŒ–@volatile @transient private var value_ : R = initialValue // Current value on master â€¦ // æ³¨å†Œäº†å½“å‰çš„ç´¯åŠ å™¨ Accumulators.register(this) â€¦, &#125;&ensp;&ensp;Accumulators.register()æºç ï¼š12345// ä¼ å…¥å‚æ•°ï¼Œæ³¨å†Œç´¯åŠ å™¨def register(a: Accumulable[_, _]): Unit = synchronized &#123;// æ„é€ æˆWeakReferenceoriginals(a.id) = new WeakReference[Accumulable[_, _]](a)&#125;è‡³æ­¤ï¼ŒDriverç«¯çš„åˆå§‹åŒ–å·²ç»å®Œæˆ Executorç«¯Executorç«¯çš„ååºåˆ—åŒ–æ˜¯ä¸€ä¸ªå¾—åˆ°æˆ‘ä»¬çš„å¯¹è±¡çš„è¿‡ç¨‹ åˆå§‹åŒ–æ˜¯åœ¨ååºåˆ—åŒ–çš„æ—¶å€™å°±å®Œæˆçš„ï¼ŒåŒæ—¶ååºåˆ—åŒ–çš„æ—¶å€™è¿˜å®Œæˆäº†Accumulatorå‘TaskContextImplçš„æ³¨å†Œ &ensp;&ensp;TaskRunnerä¸­çš„runæ–¹æ³•1234567891011121314151617181920212223242526272829303132// åœ¨è®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œä¼šå°†RDDå’Œfunctionç»è¿‡åºåˆ—åŒ–ä¹‹åä¼ ç»™Executorç«¯private[spark] class Executor( executorId: String, executorHostname: String, env: SparkEnv, userClassPath: Seq[URL] = Nil, isLocal: Boolean = false) extends Logging &#123;... class TaskRunner( execBackend: ExecutorBackend, val taskId: Long, val attemptNumber: Int, taskName: String, serializedTask: ByteBuffer) extends Runnable &#123;â€¦override def run(): Unit = &#123; â€¦val (value, accumUpdates) = try &#123; // è°ƒç”¨TaskRunnerä¸­çš„task.runæ–¹æ³•ï¼Œè§¦å‘taskçš„è¿è¡Œ val res = task.run( taskAttemptId = taskId, attemptNumber = attemptNumber, metricsSystem = env.metricsSystem) threwException = false res &#125; finally &#123; â€¦ &#125;â€¦&#125;&ensp;&ensp;Taskä¸­çš„collectAccumulators()æ–¹æ³•1234567891011121314151617private[spark] abstract class Task[T](final def run( taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem) : (T, AccumulatorUpdates) = &#123; â€¦ try &#123; // è¿”å›ç´¯åŠ å™¨ï¼Œå¹¶è¿è¡Œtask // è°ƒç”¨TaskContextImplçš„collectAccumulatorsï¼Œè¿”å›å€¼çš„ç±»å‹ä¸ºä¸€ä¸ªMap (runTask(context), context.collectAccumulators()) &#125; finally &#123; â€¦ &#125; â€¦ &#125;)&ensp;&ensp;ResultTaskä¸­çš„runTaskæ–¹æ³•123456789101112override def runTask(context: TaskContext): U = &#123; // Deserialize the RDD and the func using the broadcast variables. val deserializeStartTime = System.currentTimeMillis() val ser = SparkEnv.get.closureSerializer.newInstance() // ååºåˆ—åŒ–æ˜¯åœ¨è°ƒç”¨ResultTaskçš„runTaskæ–¹æ³•çš„æ—¶å€™åšçš„ // ä¼šååºåˆ—åŒ–å‡ºæ¥RDDå’Œè‡ªå·±å®šä¹‰çš„function val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime metrics = Some(context.taskMetrics) func(context, rdd.iterator(partition, context))&#125;&ensp;&ensp;Accumulableä¸­çš„readObjectæ–¹æ³•1234567891011121314151617181920// åœ¨ååºåˆ—åŒ–çš„è¿‡ç¨‹ä¸­ä¼šè°ƒç”¨Accumulable.readObjectæ–¹æ³• // Called by Java when deserializing an object private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123; in.defaultReadObject() // valueçš„åˆå§‹å€¼ä¸ºzeroï¼›è¯¥å€¼æ˜¯ä¼šè¢«åºåˆ—åŒ–çš„ value_ = zero deserialized = true // Automatically register the accumulator when it is deserialized with the task closure. // // Note internal accumulators sent with task are deserialized before the TaskContext is created // and are registered in the TaskContext constructor. Other internal accumulators, such SQL // metrics, still need to register here. val taskContext = TaskContext.get() if (taskContext != null) &#123; // å½“å‰ååºåˆ—åŒ–æ‰€å¾—åˆ°çš„å¯¹è±¡ä¼šè¢«æ³¨å†Œåˆ°TaskContextä¸­ // è¿™æ ·TaskContextå°±å¯ä»¥è·å–åˆ°ç´¯åŠ å™¨ // ä»»åŠ¡è¿è¡Œç»“æŸä¹‹åï¼Œå°±å¯ä»¥é€šè¿‡context.collectAccumulators()è¿”å›ç»™executor taskContext.registerAccumulator(this) &#125; &#125;&ensp;&ensp;Executor.scala12345678// åœ¨executorç«¯æ‹¿åˆ°accumuUpdateså€¼ä¹‹åï¼Œä¼šå»æ„é€ ä¸€ä¸ªDirectTaskResultval directResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.orNull)val serializedDirectResult = ser.serialize(directResult)val resultSize = serializedDirectResult.limitâ€¦// æœ€ç»ˆç”±ExecutorBackendçš„statusUpdateæ–¹æ³•å‘é€è‡³Driverç«¯// ExecutorBackendä¸ºä¸€ä¸ªTraitï¼Œæœ‰å¤šç§å®ç°execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)&ensp;&ensp;CoarseGrainedExecutorBackendä¸­çš„statusUpdateæ–¹æ³•123456789// é€šè¿‡ExecutorBackendçš„ä¸€ä¸ªå®ç°ç±»ï¼šCoarseGrainedExecutorBackend ä¸­çš„statusUpdateæ–¹æ³•// å°†æ•°æ®å‘é€è‡³Driverç«¯override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) &#123; val msg = StatusUpdate(executorId, taskId, state, data) driver match &#123; case Some(driverRef) =&gt; driverRef.send(msg) case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;) &#125; &#125;&ensp;&ensp;CoarseGrainedSchedulerBackendä¸­çš„receiveæ–¹æ³•1234567// Driverç«¯åœ¨æ¥æ”¶åˆ°æ¶ˆæ¯ä¹‹åï¼Œä¼šè°ƒç”¨CoarseGrainedSchedulerBackendä¸­çš„receiveæ–¹æ³•override def receive: PartialFunction[Any, Unit] = &#123; case StatusUpdate(executorId, taskId, state, data) =&gt; // ä¼šåœ¨DAGSchedulerçš„handleTaskCompletionæ–¹æ³•ä¸­å°†ç»“æœè¿”å› scheduler.statusUpdate(taskId, state, data.value) â€¦&#125;&ensp;&ensp;TaskSchedulerImplçš„statusUpdateæ–¹æ³•123456789101112def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) &#123; â€¦ if (state == TaskState.FINISHED) &#123; taskSet.removeRunningTask(tid) // å°†æˆåŠŸçš„Taskå…¥é˜Ÿ taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData) &#125; else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) &#123; taskSet.removeRunningTask(tid) taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData) &#125; â€¦&#125;&ensp;&ensp;TaskResultGetterçš„enqueueSuccessfulTaskæ–¹æ³•12345def enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) &#123;â€¦ result.metrics.setResultSize(size) scheduler.handleSuccessfulTask(taskSetManager, tid, result)â€¦&ensp;&ensp;TaskSchedulerImplçš„handleSuccessfulTaskæ–¹æ³•123456def handleSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, taskResult: DirectTaskResult[_]): Unit = synchronized &#123; taskSetManager.handleSuccessfulTask(tid, taskResult) &#125;&ensp;&ensp;DAGSchedulerçš„taskEndedæ–¹æ³•123456789101112def taskEnded( task: Task[_], reason: TaskEndReason, result: Any, accumUpdates: Map[Long, Any], taskInfo: TaskInfo, taskMetrics: TaskMetrics): Unit = &#123; eventProcessLoop.post( // ç»™è‡ªèº«çš„æ¶ˆæ¯å¾ªç¯ä½“å‘äº†ä¸ªCompletionEvent // è¿™ä¸ªCompletionEventä¼šè¢«handleTaskCompletionæ–¹æ³•æ‰€æ¥æ”¶åˆ° CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics)) &#125;&ensp;&ensp;DAGSchedulerçš„handleTaskCompletionæ–¹æ³•12345678910111213141516171819202122232425// ä¸ä¸Šè¿°CoarseGrainedSchedulerBackendä¸­çš„receiveæ–¹æ³•ç« èŠ‚å¯¹åº”// åœ¨handleTaskCompletionæ–¹æ³•ä¸­ï¼Œæ¥æ”¶CompletionEvent// ä¸è®ºæ˜¯ResultTaskè¿˜æ˜¯ShuffleMapTaskéƒ½ä¼šå»è°ƒç”¨updateAccumulatorsæ–¹æ³•ï¼Œæ›´æ–°ç´¯åŠ å™¨çš„å€¼private[scheduler] def handleTaskCompletion(event: CompletionEvent) &#123; â€¦ event.reason match &#123; case Success =&gt; listenerBus.post(SparkListenerTaskEnd(stageId, stage.latestInfo.attemptId, taskType, event.reason, event.taskInfo, event.taskMetrics)) stage.pendingPartitions -= task.partitionId task match &#123; case rt: ResultTask[_, _] =&gt; // Cast to ResultStage here because it&apos;s part of the ResultTask // TODO Refactor this out to a function that accepts a ResultStage val resultStage = stage.asInstanceOf[ResultStage] resultStage.activeJob match &#123; case Some(job) =&gt; if (!job.finished(rt.outputId)) &#123; updateAccumulators(event) case smt: ShuffleMapTask =&gt; val shuffleStage = stage.asInstanceOf[ShuffleMapStage] updateAccumulators(event)&#125;â€¦&#125;&ensp;&ensp;DAGSchedulerçš„updateAccumulatorsæ–¹æ³•1234567private def updateAccumulators(event: CompletionEvent): Unit = &#123; val task = event.task val stage = stageIdToStage(task.stageId) if (event.accumUpdates != null) &#123; try &#123; // è°ƒç”¨äº†ç´¯åŠ å™¨çš„addæ–¹æ³• Accumulators.add(event.accumUpdates)&ensp;&ensp;Accumulatorsçš„addæ–¹æ³•12345678910111213141516171819def add(values: Map[Long, Any]): Unit = synchronized &#123; // éå†ä¼ è¿›æ¥çš„å€¼ for ((id, value) &lt;- values) &#123; if (originals.contains(id)) &#123; // Since we are now storing weak references, we must check whether the underlying data // is valid. // æ ¹æ®idä»æ³¨å†Œçš„Mapä¸­å–å‡ºå¯¹åº”çš„ç´¯åŠ å™¨ originals(id).get match &#123; // å°†å€¼ç»™ç´¯åŠ èµ·æ¥ï¼Œæœ€ç»ˆå°†ç»“æœåŠ åˆ°valueé‡Œé¢ // ++=æ˜¯è¢«é‡è½½äº† case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value case None =&gt; throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;) &#125; &#125; else &#123; logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;) &#125; &#125; &#125;&ensp;&ensp;Accumulatorsçš„++=æ–¹æ³•1def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;&ensp;&ensp;Accumulatorsçš„valueæ–¹æ³•1234567def value: R = &#123; if (!deserialized) &#123; value_ &#125; else &#123; throw new UnsupportedOperationException(&quot;Can&apos;t read accumulator value in task&quot;) &#125; &#125;æ­¤æ—¶æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå°±å¯ä»¥é€šè¿‡ .value çš„æ–¹å¼å»è·å–è®¡æ•°å™¨çš„å€¼äº†]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç´¯åŠ å™¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§Spark2.4.0å¦‚ä½•Debugæºä»£ç ]]></title>
    <url>%2F2019%2F04%2F17%2F%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[æºç è·å–ä¸ç¼–è¯‘ç›´æ¥ä»Sparkå®˜ç½‘è·å–æºç æˆ–è€…ä»GitHubè·å–ä¸‹è½½æºç 1wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0.tgzè§£å‹æºç 1tar -zxf spark-2.4.0.tgzSparkæºç ç¼–è¯‘æ­¤å¤„ä¸å†å•°å—¦ï¼Œç›´æ¥å»è…¾è®¯è¯¾å ‚ï¼Œæœç´¢â€œè‹¥æ³½å¤§æ•°æ®â€å³å¯æ‰¾åˆ°ç¼–è¯‘è§†é¢‘ã€‚æºç å¯¼å…¥IDEAè¿è¡Œhive-thriftserver2ä»spark-2.4.0-bin-2.6.0-cdh5.7.0/sbin/start-thriftserver.sh è„šæœ¬ä¸­æ‰¾åˆ° hive-thriftserver2 çš„å…¥å£ç±»ï¼š1org.apache.spark.sql.hive.thriftserver.HiveThriftServer2é…ç½®è¿è¡Œç¯å¢ƒ1Menu -&gt; Run -&gt; Edit Configurations -&gt; é€‰æ‹© + -&gt; Application-Dspark.master=local[2] ä»£è¡¨ä½¿ç”¨æœ¬åœ°æ¨¡å¼è¿è¡ŒSparkä»£ç è¿è¡Œä¹‹å‰éœ€è¦åšä¸€ä»¶å¾ˆé‡è¦çš„äº‹æƒ…ï¼Œå°† hive-thriftserver è¿™ä¸ªå­é¡¹ç›®çš„pomä¾èµ–å…¨éƒ¨ç”±providedæ”¹ä¸ºcompileï¼š12345678910&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;æ·»åŠ è¿è¡Œä¾èµ–çš„jars1Menu -&gt; File -&gt; Project Structure -&gt; Modules -&gt; spark-hive-thriftserver_2.11 -&gt; Dependencies æ·»åŠ ä¾èµ– jars -&gt; &#123;Spark_home&#125;/assembly/target/scala-2.11/jars/ä¸­é—´é‡åˆ°çš„é—®é¢˜é—®é¢˜ä¸€12345spark\sql\hive-thriftserver\src\main\java\org\apache\hive\service\cli\thrift\ThriftCLIService.javaError:(52, 75) not found: value TCLIServicepublic abstract class ThriftCLIService extends AbstractService implements TCLIService.Iface, Runnable &#123;â€¦â€¦â€¦..è§£å†³åŠæ³•ï¼š åœ¨spark\sql\hive-thriftserver\src\gen\javaå³é”®ä¸­ç‚¹Mark Directory as -&gt; Sources Rootå³å¯é—®é¢˜äºŒ12Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal at java.lang.ClassLoader.defineClass1(Native Method)è§£å†³åŠæ³•ï¼šåœ¨ hive-thriftserve å­é¡¹ç›®çš„pomæ–‡ä»¶ä¸­æ·»åŠ ä¾èµ–12345&lt;dependency&gt; &lt;groupId&gt;xml-apis&lt;/groupId&gt; &lt;artifactId&gt;xml-apis&lt;/artifactId&gt; &lt;version&gt;1.4.01&lt;/version&gt;&lt;/dependency&gt;é—®é¢˜ä¸‰1java.net.BindException: Cannot assign requested address: Service &apos;sparkDriver&apos; failed after 16 retries (starting from 0)! Consider explicitly setting the appropriate port for the service &apos;sparkDriver&apos; (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.è§£å†³åŠæ³•ï¼š åœ¨ /etc/hosts æ–‡ä»¶ä¸­é…ç½®ç›¸åº”çš„åœ°å€æ˜ å°„ã€‚æˆåŠŸè¿è¡Œåœ¨ HiveThriftServer2 ä¸­æ‰“æ–­ç‚¹è¿›è¡Œè°ƒè¯•æºç å³å¯ã€‚æ‰“ä¸€ä¸ªæ–­ç‚¹å¦‚ä¸‹æ‰€ç¤ºï¼šå°±èƒ½çœ‹åˆ°æ–­ç‚¹æ‰€æ‰“å°å‡ºæ¥çš„ä¿¡æ¯ã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkå†…å­˜ç®¡ç†ä¹‹ä¸‰ UnifiedMemoryManageråˆ†æ]]></title>
    <url>%2F2019%2F04%2F16%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[acquireExecutionMemoryæ–¹æ³•UnifiedMemoryManagerä¸­çš„accquireExecutionMemoryæ–¹æ³•ï¼šå½“å‰çš„ä»»åŠ¡å°è¯•ä»executorä¸­è·å–numBytesè¿™ä¹ˆå¤§çš„å†…å­˜è¯¥æ–¹æ³•ç›´æ¥å‘ExecutionMemoryPoolç´¢è¦æ‰€éœ€å†…å­˜ï¼Œç´¢è¦å†…å­˜æœ‰ä»¥ä¸‹å‡ ä¸ªå…³æ³¨ç‚¹ï¼šå½“ExecutionMemory å†…å­˜å……è¶³ï¼Œåˆ™ä¸ä¼šè§¦å‘å‘Storageç”³è¯·å†…å­˜æ¯ä¸ªTaskèƒ½å¤Ÿè¢«ä½¿ç”¨çš„å†…å­˜æ˜¯è¢«é™åˆ¶çš„ç´¢è¦å†…å­˜çš„å¤§å°æˆ‘ä»¬é€šè¿‡æºç æ¥è¿›è¡Œåˆ†æUnifiedMemoryManager.scalaä¸­æˆ‘ä»¬ç‚¹è¿›å»åä¼šå‘ç°ï¼Œä¼šè°ƒç”¨ExecutionMemoryPool.acquireMemory()æ–¹æ³•ExecutionMemoryPool.scalaä¸­æˆ‘ä»¬å¯ä»¥å‘ç°æ¯Taskèƒ½å¤Ÿè¢«ä½¿ç”¨çš„å†…å­˜è¢«é™åˆ¶åœ¨ï¼špoolSize / (2 * numActiveTasks) ~ maxPoolSize / numActiveTasks ä¹‹é—´val maxMemoryPerTask = maxPoolSize /numActiveTasksval minMemoryPerTask = poolSize / (2 * numActiveTasks)UnifiedMemoryManager.scalaä¸­å…¶ä¸­maxPoolSize = maxMemory - math.min(storageMemoryUsed, storageRegionSize)maxMemory = storage + executionçš„æœ€å¤§å†…å­˜poolSize = å½“å‰è¿™ä¸ªpoolçš„å¤§å°maxPoolSize = execution poolçš„æœ€å¤§å†…å­˜UnifiedMemoryManager.scalaä¸­ä»ä¸Šè¿°ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çŸ¥é“ç´¢è¦å†…å­˜çš„å¤§å°ï¼šval memoryReclaimableFromStorage=math.max(storageMemoryPool.memoryFree, storageMemoryPool.poolSize -storageRegionSize)å–å†³äºStorageMemoryPoolçš„å‰©ä½™å†…å­˜å’Œ storageMemoryPool ä»ExecutionMemoryå€Ÿæ¥çš„å†…å­˜å“ªä¸ªå¤§ï¼Œå–æœ€å¤§çš„é‚£ä¸ªï¼Œä½œä¸ºå¯ä»¥é‡æ–°å½’è¿˜çš„æœ€å¤§å†…å­˜ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥å°±æ˜¯è¿™ä¸€ä¸ªæ ·å­ï¼šExecutionMemory èƒ½å€Ÿåˆ°çš„æœ€å¤§å†…å­˜ = StorageMemory å€Ÿçš„å†…å­˜ + StorageMemory ç©ºé—²å†…å­˜æ³¨æ„ï¼šå¦‚æœå®é™…éœ€è¦çš„å°äºèƒ½å¤Ÿå€Ÿåˆ°çš„æœ€å¤§å€¼ï¼Œåˆ™ä»¥å®é™…éœ€è¦å€¼ä¸ºå‡†èƒ½å›æ”¶çš„å†…å­˜å¤§å°ä¸ºï¼šval spaceToReclaim =storageMemoryPool.freeSpaceToShrinkPool ( math.min(extraMemoryNeeded,memoryReclaimableFromStorage))ExecutionMemoryPool.acquireMemory()è§£æ1234567891011121314151617while (true) &#123; val numActiveTasks = memoryForTask.keys.size val curMem = memoryForTask(taskAttemptId) maybeGrowPool(numBytes - memoryFree) val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks) val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem)) val toGrant = math.min(maxToGrant, memoryFree) if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123; logInfo(s&quot;TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free&quot;) lock.wait() &#125; else &#123; memoryForTask(taskAttemptId) += toGrant return toGrant &#125;&#125;æ•´ä½“æµç¨‹è§£æï¼šç¨‹åºä¸€ç›´å¤„ç†è¯¥taskçš„è¯·æ±‚ï¼Œç›´åˆ°ç³»ç»Ÿåˆ¤å®šæ— æ³•æ»¡è¶³è¯¥è¯·æ±‚æˆ–è€…å·²ç»ä¸ºè¯¥è¯·æ±‚åˆ†é…åˆ°è¶³å¤Ÿçš„å†…å­˜ä¸ºæ­¢ï¼›å¦‚æœå½“å‰executionå†…å­˜æ± å‰©ä½™å†…å­˜ä¸è¶³ä»¥æ»¡è¶³æ­¤æ¬¡è¯·æ±‚æ—¶ï¼Œä¼šå‘storageéƒ¨åˆ†è¯·æ±‚é‡Šæ”¾å‡ºè¢«å€Ÿèµ°çš„å†…å­˜ä»¥æ»¡è¶³æ­¤æ¬¡è¯·æ±‚æ ¹æ®æ­¤åˆ»executionå†…å­˜æ± çš„æ€»å¤§å°maxPoolSizeï¼Œä»¥åŠä»memoryForTaskä¸­ç»Ÿè®¡å‡ºçš„å¤„äºactiveçŠ¶æ€çš„taskçš„ä¸ªæ•°è®¡ç®—å‡ºï¼šæ¯ä¸ªtaskèƒ½å¤Ÿå¾—åˆ°çš„æœ€å¤§å†…å­˜æ•° maxMemoryPerTask = maxPoolSize / numActiveTasksæ¯ä¸ªtaskèƒ½å¤Ÿå¾—åˆ°çš„æœ€å°‘å†…å­˜æ•° minMemoryPerTask = poolSize /(2 * numActiveTasks)æ ¹æ®ç”³è¯·å†…å­˜çš„taskå½“å‰ä½¿ç”¨çš„executionå†…å­˜å¤§å°å†³å®šåˆ†é…ç»™è¯¥taskå¤šå°‘å†…å­˜ï¼Œæ€»çš„å†…å­˜ä¸èƒ½è¶…è¿‡maxMemoryPerTaskï¼›ä½†æ˜¯å¦‚æœexecutionå†…å­˜æ± èƒ½å¤Ÿåˆ†é…çš„æœ€å¤§å†…å­˜å°äºnumBytesï¼Œå¹¶ä¸”å¦‚æœæŠŠèƒ½å¤Ÿåˆ†é…çš„å†…å­˜åˆ†é…ç»™å½“å‰taskï¼Œä½†æ˜¯è¯¥taskæœ€ç»ˆå¾—åˆ°çš„executionå†…å­˜è¿˜æ˜¯å°äºminMemoryPerTaskæ—¶ï¼Œè¯¥taskè¿›å…¥ç­‰å¾…çŠ¶æ€ï¼Œç­‰å…¶ä»–taskç”³è¯·å†…å­˜æ—¶å†å°†å…¶å”¤é†’ï¼Œå”¤é†’ä¹‹åå¦‚æœæ­¤æ—¶æ»¡è¶³ï¼Œå°±ä¼šè¿”å›èƒ½å¤Ÿåˆ†é…çš„å†…å­˜æ•°ï¼Œå¹¶ä¸”æ›´æ–°memoryForTaskï¼Œå°†è¯¥taskä½¿ç”¨çš„å†…å­˜è°ƒæ•´ä¸ºåˆ†é…åçš„å€¼ä¸€ä¸ªTaskæœ€å°‘éœ€è¦minMemoryPerTaskæ‰èƒ½å¼€å§‹æ‰§è¡ŒacquireStorageMemoryæ–¹æ³•æµç¨‹å’ŒacquireExecutionMemoryç±»ä¼¼ï¼Œå½“storageçš„å†…å­˜ä¸è¶³æ—¶ï¼ŒåŒæ ·ä¼šå‘executionå€Ÿå†…å­˜ï¼Œä½†åŒºåˆ«æ˜¯å½“ä¸”ä»…å½“ExecutionMemoryæœ‰ç©ºé—²å†…å­˜æ—¶ï¼ŒStorageMemory æ‰èƒ½å€Ÿèµ°è¯¥å†…å­˜UnifiedMemoryManager.scalaä¸­ä»ä¸Šè¿°ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çŸ¥é“èƒ½å€Ÿåˆ°çš„å†…å­˜æ•°ä¸ºï¼šval memoryBorrowedFromExecution = Math.min(onHeapExecutionMemoryPool.memoryFree,numBytes)æ‰€ä»¥StorageMemoryä»ExecutionMemoryå€Ÿèµ°çš„å†…å­˜ï¼Œå®Œå…¨å–å†³äºå½“æ—¶ExecutionMemoryæ˜¯ä¸æ˜¯æœ‰ç©ºé—²å†…å­˜ï¼›å€Ÿåˆ°å†…å­˜åï¼ŒstorageMemoryPoolå¢åŠ å€Ÿåˆ°çš„è¿™éƒ¨åˆ†å†…å­˜ï¼Œä¹‹ååŒä¸Šä¸€æ ·ï¼Œä¼šè°ƒç”¨StorageMemoryPoolçš„acquireMemory()æ–¹æ³•StorageMemoryPool.acquireMemoryæ•´ä½“æµç¨‹è§£æï¼šåœ¨ç”³è¯·å†…å­˜æ—¶ï¼Œå¦‚æœnumByteså¤§äºæ­¤åˆ»storageå†…å­˜æ± çš„å‰©ä½™å†…å­˜ï¼Œå³if (numBytesToFree &gt; 0)ï¼Œé‚£ä¹ˆéœ€è¦storageå†…å­˜æ± é‡Šæ”¾ä¸€éƒ¨åˆ†å†…å­˜ä»¥æ»¡è¶³ç”³è¯·éœ€æ±‚æ³¨æ„ï¼šè¿™é‡Œçš„numBytesToFreeå¯ä»¥ç†è§£ä¸ºnumByteså¤§å°å‡å»Storageå†…å­˜æ± å‰©ä½™å¤§å°ï¼Œå¤§äº0ï¼Œå³æ‰€éœ€è¦ç”³è¯·çš„numByteså¤§äºStorageå†…å­˜æ± å‰©ä½™çš„å†…å­˜é‡Šæ”¾å†…å­˜åå¦‚æœmemoryFree &gt;= numBytesï¼Œå°±ä¼šæŠŠè¿™éƒ¨åˆ†å†…å­˜åˆ†é…ç»™ç”³è¯·å†…å­˜çš„taskï¼Œå¹¶ä¸”æ›´æ–°storageå†…å­˜æ± çš„ä½¿ç”¨æƒ…å†µåŒæ—¶StorageMemoryPoolä¸ExecutionMemoryPoolä¸åŒçš„æ˜¯ï¼Œä»–ä¸ä¼šåƒå‰è€…é‚£æ ·åˆ†ä¸åˆ°èµ„æºå°±è¿›è¡Œç­‰å¾…ï¼ŒacquireStorageMemoryåªä¼šè¿”å›ä¸€ä¸ªtrueæˆ–æ˜¯falseï¼Œå‘ŠçŸ¥å†…å­˜åˆ†é…æ˜¯å¦æˆåŠŸ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkå†…å­˜ç®¡ç†ä¹‹äºŒ ç»Ÿä¸€å†…å­˜ç®¡ç†åŠè®¾è®¡ç†å¿µ]]></title>
    <url>%2F2019%2F04%2F10%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[å †å†…å†…å­˜Spark 1.6ä¹‹åå¼•å…¥çš„ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œä¸é™æ€å†…å­˜ç®¡ç†çš„åŒºåˆ«åœ¨äºStorageå’ŒExecutionå…±äº«åŒä¸€å—å†…å­˜ç©ºé—´ï¼Œå¯ä»¥åŠ¨æ€å ç”¨å¯¹æ–¹çš„ç©ºé—²åŒºåŸŸå…¶ä¸­æœ€é‡è¦çš„ä¼˜åŒ–åœ¨äºåŠ¨æ€å ç”¨æœºåˆ¶ï¼Œå…¶è§„åˆ™å¦‚ä¸‹ï¼šè®¾å®šåŸºæœ¬çš„Storageå†…å­˜å’ŒExecutionå†…å­˜åŒºåŸŸï¼ˆspark.storage.storageFractionå‚æ•°ï¼‰ï¼Œè¯¥è®¾å®šç¡®å®šäº†åŒæ–¹å„è‡ªæ‹¥æœ‰çš„ç©ºé—´çš„èŒƒå›´åŒæ–¹çš„ç©ºé—´éƒ½ä¸è¶³æ—¶ï¼Œåˆ™å­˜å‚¨åˆ°ç¡¬ç›˜ï¼Œè‹¥å·±æ–¹ç©ºé—´ä¸è¶³è€Œå¯¹æ–¹ç©ºä½™æ—¶ï¼Œå¯å€Ÿç”¨å¯¹æ–¹çš„ç©ºé—´ï¼ˆå­˜å‚¨ç©ºé—´ä¸è¶³æ˜¯æŒ‡ä¸è¶³ä»¥æ”¾ä¸‹ä¸€ä¸ªå®Œæ•´çš„ Blockï¼‰Executionçš„ç©ºé—´è¢«å¯¹æ–¹å ç”¨åï¼Œå¯è®©å¯¹æ–¹å°†å ç”¨çš„éƒ¨åˆ†è½¬å­˜åˆ°ç¡¬ç›˜ï¼Œç„¶åâ€å½’è¿˜â€å€Ÿç”¨çš„ç©ºé—´Storageçš„ç©ºé—´è¢«å¯¹æ–¹å ç”¨åï¼Œæ— æ³•è®©å¯¹æ–¹â€å½’è¿˜â€ï¼Œå› ä¸ºéœ€è¦è€ƒè™‘ Shuffleè¿‡ç¨‹ä¸­çš„å¾ˆå¤šå› ç´ ï¼Œå®ç°èµ·æ¥è¾ƒä¸ºå¤æ‚åŠ¨æ€å†…å­˜å ç”¨æœºåˆ¶åŠ¨æ€å ç”¨æœºåˆ¶å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šå‡­å€Ÿç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶ï¼ŒSpark åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜äº†å †å†…å’Œå †å¤–å†…å­˜èµ„æºçš„åˆ©ç”¨ç‡ï¼Œé™ä½äº†å¼€å‘è€…ç»´æŠ¤ Spark å†…å­˜çš„éš¾åº¦ï¼Œä½†å¹¶ä¸æ„å‘³ç€å¼€å‘è€…å¯ä»¥é«˜æ•æ— å¿§è­¬å¦‚ï¼šå¦‚æœStorageçš„ç©ºé—´å¤ªå¤§æˆ–è€…è¯´ç¼“å­˜çš„æ•°æ®è¿‡å¤šï¼Œåè€Œä¼šå¯¼è‡´é¢‘ç¹çš„å…¨é‡åƒåœ¾å›æ”¶ï¼Œé™ä½ä»»åŠ¡æ‰§è¡Œæ—¶çš„æ€§èƒ½ï¼Œå› ä¸ºç¼“å­˜çš„ RDD æ•°æ®é€šå¸¸éƒ½æ˜¯é•¿æœŸé©»ç•™å†…å­˜çš„ã€‚æ‰€ä»¥è¦æƒ³å……åˆ†å‘æŒ¥ Spark çš„æ€§èƒ½ï¼Œéœ€è¦å¼€å‘è€…è¿›ä¸€æ­¥äº†è§£å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜å„è‡ªçš„ç®¡ç†æ–¹å¼å’Œå®ç°åŸç†å †å¤–å†…å­˜å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç›¸è¾ƒäºé™æ€å†…å­˜ç®¡ç†ï¼Œå¼•å…¥äº†åŠ¨æ€å ç”¨æœºåˆ¶è®¡ç®—å…¬å¼sparkä»1.6ç‰ˆæœ¬ä»¥åï¼Œé»˜è®¤çš„å†…å­˜ç®¡ç†æ–¹å¼å°±è°ƒæ•´ä¸ºç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼ç”±UnifiedMemoryManagerå®ç°Unified MemoryManagementæ¨¡å‹ï¼Œé‡ç‚¹æ˜¯æ‰“ç ´è¿è¡Œå†…å­˜å’Œå­˜å‚¨å†…å­˜ä¹‹é—´çš„ç•Œé™ï¼Œä½¿sparkåœ¨è¿è¡Œæ—¶ï¼Œä¸åŒç”¨é€”çš„å†…å­˜ä¹‹é—´å¯ä»¥å®ç°äº’ç›¸çš„æ‹†å€ŸReserved Memoryè¿™éƒ¨åˆ†å†…å­˜æ˜¯é¢„ç•™ç»™ç³»ç»Ÿä½¿ç”¨,åœ¨1.6.1é»˜è®¤ä¸º300MBï¼Œè¿™ä¸€éƒ¨åˆ†å†…å­˜ä¸è®¡ç®—åœ¨Executionå’ŒStorageä¸­ï¼›å¯é€šè¿‡spark.testing.reservedMemoryè¿›è¡Œè®¾ç½®ï¼›ç„¶åæŠŠå®é™…å¯ç”¨å†…å­˜å‡å»è¿™ä¸ªreservedMemorå¾—åˆ°usableMemoryExecutionMemory å’Œ StorageMemory ä¼šå…±äº«usableMemory * spark.memory.fraction(é»˜è®¤0.75)æ³¨æ„ï¼šåœ¨Spark 1.6.1 ä¸­spark.memory.fractioné»˜è®¤ä¸º0.75åœ¨Spark 2.2.0 ä¸­spark.memory.fractioné»˜è®¤ä¸º0.6User Memoryåˆ†é…Spark Memoryå‰©ä½™çš„å†…å­˜ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨åœ¨Spark 1.6.1ä¸­ï¼Œé»˜è®¤å (Java Heap - Reserved Memory) * 0.25åœ¨Spark 2.2.0ä¸­ï¼Œé»˜è®¤å (Java Heap - Reserved Memory) * 0.4Spark Memoryè®¡ç®—æ–¹å¼ä¸ºï¼š(Java Heap â€“ ReservedMemory) * spark.memory.fractionåœ¨Spark 1.6.1ä¸­ï¼Œé»˜è®¤ä¸º(Java Heap - 300M) * 0.75åœ¨Spark 2.2.0ä¸­ï¼Œé»˜è®¤ä¸º(Java Heap - 300M) * 0.6Spark Memoryåˆåˆ†ä¸ºStorage Memoryå’ŒExecution Memoryä¸¤éƒ¨åˆ†ä¸¤ä¸ªè¾¹ç•Œç”±spark.memory.storageFractionè®¾å®šï¼Œé»˜è®¤ä¸º0.5å¯¹æ¯”ç›¸å¯¹äºé™æ€å†…å­˜æ¨¡å‹ï¼ˆå³Storageå’ŒExecutionç›¸äº’éš”ç¦»ã€å½¼æ­¤ä¸å¯æ‹†å€Ÿï¼‰ï¼ŒåŠ¨æ€å†…å­˜å®ç°äº†å­˜å‚¨å’Œè®¡ç®—å†…å­˜çš„åŠ¨æ€æ‹†å€Ÿï¼šå½“è®¡ç®—å†…å­˜è¶…äº†ï¼Œå®ƒä¼šä»ç©ºé—²çš„å­˜å‚¨å†…å­˜ä¸­å€Ÿä¸€éƒ¨åˆ†å†…å­˜ä½¿ç”¨å­˜å‚¨å†…å­˜ä¸å¤Ÿç”¨çš„æ—¶å€™ï¼Œä¹Ÿä¼šå‘ç©ºé—²çš„è®¡ç®—å†…å­˜ä¸­æ‹†å€Ÿå€¼å¾—æ³¨æ„çš„åœ°æ–¹æ˜¯ï¼šè¢«å€Ÿèµ°ç”¨æ¥æ‰§è¡Œè¿ç®—çš„å†…å­˜ï¼Œåœ¨æ‰§è¡Œå®Œä»»åŠ¡ä¹‹å‰æ˜¯ä¸ä¼šé‡Šæ”¾å†…å­˜çš„é€šä¿—çš„è®²ï¼Œè¿è¡Œä»»åŠ¡ä¼šå€Ÿå­˜å‚¨çš„å†…å­˜ï¼Œä½†æ˜¯å®ƒç›´åˆ°æ‰§è¡Œå®Œä»¥åæ‰èƒ½å½’è¿˜å†…å­˜å’ŒåŠ¨æ€å†…å­˜ç›¸å…³çš„å‚æ•°spark.memory.fraction12345Spark 1.6.1 é»˜è®¤0.75ï¼ŒSpark 2.2.0 é»˜è®¤0.6 è¿™ä¸ªå‚æ•°ç”¨æ¥é…ç½®å­˜å‚¨å’Œè®¡ç®—å†…å­˜å æ•´ä¸ªå¯ç”¨å†…å­˜çš„æ¯”ä¾‹ è¿™ä¸ªå‚æ•°è®¾ç½®çš„è¶Šä½ï¼Œä¹Ÿå°±æ˜¯å­˜å‚¨å’Œè®¡ç®—å†…å­˜å å¯ç”¨çš„æ¯”ä¾‹è¶Šä½ï¼Œå°±è¶Šå¯èƒ½é¢‘ç¹çš„å‘ç”Ÿå†…å­˜çš„é‡Šæ”¾ï¼ˆå°†å†…å­˜ä¸­çš„æ•°æ®å†™ç£ç›˜æˆ–è€…ç›´æ¥ä¸¢å¼ƒæ‰ï¼‰ åä¹‹ï¼Œå¦‚æœè¿™ä¸ªå‚æ•°è¶Šé«˜ï¼Œå‘ç”Ÿé‡Šæ”¾å†…å­˜çš„å¯èƒ½æ€§å°±è¶Šå° è¿™ä¸ªå‚æ•°çš„ç›®çš„æ˜¯åœ¨jvmä¸­ç•™ä¸‹ä¸€éƒ¨åˆ†ç©ºé—´ç”¨æ¥ä¿å­˜sparkå†…éƒ¨æ•°æ®ï¼Œç”¨æˆ·æ•°æ®ç»“æ„ï¼Œå¹¶ä¸”é˜²æ­¢å¯¹æ•°æ®çš„é”™è¯¯é¢„ä¼°å¯èƒ½é€ æˆOOMçš„é£é™©ï¼Œè¿™å°±æ˜¯Otheréƒ¨åˆ†spark.memory.storageFraction1é»˜è®¤ 0.5ï¼›åœ¨ç»Ÿä¸€å†…å­˜ä¸­å­˜å‚¨å†…å­˜æ‰€å çš„æ¯”ä¾‹ï¼Œé»˜è®¤æ˜¯0.5ï¼Œå¦‚æœä½¿ç”¨çš„å­˜å‚¨å†…å­˜è¶…è¿‡äº†è¿™ä¸ªèŒƒå›´ï¼Œç¼“å­˜çš„æ•°æ®ä¼šè¢«é©±èµ¶spark.memory.useLegacyMode123456é»˜è®¤falseï¼›è®¾ç½®æ˜¯å¦ä½¿ç”¨saprk1.5åŠä»¥å‰é—ç•™çš„å†…å­˜ç®¡ç†æ¨¡å‹ï¼Œå³é™æ€å†…å­˜æ¨¡å‹ï¼Œå‰é¢çš„æ–‡ç« ä»‹ç»è¿‡è¿™ä¸ªï¼Œä¸»è¦æ˜¯è®¾ç½®ä»¥ä¸‹å‡ ä¸ªå‚æ•°ï¼š spark.storage.memoryFraction spark.storage.safetyFraction spark.storage.unrollFraction spark.shuffle.memoryFraction spark.shuffle.safetyFractionåŠ¨æ€å†…å­˜è®¾è®¡ä¸­çš„å–èˆå› ä¸ºå†…å­˜å¯ä»¥è¢«Executionå’ŒStorageæ‹†å€Ÿï¼Œæˆ‘ä»¬å¿…é¡»æ˜ç¡®åœ¨è¿™ç§æœºåˆ¶ä¸‹ï¼Œå½“å†…å­˜å‹åŠ›ä¸Šå‡çš„æ—¶å€™ï¼Œè¯¥å¦‚ä½•è¿›è¡Œå–èˆï¼Ÿä»ä¸‰ä¸ªè§’åº¦è¿›è¡Œåˆ†æï¼šå€¾å‘äºä¼˜å…ˆé‡Šæ”¾è®¡ç®—å†…å­˜å€¾å‘äºä¼˜å…ˆé‡Šæ”¾å­˜å‚¨å†…å­˜ä¸åä¸å€šï¼Œå¹³ç­‰ç«äº‰é‡Šæ”¾å†…å­˜çš„ä»£ä»·é‡Šæ”¾å­˜å‚¨å†…å­˜çš„ä»£ä»·å–å†³äºStorage Level.ï¼šå¦‚æœæ•°æ®çš„å­˜å‚¨levelæ˜¯MEMORY_ONLYçš„è¯ä»£ä»·æœ€é«˜ï¼Œå› ä¸ºå½“ä½ é‡Šæ”¾åœ¨å†…å­˜ä¸­çš„æ•°æ®çš„æ—¶å€™ï¼Œä½ ä¸‹æ¬¡å†å¤ç”¨çš„è¯åªèƒ½é‡æ–°è®¡ç®—äº†å¦‚æœæ•°æ®çš„å­˜å‚¨levelæ˜¯MEMORY_AND_DIS_SERçš„æ—¶å€™ï¼Œé‡Šæ”¾å†…å­˜çš„ä»£ä»·æœ€ä½ï¼Œå› ä¸ºè¿™ç§æ–¹å¼ï¼Œå½“å†…å­˜ä¸å¤Ÿçš„æ—¶å€™ï¼Œå®ƒä¼šå°†æ•°æ®åºåˆ—åŒ–åæ”¾åœ¨ç£ç›˜ä¸Šï¼Œé¿å…å¤ç”¨çš„æ—¶å€™å†è®¡ç®—ï¼Œå”¯ä¸€çš„å¼€é”€åœ¨I/Oç»¼è¿°ï¼šé‡Šæ”¾è®¡ç®—å†…å­˜çš„ä»£ä»·ä¸æ˜¯å¾ˆæ˜¾è€Œæ˜“è§ï¼šè¿™é‡Œæ²¡æœ‰å¤ç”¨æ•°æ®é‡è®¡ç®—çš„ä»£ä»·ï¼Œå› ä¸ºè®¡ç®—å†…å­˜ä¸­çš„ä»»åŠ¡æ•°æ®ä¼šè¢«ç§»åˆ°ç¡¬ç›˜ï¼Œæœ€åå†å½’å¹¶èµ·æ¥ï¼ˆåé¢ä¼šæœ‰æ–‡ç« ä»‹ç»åˆ°è¿™ç‚¹ï¼‰æœ€è¿‘çš„sparkç‰ˆæœ¬å°†è®¡ç®—çš„ä¸­é—´æ•°æ®è¿›è¡Œå‹ç¼©ä½¿å¾—åºåˆ—åŒ–çš„ä»£ä»·é™åˆ°äº†æœ€ä½å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šç§»åˆ°ç¡¬ç›˜çš„æ•°æ®æ€»ä¼šå†é‡æ–°è¯»å›æ¥ä»å­˜å‚¨å†…å­˜ç§»é™¤çš„æ•°æ®ä¹Ÿè®¸ä¸ä¼šè¢«ç”¨åˆ°ï¼Œæ‰€ä»¥å½“æ²¡æœ‰é‡æ–°è®¡ç®—çš„é£é™©æ—¶ï¼Œé‡Šæ”¾è®¡ç®—çš„å†…å­˜è¦æ¯”é‡Šæ”¾å­˜å‚¨å†…å­˜çš„ä»£ä»·æ›´é«˜ï¼ˆå‡ä½¿è®¡ç®—å†…å­˜éƒ¨åˆ†åˆšå¥½ç”¨äºè®¡ç®—ä»»åŠ¡çš„æ—¶å€™ï¼‰å®ç°å¤æ‚åº¦å®ç°é‡Šæ”¾å­˜å‚¨å†…å­˜çš„ç­–ç•¥å¾ˆç®€å•ï¼šæˆ‘ä»¬åªéœ€è¦ç”¨ç›®å‰çš„å†…å­˜é‡Šæ”¾ç­–ç•¥é‡Šæ”¾æ‰å­˜å‚¨å†…å­˜ä¸­çš„æ•°æ®å°±å¥½äº†å®ç°é‡Šæ”¾è®¡ç®—å†…å­˜å´ç›¸å¯¹æ¥è¯´å¾ˆå¤æ‚è¿™é‡Œæœ‰2ä¸ªé‡Šæ”¾è®¡ç®—å†…å­˜çš„æ€è·¯ï¼šå½“è¿è¡Œä»»åŠ¡è¦æ‹†å€Ÿå­˜å‚¨å†…å­˜çš„æ—¶å€™ï¼Œç»™æ‰€æœ‰è¿™äº›ä»»åŠ¡æ³¨å†Œä¸€ä¸ªå›è°ƒå‡½æ•°ä»¥ä¾¿æ—¥åè°ƒè¿™ä¸ªå‡½æ•°æ¥å›æ”¶å†…å­˜ååŒæŠ•ç¥¨æ¥è¿›è¡Œå†…å­˜çš„é‡Šæ”¾å€¼å¾—æˆ‘ä»¬æ³¨æ„çš„ä¸€ä¸ªåœ°æ–¹æ˜¯ï¼Œä»¥ä¸Šæ— è®ºå“ªç§æ–¹å¼ï¼Œéƒ½éœ€è¦è€ƒè™‘ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼šå³å¦‚æœæˆ‘è¦é‡Šæ”¾æ­£åœ¨è¿è¡Œçš„è®¡ç®—ä»»åŠ¡çš„å†…å­˜ï¼ŒåŒæ—¶æˆ‘ä»¬æƒ³è¦cacheåˆ°å­˜å‚¨å†…å­˜çš„ä¸€éƒ¨åˆ†æ•°æ®æ°å·§æ˜¯ç”±è¿™ä¸ªè®¡ç®—ä»»åŠ¡äº§ç”Ÿçš„æ­¤æ—¶ï¼Œå¦‚æœæˆ‘ä»¬ç°åœ¨é‡Šæ”¾æ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡çš„è®¡ç®—å†…å­˜ï¼Œå°±éœ€è¦è€ƒè™‘åœ¨è¿™ç§ç¯å¢ƒä¸‹ä¼šé€ æˆçš„é¥¥é¥¿æƒ…å†µï¼šå³ç”Ÿæˆcacheçš„æ•°æ®çš„è®¡ç®—ä»»åŠ¡æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ç©ºé—´æ¥è·‘å‡ºcacheçš„æ•°æ®ï¼Œè€Œä¸€ç›´å¤„äºé¥¥é¥¿çŠ¶æ€ï¼ˆå› ä¸ºè®¡ç®—å†…å­˜å·²ç»ä¸å¤Ÿäº†ï¼Œå†é‡Šæ”¾è®¡ç®—å†…å­˜æ›´åŠ ä¸å¯å–ï¼‰æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ï¼šä¸€æ—¦æˆ‘ä»¬é‡Šæ”¾æ‰è®¡ç®—å†…å­˜ï¼Œé‚£ä¹ˆé‚£äº›éœ€è¦cacheçš„æ•°æ®åº”è¯¥æ€ä¹ˆåŠï¼Ÿæœ‰2ç§æ–¹æ¡ˆï¼šæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ç­‰å¾…ï¼Œç›´åˆ°è®¡ç®—å†…å­˜æœ‰è¶³å¤Ÿçš„ç©ºé—²ï¼Œä½†æ˜¯è¿™æ ·å°±å¯èƒ½ä¼šé€ æˆæ­»é”ï¼Œå°¤å…¶æ˜¯å½“æ–°çš„æ•°æ®å—ä¾èµ–äºä¹‹å‰çš„è®¡ç®—å†…å­˜ä¸­çš„æ•°æ®å—çš„æ—¶å€™å¦ä¸€ä¸ªå¯é€‰çš„æ“ä½œå°±æ˜¯ä¸¢æ‰é‚£äº›æœ€æ–°çš„æ­£å‡†å¤‡å†™å…¥åˆ°ç£ç›˜ä¸­çš„å—å¹¶ä¸”ä¸€æ—¦å½“è®¡ç®—å†…å­˜å¤Ÿäº†åˆé©¬ä¸ŠåŠ è½½å›æ¥ã€‚ä¸ºäº†é¿å…æ€»æ˜¯ä¸¢æ‰é‚£äº›ç­‰å¾…ä¸­çš„å—ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªå°çš„å†…å­˜ç©ºé—´(æ¯”å¦‚å †å†…å­˜çš„5%)å»ç¡®ä¿å†…å­˜ä¸­è‡³å°‘æœ‰ä¸€å®šçš„æ¯”ä¾‹çš„çš„æ•°æ®å—ç»¼è¿°ï¼šæ‰€ç»™çš„ä¸¤ç§æ–¹æ³•éƒ½ä¼šå¢åŠ é¢å¤–çš„å¤æ‚åº¦ï¼Œè¿™ä¸¤ç§æ–¹å¼åœ¨ç¬¬ä¸€æ¬¡çš„å®ç°ä¸­éƒ½è¢«æ’é™¤äº†ç»¼ä¸Šç›®å‰çœ‹æ¥ï¼Œé‡Šæ”¾æ‰å­˜å‚¨å†…å­˜ä¸­çš„è®¡ç®—ä»»åŠ¡åœ¨å®ç°ä¸Šæ¯”è¾ƒç¹çï¼Œç›®å‰æš‚ä¸è€ƒè™‘å³è®¡ç®—å†…å­˜å€Ÿäº†å­˜å‚¨å†…å­˜ç”¨æ¥è®¡ç®—ä»»åŠ¡ï¼Œç„¶åé‡Šæ”¾ï¼Œè¿™ç§ä¸è€ƒè™‘ï¼›è®¡ç®—å†…å­˜å€Ÿæ¥å†…å­˜ä¹‹åï¼Œæ˜¯å¯ä»¥ä¸è¿˜çš„ç»“è®ºï¼šæˆ‘ä»¬å€¾å‘äºä¼˜å…ˆé‡Šæ”¾æ‰å­˜å‚¨å†…å­˜å³å¦‚æœå­˜å‚¨å†…å­˜æ‹†å€Ÿäº†è®¡ç®—å†…å­˜ï¼Œå½“è®¡ç®—å†…å­˜éœ€è¦è¿›è¡Œè®¡ç®—å¹¶ä¸”å†…å­˜ç©ºé—´ä¸è¶³çš„æ—¶å€™ï¼Œä¼˜å…ˆæŠŠè®¡ç®—å†…å­˜ä¸­è¿™éƒ¨åˆ†è¢«ç”¨æ¥å­˜å‚¨çš„å†…å­˜é‡Šæ”¾æ‰å¯é€‰è®¾è®¡1.è®¾è®¡æ–¹æ¡ˆç»“åˆæˆ‘ä»¬å‰é¢çš„æè¿°ï¼Œé’ˆå¯¹åœ¨å†…å­˜å‹åŠ›ä¸‹é‡Šæ”¾å­˜å‚¨å†…å­˜æœ‰ä»¥ä¸‹å‡ ä¸ªå¯é€‰è®¾è®¡ï¼šè®¾è®¡1ï¼šé‡Šæ”¾å­˜å‚¨å†…å­˜æ•°æ®å—ï¼Œå®Œå…¨å¹³æ»‘è®¡ç®—å’Œå­˜å‚¨å†…å­˜å…±äº«ä¸€ç‰‡ç»Ÿä¸€çš„åŒºåŸŸï¼Œæ²¡æœ‰è¿›è¡Œç»Ÿä¸€çš„åˆ’åˆ†å†…å­˜å‹åŠ›ä¸Šå‡ï¼Œä¼˜å…ˆé‡Šæ”¾æ‰å­˜å‚¨å†…å­˜éƒ¨åˆ†ä¸­çš„æ•°æ®å¦‚æœå‹åŠ›æ²¡æœ‰ç¼“è§£ï¼Œå¼€å§‹å°†è®¡ç®—å†…å­˜ä¸­è¿è¡Œçš„ä»»åŠ¡æ•°æ®è¿›è¡Œæº¢å†™ç£ç›˜è®¾è®¡2ï¼šé‡Šæ”¾å­˜å‚¨å†…å­˜æ•°æ®å—ï¼Œé™æ€å­˜å‚¨ç©ºé—´é¢„ç•™ï¼Œå­˜å‚¨ç©ºé—´çš„å¤§å°æ˜¯å®šæ­»çš„è¿™ç§è®¾è®¡å’Œ1è®¾è®¡å¾ˆåƒï¼Œä¸åŒçš„æ˜¯ä¼šä¸“é—¨åˆ’åˆ†ä¸€ä¸ªé¢„ç•™å­˜å‚¨å†…å­˜åŒºåŸŸï¼šåœ¨è¿™ä¸ªå†…å­˜åŒºåŸŸå†…ï¼Œå­˜å‚¨å†…å­˜ä¸ä¼šè¢«é‡Šæ”¾ï¼Œåªæœ‰å½“å­˜å‚¨å†…å­˜è¶…å‡ºè¿™ä¸ªé¢„ç•™åŒºåŸŸï¼Œæ‰ä¼šè¢«é‡Šæ”¾ï¼ˆå³è¶…è¿‡50%äº†å°±è¢«é‡Šæ”¾ï¼Œå½“ç„¶50%ä¸ºé»˜è®¤å€¼ï¼‰ã€‚è¿™ä¸ªå‚æ•°ç”±spark.memory.storageFractionï¼ˆé»˜è®¤å€¼ä¸º0.5ï¼Œå³è®¡ç®—å’Œå­˜å‚¨å†…å­˜çš„åˆ†å‰²çº¿ï¼‰é…ç½®è®¾è®¡3ï¼šé‡Šæ”¾å­˜å‚¨å†…å­˜æ•°æ®å—ï¼ŒåŠ¨æ€å­˜å‚¨ç©ºé—´é¢„ç•™è¿™ç§è®¾è®¡äºè®¾è®¡2å¾ˆç›¸ä¼¼ï¼Œä½†æ˜¯å­˜å‚¨ç©ºé—´çš„é‚£ä¸€éƒ¨åˆ†åŒºåŸŸä¸å†æ˜¯é™æ€è®¾ç½®çš„äº†ï¼Œè€Œæ˜¯åŠ¨æ€åˆ†é…ï¼›è¿™æ ·è®¾ç½®å¸¦æ¥çš„ä¸åŒæ˜¯è®¡ç®—å†…å­˜å¯ä»¥å°½å¯èƒ½å€Ÿèµ°å­˜å‚¨å†…å­˜ä¸­å¯ç”¨çš„éƒ¨åˆ†ï¼Œå› ä¸ºå­˜å‚¨å†…å­˜æ˜¯åŠ¨æ€åˆ†é…çš„ç»“è®ºï¼šæœ€ç»ˆé‡‡ç”¨çš„çš„æ˜¯è®¾è®¡32.å„ä¸ªæ–¹æ¡ˆçš„ä¼˜åŠ£è®¾è®¡1è¢«æ‹’ç»çš„åŸå› è®¾è®¡1ä¸é€‚åˆé‚£äº›å¯¹cacheå†…å­˜é‡åº¦ä¾èµ–çš„saprkä»»åŠ¡ï¼Œå› ä¸ºè®¾è®¡1ä¸­åªè¦å†…å­˜å‹åŠ›ä¸Šå‡å°±é‡Šæ”¾å­˜å‚¨å†…å­˜è®¾è®¡2è¢«æ‹’ç»çš„åŸå› è®¾è®¡2åœ¨å¾ˆå¤šæƒ…å†µä¸‹éœ€è¦ç”¨æˆ·å»è®¾ç½®å­˜å‚¨å†…å­˜ä¸­é‚£éƒ¨åˆ†æœ€å°çš„åŒºåŸŸå¦å¤–æ— è®ºæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå…·ä½“å€¼ï¼Œåªè¦å®ƒé0ï¼Œé‚£ä¹ˆè®¡ç®—å†…å­˜æœ€ç»ˆä¹Ÿä¼šè¾¾åˆ°ä¸€ä¸ªä¸Šé™ï¼Œæ¯”å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°†å­˜å‚¨å†…å­˜è®¾ç½®ä¸º0.6ï¼Œé‚£ä¹ˆæœ‰æ•ˆçš„æ‰§è¡Œå†…å­˜å°±æ˜¯ï¼šSpark 1.6.1 å¯ç”¨å†…å­˜0.40.75Spark 2.2.0 å¯ç”¨å†…å­˜0.40.6é‚£ä¹ˆå¦‚æœç”¨æˆ·æ²¡æœ‰cacheæ•°æ®ï¼Œæˆ–æ˜¯cacheçš„æ•°æ®è¾¾ä¸åˆ°è®¾ç½®çš„0.6ï¼Œé‚£ä¹ˆè¿™ç§æƒ…å†µå°±åˆå›åˆ°äº†é™æ€å†…å­˜æ¨¡å‹é‚£ç§æƒ…å†µï¼Œå¹¶æ²¡æœ‰æ”¹å–„ä»€ä¹ˆæœ€ç»ˆé€‰æ‹©è®¾è®¡3çš„åŸå› è®¾è®¡3å°±é¿å…äº†2ä¸­çš„é—®é¢˜åªè¦å­˜å‚¨å†…å­˜æœ‰ç©ºä½™çš„æƒ…å†µï¼Œé‚£ä¹ˆè®¡ç®—å†…å­˜å°±å¯ä»¥å€Ÿç”¨éœ€è¦å…³æ³¨çš„é—®é¢˜æ˜¯ï¼šå½“è®¡ç®—å†…å­˜å·²ç»ä½¿ç”¨äº†å­˜å‚¨å†…å­˜ä¸­çš„æ‰€æœ‰å¯ç”¨å†…å­˜ä½†æ˜¯åˆéœ€è¦cacheæ•°æ®çš„æ—¶å€™åº”è¯¥æ€ä¹ˆå¤„ç†æœ€æ—©çš„ç‰ˆæœ¬ä¸­ç›´æ¥é‡Šæ”¾æœ€æ–°çš„blockæ¥é¿å…å¼•å…¥æ‰§è¡Œé©±èµ¶ç­–ç•¥ï¼ˆevictionç­–ç•¥ï¼Œä¸Šè¿°ç« èŠ‚ä¸­æœ‰ä»‹ç»ï¼‰çš„å¤æ‚æ€§è®¾è®¡3æ˜¯å”¯ä¸€ä¸€ä¸ªåŒæ—¶æ»¡è¶³ä¸‹åˆ—æ¡ä»¶çš„ï¼šå­˜å‚¨å†…å­˜æ²¡æœ‰ä¸Šé™è®¡ç®—å†…å­˜æ²¡æœ‰ä¸Šé™ä¿éšœäº†å­˜å‚¨ç©ºé—´æœ‰ä¸€ä¸ªå°çš„ä¿ç•™åŒºåŸŸ]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019æ¸…æ˜-çº¿ä¸‹é¡¹ç›®ç¬¬12æœŸåœ†æ»¡ç»“æŸ]]></title>
    <url>%2F2019%2F04%2F09%2F2019%E6%B8%85%E6%98%8E-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC12%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019å¹´æ¸…æ˜3å¤©ä¸€å¥è¯ï¼Œä¸Šæµ·æ¸©åº¦é€‚å®œå°ä¼™ä¼´ä»¬æ¥è‡ªäº”æ¹–å››æµ·åŒ—äº¬ã€æˆéƒ½ã€æ·±åœ³ã€å¹¿å·æ­å·ã€å±±ä¸œã€é½é½å“ˆå°”å¤§å®¶ä¸ºäº†ä¸€ä¸ªçœŸå®ç›®æ ‡å­¦ä¹ çœŸæ­£ä¼ä¸šçº§å¤§æ•°æ®ç”Ÿäº§é¡¹ç›®2ä¸ªç”Ÿäº§é¡¹ç›®+3ä¸ªTopicåˆ†äº«ä¸€å¹´æˆ‘ä»¬åªåœ¨èŠ‚å‡æ—¥ä¸¾åŠæ¸…æ˜3å¤©+2å¤œï¼Œé”™è¿‡äº†å°±æ˜¯é”™è¿‡äº†æœŸå¾…@ç«¯åˆèŠ‚çº¿ä¸‹é¡¹ç›®ç­ç¬¬13æœŸ]]></content>
      <categories>
        <category>çº¿ä¸‹å®æˆ˜ç­</category>
      </categories>
      <tags>
        <tag>çº¿ä¸‹å®æˆ˜ç­</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkå†…å­˜ç®¡ç†ä¹‹ä¸€ é™æ€å†…å­˜ç®¡ç†]]></title>
    <url>%2F2019%2F04%2F03%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Sparkå†…å­˜ç®¡ç†ç®€ä»‹Sparkä»1.6å¼€å§‹å¼•å…¥äº†åŠ¨æ€å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œå³æ‰§è¡Œå†…å­˜å’Œå­˜å‚¨å†…å­˜ä¹‹é—´å¯ä»¥ç›¸äº’æŠ¢å Sparkæä¾›äº†2ç§å†…å­˜åˆ†é…æ¨¡å¼ï¼šé™æ€å†…å­˜ç®¡ç†ç»Ÿä¸€å†…å­˜ç®¡ç†æœ¬ç³»åˆ—æ–‡ç« å°†åˆ†åˆ«å¯¹è¿™ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼çš„ä¼˜ç¼ºç‚¹ä»¥åŠè®¾è®¡åŸç†è¿›è¡Œåˆ†æï¼ˆä¸»è¦åŸºäºSpark 1.6.1çš„å†…å­˜ç®¡ç†è¿›è¡Œåˆ†æï¼‰åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œå°†å…ˆå¯¹é™æ€å†…å­˜ç®¡ç†è¿›è¡Œä»‹ç»å †å†…å†…å­˜åœ¨Sparkæœ€åˆé‡‡ç”¨çš„é™æ€å†…å­˜ç®¡ç†æœºåˆ¶ä¸‹ï¼Œå­˜å‚¨å†…å­˜ã€æ‰§è¡Œå†…å­˜å’Œå…¶å®ƒå†…å­˜çš„å¤§å°åœ¨Sparkåº”ç”¨ç¨‹åºè¿è¡ŒæœŸé—´å‡ä¸ºå›ºå®šçš„ï¼Œä½†ç”¨æˆ·å¯ä»¥åœ¨åº”ç”¨ç¨‹åºå¯åŠ¨å‰è¿›è¡Œé…ç½®ï¼Œå †å†…å†…å­˜çš„åˆ†é…å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œsparkå†…å­˜ç®¡ç†é‡‡ç”¨unifiedæ¨¡å¼ï¼Œå¦‚æœè¦å¼€å¯é™æ€å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œéœ€è¦å°†spark.memory.useLegacyModeå‚æ•°è°ƒä¸ºtrueï¼ˆé»˜è®¤ä¸ºfalseï¼‰ï¼Œ1.6.1ç‰ˆæœ¬çš„å®˜ç½‘é…ç½®å¦‚ä¸‹æ‰€ç¤ºï¼šå°†å‚æ•°è°ƒæ•´ä¸ºtrueä¹‹åï¼Œå°±ä¼šè¿›å…¥åˆ°é™æ€å†…å­˜ç®¡ç†ä¸­æ¥ï¼Œå¯ä»¥é€šè¿‡SparkEnv.scalaä¸­å‘ç°ï¼š123å¦‚æœspark.memory.useLegacyModeä¸ºtrueï¼Œå°±è¿›å…¥åˆ°StaticMemoryManagerï¼ˆé™æ€å†…å­˜ç®¡ç†ï¼‰ï¼›å¦‚æœä¸ºfalseï¼Œå°±è¿›å…¥åˆ°UnifiedMemoryManagerï¼ˆç»Ÿä¸€å†…å­˜ç®¡ç†ï¼‰ï¼›åŒæ—¶æˆ‘ä»¬å¯ä»¥å‘ç°è¯¥å‚æ•°çš„é»˜è®¤å€¼ä¸ºfalseï¼Œå³é»˜è®¤æƒ…å†µä¸‹å°±ä¼šè°ƒç”¨ç»Ÿä¸€å†…å­˜ç®¡ç†ç±»ã€‚Executionå†…å­˜####å¯ç”¨çš„Executionå†…å­˜ç”¨äºshuffleæ“ä½œçš„å†…å­˜ï¼Œå–å†³äºjoinã€sortã€aggregationç­‰è¿‡ç¨‹é¢‘ç¹çš„IOéœ€è¦çš„Bufferä¸´æ—¶æ•°æ®å­˜å‚¨ç®€å•æ¥è¯´ï¼Œsparkåœ¨shuffle writeçš„è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªexecutorä¼šå°†æ•°æ®å†™åˆ°è¯¥executorçš„ç‰©ç†ç£ç›˜ä¸Šï¼Œä¸‹ä¸€ä¸ªstageçš„taskä¼šå»ä¸Šä¸€ä¸ªstageæ‹‰å–å…¶æ‰€éœ€è¦å¤„ç†çš„æ•°æ®ï¼Œå¹¶ä¸”æ˜¯è¾¹æ‹‰å–è¾¹è¿›è¡Œå¤„ç†çš„ï¼ˆå’ŒMapReduceçš„æ‹‰å–åˆå¹¶æ•°æ®åŸºæœ¬ä¸€æ ·ï¼‰ï¼Œè¿™ä¸ªæ—¶å€™å°±ä¼šç”¨åˆ°ä¸€ä¸ªaggregateçš„æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚hashmapè¿™ç§è¾¹æ‹‰å–æ•°æ®è¾¹è¿›è¡Œèšåˆã€‚è¿™éƒ¨åˆ†å†…å­˜å°±è¢«ç§°ä¸ºexecutionå†…å­˜ä»StaticMemoryManager.scalaä¸­çš„getMaxExecutionMemoryæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼šæ¯ä¸ªexecutoråˆ†é…ç»™executionçš„å†…å­˜ä¸ºï¼š123ExecutionMemory = systemMaxMemory * memoryFraction * safetyFraction é»˜è®¤æƒ…å†µä¸‹ä¸ºï¼šsystemMaxMemory * 0.2 * 0.8 = 0.16 * systemMaxMemory å³é»˜è®¤ä¸ºexecutoræœ€å¤§å¯ç”¨å†…å­˜ * 0.16Executionå†…å­˜å†è¿è¡Œçš„æ—¶å€™ä¼šè¢«åˆ†é…ç»™è¿è¡Œåœ¨JVMä¸Šçš„taskï¼›è¿™é‡Œä¸åŒçš„æ˜¯ï¼Œåˆ†é…ç»™æ¯ä¸ªtaskçš„å†…å­˜å¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯åŠ¨æ€çš„ï¼›sparkä¸æ˜¯ä¸€ä¸Šæ¥å°±åˆ†é…å›ºå®šå¤§å°çš„å†…å­˜å—ç»™taskï¼Œè€Œæ˜¯å…è®¸ä¸€ä¸ªtaskå æ®JVMæ‰€æœ‰executionå†…å­˜æ¯ä¸ªJVMä¸Šçš„taskå¯ä»¥æœ€å¤šç”³è¯·è‡³å¤š1/Nçš„executionå†…å­˜ï¼Œå…¶ä¸­Nä¸ºactive taskçš„ä¸ªæ•°ï¼Œç”±spark.executor.coresæŒ‡å®šï¼›å¦‚æœtaskçš„ç”³è¯·æ²¡æœ‰è¢«æ‰¹å‡†ï¼Œå®ƒä¼šé‡Šæ”¾ä¸€éƒ¨åˆ†å†…å­˜ï¼Œå¹¶ä¸”ä¸‹æ¬¡ç”³è¯·çš„æ—¶å€™ï¼Œå®ƒä¼šç”³è¯·æ›´å°çš„ä¸€éƒ¨åˆ†å†…å­˜æ³¨ï¼šæ¯ä¸ªExecutorå•ç‹¬è¿è¡Œåœ¨ä¸€ä¸ªJVMè¿›ç¨‹ä¸­ï¼Œæ¯ä¸ªTaskåˆ™æ˜¯è¿è¡Œåœ¨Executorä¸­çš„çº¿ç¨‹spark.executor.coresè®¾ç½®çš„æ˜¯æ¯ä¸ªexecutorçš„coreæ•°é‡taskçš„æ•°é‡å°±æ˜¯partitionçš„æ•°é‡ä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªcoreè®¾ç½®2~4ä¸ªpartitionæ³¨æ„ï¼š ä¸ºäº†é˜²æ­¢è¿‡å¤šçš„spillingæ•°æ®ï¼Œåªæœ‰å½“ä¸€ä¸ªtaskåˆ†é…åˆ°çš„å†…å­˜è¾¾åˆ°executionå†…å­˜1/2Nçš„æ—¶å€™æ‰ä¼šspillï¼Œå¦‚æœç›®å‰ç©ºé—²çš„å†…å­˜è¾¾ä¸åˆ°1/2Nçš„æ—¶å€™ï¼Œå†…å­˜ç”³è¯·ä¼šè¢«é˜»å¡ç›´åˆ°å…¶å®ƒçš„task spillæ‰å®ƒä»¬çš„å†…å­˜ï¼›å¦‚æœä¸è¿™æ ·é™åˆ¶ï¼Œå‡è®¾å½“å‰ä¸€ä¸ªä»»åŠ¡å æ®äº†ç»å¤§éƒ¨åˆ†å†…å­˜ï¼Œé‚£ä¹ˆæ–°æ¥çš„taskä¼šä¸€ç›´å¾€ç¡¬ç›˜spillæ•°æ®ï¼Œè¿™æ ·å°±ä¼šå¯¼è‡´æ¯”è¾ƒä¸¥é‡çš„I/Oé—®é¢˜ï¼›è€Œæˆ‘ä»¬åšäº†ä¸€å®šç¨‹åº¦çš„é™åˆ¶ï¼Œä¼šè¿›è¡Œä¸€å®šç¨‹åº¦çš„é˜»å¡ç­‰å¾…ï¼Œå¯¹äºé¢‘ç¹çš„å°æ•°æ®é›†çš„I/Oä¼šæœ‰ä¸€å®šçš„å‡ç¼“ä¾‹å­ï¼šæŸexecutorå…ˆå¯åŠ¨ä¸€ä¸ªtask Aï¼Œå¹¶åœ¨task Bå¯åŠ¨å‰å¿«é€Ÿå ç”¨äº†æ‰€æœ‰å¯ç”¨çš„å†…å­˜ï¼›åœ¨Bå¯ç”¨ä¹‹åNå˜æˆäº†2ï¼Œtask Bä¼šé˜»å¡ç›´åˆ°task A spillï¼Œè‡ªå·±å¯ä»¥è·å¾—1/2N=1/4çš„executionå†…å­˜çš„æ—¶å€™ï¼›è€Œä¸€å¤§task Bè·å–åˆ°äº†1/4çš„å†…å­˜ï¼ŒAå’ŒBå°±éƒ½æœ‰å¯èƒ½spilläº†é¢„ç•™å†…å­˜Sparkä¹‹æ‰€ä»¥ä¼šæœ‰ä¸€ä¸ªSafetyFractionè¿™æ ·çš„å‚æ•°ï¼Œæ˜¯ä¸ºäº†é¿å…æ½œåœ¨çš„OOMã€‚ä¾‹å¦‚ï¼Œè¿›è¡Œè®¡ç®—æ—¶ï¼Œæœ‰ä¸€ä¸ªæå‰æœªé¢„æ–™åˆ°çš„æ¯”è¾ƒå¤§çš„æ•°æ®ï¼Œä¼šå¯¼è‡´è®¡ç®—æ—¶é—´å»¶é•¿ç”šè‡³OOMï¼ŒsafetyFractionä¸ºstorageå’Œexecutionéƒ½æä¾›äº†é¢å¤–çš„bufferä»¥é˜²æ­¢æ­¤ç±»çš„æ•°æ®å€¾æ–œï¼›è¿™éƒ¨åˆ†å†…å­˜å«ä½œé¢„ç•™å†…å­˜####Storageå†…å­˜####å¯ç”¨çš„Storageå†…å­˜è¯¥éƒ¨åˆ†å†…å­˜ç”¨ä½œå¯¹RDDçš„ç¼“å­˜ï¼ˆå¦‚è°ƒç”¨cacheã€persistç­‰æ–¹æ³•ï¼‰ï¼ŒèŠ‚ç‚¹é—´ä¼ è¾“çš„å¹¿æ’­å˜é‡StaticMemoryManager.scalaä¸­çš„getMaxStorageMemoryæ–¹æ³•å‘ç°ï¼šæœ€åä¸ºæ¯ä¸ªexecutoråˆ†é…åˆ°çš„storageçš„å†…å­˜ï¼š123StorageMemory = systemMaxMemory * memoryFraction * safetyFraction é»˜è®¤æƒ…å†µä¸‹ä¸ºï¼šsystemMaxMemory * 0.6 * 0.9 = 0.54 * systemMaxMemory å³é»˜è®¤åˆ†é…executoræœ€å¤§å¯ç”¨å†…å­˜çš„0.54é¢„ç•™å†…å­˜åŒExecutionå†…å­˜ä¸­çš„é¢„ç•™éƒ¨åˆ†UnrollUnrollæ˜¯storageä¸­æ¯”è¾ƒç‰¹æ®Šçš„ä¸€éƒ¨åˆ†ï¼Œå®ƒé»˜è®¤å æ®storageæ€»å†…å­˜çš„20%BlockManageræ˜¯sparkè‡ªå·±å®ç°çš„å†…éƒ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ŒBlockManageræ¥å—æ•°æ®ï¼ˆå¯èƒ½ä»æœ¬åœ°æˆ–è€…å…¶ä»–èŠ‚ç‚¹ï¼‰çš„æ—¶å€™æ˜¯ä»¥iteratorçš„å½¢å¼ï¼Œå¹¶ä¸”è¿™äº›æ•°æ®æ˜¯æœ‰åºåˆ—åŒ–å’Œéåºåˆ—åŒ–çš„ï¼Œå› æ­¤éœ€è¦æ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼šIteratoråœ¨ç‰©ç†å†…å­˜ä¸Šæ˜¯ä¸è¿ç»­çš„ï¼Œå¦‚æœåç»­sparkè¦æŠŠæ•°æ®è£…è½½è¿›å†…å­˜çš„è¯ï¼Œå°±éœ€è¦æŠŠè¿™äº›æ•°æ®æ”¾è¿›ä¸€ä¸ªarrayï¼ˆç‰©ç†ä¸Šè¿ç»­ï¼‰å¦å¤–ï¼Œåºåˆ—åŒ–æ•°æ®éœ€è¦è¿›è¡Œå±•å¼€ï¼Œå¦‚æœç›´æ¥å±•å¼€åºåˆ—åŒ–çš„æ•°æ®ï¼Œä¼šé€ æˆOOMï¼Œæ‰€ä»¥BlockManagerä¼šé€æ¸çš„å±•å¼€è¿™ä¸ªiteratorï¼Œå¹¶é€æ¸æ£€æŸ¥å†…å­˜é‡Œæ˜¯å¦è¿˜æœ‰è¶³å¤Ÿçš„ç©ºé—´ç”¨æ¥å±•å¼€æ•°æ®æ”¾è¿›arrayé‡ŒStaticMemoryManager.scalaä¸­çš„maxUnrollMemoryæ–¹æ³•ï¼šUnrollçš„ä¼˜å…ˆçº§åˆ«è¿˜æ˜¯æ¯”è¾ƒé«˜çš„ï¼Œå®ƒä½¿ç”¨çš„å†…å­˜ç©ºé—´æ˜¯å¯ä»¥ä»storageä¸­å€Ÿç”¨çš„ï¼Œå¦‚æœåœ¨storageä¸­æ²¡æœ‰ç°å­˜çš„æ•°æ®blockï¼Œå®ƒç”šè‡³å¯ä»¥å æ®æ•´ä¸ªstorageç©ºé—´ï¼›å¦‚æœstorageä¸­æœ‰æ•°æ®blockï¼Œå®ƒå¯ä»¥æœ€å¤§dropæ‰å†…å­˜çš„æ•°æ®æ˜¯é€šè¿‡spark.storage.unrollFractionæ¥æ§åˆ¶çš„ï¼Œé€šè¿‡æºç å¯çŸ¥è¿™éƒ¨åˆ†çš„é»˜è®¤å€¼ä¸º0.2æ³¨æ„ï¼š è¿™ä¸ª20%çš„ç©ºé—´å¹¶ä¸æ˜¯é™æ€ä¿ç•™çš„ï¼Œè€Œæ˜¯é€šè¿‡dropæ‰å†…å­˜ä¸­çš„æ•°æ®blockæ¥åˆ†é…çš„ï¼ˆåŠ¨æ€çš„åˆ†é…è¿‡ç¨‹ï¼‰ï¼›å¦‚æœunrollå¤±è´¥äº†ï¼Œsparkä¼šæŠŠè¿™éƒ¨åˆ†æ•°æ®evictåˆ°ç¡¬ç›˜ä¸­å»evictionç­–ç•¥åœ¨sparkæŠ€æœ¯æ–‡æ¡£ä¸­ï¼Œevictionä¸€è¯ç»å¸¸å‡ºç°ï¼Œevictionå¹¶ä¸æ˜¯å•çº¯å­—é¢ä¸Šé©±é€çš„æ„æ€ã€‚è¯´å¥é¢˜å¤–è¯ï¼Œsparké€šå¸¸è¢«æˆ‘ä»¬å«åšå†…å­˜è®¡ç®—æ¡†æ¶ï¼Œä½†æ˜¯ä»ä¸¥æ ¼æ„ä¹‰ä¸Šè¯´ï¼Œsparkå¹¶ä¸æ˜¯å†…å­˜è®¡ç®—çš„æ–°æŠ€æœ¯ï¼›æ— è®ºæ˜¯cacheè¿˜æ˜¯persistè¿™ç±»ç®—å­ï¼Œsparkåœ¨å†…å­˜å®‰æ’ä¸Šï¼Œç»å¤§å¤šæ•°ç”¨çš„éƒ½æ˜¯LRUç­–ç•¥ï¼ˆLRUå¯ä»¥è¯´æ˜¯ä¸€ç§ç®—æ³•ï¼Œä¹Ÿå¯ä»¥ç®—æ˜¯ä¸€ç§åŸåˆ™ï¼Œç”¨æ¥åˆ¤æ–­å¦‚ä½•ä»Cacheä¸­æ¸…é™¤å¯¹è±¡ï¼Œè€ŒLRUå°±æ˜¯â€œè¿‘æœŸæœ€å°‘ä½¿ç”¨â€åŸåˆ™ï¼Œå½“Cacheæº¢å‡ºæ—¶ï¼Œæœ€è¿‘æœ€å°‘ä½¿ç”¨çš„å¯¹è±¡å°†è¢«ä»Cacheä¸­æ¸…é™¤ï¼‰ã€‚å³å½“å†…å­˜ä¸å¤Ÿçš„æ—¶å€™ï¼Œä¼ševictæ‰æœ€è¿œä½¿ç”¨è¿‡çš„å†…å­˜æ•°æ®blockï¼›å½“evictçš„æ—¶å€™ï¼Œsparkä¼šå°†è¯¥æ•°æ®å—evictåˆ°ç¡¬ç›˜ï¼Œè€Œä¸æ˜¯å•çº¯çš„æŠ›å¼ƒæ‰æ— è®ºæ˜¯storageè¿˜æ˜¯executionçš„å†…å­˜ç©ºé—´ï¼Œå½“å†…å­˜åŒºåŸŸçš„ç©ºé—´ä¸å¤Ÿç”¨çš„æ—¶å€™ï¼Œsparkéƒ½ä¼ševictæ•°æ®åˆ°ç¡¬ç›˜Otheréƒ¨åˆ†è¿™éƒ¨åˆ†çš„å†…å­˜ç”¨äºç¨‹åºæœ¬èº«è¿è¡Œæ‰€éœ€è¦çš„å†…å­˜ï¼Œä»¥åŠç”¨æˆ·å®šä¹‰çš„æ•°æ®ç»“æ„å’Œåˆ›å»ºçš„å¯¹è±¡ï¼Œæ­¤å†…å­˜ç”±ä¸Šé¢ä¸¤éƒ¨åˆ†ï¼šstorageã€executionå†³å®šçš„ï¼Œé»˜è®¤ä¸º0.2å †å¤–å†…å­˜Spark1.6å¼€å§‹å¼•å…¥äº†Off-heap memoryï¼ˆè¯¦è§SPARK-11389ï¼‰å †å¤–çš„ç©ºé—´åˆ†é…è¾ƒä¸ºç®€å•ï¼Œåªæœ‰å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜ï¼Œå¦‚å›¾æ‰€ç¤ºï¼šå¯ç”¨çš„æ‰§è¡Œå†…å­˜å’Œå­˜å‚¨å†…å­˜å ç”¨çš„ç©ºé—´å¤§å°ç›´æ¥ç”±å‚æ•° spark.memory.storageFraction å†³å®šï¼ˆé»˜è®¤ä¸º0.5ï¼‰ï¼Œç”±äºå †å¤–å†…å­˜å ç”¨çš„ç©ºé—´å¯ä»¥è¢«ç²¾ç¡®è®¡ç®—ï¼Œæ‰€ä»¥æ— éœ€å†è®¾å®šä¿é™©åŒºåŸŸå±€é™æ€§åœ¨Sparkçš„è®¾è®¡æ–‡æ¡£ä¸­ï¼ŒæŒ‡å‡ºäº†é™æ€å†…å­˜ç®¡ç†çš„å±€é™æ€§ï¼šæ²¡æœ‰é€‚ç”¨äºæ‰€æœ‰åº”ç”¨çš„é»˜è®¤é…ç½®ï¼Œé€šå¸¸éœ€è¦å¼€å‘äººå‘˜é’ˆå¯¹ä¸åŒçš„åº”ç”¨è¿›è¡Œä¸åŒçš„å‚æ•°è¿›è¡Œé…ç½®ï¼šæ¯”å¦‚æ ¹æ®ä»»åŠ¡çš„æ‰§è¡Œé€»è¾‘ï¼Œè°ƒæ•´shuffleå’Œstorageçš„å†…å­˜å æ¯”æ¥é€‚åº”ä»»åŠ¡çš„éœ€æ±‚è¿™æ ·éœ€è¦å¼€å‘äººå‘˜å…·å¤‡è¾ƒé«˜çš„sparkåŸç†çŸ¥è¯†é‚£äº›ä¸cacheæ•°æ®çš„åº”ç”¨åœ¨è¿è¡Œçš„æ—¶å€™åªä¼šå ç”¨ä¸€å°éƒ¨åˆ†å¯ç”¨å†…å­˜ï¼Œè€Œé»˜è®¤çš„å†…å­˜é…ç½®ä¸­storageå°±ç”¨å»äº†60%ï¼Œé€ æˆäº†æµªè´¹]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§Spark Executor Deadå¿«é€Ÿå‰–æ]]></title>
    <url>%2F2019%2F03%2F12%2F%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[é—®é¢˜ç°è±¡é€šè¿‡Spark UIæŸ¥çœ‹Executorsï¼Œå‘ç°å­˜åœ¨Executor Deadçš„æƒ…å†µè¿›ä¸€æ­¥æŸ¥çœ‹dead Executor stderræ—¥å¿—ï¼Œå‘ç°å¦‚ä¸‹æŠ¥é”™ä¿¡æ¯ï¼šè§£å†³è¿‡ç¨‹ æ‰“å¼€GCæ—¥å¿—ï¼Œé…ç½®å¦‚ä¸‹12--conf &quot;spark.executor.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;--conf &quot;spark.driver.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;æ‰“å¼€exeutor gcæ—¥å¿—ï¼Œå‘ç°ä¸€ç›´åœ¨full gcï¼Œå‡ ä¹æ¯ç§’1æ¬¡ï¼ŒåŸºæœ¬å¤„äºæ‹’ç»æœåŠ¡çŠ¶æ€è‡³æ­¤æ‰¾åˆ°é—®é¢˜åŸå› ï¼Œexecutorå†…å­˜ä¸å¤Ÿå¯¼è‡´deadï¼Œè°ƒå¤§executorå†…å­˜å³å¯ ï¼Œæ‰€ä»¥æ’é”™æ–¹æ³•å®šä½å¾ˆé‡è¦ï¼]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§Spark Streaming é»‘åå•è¿‡æ»¤æ¡ˆä¾‹]]></title>
    <url>%2F2019%2F03%2F08%2F%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[æµ‹è¯•æ•°æ®(é€šè¿‡Socketä¼ å…¥)ï¼š12320180808,zs20180808,ls20180808,wwé»‘åå•åˆ—è¡¨(ç”Ÿäº§å­˜åœ¨è¡¨)ï¼š12zslsæ€è·¯åŸå§‹æ—¥å¿—å¯ä»¥é€šè¿‡Streamingç›´æ¥è¯»å–æˆä¸€ä¸ªDStreamåå•é€šè¿‡RDDæ¥æ¨¡æ‹Ÿä¸€ä»½é€»è¾‘å®ç°å°†DStreamè½¬æˆä»¥ä¸‹æ ¼å¼(é»‘åå•åªæœ‰åå­—)(zs,(20180808,zs))(ls,(20180808,ls))(ww,( 20180808,ww))å°†é»‘åå•è½¬æˆ(zs, true)(ls, true)DStramä¸RDDè¿›è¡ŒLeftJoin(DStreamèƒ½ä¸RDDè¿›è¡ŒJoinå°±æ˜¯å€Ÿç”¨çš„transformç®—å­)å…·ä½“ä»£ç å®ç°åŠæ³¨é‡Š12345678910111213141516171819202122232425262728package com.soul.spark.Streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * @author soulChun * @create 2019-01-10-16:12 */object TransformApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(&quot;StatafulFunApp&quot;).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf,Seconds(10)) //æ„å»ºé»‘åå• val blacks = List(&quot;zs&quot;, &quot;ls&quot;) //é€šè¿‡mapæ“ä½œå°†é»‘åå•ç»“æ„è½¬æ¢æˆ(zs, true)(ls, true) val blackRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, true)) val lines = ssc.socketTextStream(&quot;localhost&quot;, 8769) //lines (20180808,zs) //lines é€šè¿‡map.split(1)ä¹‹åå–å¾—å°±æ˜¯zs,ç„¶ååŠ ä¸€ä¸ªxå°±è½¬æˆäº†(zs,(20180808,zs)).å°±å¯ä»¥å’ŒblackRDDè¿›è¡ŒJoinäº† val clicklog = lines.map(x =&gt; (x.split(&quot;,&quot;)(1), x)).transform(rdd =&gt; &#123; //Joinä¹‹åæ•°æ®ç»“æ„å°±å˜æˆäº†(zs,[(20180808,zs),true]),è¿‡æ»¤æ‰ç¬¬äºŒä¸ªå…ƒç´ ä¸­çš„ç¬¬äºŒä¸ªå…ƒç´ ç­‰äºtrueçš„ rdd.leftOuterJoin(blackRDD).filter(x =&gt; x._2._2.getOrElse(false) != true) //æˆ‘ä»¬æœ€åè¦è¾“å‡ºçš„æ ¼å¼æ˜¯(20180808,zs)ï¼Œæ‰€ä»¥å–Joinä¹‹åçš„ç¬¬äºŒä¸ªå…ƒç´ ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´  .map(x =&gt; x._2._1) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;æœ€åè¾“å‡ºï¼š]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>sparkstreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åˆšå‡ºç‚‰çš„3å®¶å¤§æ•°æ®é¢è¯•é¢˜(å«é«˜çº§),ä½ ä¼šå—ï¼Ÿ]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97%2F</url>
    <content type="text"><![CDATA[ç¬¬ä¸€å®¶å¤§æ•°æ®å¼€å‘çš„é¢è¯•é¢˜:ç¬¬äºŒå®¶å¤§æ•°æ®å¼€å‘çš„é¢è¯•é¢˜:ç¬¬ä¸‰å®¶å¤§æ•°æ®å¼€å‘çš„é¢è¯•é¢˜:]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkShuffleè¯¦è§£å‰–æ]]></title>
    <url>%2F2019%2F03%2F06%2FSparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashShuffleæ¦‚è¿°æ‰€è°“Shuffleå°±æ˜¯å°†ä¸åŒèŠ‚ç‚¹ä¸Šç›¸åŒçš„Keyæ‹‰å–åˆ°ä¸€ä¸ªèŠ‚ç‚¹çš„è¿‡ç¨‹ã€‚è¿™ä¹‹ä¸­æ¶‰åŠåˆ°å„ç§IOï¼Œæ‰€ä»¥æ‰§è¡Œæ—¶é—´åŠ¿å¿…ä¼šè¾ƒé•¿ï¼ŒSparkçš„Shuffleåœ¨1.2ä¹‹å‰é»˜è®¤çš„è®¡ç®—å¼•æ“æ˜¯HashShuffleManagerï¼Œä¸è¿‡HashShuffleManageræœ‰ä¸€ä¸ªååˆ†ä¸¥é‡çš„å¼Šç«¯ï¼Œå°±æ˜¯ä¼šäº§ç”Ÿå¤§é‡çš„ä¸­é—´æ–‡ä»¶ã€‚åœ¨1.2ä¹‹åé»˜è®¤Shuffleæ”¹ä¸ºSortShuffleManagerï¼Œç›¸å¯¹äºä¹‹å‰ï¼Œåœ¨æ¯ä¸ªTaskè™½ç„¶ä¹Ÿä¼šäº§ç”Ÿå¤§é‡ä¸­é—´æ–‡ä»¶ï¼Œä½†æ˜¯æœ€åä¼šå°†æ‰€æœ‰çš„ä¸´æ—¶æ–‡ä»¶åˆå¹¶ï¼ˆmergeï¼‰æˆä¸€ä¸ªæ–‡ä»¶ã€‚å› æ­¤Shuffle readåªéœ€è¦è¯»å–æ—¶ï¼Œæ ¹æ®ç´¢å¼•æ‹¿åˆ°æ¯ä¸ªç£ç›˜çš„éƒ¨åˆ†æ•°æ®å°±å¯ä»¥äº†æµ‹è¯•æ¡ä»¶æ¯ä¸ªExecutoråªæœ‰ä¸€ä¸ªCUPï¼ˆcoreï¼‰ï¼ŒåŒä¸€æ—¶é—´æ¯ä¸ªExecutoråªèƒ½æ‰§è¡Œä¸€ä¸ªtaskHashShuffleManageræœªä¼˜åŒ–ç‰ˆæœ¬é¦–å…ˆä»shuffle writeé˜¶æ®µï¼Œä¸»è¦æ˜¯åœ¨ä¸€ä¸ªstageç»“æŸåï¼Œä¸ºäº†ä¸‹ä¸€ä¸ªstageå¯ä»¥æ‰§è¡Œshuffleï¼Œå°†æ¯ä¸€ä¸ªtaskçš„æ•°æ®æŒ‰ç…§keyè¿›è¡Œåˆ†ç±»ï¼Œå¯¹keyè¿›è¡Œhashç®—æ³•ï¼Œä»è€Œä½¿ç›¸åŒçš„keyå†™å…¥åŒä¸€ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªç£ç›˜æ–‡ä»¶éƒ½ç”±ä¸‹æ¸¸stageçš„ä¸€ä¸ªtaskè¯»å–ã€‚åœ¨å†™å…¥ç£ç›˜æ—¶ï¼Œå…ˆå°†æ•°æ®å†™å…¥å†…å­˜ç¼“å†²ï¼Œå½“å†…å­˜ç¼“å†²å¡«æ»¡åï¼Œæ‰ä¼šæº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ï¼ˆä¼¼ä¹æ‰€ä»¥å†™æ–‡ä»¶éƒ½éœ€è¦å†™å…¥å…ˆå†™å…¥ç¼“å†²åŒºï¼Œç„¶åå†æº¢å†™ï¼Œé˜²æ­¢é¢‘ç¹IOï¼‰æˆ‘ä»¬å¯ä»¥å…ˆç®—ä¸€ä¸‹å½“å‰stageçš„ä¸€ä¸ªtaskä¼šä¸ºä¸‹ä¸€ä¸ªstageåˆ›å»ºå¤šå°‘ä¸ªç£ç›˜æ–‡ä»¶ã€‚è‹¥ä¸‹ä¸€ä¸ªstageæœ‰100ä¸ªtaskï¼Œåˆ™å½“å‰stageçš„æ¯ä¸€ä¸ªtaskéƒ½å°†åˆ›å»º100ä¸ªæ–‡ä»¶ï¼Œè‹¥å½“å‰stageè¦å¤„ç†çš„taskä¸º50ä¸ªï¼Œå…±æœ‰10ä¸ªExecutorï¼Œä¹Ÿå°±æ˜¯è¯´æ¯ä¸ªExecutorå…±æ‰§è¡Œ5ä¸ªtaskï¼Œ5x100x10=1000ã€‚ä¹Ÿå°±æ˜¯è¯´è¿™ä¹ˆä¸€ä¸ªå°è§„æ¨¡çš„æ“ä½œä¼šç”Ÿäº§5000ä¸ªæ–‡ä»¶ã€‚è¿™æ˜¯ç›¸å½“å¯è§‚çš„ã€‚è€Œshuffle read é€šå¸¸æ˜¯ä¸€ä¸ªstageä¸€å¼€å§‹è¦åšçš„äº‹æƒ…ã€‚æ­¤æ—¶stageçš„æ¯ä¸€ä¸ªtaskå»å°†ä¸Šä¸€ä¸ªstageçš„è®¡ç®—ç»“æœçš„æ‰€æœ‰ç›¸åŒçš„keyä»ä¸åŒèŠ‚ç‚¹æ‹‰åˆ°è‡ªå·±æ‰€åœ¨èŠ‚ç‚¹ã€‚è¿›è¡Œèšåˆæˆ–joinæ“ä½œã€‚åœ¨shuffle writeè¿‡ç¨‹ï¼Œæ¯ä¸ªtaskç»™ä¸‹æ¸¸çš„æ¯ä¸ªtaskéƒ½åˆ›å»ºäº†ä¸€ä¸ªç£ç›˜æ–‡ä»¶ã€‚åœ¨readè¿‡ç¨‹taskåªéœ€è¦å»ä¸Šæ¸¸stageçš„taskä¸­æ‹‰å–å±äºè‡ªå·±çš„ç£ç›˜æ–‡ä»¶ã€‚shuffle readæ˜¯è¾¹æ‹‰å–è¾¹èšåˆã€‚æ¯ä¸€ä¸ªread taskéƒ½æœ‰ä¸€ä¸ªbufferç¼“å†²ï¼Œç„¶åé€šè¿‡å†…å­˜ä¸­çš„Mapè¿›è¡Œèšåˆï¼Œæ¯æ¬¡åªæ‹‰å–bufferå¤§å°çš„æ•°æ®ï¼Œæ”¾åˆ°ç¼“å†²åŒºä¸­èšåˆï¼Œç›´åˆ°æ‰€æœ‰æ•°æ®éƒ½æ‹‰å–å®Œã€‚ä¼˜åŒ–ç‰ˆæœ¬è¿™é‡Œè¯´çš„ä¼˜åŒ–ï¼Œæ˜¯æŒ‡æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªå‚æ•°ï¼Œspark.shuffle.consolidateFilesã€‚è¯¥å‚æ•°é»˜è®¤å€¼ä¸ºfalseï¼Œå°†å…¶è®¾ç½®ä¸ºtrueå³å¯å¼€å¯ä¼˜åŒ–æœºåˆ¶ã€‚é€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨HashShuffleManagerï¼Œé‚£ä¹ˆéƒ½å»ºè®®å¼€å¯è¿™ä¸ªé€‰é¡¹ã€‚å¼€å¯è¿™ä¸ªæœºåˆ¶ä¹‹åï¼Œåœ¨shuffle writeæ—¶ï¼Œtaskå¹¶ä¸æ˜¯ä¸ºä¸‹æ¸¸çš„æ¯ä¸€ä¸ªtaskåˆ›å»ºä¸€ä¸ªç£ç›˜æ–‡ä»¶ã€‚å¼•å…¥äº†shuffleFileGroupçš„æ¦‚å¿µï¼Œæ¯ä¸ªshuffleFileGroupéƒ½å¯¹åº”ä¸€æ‰¹ç£ç›˜æ–‡ä»¶ã€‚ç£ç›˜æ–‡ä»¶æ•°é‡ä¸ä¸‹æ¸¸taskç›¸åŒã€‚åªæ˜¯ä»…ä»…ç¬¬ä¸€æ‰¹æ‰§è¡Œçš„taskä¼šåˆ›å»ºä¸€ä¸ªshuffleFIleGroupï¼Œå°†æ•°æ®å†™å…¥åˆ°å¯¹åº”ç£ç›˜æ–‡ä»¶ã€‚åœ¨æ‰§è¡Œä¸‹ä¸€æ‰¹çš„taskæ—¶ï¼Œä¼šå¤ç”¨å·²ç»åˆ›å»ºå¥½çš„shuffleFIleGroupå’Œç£ç›˜æ–‡ä»¶ï¼Œå³æ•°æ®ä¼šç»§ç»­å†™å…¥åˆ°å·²æœ‰çš„ç£ç›˜æ–‡ä»¶ã€‚è¯¥æœºåˆ¶ä¼šå…è®¸ä¸åŒtaskå¤ç”¨åŒä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå¯¹äºå¤šä¸ªtaskè¿›è¡Œäº†ä¸€å®šç¨‹åº¦çš„åˆå¹¶ï¼Œå¤§å¹…åº¦å‡å°‘shuffle writeæ—¶ï¼Œæ–‡ä»¶çš„æ•°é‡ï¼Œæå‡æ€§èƒ½ã€‚ç›¸å¯¹äºä¼˜åŒ–å‰ï¼Œæ¯ä¸ªExecutorä¹‹å‰éœ€è¦åˆ›å»ºäº”ç™¾ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå› ä¸ºä¹‹å‰éœ€è¦5ä¸ªtaskçº¿æ€§æ‰§è¡Œï¼Œè€Œä½¿ç”¨å‚æ•°ä¼˜åŒ–ä¹‹åï¼Œå°±æ¯ä¸ªExecutoråªéœ€è¦100ä¸ªå°±å¯ä»¥äº†ï¼Œè¿™æ ·10ä¸ªExecutorå°±æ˜¯1000ä¸ªæ–‡ä»¶ï¼Œè¿™æ¯”ä¼˜åŒ–å‰æ•´æ•´å‡å°‘äº†4000ä¸ªæ–‡ä»¶ã€‚SortShuffleåœ¨Spark1.2ç‰ˆæœ¬ä¹‹åï¼Œå‡ºç°äº†SortShuffleï¼Œè¿™ç§æ–¹å¼ä»¥æ›´å°‘çš„ä¸­é—´ç£ç›˜æ–‡ä»¶äº§ç”Ÿè€Œè¿œè¿œä¼˜äºHashShuffleã€‚è€Œå®ƒçš„è¿è¡Œæœºåˆ¶ä¸»è¦åˆ†ä¸ºä¸¤ç§ã€‚ä¸€ç§ä¸ºæ™®é€šæœºåˆ¶ï¼Œå¦ä¸€ç§ä¸ºbypassæœºåˆ¶ã€‚è€Œbypassæœºåˆ¶çš„å¯åŠ¨æ¡ä»¶ä¸ºï¼Œå½“shuffle read taskçš„æ•°é‡å°äºç­‰äºspark.shuffle.sort.bypassMergeThresholdå‚æ•°çš„å€¼æ—¶ï¼ˆé»˜è®¤ä¸º200ï¼‰ï¼Œå°±ä¼šå¯ç”¨bypassæœºåˆ¶ã€‚å³å½“read taskä¸æ˜¯é‚£ä¹ˆå¤šçš„æ—¶å€™ï¼Œé‡‡ç”¨bypassæœºåˆ¶æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚æ™®é€šè¿è¡Œæœºåˆ¶åœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œæ•°æ®ä¼šå…ˆå†™å…¥ä¸€ä¸ªæ•°æ®ç»“æ„ï¼Œèšåˆç®—å­å†™å…¥Mapï¼Œä¸€è¾¹é€šè¿‡Mapå±€éƒ¨èšåˆï¼Œä¸€éå†™å…¥å†…å­˜ã€‚Joinç®—å­å†™å…¥ArrayListç›´æ¥å†™å…¥å†…å­˜ä¸­ã€‚ç„¶åéœ€è¦åˆ¤æ–­æ˜¯å¦è¾¾åˆ°é˜ˆå€¼ï¼Œå¦‚æœè¾¾åˆ°å°±ä¼šå°†å†…å­˜æ•°æ®ç»“æ„çš„æ•°æ®å†™å…¥åˆ°ç£ç›˜ï¼Œæ¸…ç©ºå†…å­˜æ•°æ®ç»“æ„ã€‚åœ¨æº¢å†™ç£ç›˜å‰ï¼Œå…ˆæ ¹æ®keyè¿›è¡Œæ’åºï¼Œæ’åºè¿‡åçš„æ•°æ®ï¼Œä¼šåˆ†æ‰¹å†™å…¥åˆ°ç£ç›˜æ–‡ä»¶ä¸­ã€‚é»˜è®¤æ‰¹æ¬¡ä¸º10000æ¡ï¼Œæ•°æ®ä¼šä»¥æ¯æ‰¹ä¸€ä¸‡æ¡å†™å…¥åˆ°ç£ç›˜æ–‡ä»¶ã€‚å†™å…¥ç£ç›˜æ–‡ä»¶é€šè¿‡ç¼“å†²åŒºæº¢å†™çš„æ–¹å¼ï¼Œæ¯æ¬¡æº¢å†™éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªtaskè¿‡ç¨‹ä¼šäº§ç”Ÿå¤šä¸ªä¸´æ—¶æ–‡ä»¶ã€‚æœ€ååœ¨æ¯ä¸ªtaskä¸­ï¼Œå°†æ‰€æœ‰çš„ä¸´æ—¶æ–‡ä»¶åˆå¹¶ï¼Œè¿™å°±æ˜¯mergeè¿‡ç¨‹ï¼Œæ­¤è¿‡ç¨‹å°†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶è¯»å–å‡ºæ¥ï¼Œä¸€æ¬¡å†™å…¥åˆ°æœ€ç»ˆæ–‡ä»¶ã€‚æ„å‘³ç€ä¸€ä¸ªtaskçš„æ‰€æœ‰æ•°æ®éƒ½åœ¨è¿™ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚åŒæ—¶å•ç‹¬å†™ä¸€ä»½ç´¢å¼•æ–‡ä»¶ï¼Œæ ‡è¯†ä¸‹æ¸¸å„ä¸ªtaskçš„æ•°æ®åœ¨æ–‡ä»¶ä¸­çš„ç´¢å¼•ï¼Œstart offsetå’Œend offsetã€‚è¿™æ ·ç®—æ¥å¦‚æœç¬¬ä¸€ä¸ªstage 50ä¸ªtaskï¼Œæ¯ä¸ªExecutoræ‰§è¡Œä¸€ä¸ªtaskï¼Œé‚£ä¹ˆæ— è®ºä¸‹æ¸¸æœ‰å‡ ä¸ªtaskï¼Œå°±éœ€è¦50ä¸ªç£ç›˜æ–‡ä»¶ã€‚bypassæœºåˆ¶bypassæœºåˆ¶è¿è¡Œæ¡ä»¶ï¼šshuffle map taskæ•°é‡å°äºspark.shuffle.sort.bypassMergeThresholdå‚æ•°çš„å€¼ã€‚ä¸æ˜¯èšåˆç±»çš„shuffleç®—å­ï¼ˆæ¯”å¦‚reduceByKeyï¼‰ã€‚åœ¨è¿™ç§æœºåˆ¶ä¸‹ï¼Œå½“å‰stageçš„taskä¼šä¸ºæ¯ä¸ªä¸‹æ¸¸çš„taskéƒ½åˆ›å»ºä¸´æ—¶ç£ç›˜æ–‡ä»¶ã€‚å°†æ•°æ®æŒ‰ç…§keyå€¼è¿›è¡Œhashï¼Œç„¶åæ ¹æ®hashå€¼ï¼Œå°†keyå†™å…¥å¯¹åº”çš„ç£ç›˜æ–‡ä»¶ä¸­ï¼ˆä¸ªäººè§‰å¾—è¿™ä¹Ÿç›¸å½“äºä¸€æ¬¡å¦ç±»çš„æ’åºï¼Œå°†ç›¸åŒçš„keyæ”¾åœ¨ä¸€èµ·äº†ï¼‰ã€‚æœ€ç»ˆï¼ŒåŒæ ·ä¼šå°†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ä¾æ¬¡åˆå¹¶æˆä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå»ºç«‹ç´¢å¼•ã€‚ä¼˜ç‚¹è¯¥æœºåˆ¶ä¸æœªä¼˜åŒ–çš„hashshuffleç›¸æ¯”ï¼Œæ²¡æœ‰é‚£ä¹ˆå¤šç£ç›˜æ–‡ä»¶ï¼Œä¸‹æ¸¸taskçš„readæ“ä½œç›¸å¯¹æ€§èƒ½ä¼šæ›´å¥½ã€‚è¯¥æœºåˆ¶ä¸sortshuffleçš„æ™®é€šæœºåˆ¶ç›¸æ¯”ï¼Œåœ¨readtaskä¸å¤šçš„æƒ…å†µä¸‹ï¼Œé¦–å…ˆå†™çš„æœºåˆ¶æ˜¯ä¸åŒï¼Œå…¶æ¬¡ä¸ä¼šè¿›è¡Œæ’åºã€‚è¿™æ ·å°±å¯ä»¥èŠ‚çº¦ä¸€éƒ¨åˆ†æ€§èƒ½å¼€é”€ã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§SparkSQLå¦‚ä½•è¯»å†™æœ¬åœ°å¤–éƒ¨æ•°æ®æºåŠæ’é”™]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99%2F</url>
    <content type="text"><![CDATA[https://spark-packages.org/é‡Œæœ‰å¾ˆå¤šthird-partyæ•°æ®æºçš„packageï¼ŒsparkæŠŠåŒ…åŠ è½½è¿›æ¥å°±å¯ä»¥ä½¿ç”¨äº†csvæ ¼å¼åœ¨spark2.0ç‰ˆæœ¬ä¹‹åæ˜¯å†…ç½®çš„ï¼Œ2.0ä¹‹å‰å±äºç¬¬ä¸‰æ–¹æ•°æ®æºè¯»å–æœ¬åœ°å¤–éƒ¨æ•°æ®æºç›´æ¥è¯»å–ä¸€ä¸ªjsonæ–‡ä»¶12[hadoop@hadoop000 bin]$ ./spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.27.jar scala&gt; spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;).showè¿è¡ŒæŠ¥é”™ï¼š123456Caused by: java.lang.RuntimeException: file:/home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [49, 57, 125, 10] at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476) at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445) at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421) at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519) ... 32 moreæŸ¥çœ‹loadæ–¹æ³•çš„æºç ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by * a local or distributed file system). * * @since 1.4.0 */ def load(path: String): DataFrame = &#123; option(&quot;path&quot;, path).load(Seq.empty: _*) // force invocation of `load(...varargs...)` &#125;---------------------------------------------------------/** * Loads input in as a `DataFrame`, for data sources that support multiple paths. * Only works if the source is a HadoopFsRelationProvider. * * @since 1.6.0 */ @scala.annotation.varargs def load(paths: String*): DataFrame = &#123; if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) &#123; throw new AnalysisException(&quot;Hive data source can only be used with tables, you can not &quot; + &quot;read files of Hive data source directly.&quot;) &#125; val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf) if (classOf[DataSourceV2].isAssignableFrom(cls)) &#123; val ds = cls.newInstance() val options = new DataSourceOptions((extraOptions ++ DataSourceV2Utils.extractSessionConfigs( ds = ds.asInstanceOf[DataSourceV2], conf = sparkSession.sessionState.conf)).asJava) // Streaming also uses the data source V2 API. So it may be that the data source implements // v2, but has no v2 implementation for batch reads. In that case, we fall back to loading // the dataframe as a v1 source. val reader = (ds, userSpecifiedSchema) match &#123; case (ds: ReadSupportWithSchema, Some(schema)) =&gt; ds.createReader(schema, options) case (ds: ReadSupport, None) =&gt; ds.createReader(options) case (ds: ReadSupportWithSchema, None) =&gt; throw new AnalysisException(s&quot;A schema needs to be specified when using $ds.&quot;) case (ds: ReadSupport, Some(schema)) =&gt; val reader = ds.createReader(options) if (reader.readSchema() != schema) &#123; throw new AnalysisException(s&quot;$ds does not allow user-specified schemas.&quot;) &#125; reader case _ =&gt; null // fall back to v1 &#125; if (reader == null) &#123; loadV1Source(paths: _*) &#125; else &#123; Dataset.ofRows(sparkSession, DataSourceV2Relation(reader)) &#125; &#125; else &#123; loadV1Source(paths: _*) &#125; &#125; private def loadV1Source(paths: String*) = &#123; // Code path for data source v1. sparkSession.baseRelationToDataFrame( DataSource.apply( sparkSession, paths = paths, userSpecifiedSchema = userSpecifiedSchema, className = source, options = extraOptions.toMap).resolveRelation()) &#125;------------------------------------------------------private var source: String = sparkSession.sessionState.conf.defaultDataSourceName-------------------------------------------------------def defaultDataSourceName: String = getConf(DEFAULT_DATA_SOURCE_NAME)--------------------------------------------------------// This is used to set the default data source val DEFAULT_DATA_SOURCE_NAME = buildConf(&quot;spark.sql.sources.default&quot;) .doc(&quot;The default data source to use in input/output.&quot;) .stringConf .createWithDefault(&quot;parquet&quot;)ä»æºç ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœä¸æŒ‡å®šformatï¼Œloadé»˜è®¤è¯»å–çš„æ˜¯parquetæ–‡ä»¶12345678scala&gt; val users = spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)scala&gt; users.show()+------+--------------+----------------+ | name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+è¯»å–å…¶ä»–æ ¼å¼çš„æ–‡ä»¶ï¼Œå¿…é¡»é€šè¿‡formatæŒ‡å®šæ–‡ä»¶æ ¼å¼ï¼Œå¦‚ä¸‹ï¼š12345678910//windows ideaç¯å¢ƒä¸‹val df1 = spark.read.format(&quot;json&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).load(&quot;hdfs://192.168.137.141:9000/data/people.json&quot;)df1.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+option(â€œtimestampFormatâ€, â€œyyyy/MM/dd HH:mm:ss ZZâ€)å¿…é¡»å¸¦ä¸Šï¼Œä¸ç„¶æŠ¥é”™1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXXè¯»å–CSVæ ¼å¼æ–‡ä»¶123456789101112131415161718192021222324252627282930313233343536373839404142//æºæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š[hadoop@hadoop001 ~]$ hadoop fs -text /data/people.csvname;age;jobJorge;30;DeveloperBob;32;Developer//windows ideaç¯å¢ƒä¸‹val df2 = spark.read.format(&quot;csv&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .option(&quot;sep&quot;,&quot;;&quot;) .option(&quot;header&quot;,&quot;true&quot;) //use first line of all files as header .option(&quot;inferSchema&quot;,&quot;true&quot;) .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)df2.show()df2.printSchema()//è¾“å‡ºç»“æœï¼š+-----+---+---------+| name|age| job|+-----+---+---------+|Jorge| 30|Developer|| Bob| 32|Developer|+-----+---+---------+root |-- name: string (nullable = true) |-- age: integer (nullable = true) |-- job: string (nullable = true)-----------------------------------------------------------//å¦‚æœä¸æŒ‡å®šoption(&quot;sep&quot;,&quot;;&quot;)+------------------+| name;age;job|+------------------+|Jorge;30;Developer|| Bob;32;Developer|+------------------+//å¦‚æœä¸æŒ‡å®šoption(&quot;header&quot;,&quot;true&quot;)+-----+---+---------+| _c0|_c1| _c2|+-----+---+---------+| name|age| job||Jorge| 30|Developer|| Bob| 32|Developer|+-----+---+---------+è¯»å–csvæ ¼å¼æ–‡ä»¶è¿˜å¯ä»¥è‡ªå®šä¹‰schema12345678910111213141516171819202122val peopleschema = StructType(Array(StructField(&quot;hlwname&quot;,StringType,true), StructField(&quot;hlwage&quot;,IntegerType,true), StructField(&quot;hlwjob&quot;,StringType,true)))val df2 = spark.read.format(&quot;csv&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).option(&quot;sep&quot;,&quot;;&quot;) .option(&quot;header&quot;,&quot;true&quot;) .schema(peopleschema) .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;) //æ‰“å°æµ‹è¯• df2.show() df2.printSchema()è¾“å‡ºç»“æœï¼š+-------+------+---------+|hlwname|hlwage| hlwjob|+-------+------+---------+| Jorge| 30|Developer|| Bob| 32|Developer|+-------+------+---------+root |-- hlwname: string (nullable = true) |-- hlwage: integer (nullable = true) |-- hlwjob: string (nullable = true)å°†è¯»å–çš„æ–‡ä»¶ä»¥å…¶ä»–æ ¼å¼å†™å‡º1234567891011121314151617181920212223//å°†ä¸Šæ–‡è¯»å–çš„users.parquetä»¥jsonæ ¼å¼å†™å‡ºscala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)[hadoop@hadoop000 ~]$ cd /home/hadoop/tmp/parquet2json[hadoop@hadoop000 parquet2json]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 56 Sep 24 10:15 part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json-rw-r--r--. 1 hadoop hadoop 0 Sep 24 10:15 _SUCCESS[hadoop@hadoop000 parquet2json]$ cat part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json &#123;&quot;name&quot;:&quot;Alyssa&quot;&#125;&#123;&quot;name&quot;:&quot;Ben&quot;,&quot;favorite_color&quot;:&quot;red&quot;&#125;//å°†ä¸Šæ–‡è¯»å–çš„people.jsonä»¥csvæ ¼å¼å†™å‡ºdf1.write.format(&quot;csv&quot;) .mode(&quot;overwrite&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .save(&quot;hdfs://192.168.137.141:9000/data/formatconverttest/&quot;)------------------------------------------[hadoop@hadoop001 ~]$ hadoop fs -text /data/formatconverttest/part-00000-6fd65eff-d0d3-43e5-9549-2b11bc3ca9de-c000.csv,Michael30,Andy19,Justin//å‘ç°è‹¥æ²¡æœ‰.option(&quot;header&quot;,&quot;true&quot;)ï¼Œå†™å‡ºçš„csvä¸¢å¤±äº†é¦–è¡Œçš„age,nameä¿¡æ¯//è‹¥ä¸æŒ‡å®š.option(&quot;sep&quot;,&quot;;&quot;)ï¼Œé»˜è®¤é€—å·ä¸ºåˆ†éš”ç¬¦æ­¤æ“ä½œçš„ç›®çš„åœ¨äºå­¦ä¼šç±»å‹è½¬æ¢ï¼Œç”Ÿäº§ä¸Šæœ€å¼€å§‹è¿›æ¥çš„æ•°æ®å¤§å¤šéƒ½æ˜¯textï¼Œjsonç­‰è¡Œå¼å­˜å‚¨çš„æ–‡ä»¶ï¼Œä¸€èˆ¬éƒ½è¦è½¬æˆORCï¼Œparquetåˆ—å¼å­˜å‚¨çš„æ–‡ä»¶ï¼ŒåŠ ä¸Šå‹ç¼©ï¼Œèƒ½æŠŠæ–‡ä»¶å¤§å°å‡å°åˆ°10%å·¦å³ï¼Œå¤§å¹…åº¦å‡å°IOå’Œæ•°æ®å¤„ç†é‡ï¼Œæé«˜æ€§èƒ½æ­¤æ—¶å¦‚æœå†æ‰§è¡Œä¸€æ¬¡saveï¼Œè·¯å¾„ä¸å˜ï¼Œåˆ™ä¼šæŠ¥é”™ï¼š12345scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)org.apache.spark.sql.AnalysisException: path file:/home/hadoop/tmp/parquet2json already exists.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109) at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104).........................................................å¯ä»¥é€šè¿‡è®¾ç½®savemodeæ¥è§£å†³è¿™ä¸ªé—®é¢˜é»˜è®¤æ˜¯errorifexists1scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœ€ä½³å®è·µä¹‹Sparkå†™å…¥Hfileç»å…¸æ¡ˆä¾‹]]></title>
    <url>%2F2019%2F03%2F01%2F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡ç”±å°ä¼™ä¼´æä¾›å°†HDFSä¸Šçš„æ•°æ®è§£æå‡ºæ¥ï¼Œç„¶åé€šè¿‡hfileæ–¹å¼æ‰¹ï¥¾å†™å…¥Hbase(éœ€è¦å¤šåˆ—å†™å…¥) å†™â¼Šæ•°æ®çš„å…³é”®api:123456saveAsNewAPIHadoopFile( stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)ç‰¹æ®Šåœ°æ–¹ï¼šæœ€åˆå†™hfileè­¦å‘Š1Does it contain files in subdirectories that correspond to column family namesè¿™ä¸ªåŸå› å¤§æ¦‚ä¸‰ç§ * ä»£ç é—®é¢˜ * æ•°æ®æºé—®é¢˜ * setMapOutputKeyClass å’Œ saveAsNewAPIHadoopFileä¸­çš„Classä¸ï¥§â¼€ä¸€è‡´ è¿™é‡Œæ˜¯æˆ‘çš„æ˜¯æ•°æ®æºé—®é¢˜ æ­£å¸¸å†™putæ“ä½œçš„æ—¶å€™ï¼ŒæœåŠ¡ç«¯è‡ªåŠ¨å¸®åŠ©æ’åºï¼Œå› æ­¤åœ¨ä½¿ç”¨putæ“ä½œçš„æ—¶å€™æ²¡æœ‰æ¶‰åŠåˆ°è¿™æ ·çš„é”™è¯¯1Added a key not lexically larger than previousä½†æ˜¯åœ¨å†™hfileçš„æ—¶å€™å¦‚æœå‡ºç°æŠ¥é”™: 1Added a key not lexically larger than previous è¿™æ ·çš„é”™è¯¯ï¼Œä¸€èˆ¬ä¼šè®¤ä¸ºrowkeyæ²¡æœ‰åšå¥½æ’åºï¼Œç„¶åå‚»fufuçš„å»éªŒè¯ï¦ºä¸€ä¸‹ï¼Œrowkeyçš„ç¡®åšï¦ºæ’åºã€‚ çœŸæ­£åŸå› : `sparkå†™hfileæ—¶å€™æ˜¯æŒ‰ç…§rowkey+ï¦œæ—+åˆ—åè¿›â¾ï¨ˆæ’åºçš„ï¼Œå› æ­¤åœ¨å†™â¼Šæ•°æ®çš„æ—¶å€™ï¼Œè¦åšåˆ°æ•´ä½“æœ‰åº (äº‹æƒ…è¿˜æ²¡å®Œ)` å› ä¸ºéœ€è¦å¤šåˆ—å†™å…¥ï¼Œæœ€å¥½çš„â½…å¼:è¦ä¹ˆåå°„æ¥åŠ¨æ€è·å–ï¦œåç§°å’Œï¦œå€¼ ã€ è¦ä¹ˆé€šè¿‡datafameå»è·å–(df.columns)åå°„æ–¹å¼ï¼š12345678910111213141516171819202122232425262728293031val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] = rdd.map&#123; line =&gt; val rowkey = line.vintime val clazz = Class.forName(XXXXXXXXXXXXXXXX) val fields = clazz.getDeclaredFields var list = new ListBuffer[String]() var kvlist = new ListBuffer[KeyValue]()// if (fields != null &amp;&amp; fields.size &gt; 0) &#123; for (field &lt;- fields) &#123; field.setAccessible(true) val column = field.getName list.append(column)&#125; &#125; val newList = list.sortWith(_ &lt; _) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) for(column &lt;- newList)&#123; val declaredField: Field =line.getClass.getDeclaredField(column)&#125; declaredField.setAccessible(true) val value = declaredField.get(line).toString val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)) kvlist.append(kv)&#125;(ik, kvlist)&#125;datafameçš„æ–¹å¼: 123456789101112131415161718192021222324val tmpData: RDD[(ImmutableBytesWritable, util.LinkedList[KeyValue])] =df.rdd.map( line =&gt;&#123; val rowkey = line.getAs[String](&quot;vintime&quot;) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) var linkedList = new util.LinkedList[KeyValue]() for (column &lt;- columns) &#123; val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(line.getAs[String](column))) linkedList.add(kv) &#125; (ik, linkedList) &#125;) val result: RDD[(ImmutableBytesWritable, KeyValue)] =tmpData.flatMapValues( s =&gt; &#123; val values: Iterator[KeyValue] =JavaConverters.asScalaIteratorConverter(s.iterator()).asScala values &#125; ).sortBy(x =&gt;x._1 , true) ä»”ç»†è§‚å¯Ÿå¯ä»¥å‘ç°ï¼Œå…¶å®ä¸¤è€…éƒ½åšï¦ºæ’åºæ“ä½œï¼Œä½†æ˜¯å³ï¥¥ç»è¿‡(1)æ­¥éª¤åä»ç„¶æŠ¥é”™: 1Added a key not lexically larger than previous é‚£ä¹ˆå†å›æƒ³â¼€ä¸‹ä¹‹å‰å†™hfileçš„è¦æ±‚: rowkey+åˆ—æ—+ï¦œéƒ½è¦æœ‰åºï¼Œé‚£ä¹ˆå¦‚æœå‡ºç°æ•°æ®çš„é‡å¤ï¼Œä¹Ÿï¥§ç®—æ˜¯æœ‰åºçš„æ“ä½œ! å› ä¸ºï¼Œåšä¸€ä¸‹æ•°æ®çš„å»é‡: 12val key: RDD[(String, TransferTime)] = data.reduceByKey((x, y) =&gt; y)val unitData: RDD[TransferTime] = key.map(line =&gt; line._2) æœç„¶ï¼Œè¿™æ ·è§£å†³ï¦º:Added a key not lexically larger than previousè¿™ä¸ªå¼‚å¸¸ ä½†æ˜¯ä¼šæŠ¥å¦‚ä¸‹å¦â¼€ä¸ªå¼‚å¸¸: 1Kryo serialization failed: Buffer overflow è¿™ä¸ªæ˜¯å› ä¸ºåœ¨å¯¹â¼€äº›ç±»åškryoåºï¦œåŒ–æ—¶å€™ï¼Œæ•°æ®ï¥¾çš„ç¼“å­˜â¼¤å°è¶…è¿‡äº†é»˜è®¤å€¼ï¼Œåšâ¼€ä¸‹è°ƒæ•´å³å¯ 12sparkConf.set(&quot;spark.kryoserializer.buffer.max&quot; , &quot;256m&quot;)sparkConf.set(&quot;spark.kryoserializer.buffer&quot; , &quot;64m&quot;) å®Œæ•´ä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100object WriteTransferTime extends WriteToHbase&#123; /*** @param data è¦æ’â¼Šå…¥çš„æ•°æ® * @param tableName è¡¨å**/ override def bulkLoadData(data: RDD[Any], tableName: String ,columnFamily:String): Unit = &#123; val bean: RDD[TransferTime] = data.map(line =&gt;line.asInstanceOf[TransferTime]) val map: RDD[(String, TransferTime)] = bean.map(line =&gt; (line.vintime ,line)) val key: RDD[(String, TransferTime)] = map.reduceByKey((x, y) =&gt; y) val map1: RDD[TransferTime] = key.map(line =&gt; line._2) val by1: RDD[TransferTime] = map1.sortBy(f =&gt; f.vintime) val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] =by1.map &#123; line =&gt; val rowkey = line.vintime val clazz =Class.forName(&quot;com.dongfeng.code.Bean.message.TransferTime&quot;) val fields = clazz.getDeclaredFields var list = new ListBuffer[String]() var kvlist = new ListBuffer[KeyValue]()// if (fields != null &amp;&amp; fields.size &gt; 0) &#123; for (field &lt;- fields) &#123; field.setAccessible(true) val column = field.getName list.append(column)&#125; &#125; val newList = list.sortWith(_ &lt; _) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) for(column &lt;- newList)&#123; val declaredField: Field = line.getClass.getDeclaredField(column) declaredField.setAccessible(true) val value = declaredField.get(line).toString val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)) kvlist.append(kv) &#125; (ik, kvlist) &#125; val result: RDD[(ImmutableBytesWritable, KeyValue)] =listData.flatMapValues( s =&gt; &#123; val values: Iterator[KeyValue] = s.iterator values&#125; ) val resultDD: RDD[(ImmutableBytesWritable, KeyValue)] = result.sortBy(x=&gt;x._1 , true) WriteToHbaseDB.hfile_load(result , TableName.valueOf(tableName) ,columnFamily)&#125; &#125; def hfile_load(rdd:RDD[(ImmutableBytesWritable , KeyValue)] , tableName:TableName , columnFamily:String): Unit =&#123;//å£°æ˜è¡¨çš„ä¿¡æ¯var table: Table = null try&#123;val startTime = System.currentTimeMillis() println(s&quot;å¼€å§‹æ—¶é—´:--------&gt;$&#123;startTime&#125;&quot;) //â½£ç”Ÿæˆçš„HFileçš„ä¸´æ—¶ä¿å­˜è·¯ï¤·å¾„val stagingFolder = &quot;hdfs://cdh1:9000/hfile/&quot;+tableName+newDate().getTime//table = connection.getTable(tableName) //å¦‚æœè¡¨ä¸ï¥§å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¡¨ if(!admin.tableExists(tableName))&#123; createTable(tableName , columnFamily) &#125;//å¼€å§‹å¯¼â¼Šval job = Job.getInstance(config) job.setJobName(&quot;DumpFile&quot;) job.setMapOutputKeyClass(classOf[ImmutableBytesWritable]) job.setMapOutputValueClass(classOf[KeyValue]) rdd.sortBy(x =&gt; x._1, true).saveAsNewAPIHadoopFile( stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration) val load = new LoadIncrementalHFiles(config) val regionLocator = connection.getRegionLocator(tableName) HFileOutputFormat2.configureIncrementalLoad(job, table,regionLocator) load.doBulkLoad(new Path(stagingFolder), table.asInstanceOf[HTable])// load.doBulkLoad(new Path(stagingFolder) , connection.getAdmin ,table , regionLocator)val endTime = System.currentTimeMillis() println(s&quot;ç»“æŸæ—¶é—´:--------&gt;$&#123;endTime&#125;&quot;) println(s&quot;èŠ±è´¹çš„æ—¶é—´:-----------------&gt;$&#123;(endTime - startTime)&#125;ms&quot;) &#125;catch&#123; case e:IOException =&gt; e.printStackTrace() &#125;finally &#123; if (table != null) &#123; try &#123;// å…³é—­HTableå¯¹è±¡ table.close(); &#125; catch &#123; case e: IOException =&gt; e.printStackTrace();&#125; &#125;if (connection != null) &#123; try &#123; //å…³é—­hbaseè¿æ¥. connection.close(); &#125; catch &#123; case e: IOException =&gt; e.printStackTrace(); &#125;&#125; &#125;&#125; &#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>Spark</tag>
        <tag>Hfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§Sparkå¼€å‘è¯»å–äº‘ä¸»æœºHDFSå¼‚å¸¸å‰–ææµç¨‹]]></title>
    <url>%2F2019%2F02%2F26%2F%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[é—®é¢˜èƒŒæ™¯ï¼šäº‘ä¸»æœºæ˜¯ Linux ç¯å¢ƒï¼Œæ­å»º Hadoop ä¼ªåˆ†å¸ƒå¼å…¬ç½‘ IPï¼š139.198.xxx.xxxå†…ç½‘ IPï¼š192.168.137.2ä¸»æœºåï¼šhadoop001æœ¬åœ°çš„core-site.xmlé…ç½®å¦‚ä¸‹ï¼š12345678910&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9001&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9001/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;æœ¬åœ°çš„hdfs-site.xmlé…ç½®å¦‚ä¸‹ï¼š123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;äº‘ä¸»æœºhostsæ–‡ä»¶é…ç½®ï¼š12345[hadoop@hadoop001 ~]$ cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hostname loopback address 192.168.137.2 hadoop001äº‘ä¸»æœºå°†å†…ç½‘IPå’Œä¸»æœºåhadoop001åšäº†æ˜ å°„æœ¬åœ°hostsæ–‡ä»¶é…ç½®1139.198.18.XXX hadoop001æœ¬åœ°å·²ç»å°†å…¬ç½‘IPå’ŒåŸŸåhadoop001åšäº†æ˜ å°„é—®é¢˜ç—‡çŠ¶åœ¨äº‘ä¸»æœºä¸Šå¼€å¯ HDFSï¼ŒJPS æŸ¥çœ‹è¿›ç¨‹éƒ½æ²¡æœ‰å¼‚å¸¸ï¼Œé€šè¿‡ Shell æ“ä½œ HDFS æ–‡ä»¶ä¹Ÿæ²¡æœ‰é—®é¢˜é€šè¿‡æµè§ˆå™¨è®¿é—® 50070 ç«¯å£ç®¡ç†ç•Œé¢ä¹Ÿæ²¡æœ‰é—®é¢˜åœ¨æœ¬åœ°æœºå™¨ä¸Šä½¿ç”¨ Java API æ“ä½œè¿œç¨‹ HDFS æ–‡ä»¶ï¼ŒURI ä½¿ç”¨å…¬ç½‘ IPï¼Œä»£ç å¦‚ä¸‹ï¼š123456789val uri = new URI(&quot;hdfs://hadoop001:9001&quot;)val fs = FileSystem.get(uri,conf)val listfiles = fs.listFiles(new Path(&quot;/data&quot;),true) while (listfiles.hasNext) &#123; val nextfile = listfiles.next() println(&quot;get file path:&quot; + nextfile.getPath().toString()) &#125;------------------------------è¿è¡Œç»“æœ---------------------------------get file path:hdfs://hadoop001:9001/data/infos.txtåœ¨æœ¬åœ°æœºå™¨ä½¿ç”¨SparkSQLè¯»å–hdfsä¸Šçš„æ–‡ä»¶å¹¶è½¬æ¢ä¸ºDFçš„è¿‡ç¨‹ä¸­1234567891011object SparkSQLApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkSQLApp&quot;).master(&quot;local[2]&quot;).getOrCreate() val info = spark.sparkContext.textFile(&quot;/data/infos.txt&quot;) import spark.implicits._ val infoDF = info.map(_.split(&quot;,&quot;)).map(x=&gt;Info(x(0).toInt,x(1),x(2).toInt)).toDF() infoDF.show() spark.stop() &#125; case class Info(id:Int,name:String,age:Int)&#125;å‡ºç°å¦‚ä¸‹æŠ¥é”™ä¿¡æ¯ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667.... .... .... 19/02/23 16:07:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0) 19/02/23 16:07:00 INFO HadoopRDD: Input split: hdfs://hadoop001:9001/data/infos.txt:0+17 19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ..... .... 19/02/23 16:07:21 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry... 19/02/23 16:07:21 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 272.617680460432 msec. 19/02/23 16:07:42 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ... ... 19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3499) ... ... 19/02/23 16:08:12 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) ... ... 19/02/23 16:08:12 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry... 19/02/23 16:08:12 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11918.913311370841 msec. 19/02/23 16:08:45 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ... ... 19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException 19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException 19/02/23 16:08:45 WARN DFSClient: DFS Read org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... 19/02/23 16:08:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:648) ... ... 19/02/23 16:08:45 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job 19/02/23 16:08:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 19/02/23 16:08:45 INFO TaskSchedulerImpl: Cancelling stage 0 19/02/23 16:08:45 INFO DAGScheduler: ResultStage 0 (show at SparkSQLApp.scala:30) failed in 105.618 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... é—®é¢˜åˆ†ææœ¬åœ° Shell å¯ä»¥æ­£å¸¸æ“ä½œï¼Œæ’é™¤é›†ç¾¤æ­å»ºå’Œè¿›ç¨‹æ²¡æœ‰å¯åŠ¨çš„é—®é¢˜äº‘ä¸»æœºæ²¡æœ‰è®¾ç½®é˜²ç«å¢™ï¼Œæ’é™¤é˜²ç«å¢™æ²¡å…³çš„é—®é¢˜äº‘æœåŠ¡å™¨é˜²ç«å¢™å¼€æ”¾äº† DataNode ç”¨äºæ•°æ®ä¼ è¾“æœåŠ¡ç«¯å£ é»˜è®¤æ˜¯ 50010æˆ‘åœ¨æœ¬åœ°æ­å»ºäº†å¦ä¸€å°è™šæ‹Ÿæœºï¼Œè¯¥è™šæ‹Ÿæœºå’Œæœ¬åœ°åœ¨åŒä¸€å±€åŸŸç½‘ï¼Œæœ¬åœ°å¯ä»¥æ­£å¸¸æ“ä½œè¯¥è™šæ‹Ÿæœºçš„hdfsï¼ŒåŸºæœ¬ç¡®å®šäº†æ˜¯ç”±äºå†…å¤–ç½‘çš„åŸå› ã€‚æŸ¥é˜…èµ„æ–™å‘ç° HDFS ä¸­çš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶åéƒ½æ˜¯å­˜æ”¾åœ¨ NameNode ä¸Šï¼Œæ“ä½œä¸éœ€è¦å’Œ DataNode é€šä¿¡ï¼Œå› æ­¤å¯ä»¥æ­£å¸¸åˆ›å»ºæ–‡ä»¶å¤¹å’Œåˆ›å»ºæ–‡ä»¶è¯´æ˜æœ¬åœ°å’Œè¿œç¨‹ NameNode é€šä¿¡æ²¡æœ‰é—®é¢˜ã€‚é‚£ä¹ˆå¾ˆå¯èƒ½æ˜¯æœ¬åœ°å’Œè¿œç¨‹ DataNode é€šä¿¡æœ‰é—®é¢˜é—®é¢˜çŒœæƒ³ç”±äºæœ¬åœ°æµ‹è¯•å’Œäº‘ä¸»æœºä¸åœ¨ä¸€ä¸ªå±€åŸŸç½‘ï¼Œhadoopé…ç½®æ–‡ä»¶æ˜¯ä»¥å†…ç½‘ipä½œä¸ºæœºå™¨é—´é€šä¿¡çš„ipã€‚åœ¨è¿™ç§æƒ…å†µä¸‹,æˆ‘ä»¬èƒ½å¤Ÿè®¿é—®åˆ°namenodeæœºå™¨ï¼Œnamenodeä¼šç»™æˆ‘ä»¬æ•°æ®æ‰€åœ¨æœºå™¨çš„ipåœ°å€ä¾›æˆ‘ä»¬è®¿é—®æ•°æ®ä¼ è¾“æœåŠ¡ï¼Œä½†æ˜¯å½“å†™æ•°æ®çš„æ—¶å€™ï¼ŒNameNode å’ŒDataNode æ˜¯é€šè¿‡å†…ç½‘é€šä¿¡çš„ï¼Œè¿”å›çš„æ˜¯datanodeå†…ç½‘çš„ip,æˆ‘ä»¬æ— æ³•æ ¹æ®è¯¥IPè®¿é—®datanodeæœåŠ¡å™¨ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å…¶ä¸­ä¸€éƒ¨åˆ†æŠ¥é”™ä¿¡æ¯ï¼š123419/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.java.net.ConnectException: Connection timed out: no further information...19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue....ä»æŠ¥é”™ä¿¡æ¯ä¸­å¯ä»¥çœ‹å‡ºï¼Œè¿æ¥ä¸åˆ°192.168.137.2:50010ï¼Œä¹Ÿå°±æ˜¯datanodeçš„åœ°å€ï¼Œå› ä¸ºå¤–ç½‘å¿…é¡»è®¿é—®â€œ139.198.18.XXX:50010â€æ‰èƒ½è®¿é—®åˆ°datanodeã€‚ä¸ºäº†èƒ½å¤Ÿè®©å¼€å‘æœºå™¨è®¿é—®åˆ°hdfsï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŸŸåè®¿é—®hdfsï¼Œè®©namenodeè¿”å›ç»™æˆ‘ä»¬datanodeçš„åŸŸåã€‚é—®é¢˜è§£å†³å°è¯•ä¸€ï¼šåœ¨å¼€å‘æœºå™¨çš„hostsæ–‡ä»¶ä¸­é…ç½®datanodeå¯¹åº”çš„å¤–ç½‘ipå’ŒåŸŸåï¼ˆä¸Šæ–‡å·²ç»é…ç½®ï¼‰ï¼Œå¹¶ä¸”åœ¨ä¸hdfsäº¤äº’çš„ç¨‹åºä¸­æ·»åŠ å¦‚ä¸‹ä»£ç :12val conf = new Configuration()conf.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)æŠ¥é”™ä¾æ—§å°è¯•äºŒï¼š123456val spark = SparkSession .builder() .appName(&quot;SparkSQLApp&quot;) .master(&quot;local[2]&quot;) .config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;) .getOrCreate()æŠ¥é”™ä¾æ—§å°è¯•ä¸‰ï¼šåœ¨hdfs-site.xmlä¸­æ·»åŠ å¦‚ä¸‹é…ç½®ï¼š1234&lt;property&gt; &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;è¿è¡ŒæˆåŠŸé€šè¿‡æŸ¥é˜…èµ„æ–™ï¼Œå»ºè®®åœ¨hdfs-site.xmlä¸­å¢åŠ dfs.datanode.use.datanode.hostnameå±æ€§ï¼Œè¡¨ç¤ºdatanodeä¹‹é—´çš„é€šä¿¡ä¹Ÿé€šè¿‡åŸŸåæ–¹å¼1234&lt;property&gt; &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;è¿™æ ·èƒ½å¤Ÿä½¿å¾—æ›´æ¢å†…ç½‘IPå˜å¾—ååˆ†ç®€å•ã€æ–¹ä¾¿ï¼Œè€Œä¸”å¯ä»¥è®©ç‰¹å®šdatanodeé—´çš„æ•°æ®äº¤æ¢å˜å¾—æ›´å®¹æ˜“ã€‚ä½†ä¸æ­¤åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€ä¸ªå‰¯ä½œç”¨ï¼Œå½“DNSè§£æå¤±è´¥æ—¶ä¼šå¯¼è‡´æ•´ä¸ªHadoopä¸èƒ½æ­£å¸¸å·¥ä½œï¼Œæ‰€ä»¥è¦ä¿è¯DNSçš„å¯é æ€»ç»“ï¼šå°†é»˜è®¤çš„é€šè¿‡IPè®¿é—®ï¼Œæ”¹ä¸ºé€šè¿‡åŸŸåæ–¹å¼è®¿é—®ã€‚å‚è€ƒèµ„æ–™https://blog.csdn.net/vaf714/article/details/82996860https://www.cnblogs.com/krcys/p/9146329.htmlhttps://blog.csdn.net/dominic_tiger/article/details/71773656https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark UIç•Œé¢å®ç°åŸç†]]></title>
    <url>%2F2019%2F02%2F22%2FSpark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[å½“Sparkç¨‹åºåœ¨è¿è¡Œæ—¶ï¼Œä¼šæä¾›ä¸€ä¸ªWebé¡µé¢æŸ¥çœ‹Applicationè¿è¡ŒçŠ¶æ€ä¿¡æ¯ã€‚æ˜¯å¦å¼€å¯UIç•Œé¢ç”±å‚æ•°spark.ui.enabled(é»˜è®¤ä¸ºtrue)æ¥ç¡®å®šã€‚ä¸‹é¢åˆ—å‡ºSpark UIä¸€äº›ç›¸å…³é…ç½®å‚æ•°ï¼Œé»˜è®¤å€¼ï¼Œä»¥åŠå…¶ä½œç”¨ã€‚æœ¬æ–‡æ¥ä¸‹æ¥åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†åŸºäºSpark-1.6.0çš„æºç ï¼Œç»“åˆç¬¬äºŒéƒ¨åˆ†çš„å›¾ç‰‡å†…å®¹æ¥æè¿°UIç•Œé¢åœ¨Sparkä¸­çš„å®ç°æ–¹å¼ã€‚ç¬¬äºŒéƒ¨åˆ†ä»¥å®ä¾‹å±•ç¤ºSpark UIç•Œé¢æ˜¾ç¤ºçš„å†…å®¹ã€‚Spark UIç•Œé¢å®ç°æ–¹å¼UIç»„ä»¶ç»“æ„è¿™éƒ¨åˆ†å…ˆè®²UIç•Œé¢çš„å®ç°æ–¹å¼ï¼ŒUIç•Œé¢çš„å®ä¾‹åœ¨æœ¬æ–‡æœ€åä¸€éƒ¨åˆ†ã€‚å¦‚æœå¯¹è¿™éƒ¨åˆ†ä¸­çš„æŸäº›æ¦‚å¿µä¸æ¸…æ¥šï¼Œé‚£ä¹ˆæœ€å¥½å…ˆæŠŠç¬¬äºŒéƒ¨åˆ†äº†è§£ä¸€ä¸‹ã€‚ä»ä¸‹é¢UIç•Œé¢çš„å®ä¾‹å¯ä»¥çœ‹å‡ºï¼Œä¸åŒçš„å†…å®¹ä»¥Tabçš„å½¢å¼å±•ç°åœ¨ç•Œé¢ä¸Šï¼Œå¯¹åº”æ¯ä¸€ä¸ªTabåœ¨ä¸‹æ–¹æ˜¾ç¤ºå…·ä½“å†…å®¹ã€‚åŸºæœ¬ä¸ŠSpark UIç•Œé¢ä¹Ÿæ˜¯æŒ‰è¿™ä¸ªå±‚æ¬¡å…³ç³»å®ç°çš„ã€‚ä»¥SparkUIç±»ä¸ºå®¹å™¨ï¼Œå„ä¸ªTabï¼Œå¦‚JobsTab, StagesTab, ExecutorsTabç­‰é•¶åµŒåœ¨SparkUIä¸Šï¼Œå¯¹åº”å„ä¸ªTabï¼Œæœ‰é¡µé¢å†…å®¹å®ç°ç±»JobPage, StagePage, ExecutorsPageç­‰é¡µé¢ã€‚è¿™äº›ç±»çš„ç»§æ‰¿å’ŒåŒ…å«å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šåˆå§‹åŒ–è¿‡ç¨‹ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼ŒSparkUIç±»å‹çš„å¯¹è±¡æ˜¯UIç•Œé¢çš„æ ¹å¯¹è±¡ï¼Œå®ƒæ˜¯åœ¨SparkContextç±»ä¸­æ„é€ å‡ºæ¥çš„ã€‚12345678910private var _ui: Option[SparkUI] = None //å®šä¹‰_ui = //SparkUIå¯¹è±¡çš„ç”Ÿæˆ if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123; Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener, _env.securityManager, appName, startTime = startTime)) &#125; else &#123; // For tests, do not enable the UI None &#125;_ui.foreach(_.bind()) //å¯åŠ¨jettyã€‚bindæ–¹æ³•ç»§æ‰¿è‡ªWebUIï¼Œè¯¥ç±»è´Ÿè´£å’ŒçœŸå®çš„Jetty Server APIæ‰“äº¤é“ä¸Šé¢è¿™æ®µä»£ç ä¸­å¯ä»¥çœ‹åˆ°SparkUIå¯¹è±¡çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œç»“åˆä¸Šé¢çš„ç±»ç»“æ„å›¾ï¼Œå¯ä»¥çœ‹åˆ°bindæ–¹æ³•ç»§æ‰¿è‡ªWebUIç±»ï¼Œè¿›å…¥WebUIç±»ä¸­1234567891011121314protected val handlers = ArrayBuffer[ServletContextHandler]() // è¿™ä¸ªå¯¹è±¡åœ¨ä¸‹é¢bindæ–¹æ³•ä¸­ä¼šä½¿ç”¨åˆ°ã€‚ protected val pageToHandlers = new HashMap[WebUIPage, ArrayBuffer[ServletContextHandler]] // å°†pageç»‘å®šåˆ°handlersä¸Š /** å°†Http Serverç»‘å®šåˆ°è¿™ä¸ªWebé¡µé¢ */ def bind() &#123; assert(!serverInfo.isDefined, &quot;Attempted to bind %s more than once!&quot;.format(className)) try &#123; serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name)) logInfo(&quot;Started %s at http://%s:%d&quot;.format(className, publicHostName, boundPort)) &#125; catch &#123; case e: Exception =&gt; logError(&quot;Failed to bind %s&quot;.format(className), e) System.exit(1) &#125; &#125;ä¸Šé¢ä»£ç ä¸­handlerså¯¹è±¡ç»´æŒäº†WebUIPageå’ŒJettyä¹‹é—´çš„å…³ç³»ï¼Œorg.eclipse.jetty.servlet.ServletContextHandleræ˜¯æ ‡å‡†jettyå®¹å™¨çš„handlerã€‚è€Œå¯¹è±¡pageToHandlersç»´æŒäº†WebUIPageåˆ°ServletContextHandlerçš„å¯¹åº”å…³ç³»ã€‚å„Tabé¡µä»¥åŠè¯¥é¡µå†…å®¹çš„å®ç°ï¼ŒåŸºæœ¬ä¸Šå¤§åŒå°å¼‚ã€‚æ¥ä¸‹æ¥ä»¥AllJobsPageé¡µé¢ä¸ºä¾‹ä»”ç»†æ¢³ç†é¡µé¢å±•ç¤ºçš„è¿‡ç¨‹ã€‚SparkUIä¸­Tabçš„ç»‘å®šä»ä¸Šé¢çš„ç±»ç»“æ„å›¾ä¸­çœ‹åˆ°WebUIPageæä¾›äº†ä¸¤ä¸ªé‡è¦çš„æ–¹æ³•ï¼Œrenderå’ŒrenderJsonç”¨äºç›¸åº”é¡µé¢è¯·æ±‚ï¼Œåœ¨WebUIPageçš„å®ç°ç±»ä¸­ï¼Œå…·ä½“å®ç°äº†è¿™ä¸¤ä¸ªæ–¹æ³•ã€‚åœ¨SparkContextä¸­æ„é€ å‡ºSparkUIçš„å®ä¾‹åï¼Œä¼šæ‰§è¡ŒSparkUI#initializeæ–¹æ³•è¿›è¡Œåˆå§‹åŒ–ã€‚å¦‚ä¸‹é¢ä»£ç ä¸­ï¼Œè°ƒç”¨SparkUIä»WebUIç»§æ‰¿çš„attacheTabæ–¹æ³•ï¼Œå°†å„Tabé¡µé¢ç»‘å®šåˆ°UIä¸Šã€‚1234567891011121314def initialize() &#123; attachTab(new JobsTab(this)) attachTab(stagesTab) attachTab(new StorageTab(this)) attachTab(new EnvironmentTab(this)) attachTab(new ExecutorsTab(this)) attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;)) attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath)) attachHandler(ApiRootResource.getServletHandler(this)) // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs attachHandler(createRedirectHandler( &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest, httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;))) &#125;é¡µé¢å†…å®¹ç»‘å®šåˆ°Tabåœ¨ä¸Šä¸€èŠ‚ä¸­ï¼ŒJobsTabæ ‡ç­¾ç»‘å®šåˆ°SparkUIä¸Šä¹‹åï¼Œåœ¨JobsTabä¸Šç»‘å®šäº†AllJobsPageå’ŒJobPageç±»ã€‚AllJobsPageé¡µé¢å³è®¿é—®SparkUIé¡µé¢æ—¶åˆ—ä¸¾å‡ºæ‰€æœ‰Jobçš„é‚£ä¸ªé¡µé¢ï¼ŒJobPageé¡µé¢åˆ™æ˜¯ç‚¹å‡»å•ä¸ªJobæ—¶è·³è½¬çš„é¡µé¢ã€‚é€šè¿‡è°ƒç”¨JobsTabä»WebUITabç»§æ‰¿çš„attachPageæ–¹æ³•ä¸JobsTabè¿›è¡Œç»‘å®šã€‚1234567891011private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123; val sc = parent.sc val killEnabled = parent.killEnabled val jobProgresslistener = parent.jobProgressListener val executorListener = parent.executorsListener val operationGraphListener = parent.operationGraphListener def isFairScheduler: Boolean = jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR) attachPage(new AllJobsPage(this)) attachPage(new JobPage(this))&#125;é¡µé¢å†…å®¹çš„å±•ç¤ºçŸ¥é“äº†AllJobsPageé¡µé¢å¦‚ä½•ç»‘å®šåˆ°SparkUIç•Œé¢åï¼Œæ¥ä¸‹æ¥åˆ†æè¿™ä¸ªé¡µé¢çš„å†…å®¹æ˜¯å¦‚ä½•æ˜¾ç¤ºçš„ã€‚è¿›å…¥AllJobsPageç±»ï¼Œä¸»è¦è§‚å¯Ÿrenderæ–¹æ³•ã€‚åœ¨é¡µé¢å±•ç¤ºä¸ŠSparkç›´æ¥åˆ©ç”¨äº†Scalaå¯¹html/xmlçš„è¯­æ³•æ”¯æŒï¼Œå°†é¡µé¢çš„Htmlä»£ç åµŒå…¥Scalaç¨‹åºä¸­ã€‚å…·ä½“çš„é¡µé¢ç”Ÿæˆè¿‡ç¨‹å¯ä»¥æŸ¥çœ‹ä¸‹é¢æºç ä¸­çš„æ³¨é‡Šã€‚è¿™é‡Œå¯ä»¥ç»“åˆç¬¬äºŒéƒ¨åˆ†çš„å®ä¾‹è¿›è¡ŒæŸ¥çœ‹ã€‚123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687def render(request: HttpServletRequest): Seq[Node] = &#123; val listener = parent.jobProgresslistener //è·å–jobProgresslistenerå¯¹è±¡ï¼Œé¡µé¢å±•ç¤ºçš„æ•°æ®éƒ½æ˜¯ä»è¿™é‡Œè¯»å– listener.synchronized &#123; val startTime = listener.startTime // è·å–applicationçš„å¼€å§‹æ—¶é—´ï¼Œé»˜è®¤å€¼ä¸º-1L val endTime = listener.endTime // è·å–applicationçš„ç»“æŸæ—¶é—´ï¼Œé»˜è®¤å€¼ä¸º-1L val activeJobs = listener.activeJobs.values.toSeq // è·å–å½“å‰applicationä¸­å¤„äºactiveçŠ¶æ€çš„job val completedJobs = listener.completedJobs.reverse.toSeq // è·å–å½“å‰applicationä¸­å®ŒæˆçŠ¶æ€çš„job val failedJobs = listener.failedJobs.reverse.toSeq // è·å–å½“å‰applicationä¸­å¤±è´¥çŠ¶æ€çš„job val activeJobsTable = jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse) val completedJobsTable = jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val failedJobsTable = jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val shouldShowActiveJobs = activeJobs.nonEmpty val shouldShowCompletedJobs = completedJobs.nonEmpty val shouldShowFailedJobs = failedJobs.nonEmpty val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123; s&quot;$&#123;completedJobs.size&#125;&quot; &#125; else &#123; s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot; &#125; val summary: NodeSeq = &lt;div&gt; &lt;ul class=&quot;unstyled&quot;&gt; &lt;li&gt; &lt;strong&gt;Total Uptime:&lt;/strong&gt; // æ˜¾ç¤ºå½“å‰Sparkåº”ç”¨è¿è¡Œæ—¶é—´ &#123;// å¦‚æœè¿˜æ²¡æœ‰ç»“æŸï¼Œå°±ç”¨ç³»ç»Ÿå½“å‰æ—¶é—´å‡å¼€å§‹æ—¶é—´ã€‚å¦‚æœå·²ç»ç»“æŸï¼Œå°±ç”¨ç»“æŸæ—¶é—´å‡å¼€å§‹æ—¶é—´ if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123; UIUtils.formatDuration(System.currentTimeMillis() - startTime) &#125; else if (endTime &gt; 0) &#123; UIUtils.formatDuration(endTime - startTime) &#125; &#125; &lt;/li&gt; &lt;li&gt; &lt;strong&gt;Scheduling Mode: &lt;/strong&gt; // æ˜¾ç¤ºè°ƒåº¦æ¨¡å¼ï¼ŒFIFOæˆ–FAIR &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125; &lt;/li&gt; &#123; if (shouldShowActiveJobs) &#123; // å¦‚æœæœ‰activeçŠ¶æ€çš„jobï¼Œåˆ™æ˜¾ç¤ºActive Jobsæœ‰å¤šå°‘ä¸ª &lt;li&gt; &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;activeJobs.size&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowCompletedJobs) &#123; // å¦‚æœæœ‰å®ŒæˆçŠ¶æ€çš„jobï¼Œåˆ™æ˜¾ç¤ºCompleted Jobsçš„ä¸ªæ•° &lt;li id=&quot;completed-summary&quot;&gt; &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;completedJobNumStr&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowFailedJobs) &#123; // å¦‚æœæœ‰å¤±è´¥çŠ¶æ€çš„jobï¼Œåˆ™æ˜¾ç¤ºFailed Jobsçš„ä¸ªæ•° &lt;li&gt; &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;listener.numFailedJobs&#125; &lt;/li&gt; &#125; &#125; &lt;/ul&gt; &lt;/div&gt; var content = summary // å°†ä¸Šé¢çš„htmlä»£ç å†™å…¥contentå˜é‡ï¼Œåœ¨æœ€åç»Ÿä¸€æ˜¾ç¤ºcontentä¸­çš„å†…å®¹ val executorListener = parent.executorListener // è¿™é‡Œè·å–EventTimelineä¸­çš„ä¿¡æ¯ content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs, executorListener.executorIdToData, startTime)// ç„¶åæ ¹æ®å½“å‰applicationä¸­æ˜¯å¦å­˜åœ¨activeï¼Œ failedï¼Œ completedçŠ¶æ€çš„jobï¼Œå°†è¿™äº›ä¿¡æ¯æ˜¾ç¤ºåœ¨é¡µé¢ä¸Šã€‚ if (shouldShowActiveJobs) &#123; content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++ activeJobsTable // ç”ŸæˆactiveçŠ¶æ€jobçš„å±•ç¤ºè¡¨æ ¼ï¼Œå…·ä½“å½¢å¼å¯å‚çœ‹ç¬¬äºŒéƒ¨åˆ†ã€‚æŒ‰æäº¤æ—¶é—´å€’åºæ’åˆ— &#125; if (shouldShowCompletedJobs) &#123; content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++ completedJobsTable &#125; if (shouldShowFailedJobs) &#123; content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++ failedJobsTable &#125; val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; + &quot; Click on a job to see information about the stages of tasks inside it.&quot; UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText)) // æœ€åå°†contentä¸­çš„æ‰€æœ‰å†…å®¹å…¨éƒ¨å±•ç¤ºåœ¨é¡µé¢ä¸Š &#125; &#125;æ¥ä¸‹æ¥ä»¥activeJobsTableä»£ç ä¸ºä¾‹åˆ†æJobsä¿¡æ¯å±•ç¤ºè¡¨æ ¼çš„ç”Ÿæˆã€‚è¿™é‡Œä¸»è¦çš„æ–¹æ³•æ˜¯makeRowï¼Œæ¥æ”¶çš„æ˜¯ä¸Šé¢ä»£ç ä¸­çš„activeJobs, completedJobs, failedJobsã€‚è¿™ä¸‰ä¸ªå¯¹è±¡éƒ½æ˜¯åŒ…å«åœ¨JobProgressListenerå¯¹è±¡ä¸­çš„ï¼Œåœ¨JobProgressListenerä¸­çš„å®šä¹‰å¦‚ä¸‹ï¼š1234// è¿™ä¸‰ä¸ªå¯¹è±¡ç”¨äºå­˜å‚¨æ•°æ®çš„ä¸»è¦æ˜¯JobUIDataç±»å‹ï¼Œ val activeJobs = new HashMap[JobId, JobUIData] val completedJobs = ListBuffer[JobUIData]() val failedJobs = ListBuffer[JobUIData]()å°†ä¸Šé¢ä¸‰ä¸ªå¯¹è±¡ä¼ å…¥åˆ°ä¸‹é¢è¿™æ®µä»£ç ä¸­ï¼Œç»§ç»­æ‰§è¡Œã€‚123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123; val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined) val columns: Seq[Node] = &#123; // æ˜¾ç¤ºçš„ä¿¡æ¯åŒ…æ‹¬ï¼ŒJob Id(Job Group)ä»¥åŠJobæè¿°ï¼ŒJobæäº¤æ—¶é—´ï¼ŒJobè¿è¡Œæ—¶é—´ï¼Œæ€»çš„Stage/Taskæ•°ï¼ŒæˆåŠŸçš„Stage/Taskæ•°ï¼Œä»¥åŠä¸€ä¸ªè¿›åº¦æ¡ &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;th&gt;Submitted&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt; &#125; def makeRow(job: JobUIData): Seq[Node] = &#123; val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job) val duration: Option[Long] = &#123; job.submissionTime.map &#123; start =&gt; // Jobè¿è¡Œæ—¶é•¿ä¸ºç³»ç»Ÿæ—¶é—´ï¼Œæˆ–è€…ç»“æŸæ—¶é—´å‡å»å¼€å§‹æ—¶é—´ val end = job.completionTime.getOrElse(System.currentTimeMillis()) end - start &#125; &#125; val formattedDuration = duration.map(d =&gt; // æ ¼å¼åŒ–ä»»åŠ¡è¿è¡Œæ—¶é—´ï¼Œæ˜¾ç¤ºä¸ºa h:b m:c sæ ¼å¼UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;) val formattedSubmissionTime = // è·å–Jobæäº¤æ—¶é—´job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;) val jobDescription = UIUtils.makeDescription(lastStageDescription, parent.basePath) // è·å–ä»»åŠ¡æè¿° val detailUrl = // ç‚¹å‡»å•ä¸ªJobä¸‹é¢é“¾æ¥è·³è½¬åˆ°JobPageé¡µé¢ï¼Œä¼ å…¥å‚æ•°ä¸ºjobId &quot;%s/jobs/job?id=%s&quot;.format(UIUtils.prependBaseUri(parent.basePath), job.jobId) &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt; &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt; &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125; &lt;/td&gt; &lt;td&gt; &#123;jobDescription&#125; &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt; &lt;/td&gt; &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt; &#123;formattedSubmissionTime&#125; &lt;/td&gt; &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt; &lt;td class=&quot;stage-progress-cell&quot;&gt; &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125; &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125; &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125; &lt;/td&gt; &lt;td class=&quot;progress-cell&quot;&gt; // è¿›åº¦æ¡ &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks, failed = job.numFailedTasks, skipped = job.numSkippedTasks, total = job.numTasks - job.numSkippedTasks)&#125; &lt;/td&gt; &lt;/tr&gt; &#125; &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt; &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt; // æ˜¾ç¤ºåˆ—å &lt;tbody&gt; &#123;jobs.map(makeRow)&#125; // è°ƒç”¨ä¸Šé¢çš„rowç”Ÿæˆæ–¹æ³•ï¼Œå…·ä½“æ˜¾ç¤ºJobä¿¡æ¯ &lt;/tbody&gt; &lt;/table&gt; &#125;ä»ä¸Šé¢è¿™äº›ä»£ç ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒJobé¡µé¢æ˜¾ç¤ºçš„æ‰€æœ‰æ•°æ®ï¼Œéƒ½æ˜¯ä»JobProgressListenerå¯¹è±¡ä¸­è·å¾—çš„ã€‚SparkUIå¯ä»¥ç†è§£æˆä¸€ä¸ªJobProgressListenerå¯¹è±¡çš„æ¶ˆè´¹è€…ï¼Œé¡µé¢ä¸Šæ˜¾ç¤ºçš„å†…å®¹éƒ½æ˜¯JobProgressListenerå†…åœ¨çš„å±•ç°ã€‚Spark UIç•Œé¢å®ä¾‹é»˜è®¤æƒ…å†µä¸‹ï¼Œå½“ä¸€ä¸ªSpark Applicationè¿è¡Œèµ·æ¥åï¼Œå¯ä»¥é€šè¿‡è®¿é—®hostname:4040ç«¯å£æ¥è®¿é—®UIç•Œé¢ã€‚hostnameæ˜¯æäº¤ä»»åŠ¡çš„Sparkå®¢æˆ·ç«¯ipåœ°å€ï¼Œç«¯å£å·ç”±å‚æ•°spark.ui.port(é»˜è®¤å€¼4040ï¼Œå¦‚æœè¢«å ç”¨åˆ™é¡ºåºå¾€åæ¢æŸ¥)æ¥ç¡®å®šã€‚ç”±äºå¯åŠ¨ä¸€ä¸ªApplicationå°±ä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„UIç•Œé¢ï¼Œæ‰€ä»¥å¦‚æœå¯åŠ¨æ—¶é»˜è®¤çš„4040ç«¯å£å·è¢«å ç”¨ï¼Œåˆ™å°è¯•4041ç«¯å£ï¼Œå¦‚æœè¿˜æ˜¯è¢«å ç”¨åˆ™å°è¯•4042ï¼Œä¸€ç›´æ‰¾åˆ°ä¸€ä¸ªå¯ç”¨ç«¯å£å·ä¸ºæ­¢ã€‚ä¸‹é¢å¯åŠ¨ä¸€ä¸ªSpark ThriftServeræœåŠ¡ï¼Œå¹¶ç”¨beelineå‘½ä»¤è¿æ¥è¯¥æœåŠ¡ï¼Œæäº¤sqlè¯­å¥è¿è¡Œã€‚åˆ™ThriftServerå¯¹åº”ä¸€ä¸ªApplicationï¼Œæ¯ä¸ªsqlè¯­å¥å¯¹åº”ä¸€ä¸ªJobï¼ŒæŒ‰ç…§Jobçš„é€»è¾‘åˆ’åˆ†Stageå’ŒTaskã€‚Jobsé¡µé¢è¿æ¥ä¸Šè¯¥ç«¯å£åï¼Œæ˜¾ç¤ºçš„å°±æ˜¯ä¸Šé¢çš„é¡µé¢ï¼Œä¹Ÿæ˜¯Jobçš„ä¸»é¡µé¢ã€‚è¿™é‡Œä¼šæ˜¾ç¤ºæ‰€æœ‰Activeï¼ŒCompleted, Cancledä»¥åŠFailedçŠ¶æ€çš„Jobã€‚é»˜è®¤æƒ…å†µä¸‹æ€»å…±æ˜¾ç¤º1000æ¡Jobä¿¡æ¯ï¼Œè¿™ä¸ªæ•°å€¼ç”±å‚æ•°spark.ui.retainedJobs(é»˜è®¤å€¼1000)æ¥ç¡®å®šã€‚ä»ä¸Šé¢è¿˜çœ‹åˆ°ï¼Œé™¤äº†Jobsé€‰é¡¹å¡ä¹‹å¤–ï¼Œè¿˜å¯æ˜¾ç¤ºStages, Storage, Enviroment, Executors, SQLä»¥åŠJDBC/ODBC Serveré€‰é¡¹å¡ã€‚åˆ†åˆ«å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚Stagesé¡µé¢Storageé¡µé¢Enviromenté¡µé¢Executorsé¡µé¢å•ä¸ªJobåŒ…å«çš„Stagesé¡µé¢Taské¡µé¢]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkç›‘æ§æŠ¥é”™javax.servlet.http.HttpServletRequest.isAsyncStarted]]></title>
    <url>%2F2019%2F02%2F16%2FSpark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted%2F</url>
    <content type="text"><![CDATA[ç¯å¢ƒSpark2.2.1Hadoop2.6IntelljScala2.11pomæ–‡ä»¶1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.common.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.hdfs.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.client.version&#125;&lt;/version&gt;&lt;/dependency&gt;æŠ¥é”™ä¿¡æ¯å¦‚ä¸‹æ‰€ç¤ºï¼š12345678910111213141516171819202122232425262728293031323334353637java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Zat org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)at org.spark_project.jetty.server.Server.handle(Server.java:499)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN HttpChannel: Could not send response error 500: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z16/11/08 21:37:43 WARN HttpChannel: /jobs/java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Zat org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)at org.spark_project.jetty.server.Server.handle(Server.java:499)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN QueuedThreadPool: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletResponse.getStatus()Iat org.spark_project.jetty.server.handler.ErrorHandler.handle(ErrorHandler.java:112)at org.spark_project.jetty.server.Response.sendError(Response.java:597)at org.spark_project.jetty.server.HttpChannel.handleException(HttpChannel.java:487)at org.spark_project.jetty.server.HttpConnection$HttpChannelOverHttp.handleException(HttpConnection.java:594)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:387)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN QueuedThreadPool: Unexpected thread death: org.spark_project.jetty.util.thread.QueuedThreadPool$3@3ec5063f in SparkUI&#123;STARTED,8&lt;=8&lt;=200,i=4,q=0&#125;é—®é¢˜è§£å†³æŸ¥çœ‹æŠ¥é”™ä¿¡æ¯1java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Zæœªæ‰¾åˆ°HttpServletRequestç±»ä¸­çš„isAsyncStartedæ–¹æ³•ã€‚é—®é¢˜å®šä½ä½¿ç”¨æœç´¢åŠŸèƒ½ï¼ŒæŸ¥çœ‹è¯¥ç±»å­˜åœ¨äºå“ªäº›åŒ…ä¸‹ã€‚é—®é¢˜è§£å†³æ‰€æœ‰æ¶‰åŠåˆ°è¯¥ç±»jaræ–‡ä»¶ä¸”ç‰ˆæœ¬ä½äº3.0çš„å‡éœ€è¦è¿›è¡Œåˆ é™¤ã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å¤©èµ·åºŠç¬¬ä¸€å¥ï¼Œçœ‹çœ‹Sparkè°ƒåº¦å™¨]]></title>
    <url>%2F2019%2F01%2F18%2FSpark%E8%B0%83%E5%BA%A6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[ä¹‹å‰å‘¢ï¼Œæˆ‘ä»¬è¯¦ç»†åœ°åˆ†æäº†DAGSchedulerçš„æ‰§è¡Œè¿‡ç¨‹ï¼Œæˆ‘ä»¬çŸ¥é“ï¼ŒRDDå½¢æˆçš„DAGç»è¿‡DAGSchedulerï¼Œä¾æ®shuffleå°†DAGåˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªstageï¼Œå†ç”±taskScheduleræäº¤taskåˆ°executorä¸­æ‰§è¡Œï¼Œé‚£ä¹ˆæ‰§è¡Œtaskçš„è¿‡ç¨‹ï¼Œå°±éœ€è¦è°ƒåº¦å™¨æ¥å‚ä¸äº†ã€‚Sparkè°ƒåº¦å™¨ä¸»è¦æœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¹Ÿæ˜¯å¤§å®¶è€³ç†Ÿèƒ½è¯¦çš„FIFOå’ŒFAIRæ¨¡å¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkæ˜¯FIFOï¼ˆå…ˆå…¥å…ˆå‡ºï¼‰æ¨¡å¼ï¼Œå³è°å…ˆæäº¤è°å…ˆæ‰§è¡Œã€‚è€ŒFAIRï¼ˆå…¬å¹³è°ƒåº¦ï¼‰æ¨¡å¼ä¼šåœ¨è°ƒåº¦æ± ä¸­ä¸ºä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¯ä»¥æœ‰ä¸åŒçš„æƒé‡ï¼Œæ ¹æ®æƒé‡æ¥å†³å®šæ‰§è¡Œé¡ºåºã€‚é‚£ä¹ˆæºç ä¸­æ˜¯æ€ä¹ˆå®ç°çš„å‘¢ï¼Ÿé¦–å…ˆï¼Œå½“Stageåˆ’åˆ†å¥½ï¼Œä¼šè°ƒç”¨TaskSchedulerImpl.submitTasks()æ–¹æ³•ï¼Œä»¥TaskSetçš„å½¢å¼æäº¤ç»™TaskSchedulerï¼Œå¹¶åˆ›å»ºä¸€ä¸ªTaskSetMangerå¯¹è±¡æ·»åŠ è¿›è°ƒåº¦æ± ã€‚1234567891011override def submitTasks(taskSet: TaskSet) &#123; val tasks = taskSet.tasks //.... this.synchronized &#123; val manager = createTaskSetManager(taskSet, maxTaskFailures) val stage = taskSet.stageId val stageTaskSets = taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager]) stageTaskSets(taskSet.stageAttemptId) = manager //..... schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)SchedulerBulideré€šè¿‡TaskSchedulerImpl.initialize()è¿›è¡Œäº†å®ä¾‹åŒ–ï¼Œå¹¶è°ƒç”¨äº†SchedulerBulider.buildPools()æ–¹æ³•ã€‚å…·ä½“æ€ä¹ˆä¸ªbuildï¼Œå°±è¦çœ‹ç”¨æˆ·é€‰æ‹©çš„schedulingModeäº†ã€‚123456789101112131415def initialize(backend: SchedulerBackend) &#123; this.backend = backend schedulableBuilder = &#123; schedulingMode match &#123; case SchedulingMode.FIFO =&gt; new FIFOSchedulableBuilder(rootPool) case SchedulingMode.FAIR =&gt; new FairSchedulableBuilder(rootPool, conf) case _ =&gt; throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; + s&quot;$schedulingMode&quot;) &#125; &#125; schedulableBuilder.buildPools() &#125;ç„¶åæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸¤ä¸ªè°ƒåº¦å™¨çš„buildPools()æ–¹æ³•ã€‚123override def buildPools() &#123; // nothing &#125;FIFOä»€ä¹ˆä¹Ÿæ²¡å¹²~~1234567891011121314151617181920212223242526272829303132333435override def buildPools() &#123; var fileData: Option[(InputStream, String)] = None try &#123; fileData = schedulerAllocFile.map &#123; f =&gt; val fis = new FileInputStream(f) logInfo(s&quot;Creating Fair Scheduler pools from $f&quot;) Some((fis, f)) &#125;.getOrElse &#123; val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE) if (is != null) &#123; logInfo(s&quot;Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE&quot;) Some((is, DEFAULT_SCHEDULER_FILE)) &#125; else &#123; logWarning(&quot;Fair Scheduler configuration file not found so jobs will be scheduled in &quot; + s&quot;FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or &quot; + s&quot;set $SCHEDULER_ALLOCATION_FILE_PROPERTY to a file that contains the configuration.&quot;) None &#125; &#125; fileData.foreach &#123; case (is, fileName) =&gt; buildFairSchedulerPool(is, fileName) &#125; &#125; catch &#123; case NonFatal(t) =&gt; val defaultMessage = &quot;Error while building the fair scheduler pools&quot; val message = fileData.map &#123; case (is, fileName) =&gt; s&quot;$defaultMessage from $fileName&quot; &#125; .getOrElse(defaultMessage) logError(message, t) throw t &#125; finally &#123; fileData.foreach &#123; case (is, fileName) =&gt; is.close() &#125; &#125; // finally create &quot;default&quot; pool buildDefaultPool() &#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å†è°ˆï¼ŒæŸå¤´æ¡å…¬å¸Sparkç»“æ„åŒ–æµçš„SQLå®ç°]]></title>
    <url>%2F2019%2F01%2F10%2F%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[å‰é¢ä»‹ç»äº†å¤§æ¦‚çš„ä½¿ç”¨è¯­å¥ï¼Œæ¥ä¸‹æ¥è®²è§£åŸºæœ¬çš„åŠŸèƒ½ç‚¹çš„å®ç°ã€‚SQLè¯­å¥çš„è§£æ(è§£æéƒ¨åˆ†ä¸ºå¼€æºé¡¹ç›®flinkStreamSQLå†…å®¹ï¼Œç›´æ¥æ‹¿è¿‡æ¥ç”¨)123456789101112131415CREATE TABLE SocketTable( word String, valuecount int)WITH( type=&apos;socket&apos;, host=&apos;hadoop-sh1-core1&apos;, port=&apos;9998&apos;, delimiter=&apos; &apos;);create SINK console()WITH( type=&apos;console&apos;, outputmode=&apos;complete&apos;);insert into console select word,count(*) from SocketTable group by word;å°†createçš„å†…å®¹æ ¹æ®æ­£åˆ™è§£æå‡ºæ¥ï¼Œå°†fieldå’Œé…ç½®ç›¸å…³çš„å†…å®¹è§£æå‡ºæ¥ã€‚insert intoéƒ¨åˆ†çš„å†…å®¹åˆ™ä½¿ç”¨calsiteè§£æå‡ºinsertéƒ¨åˆ†çš„targetè¡¨å’Œå·²ç»createçš„sourceè¡¨å†…å®¹ã€‚å› ä¸ºsparkæ²¡æœ‰å®šä¹‰å¥½è¡¨ä¹‹åç›´æ¥å¯ä»¥insertçš„å†…å®¹ï¼Œæ‰€ä»¥è¦å°†éœ€è¦sinkçš„targetè§£æå‡ºæ¥å¦å¤–å¤„ç†ã€‚åˆ›å»ºsourceè¾“å…¥123456789CREATE TABLE SocketTable( word String, valuecount int)WITH( type=&apos;socket&apos;, host=&apos;hadoop-sh1-core1&apos;, port=&apos;9998&apos;, delimiter=&apos; &apos;);è§£æå‡ºtypeä¸­çš„å†…å®¹ï¼Œä½¿ç”¨åå°„å¯»æ‰¾åˆ°å¯¹åº”çš„å¤„ç†ç±»ï¼Œè§£æå„ä¸ªå‚æ•°æ˜¯å¦åˆæ³•ï¼Œè·å¾—é»˜è®¤å‚æ•°ç­‰ã€‚è¿™é‡Œå°±ä¼šä½¿ç”¨format(â€˜socketâ€™)çš„æ–¹å¼ï¼Œoptionä¸­åˆ†åˆ«æ˜¯hostå’Œportï¼Œåˆ†éš”ç¬¦æ˜¯â€™ â€˜ç©ºæ ¼ã€‚schemaçš„å®šä¹‰schemaçš„å®šä¹‰spark.readStreamåˆ›å»ºçš„æ˜¯dataframeï¼Œæ¯”å¦‚socketï¼Œå®ƒåˆ›å»ºçš„dfåªæœ‰ä¸€ä¸ªåˆ—ï¼Œschemaæ˜¯valueï¼Œå¦‚æœæ˜¯kafkaçš„è¯å°±æ›´å¤šäº†ã€‚æ¥ä¸‹æ¥å°±æ˜¯å°†å®šä¹‰çš„è¡¨ä¸­çš„fieldèµ‹ç»™dfã€‚æœ¬é¡¹ç›®ä¸­é‡‡ç”¨çš„æ˜¯jsonçš„æ–¹å¼ä¼ schemaï¼Œå…·ä½“åŸå› ä¹Ÿå¾ˆç®€å•ï¼Œtupleä¸è¡Œï¼Œcase classçš„è¯éœ€è¦åŠ¨æ€å˜åŒ–ï¼Œéš¾åº¦å¤§ï¼Œrddæ–¹å¼åœ¨é‡Œé¢è¡Œä¸é€šï¼Œå°±é€šè¿‡jsonæ¥åšäº†ã€‚çª—å£çš„å®šä¹‰flinkä¸­å…¶å®ä¹Ÿæœ‰åœ¨sqlä¸­æ·»åŠ çª—å£ç›¸å…³çš„å­—æ®µï¼Œæ¯”å¦‚group by proctime ä¹‹ç±»çš„ã€‚åœ¨StructuredStreamingInSQLä¸­æ·»åŠ ï¼Œeventtimeæˆ–è€…processtimeçš„window sqlï¼Œçœ‹æºç ä¸­ï¼Œå…¶å®å®šä¹‰ä¸€ä¸ªçª—å£ï¼Œå°±æ˜¯ä¸ºè¿™ä¸ªdfæ·»åŠ äº†ä¸€ä¸ªwindowçš„å­—æ®µï¼Œwindowä¸­æœ‰startã€endç­‰å­—æ®µï¼ŒçŸ¥é“äº†è¿™ä¸ªï¼Œæˆ‘ä»¬åœ¨dfä¸­åªè¦å®šä¹‰çª—å£çš„å­—æ®µè¦†ç›–æ‰é»˜è®¤çš„windowå­—æ®µï¼Œå°±èƒ½ä½¿ç”¨processtimeå’Œeventtimeçš„sqlè¯­å¥å•¦ï¼sinkçš„å¤„ç†å°†createçš„sourceåŠ ä¸Šå®šä¹‰fieldï¼ŒåŠ ä¸Šwindowå­—æ®µä¹‹åï¼Œå°±æ˜¯å°†insert intoçš„sqlè§£æï¼ŒæŠŠtargetçš„è¡¨æ‹¿å‡ºæ¥ï¼Œselectåçš„å†…å®¹æ˜¯é€»è¾‘çš„ä¸»ä½“ï¼Œsqlæ‰§è¡Œçš„å†…å®¹ç»“æŸä¹‹åï¼Œå°±å’Œå‰é¢ä¸€æ ·ï¼Œæ ¹æ®typeä¸­çš„å†…å®¹ï¼Œæ‰¾åˆ°å¯¹åº”çš„sinkå†…å®¹ï¼Œæ‰§è¡ŒwriteStreamã€‚åŠ¨æ€æ·»åŠ åœ¨å¤„ç†ä¸­å¯èƒ½æœ‰è¿™æ ·çš„æƒ…å†µï¼Œæƒ³è¦æ›´æ–°æ‰§è¡Œçš„sqlï¼Œä½†åˆä¸å¸Œæœ›sparkç¨‹åºåœæ­¢ï¼Œè¿™ä¸ªæ—¶å€™å°±å¯ä»¥é€šè¿‡åœ¨zkä¸Šåˆ›å»ºç›‘å¬å™¨çš„æ–¹å¼æ¥å®ç°sqlçš„åŠ¨æ€æ·»åŠ ã€‚åŠ¨æ€çš„æ›¿æ¢çš„å®ç°æ–¹å¼æ˜¯ï¼Œç»“æ„åŒ–æµæŠŠæ‰€æœ‰çš„æŸ¥è¯¢å­˜åœ¨ä¸€ä¸ªmapä¸­ï¼Œkeyæ˜¯jobidï¼Œvalueæ˜¯queryï¼Œé€šè¿‡è·å–æ—§çš„queryçš„idï¼Œå°†å…¶stopï¼Œæ–°çš„queryå°±ä¼šæ— ç¼å¯¹æ¥ï¼Œç”±äºæ˜¯æ–°çš„queryï¼Œbachidç­‰å†…å®¹éƒ½ä¼šä»å¤´å¼€å§‹è®¡ç®—ã€‚åç»­ç›‘æ§ã€è‡ªå®šä¹‰å‡½æ•°ã€å‹æµ‹ã€è°ƒä¼˜ç­‰åŠŸèƒ½(å¾…åˆ†äº«)]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç»“æ„åŒ–æµ</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019å…ƒæ—¦-çº¿ä¸‹é¡¹ç›®ç¬¬11æœŸåœ†æ»¡ç»“æŸ]]></title>
    <url>%2F2019%2F01%2F02%2F2019%E5%85%83%E6%97%A6-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC11%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019å¹´å…ƒæ—¦3å¤©ä¸€å¥è¯ï¼Œä¸Šæµ·å¤ªå†·å°ä¼™ä¼´ä»¬æ¥è‡ªäº”æ¹–å››æµ·åŒ—äº¬ã€æˆéƒ½ã€æ·±åœ³ã€å¤©æ´¥ã€å¹¿å·ã€é‡åº†ç­‰å¤§å®¶ä¸ºäº†ä¸€ä¸ªç›®æ ‡å­¦ä¹ çœŸæ­£ä¼ä¸šçº§å¤§æ•°æ®ç”Ÿäº§é¡¹ç›®ä¸€å¹´æˆ‘ä»¬åªåœ¨èŠ‚å‡æ—¥ä¸¾åŠå…ƒæ—¦3å¤©ï¼Œé”™è¿‡äº†å°±æ˜¯é”™è¿‡äº†æœŸå¾…çº¿ä¸‹é¡¹ç›®ç­ç¬¬12æœŸ]]></content>
      <categories>
        <category>çº¿ä¸‹å®æˆ˜ç­</category>
      </categories>
      <tags>
        <tag>çº¿ä¸‹å®æˆ˜ç­</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘å¸Kafka+Flink+MySQLç”Ÿäº§å®Œæ•´æ¡ˆä¾‹ä»£ç ]]></title>
    <url>%2F2018%2F12%2F20%2F%E6%88%91%E5%8F%B8Kafka%2BFlink%2BMySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1.ç‰ˆæœ¬ä¿¡æ¯ï¼šFlink Version:1.6.2Kafka Version:0.9.0.0MySQL Version:5.6.212.Kafka æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š[IP TIME URL STATU_CODE REFERER]11.74.103.143 2018-12-20 18:12:00 &quot;GET /class/130.html HTTP/1.1&quot; 404 https://search.yahoo.com/search?p=Flinkå®æˆ˜3.å·¥ç¨‹pom.xml12345678910111213141516171819202122232425262728&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;flink.version&gt;1.6.2&lt;/flink.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Flink-Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt;4.sConfç±» å®šä¹‰ä¸MySQLè¿æ¥çš„JDBCçš„å‚æ•°1234567891011package com.soul.conf;/** * @author è‹¥æ³½æ•°æ®soulChun * @create 2018-12-20-15:11 */public class sConf &#123; public static final String USERNAME = &quot;root&quot;; public static final String PASSWORD = &quot;www.ruozedata.com&quot;; public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;&#125;5.MySQLSlinkç±»123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.soul.kafka;import com.soul.conf.sConf;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * @author è‹¥æ³½æ•°æ®soulChun * @create 2018-12-20-15:09 */public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123; try &#123; if (connection == null) &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setString(3, value.f2); preparedStatement.setString(4, value.f3); preparedStatement.setString(5, value.f4); System.out.println(&quot;Start insert&quot;); preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void open(Configuration parms) throws Exception &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; public void close() throws Exception &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;6.æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»1234567891011121314151617181920212223package com.soul.utils;import org.apache.commons.lang3.time.FastDateFormat;import java.util.Date;/** * @author soulChun * @create 2018-12-19-18:44 */public class DateUtils &#123; private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;); private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;); public static Long getTime(String time) throws Exception&#123; return SOURCE_FORMAT.parse(time).getTime(); &#125; public static String parseMinute(String time) throws Exception&#123; return TARGET_FORMAT.format(new Date(getTime(time))); &#125; //æµ‹è¯•ä¸€ä¸‹ public static void main(String[] args) throws Exception&#123; String time = &quot;2018-12-19 18:55:00&quot;; System.out.println(parseMinute(time)); &#125;&#125;7.MySQLå»ºè¡¨123456789create table log_info(ID INT NOT NULL AUTO_INCREMENT,IP VARCHAR(50),TIME VARCHAR(50),CourseID VARCHAR(10),Status_Code VARCHAR(10),Referer VARCHAR(100),PRIMARY KEY ( ID ))ENGINE=InnoDB DEFAULT CHARSET=utf8;8.ä¸»ç¨‹åºï¼šä¸»è¦æ˜¯å°†timeçš„æ ¼å¼è½¬æˆyyyyMMddHHmmss,è¿˜æœ‰å–URLä¸­çš„è¯¾ç¨‹IDï¼Œå°†ä¸æ˜¯/classå¼€å¤´çš„è¿‡æ»¤æ‰ã€‚12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.soul.kafka;import com.soul.utils.DateUtils;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;import java.util.Properties;/** * @author soulChun * @create 2018-12-19-17:23 */public class FlinkCleanKafka &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafkaçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·åˆ†éš” properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeperçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·è¿›è¡Œåˆ†éš” properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flinkçš„æ¶ˆè´¹è€…çš„group.id FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties); DataStream&lt;String&gt; stream = env.addSource(myConsumer);// stream.print().setParallelism(2); DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123; String[] data = value.split(&quot;\\\t&quot;); String CourseID = null; String url = data[2].split(&quot;\\ &quot;)[2]; if (url.startsWith(&quot;/class&quot;)) &#123; String CourseHTML = url.split(&quot;\\/&quot;)[2]; CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));// System.out.println(CourseID); &#125; return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]); &#125; &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123; return value.f2 != null; &#125; &#125;); CleanData.addSink(new MySQLSink()); env.execute(&quot;Flink kafka&quot;); &#125;&#125;9.å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢123456mysql&gt; select count(*) from log_info;+----------+| count(*) |+----------+| 15137 |+----------+Kafkaè¿‡æ¥çš„æ¶ˆæ¯æ˜¯æˆ‘æ¨¡æ‹Ÿçš„ï¼Œä¸€åˆ†é’Ÿäº§ç”Ÿ100æ¡ã€‚ä»¥ä¸Šæ˜¯æˆ‘å¸ç”Ÿäº§é¡¹ç›®ä»£ç çš„æŠ½å–å‡ºæ¥çš„æ¡ˆä¾‹ä»£ç V1ã€‚ç¨åè¿˜æœ‰WaterMarkä¹‹ç±»ä¼šåšåˆ†äº«ã€‚]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkåœ¨æºç¨‹çš„å®è·µï¼ˆäºŒï¼‰]]></title>
    <url>%2F2018%2F12%2F16%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ä»¥ä¸‹å†…å®¹æ¥è‡ªç¬¬ä¸‰å±Šæºç¨‹å¤§æ•°æ®æ²™é¾™ä¸ƒã€é‡åˆ°çš„é—®é¢˜orc splitSparkè¯»å–Hiveè¡¨ç”¨çš„å„ä¸ªæ–‡ä»¶æ ¼å¼çš„InuptFormatï¼Œè®¡ç®—è¯»å–è¡¨éœ€è¦çš„taskæ•°é‡ä¾èµ–äºInputFormat#getSplitsç”±äºå¤§éƒ¨åˆ†è¡¨çš„å­˜å‚¨æ ¼å¼ä¸»è¦ä½¿ç”¨çš„æ˜¯orcï¼Œå½“ä¸€ä¸ªorcæ–‡ä»¶è¶…è¿‡256MBï¼Œsplitç®—æ³•å¹¶è¡Œå»è¯»å–orcå…ƒæ•°æ®ï¼Œæœ‰æ—¶å€™Driverå†…å­˜é£™å‡ï¼ŒOOM crashï¼ŒFull GCå¯¼è‡´network timeoutï¼Œspark context stopHiveè¯»è¿™äº›å¤§è¡¨ä¸ºä½•æ²¡æœ‰é—®é¢˜ï¼Ÿå› ä¸ºHiveé»˜è®¤ä½¿ç”¨çš„æ˜¯CombineHiveInputFormatï¼Œsplitæ˜¯åŸºäºæ–‡ä»¶å¤§å°çš„ã€‚Sparkä¹Ÿéœ€è¦å®ç°ç±»ä¼¼äºHiveçš„CombineInputFormatï¼Œè¿˜èƒ½è§£å†³å°æ–‡ä»¶è¿‡å¤šå¯¼è‡´æäº¤taskæ•°é‡è¿‡å¤šçš„é—®é¢˜ã€‚Executor Container killedExecutor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverheadåŸå› ï¼š1.Shuffle Readæ—¶nettyå †å¤–å†…å­˜çš„ä½¿ç”¨2.Window function spill thresholdè¿‡å°ï¼Œå¯¼è‡´æ¯4096æ¡æˆ–è€…64MBä¸ºä¸€ä¸ªæ–‡ä»¶å†™åˆ°ç£ç›˜å¤–éƒ¨æ’åºåŒæ—¶æ‰“å¼€æ¯ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶å ç”¨1MBçš„å †å¤–å†…å­˜ï¼Œå¯¼è‡´containerä½¿ç”¨çš„å†…å­˜è¿œè¶…è¿‡ç”³è¯·çš„å†…å­˜ï¼Œé‚è¢«yarn killã€‚è§£å†³ï¼šPatchï¼š[SPARK-19659] Fetch big blocks to disk when shuffle-read[SPARK-21369][CORE] Donâ€™t use Scala Tuple2 in common/network-å‚æ•°ï¼šspark.reducer.maxReqSizeShuffleToMem=209715200Patchï¼š[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArrayå‚æ•°ï¼šspark.sql.windowExec.buffer.in.memory.threshold=4096spark.sql.windowExec.buffer.spill.threshold= 1024 1024 * 1024 / 2å°æ–‡ä»¶é—®é¢˜Sparkå†™æ•°æ®æ—¶ç”Ÿæˆå¾ˆå¤šå°æ–‡ä»¶ï¼Œå¯¹NameNodeäº§ç”Ÿå·¨å¤§çš„å‹åŠ›ï¼Œåœ¨ä¸€å¼€å§‹Sparkç°åº¦ä¸Šçº¿çš„æ—¶å€™ï¼Œæ–‡ä»¶æ•°å’ŒBlockæ•°é£™å‡ï¼Œæ–‡ä»¶å˜å°å¯¼è‡´å‹ç¼©ç‡é™ä½ï¼Œå®¹é‡ä¹Ÿè·Ÿç€ä¸Šå»ã€‚ç§»æ¤Hive MergeFileTaskçš„å®ç°åœ¨Sparkæœ€åå†™ç›®æ ‡è¡¨çš„é˜¶æ®µè¿½åŠ å…¥äº†ä¸€ä¸ªMergeFileTaskï¼Œå‚è€ƒäº†Hiveçš„å®ç°org.apache.hadoop.hive.ql.io.merge.MergeFileTaskorg.apache.hadoop.hive.ql.exec.OrcFileMergeOperatoræ— æ•°æ®çš„æƒ…å†µä¸‹ä¸åˆ›å»ºç©ºæ–‡ä»¶[SPARK-21435][SQL]Empty files should be skipped while write to fileå…«ã€ä¼˜åŒ–1.æŸ¥è¯¢åˆ†åŒºè¡¨æ—¶æ”¯æŒbroadcast joinï¼ŒåŠ é€ŸæŸ¥è¯¢2.å‡å°‘Broadcast joinçš„å†…å­˜å‹åŠ› SPARK-221703.Fetchå¤±è´¥åèƒ½å¿«é€Ÿå¤±è´¥ï¼Œä»¥å…ä½œä¸šå¡å‡ ä¸ªå°æ—¶ SPARK-197534.Spark Thrift Serverç¨³å®šæ€§ç»å¸¸æŒ‚æ‰ï¼Œæ—¥å¿—é‡Œå¼‚å¸¸ï¼Œmore than one active taskSet for stageApply SPARK-23433ä»æœ‰å°‘æ•°æŒ‚æ‰çš„æƒ…å†µï¼Œæäº¤SPARK-24677åˆ°ç¤¾åŒºï¼Œä¿®å¤ä¹‹5.ä½œä¸šhangä½ SPARK-21834 SPARK-19326 SPARK-11334ä¹ã€æœªæ¥è®¡åˆ’è‡ªåŠ¨è°ƒä¼˜å†…å­˜æ‰‹æœºspark driverå’Œexecutorå†…å­˜ä½¿ç”¨æƒ…å†µæ ¹æ®ä½œä¸šå†å²çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œåœ¨è°ƒåº¦ç³»ç»Ÿç«¯è‡ªåŠ¨è®¾ç½®åˆé€‚çš„å†…å­˜https://github.com/uber-common/jvm-profilerspark adaptiveåŠ¨æ€è°ƒæ•´æ‰§è¡Œè®¡åˆ’ SortMergeJoinè½¬åŒ–ä¸ºBroadcastHashJoinåŠ¨æ€å¤„ç†æ•°æ®å€¾æ–œhttps://issues.apache.org/jira/browse/SPARK-23128https://github.com/Intel-bigdata/spark-adaptive]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkåœ¨æºç¨‹çš„å®è·µï¼ˆä¸€ï¼‰]]></title>
    <url>%2F2018%2F12%2F09%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ä¸€ã€Sparkåœ¨æºç¨‹åº”ç”¨çš„ç°çŠ¶é›†ç¾¤è§„æ¨¡ï¼šå¹³å‡æ¯å¤©MRä»»åŠ¡æ•°ï¼š30W+å¼€å‘å¹³å°ï¼šè°ƒåº¦ç³»ç»Ÿè¿è¡Œçš„ä»»åŠ¡æ•°ï¼š10W+æ¯å¤©è¿è¡Œä»»åŠ¡å®ä¾‹æ•°ï¼š23W+ETL/è®¡ç®—ä»»åŠ¡ï¼š~58%æŸ¥è¯¢å¹³å°:adhocæŸ¥è¯¢ï¼š2W+æ”¯æŒSpark/Hive/PrestoäºŒã€Hiveä¸Sparkçš„åŒºåˆ«Hiveï¼šä¼˜ç‚¹ï¼šè¿è¡Œç¨³å®šï¼Œå®¢æˆ·ç«¯å†…å­˜æ¶ˆè€—å°ã€‚å­˜åœ¨é—®é¢˜ï¼šç”Ÿæˆå¤šä¸ªMapReduceä½œä¸šï¼›ä¸­é—´ç»“æœè½åœ°ï¼ŒIOå¼€é”€å¤§ï¼›é¢‘ç¹ç”³è¯·å’Œé‡Šæ”¾containerï¼Œèµ„æºæ²¡æœ‰åˆç†å……åˆ†åˆ©ç”¨Sparkï¼šå¿«ï¼šé«˜æ•ˆçš„DAGæ‰§è¡Œå¼•æ“ï¼Œå¯ä»¥åŸºäºå†…å­˜æ¥é«˜æ•ˆçš„å¤„ç†æ•°æ®æµï¼ŒèŠ‚çœå¤§é‡IOå¼€é”€é€šç”¨æ€§ï¼šSparkSQLèƒ½ç›´æ¥ä½¿ç”¨HiveQLè¯­æ³•ï¼ŒHive Metastoreï¼ŒSerdesï¼ŒUDFsä¸‰ã€è¿ç§»SparkSQLçš„æŒ‘æˆ˜å…¼å®¹æ€§ï¼šHiveåŸå…ˆçš„æƒé™æ§åˆ¶SQLè¯­æ³•ï¼ŒUDFå’ŒHiveçš„å…¼å®¹æ€§ç¨³å®šæ€§ï¼šè¿ç§»é€æ˜ï¼Œä½ä¼˜å…ˆçº§ç”¨æˆ·æ— æ„ŸçŸ¥ç›‘æ§ä½œä¸šè¿ç§»åæˆåŠŸç‡åŠè¿è¡Œæ—¶é•¿å¯¹æ¯”å‡†ç¡®æ€§ï¼šæ•°æ®ä¸€è‡´åŠŸèƒ½å¢å¼ºï¼šç”¨æˆ·ä½“éªŒï¼Œæ˜¯å¦æ˜“ç”¨ï¼ŒæŠ¥é”™ä¿¡æ¯æ˜¯å¦å¯è¯»æ½œåœ¨Bugå‘¨è¾¹ç³»ç»Ÿé…åˆæ”¹é€ è¡€ç¼˜æ”¶é›†å››ã€å…¼å®¹æ€§æ”¹é€ ç§»æ¤hiveæƒé™Sparkæ²¡æœ‰æƒé™è®¤è¯æ¨¡å—ï¼Œå¯å¯¹ä»»æ„è¡¨è¿›è¡ŒæŸ¥è¯¢ï¼Œæœ‰å®‰å…¨éšæ‚£éœ€è¦ä¸Hiveå…±äº«åŒä¸€å¥—æƒé™æ–¹æ¡ˆï¼šæ‰§è¡ŒSQLæ—¶ï¼Œå¯¹SQLè§£æå¾—åˆ°LogicalPlanï¼Œå¯¹LogicalPlanè¿›è¡Œéå†ï¼Œæå–è¯»å–çš„è¡¨åŠå†™å…¥çš„è¡¨ï¼Œè°ƒç”¨Hvieçš„è®¤è¯æ–¹æ³•è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœæœ‰æƒé™åˆ™ç»§ç»­æ‰§è¡Œï¼Œå¦åˆ™æ‹’ç»è¯¥ç”¨æˆ·çš„æ“ä½œã€‚SQLè¯­æ³•å’Œhiveå…¼å®¹Sparkåˆ›å»ºçš„æŸäº›è§†å›¾ï¼Œåœ¨HiveæŸ¥è¯¢æ—¶æŠ¥é”™ï¼ŒSparkåˆ›å»ºçš„è§†å›¾ä¸ä¼šå¯¹SQLè¿›è¡Œå±•å¼€ï¼Œè§†å›¾å®šä¹‰æ²¡æœ‰å½“å‰çš„DBä¿¡æ¯ï¼ŒHiveä¸å…¼å®¹è¯»å–è¿™æ ·çš„è§†å›¾æ–¹æ¡ˆï¼šã€ä¿æŒä¸Hiveä¸€è‡´ï¼Œåœ¨Sparkåˆ›å»ºå’Œä¿®æ”¹è§†å›¾æ—¶ï¼Œä½¿ç”¨hive cli driverå»æ‰§è¡Œcreate/alter view sqlUDFä¸hiveå…¼å®¹UDFè®¡ç®—ç»“æœä¸ä¸€æ ·ï¼Œå³ä½¿æ˜¯æ­£å¸¸æ•°æ®ï¼ŒSparkè¿”å›nullï¼ŒHiveç»“æœæ­£ç¡®ï¼›å¼‚å¸¸æ•°æ®ï¼ŒSparkæŠ›exceptionå¯¼è‡´ä½œä¸šå¤±è´¥ï¼ŒHiveè¿”å›çš„nullã€‚æ–¹æ¡ˆï¼šSparkå‡½æ•°ä¿®å¤ï¼Œæ¯”å¦‚roundå‡½æ•°å°†hiveä¸€äº›å‡½æ•°ç§»æ¤ï¼Œå¹¶æ³¨å†Œæˆæ°¸ä¹…å‡½æ•°æ•´ç†Sparkå’ŒHiveè¯­æ³•å’ŒUDFå·®å¼‚äº”ã€ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ç¨³å®šæ€§ï¼šè¿ç§»é€æ˜ï¼šè°ƒåº¦ç³»ç»Ÿå¯¹ä½ä¼˜å…ˆçº§ä½œä¸šï¼ŒæŒ‰ä½œä¸šç²’åº¦åˆ‡æ¢æˆSparkæ‰§è¡Œï¼Œå¤±è´¥åå†åˆ‡æ¢æˆhiveç°åº¦å˜æ›´ï¼Œå¤šç§å˜æ›´è§„åˆ™ï¼šæ”¯æŒå¤šç‰ˆæœ¬Sparkï¼Œè‡ªåŠ¨åˆ‡æ¢å¼•æ“ï¼ŒSpark v2 -&gt; Spark v1 -&gt; Hiveï¼›ç°åº¦æ¨é€å‚æ•°ï¼Œè°ƒä¼˜å‚æ•°ï¼ŒæŸäº›åŠŸèƒ½ç›‘æ§ï¼šæ¯æ—¥ç»Ÿè®¡sparkå’Œhiveè¿è¡Œå¯¹æ¯”ï¼Œæ¯æ—¶æ”¶é›†ä½œä¸šç²’åº¦å¤±è´¥çš„Sparkä½œä¸šï¼Œåˆ†æå¤±è´¥åŸå› å‡†ç¡®æ€§ï¼šæ•°æ®è´¨é‡ç³»ç»Ÿï¼šæ ¡éªŒä»»åŠ¡ï¼Œæ£€æŸ¥æ•°æ®å‡†ç¡®æ€§å…­ã€åŠŸèƒ½å¢å¼ºSpark Thrift Serverï¼š1.åŸºäºdelegation tokençš„impersontionDriverï¼šä¸ºä¸åŒçš„ç”¨æˆ·æ‹¿delegation tokenï¼Œå†™åˆ°stagingç›®å½•ï¼Œè®°å½•User-&gt;SQL-&gt;Jobæ˜ å°„å…³ç³»ï¼Œåˆ†å‘taskå¸¦ä¸Šå¯¹åº”çš„usernameExecutorï¼šæ ¹æ®taskä¿¡æ¯å¸¦çš„usernameæ‰¾åˆ°stagingç›®å½•ä¸‹çš„tokenï¼ŒåŠ åˆ°å½“å‰proxy userçš„ugiï¼Œå®ç°impersonate2.åŸºäºzookeeperçš„æœåŠ¡å‘ç°ï¼Œæ”¯æŒå¤šå°serverè¿™ä¸€å—ä¸»è¦ç§»æ¤äº†Hive zookeeperçš„å®ç°3.é™åˆ¶å¤§æŸ¥è¯¢ä½œä¸šï¼Œé˜²æ­¢driver OOMé™åˆ¶æ¯ä¸ªjobäº§ç”Ÿçš„taskæœ€å¤§æ•°é‡é™åˆ¶æŸ¥è¯¢SQLçš„æœ€å¤§è¡Œæ•°ï¼Œå®¢æˆ·ç«¯æŸ¥è¯¢å¤§æ‰¹é‡æ•°æ®ï¼Œæ•°æ®æŒ¤å‹åœ¨Thrift Serverï¼Œå †å†…å†…å­˜é£™å‡ï¼Œå¼ºåˆ¶åœ¨åªæœ‰æŸ¥çš„SQLåŠ ä¸Šlimité™åˆ¶æŸ¥è¯¢SQLçš„ç»“æœé›†æ•°æ®å¤§å°4.ç›‘æ§å¯¹æ¯ä¸ªserverå®šæ—¶æŸ¥è¯¢ï¼Œæ£€æµ‹æ˜¯å¦å¯ç”¨å¤šè¿è¡Œæ—¶é•¿è¾ƒä¹…çš„ä½œä¸šï¼Œä¸»åŠ¨killç”¨æˆ·ä½“éªŒç”¨æˆ·çœ‹åˆ°çš„æ˜¯ç±»ä¼¼Hive MRè¿›åº¦çš„æ—¥å¿—ï¼ŒINFOçº§åˆ«æ—¥å¿—æ”¶é›†åˆ°ESï¼Œå¯ä¾›æ—¥å¿—çš„åˆ†æå’Œæ’æŸ¥é—®é¢˜æ”¶é›†ç”Ÿæˆçš„è¡¨æˆ–è€…åˆ†åŒºçš„numRows numFile totalSizeï¼Œè¾“å‡ºåˆ°æ—¥å¿—å¯¹ç®€å•çš„è¯­å¥ï¼Œå¦‚DDLè¯­å¥ï¼Œè‡ªåŠ¨ä½¿ç”¨â€“master=localæ–¹å¼å¯åŠ¨Combine input Formatåœ¨HadoopTableReader#makeRDDForTableï¼Œæ‹¿åˆ°å¯¹åº”tableçš„InputFormatClassï¼Œè½¬æ¢æˆå¯¹åº”æ ¼å¼çš„CombineInputFormaté€šè¿‡å¼€å…³æ¥å†³å®šæ˜¯å¦å¯ç”¨è¿™ä¸ªç‰¹æ€§set spark.sql.combine.input.splits.enable=trueé€šè¿‡å‚æ•°æ¥è°ƒæ•´æ¯ä¸ªsplitçš„total input sizemapreduce.input.fileinputformat.split.maxsize=256MB 10241024ä¹‹å‰driverè¯»å¤§è¡¨é«˜å³°æ—¶æ®µsplitéœ€è¦30åˆ†é’Ÿä¸æ­¢ï¼Œæ‰æŠŠä»»åŠ¡æäº¤ä¸Šï¼Œç°åœ¨åªè¦å‡ åˆ†é’Ÿå°±ç®—å¥½splitçš„æ•°é‡å¹¶æäº¤ä»»åŠ¡ï¼Œä¹Ÿè§£å†³äº†ä¸€äº›è¡¨ä¸å¤§ï¼Œå°æ–‡ä»¶å¤šï¼Œèƒ½åˆå¹¶åˆ°åŒä¸€ä¸ªtaskè¿›è¡Œè¯»å–]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç  | Sparkè¯»å–mongoDBæ•°æ®å†™å…¥Hiveæ™®é€šè¡¨å’Œåˆ†åŒºè¡¨]]></title>
    <url>%2F2018%2F11%2F20%2FSpark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[ç‰ˆæœ¬ï¼šspark 2.2.0hive 1.1.0scala 2.11.8hadoop-2.6.0-cdh5.7.0jdk 1.8MongoDB 3.6.4ä¸€ åŸå§‹æ•°æ®åŠHiveè¡¨MongoDBæ•°æ®æ ¼å¼1234567&#123; &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;), &quot;id&quot; : &quot;1&quot;, &quot;name&quot; : &quot;lisi&quot;, &quot;age&quot; : &quot;18&quot;, &quot;deptno&quot; : &quot;01&quot;&#125;Hiveæ™®é€šè¡¨123456create table mg_hive_test(id string,name string,age string,deptno string)row format delimited fields terminated by &apos;\t&apos;;Hiveåˆ†åŒºè¡¨1234567create table mg_hive_external(id string,name string,age string)partitioned by (deptno string)row format delimited fields terminated by &apos;\t&apos;;äºŒ IDEA+Maven+Javaä¾èµ–1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt;ä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/2 21:00 * */import com.mongodb.spark.MongoSpark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.RowFactory;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.hive.HiveContext;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructField;import org.apache.spark.sql.types.StructType;import org.bson.Document;import java.io.File;import java.util.ArrayList;import java.util.List;public class sparkreadmgtohive &#123; public static void main(String[] args) &#123; //spark 2.x String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath(); SparkSession spark = SparkSession.builder() .master(&quot;local[2]&quot;) .appName(&quot;SparkReadMgToHive&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;) .enableHiveSupport() .getOrCreate(); JavaSparkContext sc = new JavaSparkContext(spark.sparkContext()); //spark 1.x// JavaSparkContext sc = new JavaSparkContext(conf);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);// SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);// conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);// conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);// HiveContext sqlContext = new HiveContext(sc);// //create df from mongo// Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();// df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show(); String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;; String opType =&quot;P&quot;; SQLUtils sqlUtils = new SQLUtils(); List&lt;String&gt; column = sqlUtils.getColumns(querysql); //create rdd from mongo JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc); //å°†Documentè½¬æˆObject JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123; public Object call(Document document)&#123; List list = new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; list.add(String.valueOf(document.get(column.get(i)))); &#125; return list;// return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;); &#125; &#125;); System.out.println(Ordd.first()); //é€šè¿‡ç¼–ç¨‹æ–¹å¼å°†RDDè½¬æˆDF List ls= new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; ls.add(column.get(i)); &#125; String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;); System.out.println(schemaString); List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); for (String fieldName : schemaString.split(&quot;,&quot;)) &#123; StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true); fields.add(field); &#125; StructType schema = DataTypes.createStructType(fields); JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123; List fileds = (List) record;// String[] attributes = record.toString().split(&quot;,&quot;); return RowFactory.create(fileds.toArray()); &#125;); Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema); //å°†DFå†™å…¥åˆ°Hiveä¸­ //é€‰æ‹©Hiveæ•°æ®åº“ spark.sql(&quot;use datalake&quot;); //æ³¨å†Œä¸´æ—¶è¡¨ df.registerTempTable(&quot;mgtable&quot;); if (&quot;O&quot;.equals(opType.trim())) &#123; System.out.println(&quot;æ•°æ®æ’å…¥åˆ°Hive ordinary table&quot;); Long t1 = System.currentTimeMillis(); spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;); Long t2 = System.currentTimeMillis(); System.out.println(&quot;å…±è€—æ—¶ï¼š&quot; + (t2 - t1) / 60000 + &quot;åˆ†é’Ÿ&quot;); &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123; System.out.println(&quot;æ•°æ®æ’å…¥åˆ°Hive dynamic partition table&quot;); Long t3 = System.currentTimeMillis(); //å¿…é¡»è®¾ç½®ä»¥ä¸‹å‚æ•° å¦åˆ™æŠ¥é”™ spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;); //deptonä¸ºåˆ†åŒºå­—æ®µ selectè¯­å¥æœ€åä¸€ä¸ªå­—æ®µå¿…é¡»æ˜¯deptno spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;); Long t4 = System.currentTimeMillis(); System.out.println(&quot;å…±è€—æ—¶ï¼š&quot;+(t4 -t3)/60000+ &quot;åˆ†é’Ÿ&quot;); &#125; spark.stop(); &#125;&#125;å·¥å…·ç±»1234567891011121314151617181920212223242526272829303132333435package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/3 23:20 * */import java.util.ArrayList;import java.util.List;public class SQLUtils &#123; public List&lt;String&gt; getColumns(String querysql)&#123; List&lt;String&gt; column = new ArrayList&lt;String&gt;(); String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6, querysql.indexOf(&quot;from&quot;)).trim(); if (tmp.indexOf(&quot;*&quot;) == -1)&#123; String cols[] = tmp.split(&quot;,&quot;); for (String c:cols)&#123; column.add(c); &#125; &#125; return column; &#125; public String getTBname(String querysql)&#123; String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim(); int sx = tmp.indexOf(&quot; &quot;); if(sx == -1)&#123; return tmp; &#125;else &#123; return tmp.substring(0,sx); &#125; &#125;&#125;ä¸‰ é”™è¯¯è§£å†³åŠæ³•1 IDEAä¼šè·å–ä¸åˆ°Hiveçš„æ•°æ®åº“å’Œè¡¨ï¼Œå°†hive-site.xmlæ”¾å…¥resourcesæ–‡ä»¶ä¸­ã€‚å¹¶ä¸”å°†resourcesè®¾ç½®æˆé…ç½®æ–‡ä»¶(è®¾ç½®æˆåŠŸæ–‡ä»¶å¤¹æ˜¯è“è‰²å¦åˆ™æ˜¯ç°è‰²)fileâ€“&gt;Project Structureâ€“&gt;Modulesâ€“&gt;Source2 ä¸Šé¢é”™è¯¯å¤„ç†å®Œåå¦‚æœæŠ¥JDOç±»å‹çš„é”™è¯¯ï¼Œé‚£ä¹ˆæ£€æŸ¥HIVE_HOME/libä¸‹æ—¶å€™å¦mysqlé©±åŠ¨ï¼Œå¦‚æœç¡®å®šæœ‰ï¼Œé‚£ä¹ˆå°±æ˜¯IDEAè·å–ä¸åˆ°ã€‚è§£å†³æ–¹æ³•å¦‚ä¸‹ï¼šå°†mysqlé©±åŠ¨æ‹·è´åˆ°jdk1.8.0_171.jdk/Contents/Home/jre/lib/extè·¯å¾„ä¸‹(jdk/jre/lib/ext)åœ¨IDEAé¡¹ç›®External Librariesä¸‹çš„&lt;1.8&gt;é‡Œé¢æ·»åŠ mysqlé©±åŠ¨å›› æ³¨æ„ç‚¹ç”±äºå°†MongoDBæ•°æ®è¡¨æ³¨å†Œæˆäº†ä¸´æ—¶è¡¨å’ŒHiveè¡¨è¿›è¡Œäº†å…³è”ï¼Œæ‰€ä»¥è¦å°†MongoDBä¸­çš„idå­—æ®µè®¾ç½®æˆç´¢å¼•å­—æ®µï¼Œå¦åˆ™æ€§èƒ½ä¼šå¾ˆæ…¢ã€‚MongoDBè®¾ç½®ç´¢å¼•æ–¹æ³•ï¼š1db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;æŸ¥çœ‹ç´¢å¼•ï¼š12db.getCollection(&apos;mgtest&apos;).getIndexes()MongoSparkç½‘å€ï¼šhttps://docs.mongodb.com/spark-connector/current/java-api/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœ€å…¨çš„Flinkéƒ¨ç½²åŠå¼€å‘æ¡ˆä¾‹(KafkaSource+SinkToMySQL)]]></title>
    <url>%2F2018%2F11%2F10%2F%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource%2BSinkToMySQL)%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½Flinkå®‰è£…åŒ…flinkä¸‹è½½åœ°å€https://archive.apache.org/dist/flink/flink-1.5.0/å› ä¸ºä¾‹å­ä¸éœ€è¦hadoopï¼Œä¸‹è½½flink-1.5.0-bin-scala_2.11.tgzå³å¯ä¸Šä¼ è‡³æœºå™¨çš„/optç›®å½•ä¸‹2.è§£å‹tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/3.é…ç½®masterèŠ‚ç‚¹é€‰æ‹©ä¸€ä¸ª masterèŠ‚ç‚¹(JobManager)ç„¶ååœ¨conf/flink-conf.yamlä¸­è®¾ç½®jobmanager.rpc.address é…ç½®é¡¹ä¸ºè¯¥èŠ‚ç‚¹çš„IP æˆ–è€…ä¸»æœºåã€‚ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹æœ‰æœ‰ä¸€æ ·çš„jobmanager.rpc.address é…ç½®ã€‚jobmanager.rpc.address: node1(é…ç½®ç«¯å£å¦‚æœè¢«å ç”¨ä¹Ÿè¦æ”¹ å¦‚é»˜è®¤8080å·²ç»è¢«sparkå ç”¨ï¼Œæ”¹æˆäº†8088)rest.port: 8088æœ¬æ¬¡å®‰è£… masterèŠ‚ç‚¹ä¸ºnode1ï¼Œå› ä¸ºå•æœºï¼ŒslaveèŠ‚ç‚¹ä¹Ÿä¸ºnode14.é…ç½®slaveså°†æ‰€æœ‰çš„ worker èŠ‚ç‚¹ ï¼ˆTaskManagerï¼‰çš„IP æˆ–è€…ä¸»æœºåï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰å¡«å…¥conf/slaves æ–‡ä»¶ä¸­ã€‚5.å¯åŠ¨flinké›†ç¾¤bin/start-cluster.shæ‰“å¼€ http://node1:8088 æŸ¥çœ‹webé¡µé¢Task Managersä»£è¡¨å½“å‰çš„flinkåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªtaskè¿˜æœ‰ä¸¤ä¸ªslots6.æµ‹è¯•ä¾èµ–123456789101112131415161718192021222324252627&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;7.Socketæµ‹è¯•ä»£ç 12345678910111213141516171819202122232425262728293031public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; final String hostName; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(&quot;port&quot;); hostName = params.get(&quot;hostname&quot;); &#125; catch (Exception e) &#123; System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + &quot; : &quot; + count; &#125; &#125;&#125;æ‰“åŒ…mvn clean install (å¦‚æœæ‰“åŒ…è¿‡ç¨‹ä¸­æŠ¥é”™java.lang.OutOfMemoryError)åœ¨å‘½ä»¤è¡Œset MAVEN_OPTS= -Xms128m -Xmx512mç»§ç»­æ‰§è¡Œmvn clean installç”ŸæˆFlinkTest.jaræ‰¾åˆ°æ‰“æˆçš„jarï¼Œå¹¶uploadï¼Œå¼€å§‹ä¸Šä¼ è¿è¡Œå‚æ•°ä»‹ç»æäº¤ç»“æŸä¹‹åå»overviewç•Œé¢çœ‹ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå¯ç”¨çš„slotså˜æˆäº†ä¸€ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬çš„socketç¨‹åºå ç”¨äº†ä¸€ä¸ªï¼Œæ­£åœ¨runningçš„jobå˜æˆäº†ä¸€ä¸ªå‘é€æ•°æ®12345[root@hadoop000 flink-1.5.0]# nc -l 8099aaa bbbaaa cccaaa bbbbbb cccç‚¹å¼€runningçš„jobï¼Œä½ å¯ä»¥çœ‹è§æ¥æ”¶çš„å­—èŠ‚æ•°ç­‰ä¿¡æ¯åˆ°logç›®å½•ä¸‹å¯ä»¥æ¸…æ¥šçš„çœ‹è§è¾“å‡º1234567891011[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.outaaa : 1ccc : 1ccc : 1bbb : 1ccc : 1bbb : 1bbb : 1ccc : 1bbb : 1ccc : 1é™¤äº†å¯ä»¥åœ¨ç•Œé¢æäº¤ï¼Œè¿˜å¯ä»¥å°†jarä¸Šä¼ çš„linuxä¸­è¿›è¡Œæäº¤ä»»åŠ¡è¿è¡Œflinkä¸Šä¼ çš„jar1bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1å…¶ä»–æ­¥éª¤ä¸€è‡´ã€‚8.ä½¿ç”¨kafkaä½œä¸ºsourceåŠ ä¸Šä¾èµ–1234&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;1234567891011121314151617public class KakfaSource010 &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); //å¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…åˆé€‚çš„topic FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties); //é…ç½®ä»æœ€æ–°çš„åœ°æ–¹å¼€å§‹æ¶ˆè´¹ kafkaSource.setStartFromLatest(); //ä½¿ç”¨addsourceï¼Œå°†kafkaçš„è¾“å…¥è½¬å˜ä¸ºdatastream DataStream&lt;String&gt; consume = env.addSource(wordfre); ... //process and sink env.execute(&quot;KakfaSource010&quot;); &#125;&#125;9.ä½¿ç”¨mysqlä½œä¸ºsinkflinkæœ¬èº«å¹¶æ²¡æœ‰æä¾›datastreamè¾“å‡ºåˆ°mysqlï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±å»å®ç°é¦–å…ˆï¼Œå¯¼å…¥ä¾èµ–12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt;è‡ªå®šä¹‰sinkï¼Œé¦–å…ˆæƒ³åˆ°çš„æ˜¯extends SinkFunctionï¼Œé›†æˆflinkè‡ªå¸¦çš„sinkfunctionï¼Œå†å½“ä¸­å®ç°æ–¹æ³•ï¼Œå®ç°å¦‚ä¸‹12345678910111213141516171819202122public class MysqlSink implements SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; String username = &quot;mysql.user&quot;; String password = &quot;mysql.password&quot;; String drivername = &quot;mysql.driver&quot;; String dburl = &quot;mysql.url&quot;; @Override public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;è¿™æ ·å®ç°æœ‰ä¸ªé—®é¢˜ï¼Œæ¯ä¸€æ¡æ•°æ®ï¼Œéƒ½è¦æ‰“å¼€mysqlè¿æ¥ï¼Œå†å…³é—­ï¼Œæ¯”è¾ƒè€—æ—¶ï¼Œè¿™ä¸ªå¯ä»¥ä½¿ç”¨flinkä¸­æ¯”è¾ƒå¥½çš„Richæ–¹å¼æ¥å®ç°ï¼Œä»£ç å¦‚ä¸‹12345678910111213141516171819202122232425262728public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement preparedStatement = null; private String userName = null; private String password = null; private String driverName = null; private String DBUrl = null; public MysqlSink() &#123; userName = &quot;mysql.username&quot;; password = &quot;mysql.password&quot;; driverName = &quot;mysql.driverName&quot;; DBUrl = &quot;mysql.DBUrl&quot;; &#125; public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; if(connection==null)&#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; String sql =&quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,value.f0); preparedStatement.setString(2,value.f1); preparedStatement.executeUpdate();//è¿”å›æˆåŠŸçš„è¯å°±æ˜¯ä¸€ä¸ªï¼Œå¦åˆ™å°±æ˜¯0 &#125; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; @Override public void close() throws Exception &#123; if(preparedStatement!=null)&#123; preparedStatement.close(); &#125; if(connection!=null)&#123; connection.close(); &#125; &#125;&#125;Richæ–¹å¼çš„ä¼˜ç‚¹åœ¨äºï¼Œæœ‰ä¸ªopenå’Œcloseæ–¹æ³•ï¼Œåœ¨åˆå§‹åŒ–çš„æ—¶å€™å»ºç«‹ä¸€æ¬¡è¿æ¥ï¼Œä¹‹åä¸€ç›´ä½¿ç”¨è¿™ä¸ªè¿æ¥å³å¯ï¼Œç¼©çŸ­å»ºç«‹å’Œå…³é—­è¿æ¥çš„æ—¶é—´ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿æ¥æ± å®ç°ï¼Œè¿™é‡Œåªæ˜¯æä¾›è¿™æ ·ä¸€ç§æ€è·¯ã€‚ä½¿ç”¨è¿™ä¸ªmysqlsinkä¹Ÿéå¸¸ç®€å•1//ç›´æ¥addsinkï¼Œå³å¯è¾“å‡ºåˆ°è‡ªå®šä¹‰çš„mysqlä¸­ï¼Œä¹Ÿå¯ä»¥å°†mysqlçš„å­—æ®µç­‰å†™æˆå¯é…ç½®çš„ï¼Œæ›´åŠ æ–¹ä¾¿å’Œé€šç”¨proceDataStream.addSink(new MysqlSink());10.æ€»ç»“æœ¬æ¬¡çš„ç¬”è®°åšäº†ç®€å•çš„éƒ¨ç½²ã€æµ‹è¯•ã€kafkademoï¼Œä»¥åŠè‡ªå®šä¹‰å®ç°mysqlsinkçš„ä¸€äº›å†…å®¹ï¼Œå…¶ä¸­æ¯”è¾ƒé‡è¦çš„æ˜¯Richçš„ä½¿ç”¨ï¼Œå¸Œæœ›å¤§å®¶èƒ½æœ‰æ‰€æ”¶è·ã€‚]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹sparkdemo.jarè¿è¡Œåœ¨yarnä¸Šè¿‡ç¨‹]]></title>
    <url>%2F2018%2F09%2F28%2F19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.å°†ä¹‹å‰æ‰“åŒ…çš„jaråŒ…ä¸Šä¼ [root@sht-sgmhadoopnn-01 spark]# pwd/root/learnproject/app/spark[root@sht-sgmhadoopnn-01 spark]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring sparkdemo.jarâ€¦100% 164113 KB 421 KB/sec 00:06:29 0 Errors2.ä»¥ä¸‹æ˜¯é”™è¯¯2.112ERROR1: Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributesIDEAæ‰“åŒ…çš„jaråŒ…,éœ€è¦ä½¿ç”¨zipåˆ é™¤æŒ‡å®šæ–‡ä»¶1zip -d sparkdemo.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF2.21ERROR2: Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/learn/java/main/OnLineLogAnalysis2 : Unsupported major.minor version 52.0yarnç¯å¢ƒçš„jdkç‰ˆæœ¬ä½äºç¼–è¯‘jaråŒ…çš„jdkç‰ˆæœ¬(éœ€è¦ä¸€è‡´æˆ–è€…é«˜äº;æ¯ä¸ªèŠ‚ç‚¹éœ€è¦å®‰è£…jdk,åŒæ—¶ä¿®æ”¹æ¯ä¸ªèŠ‚ç‚¹çš„hadoop-env.shæ–‡ä»¶çš„JAVA_HOMEå‚æ•°æŒ‡å‘)2.31234567891011ERROR3: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; 17/02/15 17:30:35 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; at org.influxdb.impl.InfluxDBImpl.ping(InfluxDBImpl.java:178) at org.influxdb.impl.InfluxDBImpl.version(InfluxDBImpl.java:201) at com.learn.java.main.OnLineLogAnalysis2.main(OnLineLogAnalysis2.java:69) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)æŠ›é”™ä¿¡æ¯ä¸ºNoSuchMethodErrorï¼Œè¡¨ç¤º guavaå¯èƒ½æœ‰å¤šç‰ˆæœ¬ï¼Œåˆ™ä½ç‰ˆæœ¬12345678910111213141516171819202122232425[root@sht-sgmhadoopnn-01 app]# pwd /root/learnproject/app [root@sht-sgmhadoopnn-01 app]# ll total 470876 -rw-r--r-- 1 root root 7509833 Jan 16 22:11 AdminLTE.zip drwxr-xr-x 12 root root 4096 Feb 14 11:21 hadoop -rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz drwxr-xr-x 7 root root 4096 Feb 7 11:16 kafka-manager-1.3.2.1 -rw-r--r-- 1 root root 59682993 Dec 26 14:44 kafka-manager-1.3.2.1.zip drwxr-xr-x 2 root root 4096 Jan 7 16:21 kafkaoffsetmonitor drwxr-xr-x 2 777 root 4096 Feb 14 14:48 pid drwxrwxr-x 4 1000 1000 4096 Oct 29 01:46 sbt -rw-r--r-- 1 root root 1049906 Dec 25 21:29 sbt-0.13.13.tgz drwxrwxr-x 6 root root 4096 Mar 4 2016 scala -rw-r--r-- 1 root root 28678231 Mar 4 2016 scala-2.11.8.tgz drwxr-xr-x 13 root root 4096 Feb 15 17:01 spark -rw-r--r-- 1 root root 187426587 Nov 12 06:54 spark-2.0.2-bin-hadoop2.7.tgz [root@sht-sgmhadoopnn-01 app]# [root@sht-sgmhadoopnn-01 app]# find ./ -name *guava* [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/yarn/lib/[root@sht-sgmhadoopnn-01 app]# mv ./spark/jars/guava-14.0.1.jar ./spark/jars/guava-14.0.1.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./spark/jars/ [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/common/lib/3.åå°æäº¤jaråŒ…è¿è¡Œ123456789101112131415161718[root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# nohup /root/learnproject/app/spark/bin/spark-submit \&gt; --name onlineLogsAnalysis \&gt; --master yarn \&gt; --deploy-mode cluster \&gt; --conf &quot;spark.scheduler.mode=FAIR&quot; \&gt; --conf &quot;spark.sql.codegen=true&quot; \&gt; --driver-memory 2G \&gt; --executor-memory 2G \&gt; --executor-cores 1 \&gt; --num-executors 3 \&gt; --class com.learn.java.main.OnLineLogAnalysis2 \&gt; /root/learnproject/app/spark/sparkdemo.jar &amp;[1] 22926[root@sht-sgmhadoopnn-01 spark]# nohup: ignoring input and appending output to `nohup.out&apos;[root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# tail -f nohup.out4.yarn webç•Œé¢æŸ¥çœ‹è¿è¡ŒlogApplicationMasterï¼šæ‰“å¼€ä¸ºspark history server webç•Œé¢logsï¼š æŸ¥çœ‹stderr å’Œ stdoutæ—¥å¿— (system.out.printlnæ–¹æ³•è¾“å‡ºåˆ°stdoutæ—¥å¿—ä¸­)5.æŸ¥çœ‹spark history web6.æŸ¥çœ‹DashBoard ,å®æ—¶å¯è§†åŒ–]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18Spark on Yarné…ç½®æ—¥å¿—Web UI(HistoryServeræœåŠ¡)]]></title>
    <url>%2F2018%2F09%2F26%2F18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)%2F</url>
    <content type="text"><![CDATA[1.è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶12[root@sht-sgmhadoopnn-01 ~]# cd /root/learnproject/app/spark/conf[root@sht-sgmhadoopnn-01 conf]# cp spark-defaults.conf.template spark-defaults.conf2.åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š(å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š)1234567891011 [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls / Found 3 items drwxr-xr-x - root root 0 2017-02-14 12:43 /spark drwxrwx--- - root root 0 2017-02-14 12:58 /tmp drwxr-xr-x - root root 0 2017-02-14 12:58 /user You have new mail in /var/spool/mail/root [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /spark Found 1 items drwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@sht-sgmhadoopnn-01 conf]# hdfs dfs -mkdir /spark/historylog#åœ¨HDFSä¸­åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œç”¨äºä¿å­˜Sparkè¿è¡Œæ—¥å¿—ä¿¡æ¯ã€‚Spark History Serverä»æ­¤ç›®å½•ä¸­è¯»å–æ—¥å¿—ä¿¡æ¯3.é…ç½®12345678[root@sht-sgmhadoopnn-01 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080#spark.eventLog.dirä¿å­˜æ—¥å¿—ç›¸å…³ä¿¡æ¯çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯hdfs://å¼€å¤´çš„HDFSè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯file://å¼€å¤´çš„æœ¬åœ°è·¯å¾„ï¼Œéƒ½éœ€è¦æå‰åˆ›å»º#spark.yarn.historyServer.address : Spark history serverçš„åœ°å€(ä¸åŠ http://).è¿™ä¸ªåœ°å€ä¼šåœ¨Sparkåº”ç”¨ç¨‹åºå®Œæˆåæäº¤ç»™YARN RMï¼Œç„¶åå¯ä»¥åœ¨RM UIä¸Šç‚¹å‡»é“¾æ¥è·³è½¬åˆ°history server UIä¸Š.4.æ·»åŠ SPARK_HISTORY_OPTSå‚æ•°123456789101112 [root@sht-sgmhadoopnn-01 conf]# vi spark-env.sh #!/usr/bin/env bash export SCALA_HOME=/root/learnproject/app/scala export JAVA_HOME=/usr/java/jdk1.8.0_111 export SPARK_MASTER_IP=172.16.101.55 export SPARK_WORKER_MEMORY=1g export SPARK_PID_DIR=/root/learnproject/app/pid export HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹1234567891011121314151617 [root@sht-sgmhadoopnn-01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out [root@sht-sgmhadoopnn-01 ~]# jps 28905 HistoryServer 30407 ProdServerStart 30373 ResourceManager 30957 NameNode 16949 Jps 30280 DFSZKFailoverController31445 JobHistoryServer[root@sht-sgmhadoopnn-01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/java [root@sht-sgmhadoopnn-01 ~]#]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[17ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹ä½¿ç”¨IDEAå°†å·¥ç¨‹BuildæˆjaråŒ…]]></title>
    <url>%2F2018%2F09%2F25%2F17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85%2F</url>
    <content type="text"><![CDATA[1.Fileâ€“&gt;Project Structure2.Artifactsâ€“&gt;+â€“&gt;JARâ€“&gt;From modules with dependencies3. å•å‡»â€¦ â€“&gt;é€‰æ‹©OnLineLogAnalysis24.é€‰æ‹©é¡¹ç›®çš„æ ¹ç›®å½•5.ä¿®æ”¹Nameâ€“&gt;é€‰æ‹©è¾“å‡ºç›®å½•â€“&gt;é€‰æ‹©Output directoryâ€“&gt;Applyâ€“&gt;OK6.Buildâ€“&gt;Build Artifactsâ€“&gt;Build===================================è¯´æ˜:1.æ‰“åŒ…æ–¹å¼å¾ˆå¤šï¼Œå¤§å®¶è‡ªè¡Œgoogle.2.ç”±äºæˆ‘æ˜¯å¼•ç”¨influxdbçš„æºç åŒ…,éœ€è¦å¼•å…¥è®¸å¤šä¾èµ–jaråŒ…,æ‰€ä»¥æˆ‘éœ€è¦å°†ç›¸å…³ä¾èµ–jaråŒ…å…¨éƒ¨æ‰“åŒ…åˆ°æœ¬ç¨‹åºçš„jaråŒ…,æ•…è¯¥jaråŒ…å¤§æ¦‚160Mã€‚(å½“ç„¶ä¹Ÿå¯ä»¥åªéœ€è¦æ‰“æœ¬ç¨‹åºçš„jaråŒ…ï¼Œåªä¸è¿‡éœ€è¦äº‹å…ˆå°†ç›¸å…³çš„æ‰€æœ‰æˆ–è€…éƒ¨åˆ†ä¾èµ–jaråŒ…ï¼Œå‰æä¸Šä¼ åˆ°é›†ç¾¤ï¼Œç„¶åspark-submitä½¿ç”¨â€“jarså¼•ç”¨å³å¯)]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹grafana-4.1.1 Installå’Œæ–°å»ºæ—¥å¿—åˆ†æçš„DashBoard]]></title>
    <url>%2F2018%2F09%2F19%2F16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½wget https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz2.è§£å‹tar -zxvf grafana-4.1.1-1484211277.linux-x64.tar.gz3.é…ç½®æ–‡ä»¶cd grafana-4.1.1-1484211277cp conf/sample.ini conf/custom.ini#make changes to conf/custom.ini then start grafana-server4.åå°å¯åŠ¨./bin/grafana-server &amp;5.æ‰“å¼€webhttp://172.16.101.66:3000/ admin/admin6.é…ç½®æ•°æ®æºinfluxdbè¿˜è¦å¡«å†™Database ä¸º online_log_analysis7.IDEAæœ¬æœºè¿è¡ŒOnLineLogAanlysis2.classï¼Œå®æ—¶è®¡ç®—å­˜å‚¨åˆ°influxdb8.æ–°å»ºdashboardå’Œ cdh_hdfs_warnæ›²çº¿å›¾å‚è€ƒ:http://grafana.org/download/http://docs.grafana.org/]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹åŸºäºSpark Streaming+Saprk SQLå¼€å‘OnLineLogAanlysis2]]></title>
    <url>%2F2018%2F09%2F18%2F15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%2BSaprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2%2F</url>
    <content type="text"><![CDATA[1.influxdbåˆ›å»ºdatabase[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0create database online_log_analysis2.å¯¼å…¥æºä»£ç é¡¹ç›®ä¸­åŸæœ¬æƒ³å°† influxdb-java https://github.com/influxdata/influxdb-javaçš„InfluxDBTest.java æ–‡ä»¶çš„åŠ åˆ°é¡¹ç›®ä¸­ï¼Œæ‰€ä»¥å¿…é¡»è¦å¼•å…¥ influxdb-java çš„åŒ…ï¼›ä½†æ˜¯ç”±äºGitHubçš„ä¸Šçš„classæ–‡ä»¶çš„æŸäº›æ–¹æ³•ï¼Œæ˜¯ç‰ˆæœ¬æ˜¯2.6ï¼Œè€Œmavenä¸­çš„æœ€é«˜ä¹Ÿå°±2.5ç‰ˆæœ¬ï¼Œæ‰€ä»¥å°†Githubçš„æºä»£ç ä¸‹è½½å¯¼å…¥åˆ°ideaä¸­ï¼Œç¼–è¯‘å¯¼å‡º2.6.jaråŒ…ï¼›å¯æ˜¯ å¼•å…¥2.6jaråŒ…ï¼Œå…¶åœ¨InfluxDBTest.classæ–‡ä»¶çš„ æ— æ³•import org.influxdbï¼ˆç™¾åº¦è°·æ­Œå¾ˆé•¿æ—¶é—´ï¼Œå°è¯•å¾ˆå¤šæ–¹æ³•ä¸è¡Œï¼‰ã€‚æœ€åç´¢æ€§å°† influx-javaçš„æºä»£ç å…¨éƒ¨æ·»åŠ åˆ°é¡¹ç›®ä¸­å³å¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚3.è¿è¡ŒOnLineLogAanlysis2.javahttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.javaæ¯”å¦‚ logtype_count,host_service_logtype=hadoopnn-01_namenode_WARN count=12logtype_count æ˜¯è¡¨host_service_logtype=hadoopnn-01_namenode_WARN æ˜¯ tagâ€“æ ‡ç­¾ï¼Œåœ¨InfluxDBä¸­ï¼Œtagæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†ï¼Œè¡¨å+tagä¸€èµ·ä½œä¸ºæ•°æ®åº“çš„ç´¢å¼•ï¼Œæ˜¯â€œkey-valueâ€çš„å½¢å¼ã€‚count=12 æ˜¯ fieldâ€“æ•°æ®ï¼Œfieldä¸»è¦æ˜¯ç”¨æ¥å­˜æ”¾æ•°æ®çš„éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯â€œkey-valueâ€çš„å½¢å¼ã€‚tagã€field ä¸­é—´æ˜¯è¦æœ‰ç©ºæ ¼çš„4.influxdbæŸ¥è¯¢æ•°æ®]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹èˆå¼ƒRedis+echarts3,é€‰æ‹©InfluxDB+Grafana]]></title>
    <url>%2F2018%2F09%2F17%2F13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis%2Becharts3%2C%E9%80%89%E6%8B%A9InfluxDB%2BGrafana%2F</url>
    <content type="text"><![CDATA[1.æœ€åˆé€‰æ‹©Redisä½œä¸ºå­˜å‚¨ï¼Œæ˜¯ä¸»è¦æœ‰4ä¸ªåŸå› :a.redisæ˜¯ä¸€ä¸ªkey-valueçš„å­˜å‚¨ç³»ç»Ÿï¼Œæ•°æ®æ˜¯å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè¯»å†™æ€§èƒ½å¾ˆé«˜ï¼›b.æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼Œå¦‚set,zset,list,hash,stringï¼›c.keyè¿‡æœŸç­–ç•¥ï¼›d.æœ€ä¸»è¦æ˜¯ç½‘ä¸Šçš„åšå®¢å…¨æ˜¯sparkstreaming+redisï¼Œéƒ½äº’ç›¸æ¨¡ä»¿ï¼›è‡³äºç¼ºç‚¹ï¼Œå½“æ—¶è¿˜æ²¡è€ƒè™‘åˆ°ã€‚2.ç„¶åå¼€å§‹æ·»åŠ CDHRolelog.classç±»å’Œå°†redisæ¨¡å—åŠ å…¥ä»£ç ä¸­ï¼Œä½¿è®¡ç®—ç»“æœï¼ˆæœ¬æ¬¡ä½¿ç”¨spark streaming+spark sqlï¼Œä¹‹å‰ä»…ä»…æ˜¯spark streamingï¼Œå…·ä½“çœ‹ä»£ç ï¼‰å­˜å‚¨åˆ°redisä¸­ï¼Œå½“ç„¶å­˜å‚¨åˆ°redisä¸­ï¼Œæœ‰ä¸¤ç§å­˜å‚¨æ ¼å¼ã€‚2.1 keyä¸ºæœºå™¨åç§°,æœåŠ¡åç§°,æ—¥å¿—çº§åˆ«æ‹¼æ¥çš„å­—ç¬¦ä¸²ï¼Œå¦‚hadoopnn-01_namenode_WARNï¼Œvalueä¸ºæ•°æ®ç±»å‹listï¼Œå…¶å­˜å‚¨ä¸ºjsonæ ¼å¼çš„ [{â€œtimeStampâ€: â€œ2017-02-09 17:16:14.249â€,â€hostNameâ€: â€œhadoopnn-01â€,â€serviceNameâ€: â€œnamenodeâ€,â€logTypeâ€:â€WARNâ€,â€countâ€:â€12â€ }]ä»£ç url,ä¸‹è½½å¯¼å…¥idea,è¿è¡Œå³å¯:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java2.2 keyä¸ºtimestampå¦‚ 2017-02-09 18:09:02.462,value ä¸º [ {â€œhost_service_logtypeâ€: â€œhadoopnn-01_namenode_INFOâ€,â€countâ€:â€110â€ }, {â€œhost_service_logtypeâ€: â€œhadoopnn-01_namenode_DEBUGâ€,â€countâ€:â€678â€ }, {â€œhost_service_logtypeâ€: â€œhadoopnn-01_namenode_WARNâ€,â€countâ€:â€12â€ }]ä»£ç url,ä¸‹è½½å¯¼å…¥idea,è¿è¡Œå³å¯:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java3.åšå¯è§†åŒ–è¿™å—ï¼Œæˆ‘ä»¬é€‰æ‹©adminLTE+flask+echarts3, è®¡åˆ’å’Œç¼–ç¨‹å¼€å‘å°è¯•å»ä»rediså®æ—¶è¯»å–æ•°æ®ï¼ŒåŠ¨æ€ç»˜åˆ¶å›¾è¡¨ï¼›åæ¥å¼€å‘è°ƒç ”å¤§æ¦‚1å‘¨ï¼Œæœ€ç»ˆ2.1 å’Œ2.2æ–¹æ³•çš„å­˜å‚¨æ ¼å¼éƒ½ä¸èƒ½æœ‰æ•ˆé€‚åˆæˆ‘ä»¬ï¼Œè¿›è¡Œå¼€å‘å¯è§†åŒ–Dashboardï¼Œæ‰€ä»¥æˆ‘ä»¬æœ€ç»ˆè°ƒç ”é‡‡å–InfluxDB+Grafanaæ¥åšå­˜å‚¨å’Œå¯è§†åŒ–å±•ç¤ºåŠé¢„è­¦ã€‚4.InfluxDBæ˜¯æ—¶åºæ•°æ®åº“https://docs.influxdata.com/influxdb/v1.2/5.Grafanaæ˜¯å¯è§†åŒ–ç»„ä»¶http://grafana.org/https://github.com/grafana/grafana]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹influxdb-1.2.0 Installå’Œæ¦‚å¿µï¼Œè¯­æ³•ç­‰å­¦ä¹ ]]></title>
    <url>%2F2018%2F09%2F17%2F14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½rpmhttps://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpmæˆ‘é€‰æ‹©ç”¨window7 æµè§ˆå™¨ä¸‹è½½ï¼Œç„¶årzä¸Šä¼ åˆ°linuxæœºå™¨ä¸Š2.å®‰è£…yum install influxdb-1.2.0.x86_64.rpm3.å¯åŠ¨service influxdb startå‚è€ƒ:https://docs.influxdata.com/influxdb/v1.2/introduction/installation/ç¼–è¯‘å®‰è£…:https://anomaly.io/compile-influxdb/4.è¿›å…¥123[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0è¯­æ³•å‚è€ƒ:https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/å­¦ä¹ url:http://www.linuxdaxue.com/influxdb-study-series-manual.html]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹RedisLiveç›‘æ§å·¥å…·çš„è¯¦ç»†å®‰è£…]]></title>
    <url>%2F2018%2F09%2F14%2F12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1GitHub: https://github.com/nkrode/RedisLive1.å®‰è£…python2.7.5 å’Œpip1http://blog.itpub.net/30089851/viewspace-2132450/2.ä¸‹è½½RedisLive123456789101112[root@sht-sgmhadoopdn-04 app]# wget https://github.com/nkrode/RedisLive/archive/master.zip[root@sht-sgmhadoopdn-04 app]# unzip master [root@sht-sgmhadoopdn-04 app]# mv RedisLive-master RedisLive[root@sht-sgmhadoopdn-04 app]# cd RedisLive/[root@sht-sgmhadoopdn-04 RedisLive]# lltotal 20drwxr-xr-x 2 root root 4096 Aug 20 2015 design-rw-r--r-- 1 root root 1067 Aug 20 2015 MIT-LICENSE.txt-rw-r--r-- 1 root root 902 Aug 20 2015 README.md-rw-r--r-- 1 root root 58 Aug 20 2015 requirements.txtdrwxr-xr-x 7 root root 4096 Aug 20 2015 src[root@sht-sgmhadoopdn-04 RedisLive]#3.æŸ¥çœ‹ç‰ˆæœ¬è¦æ±‚(åˆšå¼€å§‹å®‰è£…æ²¡æ³¨æ„ç‰ˆæœ¬ï¼Œç›´æ¥pipå¯¼è‡´åé¢å„ç§é—®é¢˜ï¼Œæ‰€ä»¥è¯·ä»”ç»†çœ‹ä¸‹é¢è¿‡ç¨‹)123456[root@sht-sgmhadoopdn-04 RedisLive]# cat requirements.txtargparse==1.2.1python-dateutil==1.5redistornado==2.1.1[root@sht-sgmhadoopdn-04 RedisLive]# cd ../4.pipå®‰è£…ç¯å¢ƒè¦æ±‚1234[root@sht-sgmhadoopdn-04 app]# pip install tornado[root@sht-sgmhadoopdn-04 app]# pip install redis[root@sht-sgmhadoopdn-04 app]# pip install python-dateutil[root@sht-sgmhadoopdn-04 app]# pip install argparse5.è¿›å…¥ /root/learnproject/app/RedisLive/srcç›®å½•,é…ç½®redis-live.confæ–‡ä»¶12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopdn-04 app]# cd -/root/learnproject/app/RedisLive[root@sht-sgmhadoopdn-04 RedisLive]# cd src[root@sht-sgmhadoopdn-04 src]# lltotal 40drwxr-xr-x 4 root root 4096 Aug 20 2015 apidrwxr-xr-x 2 root root 4096 Aug 20 2015 dataproviderdrwxr-xr-x 2 root root 4096 Aug 20 2015 db-rw-r--r-- 1 root root 0 Aug 20 2015 __init__.py-rw-r--r-- 1 root root 381 Aug 20 2015 redis-live.conf.example-rwxr-xr-x 1 root root 1343 Aug 20 2015 redis-live.py-rwxr-xr-x 1 root root 9800 Aug 20 2015 redis-monitor.pydrwxr-xr-x 2 root root 4096 Aug 20 2015 utildrwxr-xr-x 4 root root 4096 Aug 20 2015 wwwYou have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp redis-live.conf.example redis-live.conf[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# vi redis-live.conf&#123; &quot;RedisServers&quot;: [ &#123; &quot;server&quot;: &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125; ], &quot;DataStoreType&quot; : &quot;redis&quot;, &quot;RedisStatsServer&quot;: &#123; &quot;server&quot; : &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125;&#125;6.ç¬¬ä¸€æ¬¡å°è¯•å¯åŠ¨redis-monitor.pyæŠ›é”™ _sqlite312345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named _sqlite3[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite-devel[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite[root@sht-sgmhadoopdn-04 ~]# find / -name _sqlite3.so/usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so/usr/local/Python-2.7.5/build/lib.linux-x86_64-2.7/_sqlite3.so/usr/lib64/python2.6/lib-dynload/_sqlite3.so[root@sht-sgmhadoopdn-04 ~]# cp /usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 ~]# pythonPython 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sqlite3&gt;&gt;&gt;å‚è€ƒ: http://ju.outofmemory.cn/entry/976587.ç¬¬äºŒæ¬¡å°è¯•å¯åŠ¨redis-monitor.pyæŠ›é”™ redis12345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named redis[root@sht-sgmhadoopdn-04 src]# find / -name redis/etc/rc.d/init.d/redis/root/learnproject/app/redis/root/learnproject/app/redis-monitor/src/main/java/sun/redis/root/learnproject/app/redis-monitor/src/test/java/sun/redis/usr/local/redis/usr/local/python27/lib/python2.7/site-packages/redis[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/redis /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 src]# python Python 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import redis8.ç¬¬ä¸‰æ¬¡å°è¯•å¯åŠ¨redis-monitor.pyï¼ŒæˆåŠŸï¼›æŒ‰ctrl+cä¸­æ–­æ‰1234[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ^Cshutting down...You have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]#9.å°è¯•ç¬¬ä¸€æ¬¡å¯åŠ¨redis-live.py ï¼Œtornado.ioloop12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 3, in &lt;module&gt; import tornado.ioloopImportError: No module named tornado.ioloop[root@sht-sgmhadoopdn-04 src]# find / -name tornado/usr/local/python27/lib/python2.7/site-packages/tornado[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/tornado /usr/local/lib/python2.7/lib-dynload/10.å°è¯•ç¬¬äºŒæ¬¡å¯åŠ¨redis-live.py ï¼Œsingledispatch123456789[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 6, in &lt;module&gt; import tornado.web File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/web.py&quot;, line 84, in &lt;module&gt; from tornado import gen File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/gen.py&quot;, line 98, in &lt;module&gt; from singledispatch import singledispatch # backportImportError: No module named singledispatchè¿™ä¸ª singledispatch é”™è¯¯ï¼Œå…¶å®å°±æ˜¯åœ¨tornadoé‡Œçš„ï¼Œè°·æ­Œå’Œæ€è€ƒè¿‡åï¼Œæ€€ç–‘æ˜¯ç‰ˆæœ¬é—®é¢˜ï¼Œäºæ˜¯æœæ–­å¸è½½tornado12345[root@sht-sgmhadoopdn-04 src]# pip uninstall tornado[root@sht-sgmhadoopdn-04 src]# rm -rf /usr/local/lib/python2.7/lib-dynload/tornado[root@sht-sgmhadoopdn-04 src]# find / -name tornado[root@sht-sgmhadoopdn-04 src]# å‡å¦‚findæœ‰çš„è¯ ï¼Œå°±è¦æ‰‹å·¥åˆ é™¤æ‰11.äºæ˜¯æƒ³æƒ³å…¶ä»–ä¹Ÿæ˜¯è¦å¸è½½æ‰12345[root@sht-sgmhadoopdn-04 src]# pip uninstall argparse[root@sht-sgmhadoopdn-04 src]# pip uninstall python-dateutil[root@sht-sgmhadoopdn-04 src]# find / -name argparse[root@sht-sgmhadoopdn-04 src]# find / -name python-dateutilå‡å¦‚findæœ‰çš„è¯ ï¼Œå°±è¦æ‰‹å·¥åˆ é™¤æ‰12.å…³é”®ä¸€æ­¥: æ ¹æ®step3çš„æŒ‡å®šç‰ˆæœ¬æ¥å®‰è£…123[root@sht-sgmhadoopdn-04 src]# pip install -v tornado==2.1.1[root@sht-sgmhadoopdn-04 src]# pip install -v argparse==1.2.1[root@sht-sgmhadoopdn-04 src]# pip install -v python-dateutil==1.513.å†æ¬¡å°è¯•å¯åŠ¨redis-live.py ï¼ŒæŠ›é”™dateutil.parser1234567891011121314151617[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 10, in &lt;module&gt; from api.controller.ServerListController import ServerListController File &quot;/root/learnproject/app/RedisLive/src/api/controller/ServerListController.py&quot;, line 1, in &lt;module&gt; from BaseController import BaseController File &quot;/root/learnproject/app/RedisLive/src/api/controller/BaseController.py&quot;, line 4, in &lt;module&gt; import dateutil.parserImportError: No module named dateutil.parser[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# find / -name dateutil/usr/local/python27/lib/python2.7/site-packages/dateutil[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/dateutil /usr/local/lib/python2.7/lib-dynload/You have mail in /var/spool/mail/root14.å†åœ¨å°è¯•å¯åŠ¨redis-live.py ï¼ŒæˆåŠŸäº†ï¼Œç„¶åæŒ‰ctrl+cä¸­æ–­æ‰12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py ^CTraceback (most recent call last): File &quot;./redis-live.py&quot;, line 36, in &lt;module&gt; tornado.ioloop.IOLoop.instance().start() File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/ioloop.py&quot;, line 283, in start event_pairs = self._impl.poll(poll_timeout)KeyboardInterrupt[root@sht-sgmhadoopdn-04 src]#15.å¯åŠ¨12[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 &amp;[root@sht-sgmhadoopdn-04 src]# ./redis-live.py &amp;æ‰“å¼€webç•Œé¢http://172.16.101.66:8888/index.html16.æ€»ç»“a.å®‰è£… python2.7+pipb.pipæŒ‡å®šç‰ˆæœ¬å»å®‰è£…é‚£å‡ ä¸ªç»„ä»¶17.è¯´æ˜:redis live å®æ—¶redisç›‘æ§é¢æ¿å¯ä»¥åŒæ—¶ç›‘æ§å¤šä¸ªrediså®ä¾‹ , åŒ…æ‹¬ å†…å­˜ä½¿ç”¨ ã€åˆ†dbæ˜¾ç¤ºçš„keyæ•°ã€å®¢æˆ·ç«¯è¿æ¥æ•°ã€ å‘½ä»¤å¤„ç†æ•°ã€ ç³»ç»Ÿè¿è¡Œæ—¶é—´ , ä»¥åŠå„ç§ç›´è§‚çš„æŠ˜çº¿å›¾æŸ±çŠ¶å›¾.ç¼ºç‚¹æ˜¯ä½¿ç”¨äº†monitor å‘½ä»¤ç›‘æ§ , å¯¹æ€§èƒ½æœ‰å½±å“ ,æœ€å¥½ä¸è¦é•¿æ—¶é—´å¯åŠ¨ .redis-monitor.py:ç”¨æ¥è°ƒç”¨redisçš„monitorå‘½ä»¤æ¥æ”¶é›†redisçš„å‘½ä»¤æ¥è¿›è¡Œç»Ÿè®¡redis-live.py:å¯åŠ¨webæœåŠ¡]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹redis-3.2.5 install(å•èŠ‚ç‚¹)]]></title>
    <url>%2F2018%2F09%2F12%2F11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)%2F</url>
    <content type="text"><![CDATA[1.å®‰è£…jdk1.8123456789101112[root@sht-sgmhadoopdn-04 ~]# cd /usr/java/[root@sht-sgmhadoopdn-04 java]# wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# tar -zxvf jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.8.0_111export path=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-04 java]# source /etc/profile[root@sht-sgmhadoopdn-04 java]# java -versionjava version &quot;1.8.0_111&quot;Java(TM) SE Runtime Environment (build 1.8.0_111-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)[root@sht-sgmhadoopdn-04 java]#2.å®‰è£… redis 3.2.52.1 å®‰è£…ç¼–ç»æ‰€éœ€åŒ…gcc,tcl12[root@sht-sgmhadoopdn-04 local]# yum install gcc[root@sht-sgmhadoopdn-04 local]# yum install tcl2.2 ä¸‹è½½redis-3.2.5123456789[root@sht-sgmhadoopdn-04 local]# wget http://download.redis.io/releases/redis-3.2.5.tar.gz--2016-11-12 20:16:40-- http://download.redis.io/releases/redis-3.2.5.tar.gzResolving download.redis.io (download.redis.io)... 109.74.203.151Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1544040 (1.5M) [application/x-gzip]Saving to: â€˜redis-3.2.5.tar.gzâ€™100%[==========================================================================================================================&gt;] 1,544,040 221KB/s in 6.8s 2016-11-12 20:16:47 (221 KB/s) - â€˜redis-3.2.5.tar.gzâ€™ saved [1544040/1544040]2.3 å®‰è£…redis12345678910111213[root@sht-sgmhadoopdn-04 local]# mkdir /usr/local/redis[root@sht-sgmhadoopdn-04 local]# tar xzvf redis-3.2.5.tar.gz[root@sht-sgmhadoopdn-04 local]# cd redis-3.2.5[root@sht-sgmhadoopdn-04 redis-3.2.5]# make PREFIX=/usr/local/redis install[root@sht-sgmhadoopdn-04 redis-3.2.5]# cd ../[root@sht-sgmhadoopdn-04 redis-3.2.5]# ll /usr/local/redis/bin/total 15056-rwxr-xr-x 1 root root 2431728 Nov 12 20:45 redis-benchmark-rwxr-xr-x 1 root root 25165 Nov 12 20:45 redis-check-aof-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-check-rdb-rwxr-xr-x 1 root root 2584443 Nov 12 20:45 redis-clilrwxrwxrwx 1 root root 12 Nov 12 20:45 redis-sentinel -&gt; redis-server-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-server2.4 é…ç½®redisä¸ºæœåŠ¡1234567891011121314[root@server redis-3.2.5]# cp utils/redis_init_script /etc/rc.d/init.d/redis[root@server redis-3.2.5]# vi /etc/rc.d/init.d/redis åœ¨ç¬¬äºŒè¡Œæ·»åŠ ï¼š#chkconfig: 2345 80 90EXEC=/usr/local/bin/redis-server ä¿®æ”¹æˆ EXEC=/usr/local/redis/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cli ä¿®æ”¹æˆ CLIEXEC=/usr/local/redis/bin/redis-cliCONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot; ä¿®æ”¹æˆ CONF=&quot;/usr/local/redis/conf/$&#123;REDISPORT&#125;.conf&quot;$EXEC $CONF ä¿®æ”¹æˆ $EXEC $CONF &amp;[root@server redis-3.2.5]# mkdir /usr/local/redis/conf/[root@server redis-3.2.5]# chkconfig --add redis[root@server redis-3.2.5]# cp redis.conf /usr/local/redis/conf/6379.conf [root@server redis-3.2.5]# vi /usr/local/redis/conf/6379.conf daemonize yespidfile /var/run/redis_6379.pidbind 172.16.101.662.5 å¯åŠ¨redis123456[root@server redis-3.2.5]# cd ../redis[root@sht-sgmhadoopdn-04 redis]# service redis startStarting Redis server...[root@sht-sgmhadoopdn-04 redis]# netstat -tnlp|grep redistcp 0 0 172.16.100.79:6379 0.0.0.0:* LISTEN 30032/redis-server [root@sht-sgmhadoopdn-04 redis]#2.6 æ·»åŠ ç¯å¢ƒå˜é‡123456[root@sht-sgmhadoopdn-04 redis]# vi /etc/profileexport REDIS_HOME=/usr/local/redisexport PATH=$REDIS_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@sht-sgmhadoopdn-04 redis]# source /etc/profile[root@sht-sgmhadoopdn-04 redis]# which redis-cli/usr/local/redis/bin/redis-cli2.7 æµ‹è¯• å’Œ è®¾ç½®å¯†ç (æœ¬æ¬¡å®éªŒæœªè®¾ç½®å¯†ç )1234567891011121314151617181920212223242526[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; sht-sgmhadoopdn-04:6379&gt; set testkey testvalue OKsht-sgmhadoopdn-04:6379&gt; get test(nil)sht-sgmhadoopdn-04:6379&gt; get testkey&quot;testvalue&quot;sht-sgmhadoopdn-04:6379&gt;[root@sht-sgmhadoopdn-04 redis]# vi /usr/local/redis/conf/6379.conf /*æ·»åŠ ä¸€ä¸ªéªŒè¯å¯†ç */requirepass 123456[root@sht-sgmhadoopdn-04 redis]# service redis stop[root@sht-sgmhadoopdn-04 redis]# service redis start[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; set key ss(error) NOAUTH Authentication required. [root@server redis-3.2.5]# redis-cli -h sht-sgmhadoopdn-04 -a 123456sht-sgmhadoopdn-04:6379&gt; set a bOKsht-sgmhadoopdn-04:6379&gt; get a&quot;b&quot;sht-sgmhadoopdn-04:6379&gt; exit;[root@sht-sgmhadoopdn-04 redis]#]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹åŸºäºSpark Streamingå¼€å‘OnLineLogAanlysis1]]></title>
    <url>%2F2018%2F09%2F11%2F10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1%2F</url>
    <content type="text"><![CDATA[1.GitHubhttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java2.ä½¿ç”¨IDEA æœ¬åœ°è¿è¡Œæµ‹è¯•ï¼ˆæœªæ‰“jaråŒ…ï¼‰]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹åŸºäºSpark Streaming Directæ–¹å¼çš„WordCountæœ€è¯¦ç»†æ¡ˆä¾‹(javaç‰ˆ)]]></title>
    <url>%2F2018%2F09%2F10%2F09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)%2F</url>
    <content type="text"><![CDATA[1.å‰æa. flume æ”¶é›†â€“ã€‹flume èšåˆâ€“ã€‹kafka ï¼Œå¯åŠ¨è¿›ç¨‹å’Œå¯åŠ¨kafka managerç›‘æ§ï¼Œå‚è€ƒ08ã€åœ¨çº¿æ—¥å¿—åˆ†æã€‘ä¹‹Flume Agent(èšåˆèŠ‚ç‚¹) sink to kafka clusterb.window7 å®‰è£…jdk1.7 æˆ–è€…1.8(æœ¬æ¬¡ç¯å¢ƒæ˜¯1.8)c.window7 å®‰è£…IDEAå¼€å‘å·¥å…·(ä»¥ä¸‹ä»…ä¾›å‚è€ƒ)ä½¿ç”¨IntelliJ IDEA é…ç½®Mavenï¼ˆå…¥é—¨ï¼‰:http://blog.csdn.net/qq_32588349/article/details/51461182IDEA Java/Scalaæ··åˆé¡¹ç›®Mavenæ‰“åŒ…:http://blog.csdn.net/rongyongfeikai2/article/details/51404611Intellij ideaä½¿ç”¨javaç¼–å†™å¹¶æ‰§è¡Œsparkç¨‹åº:http://blog.csdn.net/yhao2014/article/details/442390212.æºä»£ç ï¼ˆå¯ä¸‹è½½å•ä¸ªjavaæ–‡ä»¶ï¼ŒåŠ å…¥projet æˆ–è€… æ•´ä¸ªå·¥ç¨‹ä¸‹è½½ï¼ŒIDEAé€‰æ‹©open å³å¯ï¼‰GitHub: https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java3.ä½¿ç”¨IDEA æœ¬åœ°è¿è¡Œæµ‹è¯•ï¼ˆæœªæ‰“jaråŒ…ï¼‰æµ·åº·å¨è§†æ ¡æ‹›ç”µè¯é¢è¯•ï¼š1.æ•°æ®å€¾æ–œçš„è§£å†³ï¼Œæ€ä¹ˆçŸ¥é“å“ªé‡Œå€¾æ–œ2.è‡ªå®šä¹‰ç±»çš„å¹¿æ’­3.cacheæœºåˆ¶ï¼Œrddå’Œdfçš„cacheä»€ä¹ˆåŒºåˆ«4.sparkåŠ¨æ€å†…å­˜ï¼Œå †å†…å’Œå †å¤–5.rddç®—å­ï¼Œmap,mappartitions,foreachï¼Œunion6.å®½ä¾èµ–ï¼Œçª„ä¾èµ–7.spark DAGè¿‡ç¨‹ï¼ŒdoOnreciveï¼Œeventloopæ‰§è¡Œè¿‡ç¨‹8.stageå’Œtaskæ€ä¹ˆåˆ†ç±»9.sparkè°ƒä¼˜10.æ¦‚å¿µï¼Œexecutorï¼Œworkerï¼Œjob,taskå’Œpartitionçš„å…³ç³»11.ç”¨æ²¡ç”¨è¿‡sparkä»€ä¹ˆlogï¼Œæ²¡è®°ä½12.è®²è®²sparkSQLæ•°æ®æ¸…æ´—è¿‡ç¨‹13.æå¸¦ä¸€ç‚¹é¡¹ç›®]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹Flume Agent(èšåˆèŠ‚ç‚¹) sink to kafka cluster]]></title>
    <url>%2F2018%2F09%2F07%2F08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster%2F</url>
    <content type="text"><![CDATA[1.åˆ›å»ºlogtopic1[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic logtopic2.åˆ›å»ºavro_memory_kafka.properties (kafka sink)12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# cp avro_memory_hdfs.properties avro_memory_kafka.properties[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_kafka.properties #Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = logtopica1.sinks.k1.kafka.bootstrap.servers = 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092a1.sinks.k1.kafka.flumeBatchSize = 6000a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1a1.sinks.ki.kafka.producer.compression.type = snappy#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90a1.channels.c1.capacity = 2000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c13.åå°å¯åŠ¨ flume-ng agent(èšåˆèŠ‚ç‚¹)å’ŒæŸ¥çœ‹nohup.out123456789[root@sht-sgmhadoopcm-01 ~]# source /etc/profile[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/[root@sht-sgmhadoopcm-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_kafka.properties -n a1 -Dflume.root.logger=INFO,console &amp;[1] 4971[root@sht-sgmhadoopcm-01 flume-ng]# nohup: ignoring input and appending output to `nohup.out&apos;[root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# cat nohup.out4.æ£€æŸ¥logæ”¶é›†çš„ä¸‰å°(æ”¶é›†èŠ‚ç‚¹)å¼€å¯æ²¡12345678[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;5.æ‰“å¼€kafka managerç›‘æ§http://172.16.101.55:9999]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹kafka-managerç›‘æ§å·¥å…·çš„æ­å»º(sbtå®‰è£…ä¸ç¼–è¯‘)]]></title>
    <url>%2F2018%2F09%2F06%2F07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½sbt123456http://www.scala-sbt.org/download.html[root@sht-sgmhadoopnn-01 app]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring sbt-0.13.13.tgz... 100% 1025 KB 1025 KB/sec 00:00:01 0 Errors2.è§£å‹1234567891011[root@sht-sgmhadoopnn-01 app]# tar -zxvf sbt-0.13.13.tgzsbt-launcher-packaging-0.13.13/sbt-launcher-packaging-0.13.13/conf/sbt-launcher-packaging-0.13.13/conf/sbtconfig.txtsbt-launcher-packaging-0.13.13/conf/sbtoptssbt-launcher-packaging-0.13.13/bin/sbt-launcher-packaging-0.13.13/bin/sbt.batsbt-launcher-packaging-0.13.13/bin/sbtsbt-launcher-packaging-0.13.13/bin/sbt-launch.jarsbt-launcher-packaging-0.13.13/bin/sbt-launch-lib.bash[root@sht-sgmhadoopnn-01 app]# mv sbt-launcher-packaging-0.13.13 sbt3.æ·»åŠ è„šæœ¬æ–‡ä»¶12345[root@sht-sgmhadoopnn-01 bin]# vi sbt#!/usr/bin/env bashBT_OPTS=&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;java $SBT_OPTS -jar /root/learnproject/app/sbt/bin/sbt-launch.jar &quot;$@&quot;4.ä¿®æ”¹æƒé™å’Œç¯å¢ƒå˜é‡123456[root@sht-sgmhadoopnn-01 bin]# chmod u+x sbt[root@sht-sgmhadoopnn-01 bin]# vi /etc/profileexport SBT_HOME=/root/learnproject/app/sbtexport PATH=$SBT_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&quot;/etc/profile&quot; 94L, 2265C written[root@sht-sgmhadoopnn-01 bin]# source /etc/profile5.æµ‹è¯•12345678/*ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ï¼Œä¼šä¸‹è½½ä¸€äº›æ–‡ä»¶åŒ…ï¼Œç„¶åæ‰èƒ½æ­£å¸¸ä½¿ç”¨ï¼Œè¦ç¡®ä¿è”ç½‘äº†ï¼Œå®‰è£…æˆåŠŸåæ˜¾ç¤ºå¦‚ä¸‹*/[root@sht-sgmhadoopnn-01 bin]# sbt sbt-version[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[root@sht-sgmhadoopnn-01 bin]#]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹ KafkaOffsetMonitorç›‘æ§å·¥å…·çš„æ­å»º]]></title>
    <url>%2F2018%2F09%2F05%2F06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½åœ¨window7 æ‰‹å·¥ä¸‹è½½å¥½ä¸‹é¢çš„é“¾æ¥1https://github.com/quantifind/KafkaOffsetMonitor/releases/tag/v0.2.112345678910[root@sht-sgmhadoopnn-01 app]# mkdir kafkaoffsetmonitor[root@sht-sgmhadoopnn-01 app]# cd kafkaoffsetmonitor#ä½¿ç”¨rzå‘½ä»¤ä¸Šä¼ [root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring KafkaOffsetMonitor-assembly-0.2.1.jar... 100% 51696 KB 12924 KB/sec 00:00:04 0 Errors You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#2.æ–°å»ºä¸€ä¸ªkafkaMonitor.shæ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š12345678910[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# vi kafkaoffsetmonitor.sh! /bin/bashjava -cp KafkaOffsetMonitor-assembly-0.2.1.jar \com.quantifind.kafka.offsetapp.OffsetGetterWeb \--zk 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka \--port 8089 \--refresh 5.seconds \--retain 7.days[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# chmod +x *.sh[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#å‚æ•°è¯´æ˜ï¼šâ€“zk è¿™é‡Œå†™çš„åœ°å€å’Œç«¯å£ï¼Œæ˜¯zookeeperé›†ç¾¤çš„å„ä¸ªåœ°å€å’Œç«¯å£ã€‚åº”å’Œkafka/binæ–‡ä»¶å¤¹ä¸­çš„zookeeper.propertiesä¸­çš„host.nameå’ŒclientPortä¸€è‡´ã€‚â€“port è¿™ä¸ªæ˜¯æœ¬è½¯ä»¶KafkaOffsetMonitorçš„ç«¯å£ã€‚æ³¨æ„ä¸è¦ä½¿ç”¨é‚£äº›è‘—åçš„ç«¯å£å·ï¼Œä¾‹å¦‚80,8080ç­‰ã€‚æˆ‘é‡‡ç”¨äº†8089.â€“refresh è¿™ä¸ªæ˜¯è½¯ä»¶åˆ·æ–°é—´éš”æ—¶é—´ï¼Œä¸è¦å¤ªçŸ­ä¹Ÿä¸è¦å¤ªé•¿ã€‚â€“retain è¿™ä¸ªæ˜¯æ•°æ®åœ¨æ•°æ®åº“ä¸­ä¿å­˜çš„æ—¶é—´ã€‚3.åå°å¯åŠ¨1234567891011121314 1[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# nohup ./kafkaoffsetmonitor.sh &amp; 2serving resources from: jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp 3SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;. 4SLF4J: Defaulting to no-operation (NOP) logger implementation 5SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. 6log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection). 7log4j:WARN Please initialize the log4j system properly. 8log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 9log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).10log4j:WARN Please initialize the log4j system properly.11log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.122016-12-25 22:00:24.252:INFO:oejs.Server:jetty-7.x.y-SNAPSHOT132016-12-25 22:00:24.319:INFO:oejsh.ContextHandler:started o.e.j.s.ServletContextHandler&#123;/,jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp&#125;142016-12-25 22:00:24.328:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:80894.IEæµè§ˆå™¨æ‰“å¼€1http://172.16.101.55:8089]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹Kafka 0.10.1.0 Clusterçš„æ­å»ºå’ŒTopicç®€å•æ“ä½œå®éªŒ]]></title>
    <url>%2F2018%2F09%2F04%2F05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[ã€kafka clusteræœºå™¨ã€‘:æœºå™¨åç§° ç”¨æˆ·åç§°sht-sgmhadoopdn-01/02/03 rootã€å®‰è£…ç›®å½•ã€‘: /root/learnproject/app1.å°†scalaæ–‡ä»¶å¤¹åŒæ­¥åˆ°é›†ç¾¤å…¶ä»–æœºå™¨(scala 2.11ç‰ˆæœ¬ï¼Œå¯å•ç‹¬ä¸‹è½½è§£å‹)123456789101112131415161718192021[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-01:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-02:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-03:/root/learnproject/app/#ç¯å¢ƒå˜é‡[root@sht-sgmhadoopdn-01 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 app]# source /etc/profile[root@sht-sgmhadoopdn-02 app]# source /etc/profile[root@sht-sgmhadoopdn-03 app]# source /etc/profile2.ä¸‹è½½åŸºäºScala 2.11çš„kafkaç‰ˆæœ¬ä¸º0.10.1.012345[root@sht-sgmhadoopdn-01 app]# pwd/root/learnproject/app[root@sht-sgmhadoopdn-01 app]# wget http://www-eu.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz[root@sht-sgmhadoopdn-01 app]# tar xzvf kafka_2.11-0.10.1.0.tgz [root@sht-sgmhadoopdn-01 app]# mv kafka_2.11-0.10.1.0 kafka3.åˆ›å»ºlogsç›®å½•å’Œä¿®æ”¹server.properties(å‰æzookeeper clusteréƒ¨ç½²å¥½ï¼Œè§â€œ03ã€åœ¨çº¿æ—¥å¿—åˆ†æã€‘ä¹‹hadoop-2.7.3ç¼–è¯‘å’Œæ­å»ºé›†ç¾¤ç¯å¢ƒ(HDFS HA,Yarn HA)â€ )123456789[root@sht-sgmhadoopdn-01 app]# cd kafka[root@sht-sgmhadoopdn-01 kafka]# mkdir logs[root@sht-sgmhadoopdn-01 kafka]# cd config/[root@sht-sgmhadoopdn-01 config]# vi server.propertiesbroker.id=1port=9092host.name=172.16.101.58log.dirs=/root/learnproject/app/kafka/logszookeeper.connect=172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka4.åŒæ­¥åˆ°02/03æœåŠ¡å™¨ï¼Œæ›´æ”¹broker.id åŠhost.name123456789101112[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-02 config]# vi server.properties broker.id=2port=9092host.name=172.16.101.59[root@sht-sgmhadoopdn-03 config]# vi server.properties broker.id=3port=9092host.name=172.16.101.605.ç¯å¢ƒå˜é‡1234567891011[root@sht-sgmhadoopdn-01 kafka]# vi /etc/profileexport KAFKA_HOME=/root/learnproject/app/kafkaexport PATH=$KAFKA_HOME/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-02:/etc/profile[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-03:/etc/profile[root@sht-sgmhadoopdn-01 kafka]#[root@sht-sgmhadoopdn-01 kafka]# source /etc/profile[root@sht-sgmhadoopdn-02 kafka]# source /etc/profile[root@sht-sgmhadoopdn-03 kafka]# source /etc/profile6.å¯åŠ¨/åœæ­¢123456[root@sht-sgmhadoopdn-01 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-02 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-03 kafka]# nohup kafka-server-start.sh config/server.properties &amp;###åœæ­¢bin/kafka-server-stop.sh7.topicç›¸å…³çš„æ“ä½œ12345678910111213141516171819202122232425262728293031323334353637a.åˆ›å»ºtopicï¼Œå¦‚èƒ½æˆåŠŸåˆ›å»ºtopicåˆ™è¡¨ç¤ºé›†ç¾¤å®‰è£…å®Œæˆï¼Œä¹Ÿå¯ä»¥ç”¨jpså‘½ä»¤æŸ¥çœ‹kafkaè¿›ç¨‹æ˜¯å¦å­˜åœ¨ã€‚[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic testb.é€šè¿‡listå‘½ä»¤æŸ¥çœ‹åˆ›å»ºçš„topic:[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --list --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafkac.æŸ¥çœ‹åˆ›å»ºçš„Topic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:1 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2[root@sht-sgmhadoopdn-01 kafka]# ç¬¬ä¸€è¡Œåˆ—å‡ºäº†è¿™ä¸ªtopicçš„æ€»ä½“æƒ…å†µï¼Œå¦‚topicåç§°ï¼Œåˆ†åŒºæ•°é‡ï¼Œå‰¯æœ¬æ•°é‡ç­‰ã€‚ç¬¬äºŒè¡Œå¼€å§‹ï¼Œæ¯ä¸€è¡Œåˆ—å‡ºäº†ä¸€ä¸ªåˆ†åŒºçš„ä¿¡æ¯ï¼Œå¦‚å®ƒæ˜¯ç¬¬å‡ ä¸ªåˆ†åŒºï¼Œè¿™ä¸ªåˆ†åŒºçš„leaderæ˜¯å“ªä¸ªbrokerï¼Œå‰¯æœ¬ä½äºå“ªäº›brokerï¼Œæœ‰å“ªäº›å‰¯æœ¬å¤„ç†åŒæ­¥çŠ¶æ€ã€‚Partitionï¼š åˆ†åŒºLeader ï¼š è´Ÿè´£è¯»å†™æŒ‡å®šåˆ†åŒºçš„èŠ‚ç‚¹Replicas ï¼š å¤åˆ¶è¯¥åˆ†åŒºlogçš„èŠ‚ç‚¹åˆ—è¡¨Isr ï¼š â€œin-syncâ€ replicasï¼Œå½“å‰æ´»è·ƒçš„å‰¯æœ¬åˆ—è¡¨ï¼ˆæ˜¯ä¸€ä¸ªå­é›†ï¼‰ï¼Œå¹¶ä¸”å¯èƒ½æˆä¸ºLeaderæˆ‘ä»¬å¯ä»¥é€šè¿‡Kafkaè‡ªå¸¦çš„bin/kafka-console-producer.shå’Œbin/kafka-console-consumer.shè„šæœ¬ï¼Œæ¥éªŒè¯æ¼”ç¤ºå¦‚æœå‘å¸ƒæ¶ˆæ¯ã€æ¶ˆè´¹æ¶ˆæ¯ã€‚d.åˆ é™¤topic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --delete --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic teste.ä¿®æ”¹topicä½¿ç”¨â€”-alertåŸåˆ™ä¸Šå¯ä»¥ä¿®æ”¹ä»»ä½•é…ç½®ï¼Œä»¥ä¸‹åˆ—å‡ºäº†ä¸€äº›å¸¸ç”¨çš„ä¿®æ”¹é€‰é¡¹ï¼šï¼ˆ1ï¼‰æ”¹å˜åˆ†åŒºæ•°é‡[root@sht-sgmhadoopdn-02 kafka]#bin/kafka-topics.sh --alter --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test --partitions 3[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:3 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: test Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: test Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1[root@sht-sgmhadoopdn-02 kafka]#ï¼ˆ2ï¼‰å¢åŠ ã€ä¿®æ”¹æˆ–è€…åˆ é™¤ä¸€ä¸ªé…ç½®å‚æ•° bin/kafka-topics.sh â€”alter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --config key=value bin/kafka-topics.sh â€”alter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --deleteConfig key8.æ¨¡æ‹Ÿå®éªŒ11234567åœ¨ä¸€ä¸ªç»ˆç«¯ï¼Œå¯åŠ¨Producerï¼Œå¹¶å‘æˆ‘ä»¬ä¸Šé¢åˆ›å»ºçš„åç§°ä¸ºmy-replicated-topic5çš„Topicä¸­ç”Ÿäº§æ¶ˆæ¯ï¼Œæ‰§è¡Œå¦‚ä¸‹è„šæœ¬ï¼š[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-console-producer.sh --broker-list 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092 --topic teståœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼Œå¯åŠ¨Consumerï¼Œå¹¶è®¢é˜…æˆ‘ä»¬ä¸Šé¢åˆ›å»ºçš„åç§°ä¸ºmy-replicated-topic5çš„Topicä¸­ç”Ÿäº§çš„æ¶ˆæ¯ï¼Œæ‰§è¡Œå¦‚ä¸‹è„šæœ¬ï¼š[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --from-beginning --topic testå¯ä»¥åœ¨Producerç»ˆç«¯ä¸Šè¾“å…¥å­—ç¬¦ä¸²æ¶ˆæ¯è¡Œï¼Œå°±å¯ä»¥åœ¨Consumerç»ˆç«¯ä¸Šçœ‹åˆ°æ¶ˆè´¹è€…æ¶ˆè´¹çš„æ¶ˆæ¯å†…å®¹ã€‚]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹Flume Agentçš„3å°æ”¶é›†+1å°èšåˆåˆ°hdfsçš„æ­å»º]]></title>
    <url>%2F2018%2F09%2F03%2F04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86%2B1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[ã€logæ”¶é›†ã€‘:æœºå™¨åç§° æœåŠ¡åç§° ç”¨æˆ·flume-agent-01: namenode hdfsflume-agent-02: datanode hdfsflume-agent-03: datanode hdfsã€logèšåˆã€‘:æœºå™¨åç§° ç”¨æˆ·sht-sgmhadoopcm-01(172.16.101.54) rootã€sinkåˆ°hdfsã€‘:hdfs://172.16.101.56:8020/testwjp/1.ä¸‹è½½apache-flume-1.7.0-bin.tar.gz1234567891011[hdfs@flume-agent-01 tmp]$ wget http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz--2017-01-04 20:40:10-- http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gzResolving www-eu.apache.org... 88.198.26.2, 2a01:4f8:130:2192::2Connecting to www-eu.apache.org|88.198.26.2|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 55711670 (53M) [application/x-gzip]Saving to: â€œapache-flume-1.7.0-bin.tar.gzâ€100%[===============================================================================================================================================================================================&gt;] 55,711,670 473K/s in 74s 2017-01-04 20:41:25 (733 KB/s) - â€œapache-flume-1.7.0-bin.tar.gzâ€ saved [55711670/55711670]2.è§£å‹é‡å‘½å1234[hdfs@flume-agent-01 tmp]$ [hdfs@flume-agent-01 tmp]$ tar -xzvf apache-flume-1.7.0-bin.tar.gz [hdfs@flume-agent-01 tmp]$ mv apache-flume-1.7.0-bin flume-ng[hdfs@flume-agent-01 tmp]$ cd flume-ng/conf3.æ‹·è´flumeç¯å¢ƒé…ç½®å’Œagenté…ç½®æ–‡ä»¶12[hdfs@flume-agent-01 tmp]$ cp flume-env.sh.template flume-env.sh[hdfs@flume-agent-01 tmp]$ cp flume-conf.properties.template exec_memory_avro.properties4.æ·»åŠ hdfsç”¨æˆ·çš„ç¯å¢ƒå˜é‡æ–‡ä»¶123456789101112131415161718192021[hdfs@flume-agent-01 tmp]$ cd[hdfs@flume-agent-01 ~]$ ls -latotal 24drwxr-xr-x 3 hdfs hadoop 4096 Jul 8 14:05 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_historydrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo[hdfs@flume-agent-01 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-01 ~]$ ls -latotal 36drwxr-xr-x 3 hdfs hadoop 4096 Jan 4 20:49 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_history-rw-r--r-- 1 hdfs hdfs 18 Jan 4 20:49 .bash_logout-rw-r--r-- 1 hdfs hdfs 176 Jan 4 20:49 .bash_profile-rw-r--r-- 1 hdfs hdfs 124 Jan 4 20:49 .bashrcdrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo5.æ·»åŠ flumeçš„ç¯å¢ƒå˜é‡123456[hdfs@flume-agent-01 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-01 ~]$ . .bash_profile6.ä¿®æ”¹flumeç¯å¢ƒé…ç½®æ–‡ä»¶12[hdfs@flume-agent-01 conf]$ vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_257.å°†åŸºäºFlume-ng Exec Sourceå¼€å‘è‡ªå®šä¹‰æ’ä»¶AdvancedExecSourceçš„AdvancedExecSource.jaråŒ…ä¸Šä¼ åˆ°$FLUME_HOME/lib/1http://blog.itpub.net/30089851/viewspace-2131995/12345[hdfs@LogshedNameNodeLogcollector lib]$ pwd/tmp/flume-ng/lib[hdfs@LogshedNameNodeLogcollector lib]$ ll AdvancedExecSource.jar -rw-r--r-- 1 hdfs hdfs 10618 Jan 5 23:50 AdvancedExecSource.jar[hdfs@LogshedNameNodeLogcollector lib]$8.ä¿®æ”¹flumeçš„agenté…ç½®æ–‡ä»¶1234567891011121314151617181920212223242526[hdfs@flume-agent-01 conf]$ vi exec_memory_avro.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the custom exec sourcea1.sources.r1.type = com.onlinelog.analysis.AdvancedExecSourcea1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-NAMENODE-flume-agent-01.log.outa1.sources.r1.hostname = flume-agent-01a1.sources.r1.servicename = namenode#Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 172.16.101.54a1.sinks.k1.port = 4545#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 60 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 2000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c19.å°†flume-agent-01çš„flume-ngæ‰“åŒ…,scpåˆ°flume-agent-02/03 å’Œ sht-sgmhadoopcm-01(172.16.101.54)12345[hdfs@flume-agent-01 tmp]$ zip -r flume-ng.zip flume-ng/*[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-02:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-03:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip sht-sgmhadoopcm-01:/tmp/10.åœ¨flume-agent-02é…ç½®hdfsç”¨æˆ·ç¯å¢ƒå˜é‡å’Œè§£å‹ï¼Œä¿®æ”¹agenté…ç½®æ–‡ä»¶12345678910111213141516171819[hdfs@flume-agent-02 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-02 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-02 ~]$ . .bash_profile[hdfs@flume-agent-02 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-02 tmp]$ cd flume-ng/conf##ä¿®æ”¹ä»¥ä¸‹å‚æ•°å³å¯[hdfs@flume-agent-02 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-02.log.outa1.sources.r1.hostname = flume-agent-02a1.sources.r1.servicename = datanode###è¦æ£€æŸ¥flume-env.shçš„JAVA_HOMEç›®å½•æ˜¯å¦å­˜åœ¨11.åœ¨flume-agent-03é…ç½®hdfsç”¨æˆ·ç¯å¢ƒå˜é‡å’Œè§£å‹ï¼Œä¿®æ”¹agenté…ç½®æ–‡ä»¶12345678910111213141516171819[hdfs@flume-agent-03 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-03 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-03 ~]$ . .bash_profile[hdfs@flume-agent-03 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-03 tmp]$ cd flume-ng/conf##ä¿®æ”¹ä»¥ä¸‹å‚æ•°å³å¯[hdfs@flume-agent-03 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-03.log.outa1.sources.r1.hostname = flume-agent-03a1.sources.r1.servicename = datanode###è¦æ£€æŸ¥flume-env.shçš„JAVA_HOMEç›®å½•æ˜¯å¦å­˜åœ¨12.èšåˆç«¯ sht-sgmhadoopcm-01ï¼Œé…ç½®rootç”¨æˆ·ç¯å¢ƒå˜é‡å’Œè§£å‹ï¼Œä¿®æ”¹agenté…ç½®æ–‡ä»¶1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@sht-sgmhadoopcm-01 tmp]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$FLUME_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopcm-01 tmp]# source /etc/profile[root@sht-sgmhadoopcm-01 tmp]#[root@sht-sgmhadoopcm-01 tmp]# unzip flume-ng.zip[root@sht-sgmhadoopcm-01 tmp]# cd flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera ###æµ‹è¯•: å…ˆèšåˆ, sinkåˆ°hdfsç«¯[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_hdfs.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://172.16.101.56:8020/testwjp/a1.sinks.k1.hdfs.filePrefix = logsa1.sinks.k1.hdfs.inUsePrefix = .a1.sinks.k1.hdfs.rollInterval = 0###roll 16 m = 16777216 bytesa1.sinks.k1.hdfs.rollSize = 1048576a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.batchSize = 6000a1.sinks.k1.hdfs.writeFormat = texta1.sinks.k1.hdfs.fileType = DataStream#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c113.åå°å¯åŠ¨123456789101112[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[root@sht-sgmhadoopnn-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_hdfs.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;14.æ ¡éªŒï¼šå°†é›†ç¾¤çš„æ—¥å¿—ä¸‹è½½åˆ°æœ¬åœ°ï¼Œæ‰“å¼€æŸ¥çœ‹å³å¯(ç•¥)12345678910111213141516171819202122232425262728293031------------------------------------------------------------------------------------------------------------------------------------------------ã€å¤‡æ³¨ã€‘: 1.é”™è¯¯1 flume-ngå®‰è£…çš„æœºå™¨ä¸Šæ²¡æœ‰hadoopç¯å¢ƒï¼Œæ‰€ä»¥å‡å¦‚sinkåˆ°hdfsè¯ï¼Œéœ€è¦ç”¨åˆ°hdfsçš„jaråŒ…[ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:146)] Failed to start agent because dependencies were not found in classpath. Error follows.java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionTypeåªéœ€åœ¨å…¶ä»–å®‰è£…hadoopæœºå™¨ä¸Šæœç´¢ä»¥ä¸‹5ä¸ªjaråŒ…ï¼Œæ‹·è´åˆ°$FLUME_HOME/libç›®å½•å³å¯ã€‚æœç´¢æ–¹æ³•: find $HADOOP_HOME/ -name commons-configuration*.jarcommons-configuration-1.6.jarhadoop-auth-2.7.3.jarhadoop-common-2.7.3.jarhadoop-hdfs-2.7.3.jarhadoop-mapreduce-client-core-2.7.3.jarprotobuf-java-2.5.0.jarhtrace-core-3.1.0-incubating.jarcommons-io-2.4.jar2.é”™è¯¯2 æ— æ³•åŠ è½½è‡ªå®šä¹‰æ’ä»¶çš„ç±» Unable to load source type: com.onlinelog.analysis.AdvancedExecSource2017-01-06 21:10:48,278 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:142)] Failed to load configuration data. Exception follows.org.apache.flume.FlumeException: Unable to load source type: com.onlinelog.analysis.AdvancedExecSource, class: com.onlinelog.analysis.AdvancedExecSourceæ‰§è¡Œhdfsæˆ–è€…rootç”¨æˆ·çš„ç¯å¢ƒå˜é‡å³å¯[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹hadoop-2.7.3ç¼–è¯‘å’Œæ­å»ºé›†ç¾¤ç¯å¢ƒ(HDFS HA,Yarn HA)]]></title>
    <url>%2F2018%2F09%2F03%2F03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA%2CYarn%20HA)%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½hadoop2.7.3æœ€æ–°æºç 123456789101112131415161718192021222324252627282930313233343536373839404142[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt Build instructions for Hadoop----------------------------------------------------------------------------------Requirements:* Unix System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac* Zlib devel (if compiling native code)* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )* Internet connection for first build (to fetch all Maven and Hadoop dependencies)----------------------------------------------------------------------------------Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:* Oracle JDK 1.7 (preferred) $ sudo apt-get purge openjdk* $ sudo apt-get install software-properties-common $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-java7-installer* Maven $ sudo apt-get -y install maven* Native libraries $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev* ProtocolBuffer 2.5.0 (required) $ sudo apt-get -y install libprotobuf-dev protobuf-compilerOptional packages:* Snappy compression $ sudo apt-get install snappy libsnappy-dev* Bzip2 $ sudo apt-get install bzip2 libbz2-dev* Jansson (C Library for JSON) $ sudo apt-get install libjansson-dev* Linux FUSE $ sudo apt-get install fuse libfuse-dev2.å®‰è£…ä¾èµ–åŒ…1[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*3.å®‰è£…jdk12345678910[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport PATH=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# java -versionjava version &quot;1.7.0_67&quot;Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 compilesoft]#4.å®‰è£…maven123456789101112131415161718[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9#åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ä¸ºäº†é˜²æ­¢Javaå†…å­˜æº¢å‡ºï¼Œéœ€è¦åŠ å…¥ä»¥ä¸‹ç¯å¢ƒå˜é‡export MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# mvn -versionApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9Java version: 1.7.0_67, vendor: Oracle CorporationJava home: /usr/java/jdk1.7.0_67-cloudera/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;You have new mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#5.ç¼–è¯‘å®‰è£…protobuf12345678910111213[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/[root@hadoop-01 protobuf-2.5.0]# ./configure [root@hadoop-01 protobuf-2.5.0]# make[root@hadoop-01 protobuf-2.5.0]# make install#æŸ¥çœ‹protobufç‰ˆæœ¬ä»¥æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸ[root@hadoop-01 protobuf-2.5.0]# protoc --versionprotoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib[root@hadoop-01 protobuf-2.5.0]# protoc --versionlibprotoc 2.5.0[root@hadoop-01 protobuf-2.5.0]#6.å®‰è£…snappy123456789101112131415[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz#ç”¨rootç”¨æˆ·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install#æŸ¥çœ‹snappyåº“æ–‡ä»¶[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy-rw-r--r-- 1 root root 229K Jun 21 15:46 libsnappy.a-rwxr-xr-x 1 root root 953 Jun 21 15:46 libsnappy.lalrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0lrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0-rwxr-xr-x 1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0[root@sht-sgmhadoopnn-01 snappy-1.1.1]#7.ç¼–è¯‘123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-srcmvn clean package -Pdist,native -DskipTests -Dtaræˆ–mvn package -Pdist,native -DskipTests -Dtar[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package â€“Pdist,native â€“DskipTests â€“Dtar[INFO] Executing tasksmain: [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3 [exec] $ gzip -f hadoop-2.7.3.tar [exec] [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz [exec] [INFO] Executed tasks[INFO] [INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s][INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 6.832 s][INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s][INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s][INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.411 s][INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 4.814 s][INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s][INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min][INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s][INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s][INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min][INFO] Apache Hadoop NFS .................................. SUCCESS [ 9.285 s][INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s][INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.049 s][INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min][INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min][INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s][INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 4.599 s][INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.044 s][INFO] hadoop-yarn ........................................ SUCCESS [ 0.043 s][INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min][INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s][INFO] hadoop-yarn-server ................................. SUCCESS [ 0.041 s][INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s][INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s][INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 6.415 s][INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s][INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s][INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 7.751 s][INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s][INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [ 5.612 s][INFO] hadoop-yarn-applications ........................... SUCCESS [ 0.038 s][INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [ 4.029 s][INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 2.611 s][INFO] hadoop-yarn-site ................................... SUCCESS [ 0.077 s][INFO] hadoop-yarn-registry ............................... SUCCESS [ 8.045 s][INFO] hadoop-yarn-project ................................ SUCCESS [ 5.456 s][INFO] hadoop-mapreduce-client ............................ SUCCESS [ 0.226 s][INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s][INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s][INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 6.697 s][INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s][INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 9.328 s][INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s][INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [ 2.412 s][INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 8.876 s][INFO] hadoop-mapreduce ................................... SUCCESS [ 4.237 s][INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s][INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s][INFO] Apache Hadoop Archives ............................. SUCCESS [ 3.069 s][INFO] Apache Hadoop Rumen ................................ SUCCESS [ 7.446 s][INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 5.765 s][INFO] Apache Hadoop Data Join ............................ SUCCESS [ 3.752 s][INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 2.771 s][INFO] Apache Hadoop Extras ............................... SUCCESS [ 5.612 s][INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s][INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 7.131 s][INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min][INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s][INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s][INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 1.142 s][INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 7.354 s][INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s][INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.035 s][INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 26:29 min[INFO] Finished at: 2016-12-24T21:07:09+08:00[INFO] Final Memory: 214M/740M[INFO] ------------------------------------------------------------------------You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# [root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../[root@sht-sgmhadoopnn-01 learnproject]# lltotal 193152drwxr-xr-x 5 root root 4096 Dec 24 20:24 compilesoft-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz[root@sht-sgmhadoopnn-01 learnproject]#8.æ­å»ºHDFS HA,YARN HAé›†ç¾¤ï¼ˆ5ä¸ªèŠ‚ç‚¹ï¼‰å‚è€ƒ:http://blog.itpub.net/30089851/viewspace-1994585/https://github.com/Hackeruncle/Hadoop9.æ­å»ºé›†ç¾¤,éªŒè¯ç‰ˆæœ¬å’Œæ”¯æŒçš„å‹ç¼©ä¿¡æ¯1234567891011121314151617181920[root@sht-sgmhadoopnn-01 app]# hadoop versionHadoop 2.7.3Subversion Unknown -r UnknownCompiled by root on 2016-12-24T12:45ZCompiled with protoc 2.5.0From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar[root@sht-sgmhadoopnn-01 app]# hadoop checknative16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0zlib: true /lib64/libz.so.1snappy: true /usr/local/lib/libsnappy.so.1lz4: true revision:99bzip2: true /lib64/libbz2.so.1openssl: true /usr/lib64/libcrypto.so[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped[root@sht-sgmhadoopnn-01 app]#[å‚è€ƒ]http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.htmlhttp://blog.csdn.net/haohaixingyun/article/details/52800048]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹Flume-1.7.0æºç ç¼–è¯‘å¯¼å…¥eclipse]]></title>
    <url>%2F2018%2F08%2F28%2F02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse%2F</url>
    <content type="text"><![CDATA[ã€å‰æã€‘:1.windows 7 å®‰è£…maven-3.3.9å…¶ä¸­åœ¨conf/setting.xmlæ–‡ä»¶æ·»åŠ D:\software\apache-maven-3.3.9\repositoryhttp://blog.csdn.net/defonds/article/details/419572872.windows 7 å®‰è£…eclipse 64ä½(ç™¾åº¦ä¸‹è½½ï¼Œè§£å‹å³å¯)3.eclipseå®‰è£…mavenæ’ä»¶ï¼Œé€‰æ‹©ç¬¬äºŒç§æ–¹å¼linkhttp://blog.csdn.net/lfsfxy9/article/details/9397937å…¶ä¸­ eclipse-maven3-plugin.7z è¿™ä¸ªåŒ…å¯ä»¥åŠ ç¾¤258669058æ‰¾æˆ‘ï¼Œåˆ†äº«ç»™ä½ ã€flume-ng 1.7.0æºç çš„ç¼–è¯‘å¯¼å…¥eclipseã€‘:1.ä¸‹è½½å®˜ç½‘çš„æºç (ä¸è¦ä¸‹è½½GitHubä¸Šæºç ï¼Œå› ä¸ºè¿™æ—¶pomæ–‡ä»¶ä¸­ç‰ˆæœ¬ä¸º1.8.0ï¼Œç¼–è¯‘ä¼šæœ‰é—®é¢˜)http://archive.apache.org/dist/flume/1.7.0/a.ä¸‹è½½apache-flume-1.7.0-src.tar.gzb.è§£å‹é‡å‘½åä¸ºflume-1.7.02.ä¿®æ”¹pom.xml (å¤§æ¦‚åœ¨621è¡Œï¼Œå°†è‡ªå¸¦çš„repositoryæ³¨é‡Šæ‰ï¼Œæ·»åŠ ä»¥ä¸‹çš„)1234&lt;repository&gt; &lt;id&gt;maven.tempo-db.com&lt;/id&gt; &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt; &lt;/repository&gt;3.æ‰“å¼€cmd,ç¼–è¯‘cd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0mvn compile4.æ‰“å¼€eclipse,å•å‡»Windowâ€“&gt;Perferencesâ€“&gt;å·¦ä¾§çš„Mavenâ€“&gt;User Settingsç„¶åè®¾ç½®è‡ªå·±çš„mvnçš„setting.xmlè·¯å¾„å’ŒLocal Repository(æœ€å¥½ä½¿ç”¨Maven3.3.xç‰ˆæœ¬ä»¥ä¸Šï¼Œæˆ‘æ˜¯3.3.9)5.å…³é—­eclipseçš„ Projectâ€“&gt;Buid Automatically6.å…³é—­eclipseçš„Download repository index updates on startup7.å¯¼å…¥flume1.7.0æºç a.Fileâ€“&gt;Importâ€“&gt;Mavenâ€“&gt;Existing Maven Projectsâ€“&gt;Nextb.é€‰æ‹©ç›®å½•â€“&gt; Finish8.æ£€æŸ¥æºç ï¼Œæ²¡æœ‰æŠ›ä»»ä½•é”™è¯¯]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è¿™æ˜¯ä¸€ç¯‡çƒ­è…¾è…¾çš„é¢ç»]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[ä¼Ÿæ¢¦ï¼š1.ä¸»è¦è¿˜æ˜¯é¡¹ç›®ï¼ŸåŸºæœ¬ä¸Šæ²¡é—®ä»€ä¹ˆæŠ€æœ¯ï¼Œæˆ‘å°±è¯´äº†ä¸€éé¡¹ç›®æµç¨‹ï¼Œç„¶åè¯´å‡ ä¸ªä¼˜åŒ–ç‚¹ï¼Œæ¯”å¦‚ä¸Šæ¬¡è®²çš„è¡€æ¡ˆï¼Œæˆ‘ä¹Ÿé¡ºå¸¦æäº†ä¸€ä¸‹ã€‚2.åœ¨å¤§æ•°æ®ä¸­ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆæ˜¯ä¸è¶³çš„ï¼Œé‡åˆ°è¿‡ä»€ä¹ˆé—®é¢˜ï¼Ÿå¾®ç›Ÿï¼š1.SparkStreamingå¤„ç†å®Œä¸€æ‰¹æ¬¡çš„æ•°æ®ï¼Œå†™åç§»é‡ä¹‹å‰æŒ‚äº†ï¼Œæ•°æ®æ€ä¹ˆä¿è¯ä¸é‡ï¼Ÿ2.Maxwellçš„åº•å±‚åŸç†ï¼Ÿ3.æ‰‹å†™Springï¼Ÿ4.éå†äºŒå‰æ ‘ï¼Ÿ5.ç”¨è¿‡ä»€ä¹ˆç®—æ³•ï¼Ÿ6.å¤šçº¿ç¨‹æ–¹é¢ï¼Œæ€ä¹ˆå®ç°ä¸€ä¸ªä¸»çº¿ç¨‹ï¼Œç­‰å¾…å…¶ä»–å­çº¿ç¨‹å®Œæˆåå†è¿è¡Œï¼Ÿ7.Maxwellå’ŒCannalçš„æ¯”è¾ƒï¼Ÿ8.directæ¯”è¾ƒreceiverçš„ä¼˜åŠ¿ï¼Ÿ9.åŸæ¥æ˜¯æŠŠæ•°æ®ä¼ å…¥åˆ°Hiveï¼Œä¹‹åæ”¹äº†æ¶æ„ï¼Œæ€ä¹ˆæŠŠHiveçš„æ•°æ®å¯¼å…¥åˆ°Hbaseï¼Ÿ10.ä¸ºä»€ä¹ˆç”¨Kafkaè‡ªå·±å­˜å‚¨offsetæ¥æ›¿ä»£checkpointï¼Œæ€ä¹ˆé˜²æ­¢äº†æ•°æ®åŒä»½è½åœ°ï¼Œæ•°æ®åŒä»½æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ11.å•ä¾‹ç”¨è¿‡å—ï¼Ÿå¹³å®‰ï¼š1.é—®é¡¹ç›®ï¼Œæµç¨‹ï¼Œä¸šåŠ¡ï¼Ÿ2.æ•°æ®é‡ï¼Œå¢é‡ï¼Ÿ3.å‡ ä¸ªäººå¼€å‘çš„ï¼Œä»£ç é‡å¤šå°‘ï¼Ÿ4.ä½ ä¸»è¦åšä»€ä¹ˆçš„ï¼Ÿ5.ä»€ä¹ˆåœºæ™¯ï¼Œç”¨SparkSqlåˆ†æä»€ä¹ˆä¸œè¥¿ï¼Ÿæ€»ç»“ï¼šåŸºæœ¬ä¸Šéƒ½æ˜¯å›´ç»•é¡¹ç›®æ¥é¢ï¼Œç¬¬ä¸€å®¶é—®çš„æ¯”è¾ƒå°‘ï¼Œè€Œä¸”éƒ½æ˜¯å…³äºé¡¹ç›®ï¼›å¾®ç›Ÿçš„é¢è¯•å®˜åšçš„é¡¹ç›®ï¼Œè·Ÿç®€å†ä¸Šçš„é¡¹ç›®ï¼Œæ¶æ„ä¸ŠåŸºæœ¬ä¸€æ ·ï¼Œæ‰€ä»¥é—®çš„æ¯”è¾ƒæ·±ï¼Œé—®æˆ‘Maxwellçš„åº•å±‚åŸç†ï¼Œå¯¹æ¯”Cannalæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Œä¸ºä»€ä¹ˆé€‰æ‹©å®ƒï¼Œè¿™ä¸ªæˆ‘æ²¡å›ç­”ä¸Šæ¥ï¼Œåæ¥è®©æ‰‹å†™Springï¼Œç®—æ³•ï¼Œåæ¥å°±è®©æˆ‘èµ°äº†ï¼›å¹³å®‰ä¹Ÿæ˜¯åŸºæœ¬å›´ç»•é¡¹ç›®ï¼Œä¸šåŠ¡ï¼Œæ•°æ®é‡ï¼Œæ²¡é—®ä»€ä¹ˆæŠ€æœ¯ï¼Œè€Œä¸”æˆ‘è¯´äº†å…³äºä¼˜åŒ–çš„ç‚¹(é¢è¯•å®˜è¯´ä¸è¦è¯´ç½‘ä¸Šéƒ½æœ‰çš„ä¸œè¥¿)ã€‚]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®ä¹‹é¡¹ç›®æ¦‚è¿°]]></title>
    <url>%2F2018%2F08%2F27%2F01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1.å‰æœŸåŸºæœ¬æ¶æ„å›¾2.æœ€ç»ˆåŸºæœ¬æ¶æ„å›¾3.ç‰ˆæœ¬ç»„ä»¶ç‰ˆæœ¬Flume:1.7Hadoop:2.7.3Scala:2.11Kafka:0.10.1.0Spark:2.0.2InfluxDB:1.2.0Grafana:4.1.1maven:3.3.94.ä¸»è¦ç›®çš„ä¸»è¦æ˜¯æƒ³åŸºäºExec Sourceå¼€å‘è‡ªå®šä¹‰æ’ä»¶AdvancedExecSourceï¼Œå°†æœºå™¨åç§° å’Œ æœåŠ¡åç§° æ·»åŠ åˆ°cdh æœåŠ¡çš„è§’è‰²logæ•°æ®çš„æ¯ä¸€è¡Œå‰é¢ï¼Œåˆ™æ ¼å¼ä¸ºï¼šæœºå™¨åç§° æœåŠ¡åç§° å¹´æœˆæ—¥ æ—¶åˆ†ç§’.æ¯«ç§’ æ—¥å¿—çº§åˆ« æ—¥å¿—ä¿¡æ¯ ï¼›ç„¶ååœ¨åé¢çš„spark streaming å®æ—¶è®¡ç®—æˆ‘ä»¬æ‰€éœ€æ±‚ï¼šæ¯”å¦‚ç»Ÿè®¡æ¯å°æœºå™¨çš„æœåŠ¡çš„æ¯ç§’å‡ºç°çš„erroræ¬¡æ•° ã€ç»Ÿè®¡æ¯5ç§’çš„warnï¼Œerroræ¬¡æ•°ç­‰ç­‰ï¼›æ¥å®æ—¶å¯è§†åŒ–å±•ç¤ºå’Œé‚®ä»¶çŸ­ä¿¡ã€å¾®ä¿¡ä¼ä¸šå·é€šçŸ¥ã€‚å…¶å®ä¸»è¦æˆ‘ä»¬ç°åœ¨çš„å¾ˆå¤šç›‘æ§æœåŠ¡åŸºæœ¬è¾¾ä¸åˆ°ç§’çº§çš„é€šçŸ¥ï¼Œéƒ½ä¸º5åˆ†é’Ÿç­‰ç­‰ï¼Œä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬è‡ªå·±çš„ç»´æŠ¤ï¼›å…¶å®å¯¹ä¸€äº›å³å°†å‡ºç°çš„é—®é¢˜å¯ä»¥æå‰é¢„çŸ¥ï¼›å…¶å®æœ€ä¸»è¦å¯ä»¥æœ‰æ•ˆæ‰©å±•åˆ°å®æ—¶è®¡ç®—æ•°æ®åº“çº§åˆ«æ—¥å¿—ï¼Œæ¯”å¦‚MySQLæ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œnginxï¼Œtomcatï¼Œlinuxçš„ç³»ç»Ÿçº§åˆ«æ—¥å¿—ç­‰ç­‰ã€‚5.å¤§æ¦‚æµç¨‹1.æ­å»ºhadoop cluster2.eclipse å¯¼å…¥flumeæºä»£ç ï¼ˆwindow7 å®‰è£…mavenï¼Œeclipseï¼Œeclipseä¸mavené›†æˆï¼‰3.å¼€å‘flume-ng è‡ªå®šä¹‰æ’ä»¶4.flume æ”¶é›†ï¼Œæ±‡èšåˆ°hdfs(ä¸»è¦æµ‹è¯•æ˜¯å¦æ±‡èšæˆåŠŸï¼ŒåæœŸä¹Ÿå¯ä»¥åšç¦»çº¿å¤„ç†)5.flume æ”¶é›†ï¼Œæ±‡èšåˆ°kafka6.æ­å»ºkafka monitor7.æ­å»º spark client8.window7è£…iedaå¼€å‘å·¥å…·9.ideaå¼€å‘ spark streaming çš„wc10.è¯»å–kafkaæ—¥å¿—ï¼Œå¼€å‘spark streamingçš„è¿™å—æ—¥å¿—åˆ†æ11.å†™å…¥influxdb12.grafanaå¯è§†åŒ–å±•ç¤º13.é›†æˆé‚®ä»¶###è¯´æ˜ï¼šé’ˆå¯¹è‡ªèº«æƒ…å†µï¼Œè‡ªè¡Œé€‰æ‹©ï¼Œæ­¥éª¤å¦‚ä¸Šï¼Œä½†ä¸æ˜¯å›ºå®šçš„ï¼Œæœ‰äº›é¡ºåºæ˜¯å¯ä»¥æ‰“ä¹±çš„ï¼Œä¾‹å¦‚å¼€å‘å·¥å…·çš„å®‰è£…ï¼Œå¯ä»¥ä¸€èµ·æ“ä½œçš„ï¼Œå†å¦‚è¿™å‡ ä¸ªç»„ä»¶çš„ä¸‹è½½ç¼–è¯‘ï¼Œå¦‚æœä¸æƒ³ç¼–è¯‘å¯ä»¥ç›´æ¥ä¸‹taråŒ…çš„ï¼Œè‡ªè¡Œé€‰æ‹©å°±å¥½ï¼Œä½†æ˜¯å»ºè®®è¿˜æ˜¯è‡ªå·±ç¼–è¯‘ï¼Œé‡åˆ°å‘æ‰èƒ½æ›´å¥½çš„è®°ä½è¿™ä¸ªä¸œè¥¿ï¼Œæœ¬èº«è¿™ä¸ªé¡¹ç›®å°±æ˜¯å­¦ä¹ æå‡çš„è¿‡ç¨‹ï¼Œè¦æ˜¯ä»€ä¹ˆéƒ½æ˜¯ç°æˆçš„ï¼Œé‚£å°±æ²¡ä»€ä¹ˆæ„ä¹‰äº†]]></content>
      <categories>
        <category>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>ç”Ÿäº§é¢„è­¦å¹³å°é¡¹ç›®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkä¸­é…ç½®å¯ç”¨LZOå‹ç¼©]]></title>
    <url>%2F2018%2F08%2F20%2Fspark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Sparkä¸­é…ç½®å¯ç”¨LZOå‹ç¼©ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼šä¸€ã€spark-env.shé…ç½®123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*äºŒã€spark-defaults.confé…ç½®12spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jaræ³¨ï¼šæŒ‡å‘ç¼–è¯‘ç”Ÿæˆlzoçš„jaråŒ…ä¸‰ã€æµ‹è¯•1ã€è¯»å–Lzoæ–‡ä»¶123spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)2ã€å†™å‡ºlzoæ–‡ä»¶1234spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val lzoTest = sc.parallelize(1 to 10)scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])ç»“æœï¼š12345[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzoFound 3 items-rw-r--r-- 1 hadoop supergroup 0 2018-03-16 23:24 /input/test_lzo/_SUCCESS-rw-r--r-- 1 hadoop supergroup 60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo-rw-r--r-- 1 hadoop supergroup 61 2018-03-16 23:24 /input/test_lzo/part-00001.lzoè‡³æ­¤é…ç½®ä¸æµ‹è¯•å®Œæˆã€‚å››ã€é…ç½®ä¸æµ‹è¯•ä¸­å­˜é—®é¢˜1ã€å¼•ç”¨nativeï¼Œç¼ºå°‘LD_LIBRARY_PATH1.1ã€é”™è¯¯æç¤ºï¼š1234567891011121314151617181920Caused by: java.lang.RuntimeException: native-lzo library not available at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120) at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178) at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246) at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)1.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-evn.shï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*2ã€æ— æ³•æ‰¾åˆ°LzopCodecç±»2.1ã€é”™è¯¯æç¤ºï¼š1234567Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found. at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135) at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175) at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980) at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)2.2ã€è§£å†³åŠæ³•ï¼šåœ¨sparkçš„confä¸­é…ç½®spark-defaults.confï¼Œå¢åŠ ä»¥ä¸‹å†…å®¹ï¼š123spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFSä¹‹åƒåœ¾å›æ”¶ç®±é…ç½®åŠä½¿ç”¨]]></title>
    <url>%2F2018%2F07%2F18%2FHDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HDFSä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºä¸€ä¸ªå›æ”¶ç«™:ç›®å½•:/user/ç”¨æˆ·/.Trash/Current, ç³»ç»Ÿå›æ”¶ç«™éƒ½æœ‰ä¸€ä¸ªå‘¨æœŸ,å‘¨æœŸè¿‡åhdfsä¼šå½»åº•åˆ é™¤æ¸…ç©º,å‘¨æœŸå†…å¯ä»¥æ¢å¤ã€‚ä¸€ã€HDFSåˆ é™¤æ–‡ä»¶,æ— æ³•æ¢å¤12[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.logDeleted /123.logäºŒã€ å¯ç”¨å›æ”¶ç«™åŠŸèƒ½12345678910111213[hadoop@hadoop001 hadoop]$ vim core-site.xml&lt;property&gt;&lt;!--å¤šé•¿æ—¶é—´åˆ›å»ºCheckPoint NameNodeèŠ‚ç‚¹ä¸Šè¿è¡Œçš„CheckPointer ä»Currentæ–‡ä»¶å¤¹åˆ›å»ºCheckPoint; é»˜è®¤: 0 ç”±fs.trash.intervalé¡¹æŒ‡å®š --&gt;&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--å¤šå°‘åˆ†é’Ÿ.Trashä¸‹çš„CheckPointç›®å½•ä¼šè¢«åˆ é™¤,è¯¥é…ç½®æœåŠ¡å™¨è®¾ç½®ä¼˜å…ˆçº§å¤§äºå®¢æˆ·ç«¯ï¼Œé»˜è®¤:ä¸å¯ç”¨ --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; -- æ¸…é™¤å‘¨æœŸåˆ†é’Ÿ(24å°æ—¶)&lt;/property&gt;1ã€é‡å¯hdfsæœåŠ¡12[hadoop@hadoop001 sbin]$ ./stop-dfs.sh[hadoop@hadoop001 sbin]$ ./start-dfs.sh2ã€æµ‹è¯•å›æ”¶ç«™åŠŸèƒ½123[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /[hadoop@hadoop001 opt]$ hdfs dfs -ls /-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /123.logæ–‡ä»¶åˆ é™¤æˆåŠŸå­˜æ”¾å›æ”¶ç«™è·¯å¾„ä¸‹12345[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log[hadoop@hadoop001 opt]$ hdfs dfs -ls /Found 1 itemsdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /useræ¢å¤æ–‡ä»¶12345[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log[hadoop@hadoop001 ~]$ hdfs dfs -ls /Found 2 items-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /456.logdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /useråˆ é™¤æ–‡ä»¶è·³è¿‡å›æ”¶ç«™123[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.logDeleted /456.logæºç å‚è€ƒï¼šhttps://blog.csdn.net/tracymkgld/article/details/17557655]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkåºåˆ—åŒ–ï¼Œä½ äº†è§£å—]]></title>
    <url>%2F2018%2F07%2F16%2FSpark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[åºåˆ—åŒ–åœ¨åˆ†å¸ƒå¼åº”ç”¨çš„æ€§èƒ½ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ ¼å¼åŒ–å¯¹è±¡ç¼“æ…¢ï¼Œæˆ–è€…æ¶ˆè€—å¤§é‡çš„å­—èŠ‚æ ¼å¼åŒ–ï¼Œä¼šå¤§å¤§é™ä½è®¡ç®—æ€§èƒ½ã€‚é€šå¸¸è¿™æ˜¯åœ¨sparkåº”ç”¨ä¸­ç¬¬ä¸€ä»¶éœ€è¦ä¼˜åŒ–çš„äº‹æƒ…ã€‚Sparkçš„ç›®æ ‡æ˜¯åœ¨ä¾¿åˆ©ä¸æ€§èƒ½ä¸­å–å¾—å¹³è¡¡ï¼Œæ‰€ä»¥æä¾›2ç§åºåˆ—åŒ–çš„é€‰æ‹©ã€‚Java serializationåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä¼šä½¿ç”¨Javaçš„ObjectOutputStreamæ¡†æ¶å¯¹å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä»»ä½•å®ç°java.io.Serializableçš„ç±»ä¸€èµ·å·¥ä½œã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æ‰©å±•java.io.Externalizableæ¥æ›´ç´§å¯†åœ°æ§åˆ¶åºåˆ—åŒ–çš„æ€§èƒ½ã€‚Javaåºåˆ—åŒ–æ˜¯çµæ´»çš„ï¼Œä½†é€šå¸¸ç›¸å½“æ…¢ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´è®¸å¤šç±»çš„å¤§å‹åºåˆ—åŒ–æ ¼å¼ã€‚æµ‹è¯•ä»£ç ï¼š12345678910111213141516171819202122232425262728package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;æµ‹è¯•ç»“æœï¼šKryo serializationSparkè¿˜å¯ä»¥ä½¿ç”¨Kryoåº“ï¼ˆç‰ˆæœ¬2ï¼‰æ¥æ›´å¿«åœ°åºåˆ—åŒ–å¯¹è±¡ã€‚Kryoæ¯”Javaä¸²è¡ŒåŒ–ï¼ˆé€šå¸¸å¤šè¾¾10å€ï¼‰è¦å¿«å¾—å¤šï¼Œä¹Ÿæ›´ç´§å‡‘ï¼Œä½†æ˜¯ä¸æ”¯æŒæ‰€æœ‰å¯ä¸²è¡ŒåŒ–ç±»å‹ï¼Œå¹¶ä¸”è¦æ±‚æ‚¨æå‰æ³¨å†Œæ‚¨å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„ç±»ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚æµ‹è¯•ä»£ç ï¼š12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;æµ‹è¯•ç»“æœä¸­å‘ç°ï¼Œä½¿ç”¨ Kryo serialization çš„åºåˆ—åŒ–å¯¹è±¡ æ¯”ä½¿ç”¨ Java serializationçš„åºåˆ—åŒ–å¯¹è±¡è¦å¤§ï¼Œä¸æè¿°çš„ä¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼ŸæŸ¥æ‰¾å®˜ç½‘ï¼Œå‘ç°è¿™ä¹ˆä¸€å¥è¯ Finally, if you donâ€™t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.ã€‚ä¿®æ”¹ä»£ç ååœ¨æµ‹è¯•ä¸€æ¬¡12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .registerKryoClasses(Array(classOf[Student])) // å°†è‡ªå®šä¹‰çš„ç±»æ³¨å†Œåˆ°Kryo val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125;æµ‹è¯•ç»“æœï¼šæ€»ç»“ï¼šKryo serialization æ€§èƒ½å’Œåºåˆ—åŒ–å¤§å°éƒ½æ¯”é»˜è®¤æä¾›çš„ Java serialization è¦å¥½ï¼Œä½†æ˜¯ä½¿ç”¨Kryoéœ€è¦å°†è‡ªå®šä¹‰çš„ç±»å…ˆæ³¨å†Œè¿›å»ï¼Œä½¿ç”¨èµ·æ¥æ¯”Java serializationéº»çƒ¦ã€‚è‡ªä»Spark 2.0.0ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ç®€å•ç±»å‹ã€ç®€å•ç±»å‹æ•°ç»„æˆ–å­—ç¬¦ä¸²ç±»å‹çš„ç®€å•ç±»å‹æ¥è°ƒæ•´RDDsæ—¶ï¼Œåœ¨å†…éƒ¨ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨ã€‚é€šè¿‡æŸ¥æ‰¾sparkcontextåˆå§‹åŒ–çš„æºç ï¼Œå¯ä»¥å‘ç°æŸäº›ç±»å‹å·²ç»åœ¨sparkcontextåˆå§‹åŒ–çš„æ—¶å€™è¢«æ³¨å†Œè¿›å»ã€‚12345678910111213141516171819202122232425262728 /** * Component which configures serialization, compression and encryption for various Spark * components, including automatic selection of which [[Serializer]] to use for shuffles. */private[spark] class SerializerManager( defaultSerializer: Serializer, conf: SparkConf, encryptionKey: Option[Array[Byte]]) &#123; def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None) private[this] val kryoSerializer = new KryoSerializer(conf) private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]] private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123; val primitiveClassTags = Set[ClassTag[_]]( ClassTag.Boolean, ClassTag.Byte, ClassTag.Char, ClassTag.Double, ClassTag.Float, ClassTag.Int, ClassTag.Long, ClassTag.Null, ClassTag.Short ) val arrayClassTags = primitiveClassTags.map(_.wrap) primitiveClassTags ++ arrayClassTags]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming çŠ¶æ€ç®¡ç†å‡½æ•°ï¼Œä½ äº†è§£å—]]></title>
    <url>%2F2018%2F06%2F25%2FSpark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[Spark Streaming çŠ¶æ€ç®¡ç†å‡½æ•°åŒ…æ‹¬updateStateByKeyå’ŒmapWithStateä¸€ã€updateStateByKeyå®˜ç½‘åŸè¯ï¼šIn every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.ç»Ÿè®¡å…¨å±€çš„keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å°±ç®—æ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä»–ä¹Ÿä¼šåœ¨æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶å€™è¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ã€‚è¿™æ ·çš„ç¼ºç‚¹ï¼šå¦‚æœæ•°æ®é‡å¤ªå¤§çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦checkpointæ•°æ®ä¼šå ç”¨è¾ƒå¤§çš„å­˜å‚¨ã€‚è€Œä¸”æ•ˆç‡ä¹Ÿä¸é«˜123456789101112131415161718192021222324252627282930//[root@bda3 ~]# nc -lk 9999 object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //æ³¨æ„ï¼šupdateStateByKeyå¿…é¡»è®¾ç½®checkpointç›®å½• ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() // ä¸€å®šè¦å†™ ssc.awaitTermination() &#125; /*çŠ¶æ€æ›´æ–°å‡½æ•° * @param currentValues keyç›¸åŒvalueå½¢æˆçš„åˆ—è¡¨ * @param preValues keyå¯¹åº”çš„valueï¼Œå‰ä¸€çŠ¶æ€ * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seqåˆ—è¡¨ä¸­æ‰€æœ‰valueæ±‚å’Œ val pre = preValues.getOrElse(0) //è·å–ä¸Šä¸€çŠ¶æ€å€¼ Some(curr + pre) &#125; &#125;äºŒã€mapWithState (æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨)mapWithStateï¼šä¹Ÿæ˜¯ç”¨äºå…¨å±€ç»Ÿè®¡keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å®ƒå¦‚æœæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä¾¿ä¸ä¼šè¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ï¼Œæœ‰ä¸€ç‚¹å¢é‡çš„æ„Ÿè§‰ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åªæ˜¯å…³å¿ƒé‚£äº›å·²ç»å‘ç”Ÿçš„å˜åŒ–çš„keyï¼Œå¯¹äºæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œåˆ™ä¸ä¼šè¿”å›é‚£äº›æ²¡æœ‰å˜åŒ–çš„keyçš„æ•°æ®ã€‚è¿™æ ·çš„è¯ï¼Œå³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œcheckpointä¹Ÿä¸ä¼šåƒupdateStateByKeyé‚£æ ·ï¼Œå ç”¨å¤ªå¤šçš„å­˜å‚¨ã€‚å®˜æ–¹ä»£ç å¦‚ä¸‹ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every * second starting with initial value of word count. * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive * data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example * org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999` */ object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;) // Create the context with a 1 second batch size val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) // Initial state RDD for mapWithState operation val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1))) // Create a ReceiverInputDStream on target ip:port and count the // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) // Update the cumulative count using mapWithState // This will give a DStream made of state (which is the cumulative count of the words) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Sparkå’ŒDL/AIç»“åˆï¼Œè°ä¸äº‰é”‹? æœŸå¾…Spark3.0çš„åˆ°æ¥ï¼]]></title>
    <url>%2F2018%2F06%2F22%2FAI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20%2F</url>
    <content type="text"><![CDATA[ä¸çŸ¥å„ä½ï¼Œæ˜¯å¦å…³æ³¨ç¤¾åŒºçš„å‘å±•ï¼Ÿå…³æ³¨Sparkå‘¢ï¼Ÿå®˜ç½‘çš„Sparkå›¾æ ‡å’Œè§£é‡Šè¯­å·²ç»å‘ç”Ÿå˜åŒ–äº†ã€‚ç„¶è€Œåœ¨6-18å·ï¼Œç¤¾åŒºæå‡ºSpark and DL/AIç›¸ç»“åˆï¼Œè¿™æ— æ¯”å†ä¸€æ¬¡è¯´æ˜ï¼ŒSparkåœ¨å¤§æ•°æ®çš„åœ°ä½æ˜¯æ— æ³•æ’¼åŠ¨çš„ï¼æœŸå¾…Spark3.0çš„åˆ°æ¥ï¼æ¥ä¸‹æ¥å¯¹SPARK-24579çš„ç¿»è¯‘:åœ¨å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½çš„åå­—è·¯å£ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†Apache Sparkä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åˆ†æå¼•æ“ä»¥åŠAIæ¡†æ¶å¦‚TensorFlowå’ŒApache MXNet (æ­£åœ¨å­µåŒ–ä¸­)çš„å…´èµ·åŠè¿™ä¸¤å¤§å—çš„å·¨å¤§æˆåŠŸ ã€‚å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½éƒ½æ˜¯æ¨åŠ¨ä¼ä¸šåˆ›æ–°çš„ä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ï¼Œ ä¸¤ä¸ªç¤¾åŒºçš„å¤šæ¬¡å°è¯•ï¼Œä½¿ä»–ä»¬ç»“åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çœ‹åˆ°AIç¤¾åŒºçš„åŠªåŠ›ï¼Œä¸ºAIæ¡†æ¶å®ç°æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œå¦‚TF.DATAå’ŒTF.Trorã€‚ç„¶è€Œï¼Œ50+ä¸ªæ•°æ®æºå’Œå†…ç½®SQLã€æ•°æ®æµå’Œæµç‰¹å¾ï¼ŒSparkä»ç„¶æ˜¯å¯¹äºå¤§æ•°æ®ç¤¾åŒºé€‰æ‹©ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çœ‹åˆ°è®¸å¤šåŠªåŠ›,å°†DL/AIæ¡†æ¶ä¸Sparkç»“åˆèµ·æ¥ï¼Œä»¥åˆ©ç”¨å®ƒçš„åŠ›é‡ï¼Œä¾‹å¦‚ï¼ŒSparkæ•°æ®æºTFRecordsã€TensorFlowOnSpark, TensorFramesç­‰ã€‚ä½œä¸ºé¡¹ç›®Hydrogençš„ä¸€éƒ¨åˆ†ï¼Œè¿™ä¸ªSPIPå°†Spark+AIä»ä¸åŒçš„è§’åº¦ç»Ÿä¸€èµ·æ¥ã€‚æ²¡æœ‰åœ¨Sparkå’Œå¤–éƒ¨DL/AIæ¡†æ¶ä¹‹é—´äº¤æ¢æ•°æ®ï¼Œè¿™äº›é›†æˆéƒ½æ˜¯ä¸å¯èƒ½çš„,ä¹Ÿæœ‰æ€§èƒ½é—®é¢˜ã€‚ç„¶è€Œï¼Œç›®å‰è¿˜æ²¡æœ‰ä¸€ç§æ ‡å‡†çš„æ–¹å¼æ¥äº¤æ¢æ•°æ®ï¼Œå› æ­¤å®ç°å’Œæ€§èƒ½ä¼˜åŒ–å°±é™·å…¥äº†å›°å¢ƒã€‚ä¾‹å¦‚ï¼Œåœ¨Pythonä¸­ï¼ŒTensorFlowOnSparkä½¿ç”¨Hadoop InputFormat/OutputFormatä½œä¸ºTensorFlowçš„TFRecordsï¼Œæ¥åŠ è½½å’Œä¿å­˜æ•°æ®ï¼Œå¹¶å°†RDDæ•°æ®ä¼ é€’ç»™TensorFlowã€‚TensorFramesä½¿ç”¨TensorFlowçš„Java APIï¼Œè½¬æ¢ä¸º Spark DataFrames Rows to/from TensorFlow Tensors ã€‚æˆ‘ä»¬æ€æ ·æ‰èƒ½é™ä½å¤æ‚æ€§å‘¢?è¿™é‡Œçš„å»ºè®®æ˜¯æ ‡å‡†åŒ–Sparkå’ŒDL/AIæ¡†æ¶ä¹‹é—´çš„æ•°æ®äº¤æ¢æ¥å£(æˆ–æ ¼å¼)ï¼Œå¹¶ä¼˜åŒ–ä»/åˆ°è¿™ä¸ªæ¥å£çš„æ•°æ®è½¬æ¢ã€‚å› æ­¤ï¼ŒDL/AIæ¡†æ¶å¯ä»¥åˆ©ç”¨Sparkä»ä»»ä½•åœ°æ–¹åŠ è½½æ•°æ®ï¼Œè€Œæ— éœ€èŠ±è´¹é¢å¤–çš„ç²¾åŠ›æ„å»ºå¤æ‚çš„æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œæ¯”å¦‚ä»ç”Ÿäº§æ•°æ®ä»“åº“è¯»å–ç‰¹æ€§æˆ–æµæ¨¡å‹æ¨æ–­ã€‚Sparkç”¨æˆ·å¯ä»¥ä½¿ç”¨DL/AIæ¡†æ¶ï¼Œè€Œæ— éœ€å­¦ä¹ é‚£é‡Œå®ç°çš„ç‰¹å®šæ•°æ®apiã€‚è€Œä¸”åŒæ–¹çš„å¼€å‘äººå‘˜éƒ½å¯ä»¥ç‹¬ç«‹åœ°è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ï¼Œå› ä¸ºæ¥å£æœ¬èº«ä¸ä¼šå¸¦æ¥å¾ˆå¤§çš„å¼€é”€ã€‚ISSUE: https://issues.apache.org/jira/browse/SPARK-24579è‹¥æ³½æ•°æ®ï¼Œæ˜Ÿæ˜Ÿæœ¬äººæ°´å¹³æœ‰é™ï¼Œç¿»è¯‘å¤šå¤šåŒ…æ¶µã€‚å¯¹äº†å¿˜è®°è¯´äº†ï¼Œæœ¬ISSUEæœ‰ä¸ªPDFæ–‡æ¡£ï¼Œèµ¶å¿«å»ä¸‹è½½å§ã€‚https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¼€å‘å¿…ç”¨-Spark RDDè½¬DataFrameçš„ä¸¤ç§æ–¹æ³•]]></title>
    <url>%2F2018%2F06%2F14%2F%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[æœ¬ç¯‡æ–‡ç« å°†ä»‹ç»Spark SQLä¸­çš„DataFrameï¼Œå…³äºDataFrameçš„ä»‹ç»å¯ä»¥å‚è€ƒ:https://blog.csdn.net/lemonzhaotao/article/details/80211231åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œå°†ä»‹ç»RDDè½¬æ¢ä¸ºDataFrameçš„2ç§æ–¹å¼å®˜ç½‘ä¹‹RDDè½¬DF:http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rddsDataFrame ä¸ RDD çš„äº¤äº’Spark SQLå®ƒæ”¯æŒä¸¤ç§ä¸åŒçš„æ–¹å¼è½¬æ¢å·²ç»å­˜åœ¨çš„RDDåˆ°DataFrameæ–¹æ³•ä¸€ç¬¬ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨åå°„çš„æ–¹å¼ï¼Œç”¨åå°„å»æ¨å€’å‡ºæ¥RDDé‡Œé¢çš„schemaã€‚è¿™ä¸ªæ–¹å¼ç®€å•ï¼Œä½†æ˜¯ä¸å»ºè®®ä½¿ç”¨ï¼Œå› ä¸ºåœ¨å·¥ä½œå½“ä¸­ï¼Œä½¿ç”¨è¿™ç§æ–¹å¼æ˜¯æœ‰é™åˆ¶çš„ã€‚å¯¹äºä»¥å‰çš„ç‰ˆæœ¬æ¥è¯´ï¼Œcase classæœ€å¤šæ”¯æŒ22ä¸ªå­—æ®µå¦‚æœè¶…è¿‡äº†22ä¸ªå­—æ®µï¼Œæˆ‘ä»¬å°±å¿…é¡»è¦è‡ªå·±å¼€å‘ä¸€ä¸ªç±»ï¼Œå®ç°productæ¥å£æ‰è¡Œã€‚å› æ­¤è¿™ç§æ–¹å¼è™½ç„¶ç®€å•ï¼Œä½†æ˜¯ä¸é€šç”¨ï¼›å› ä¸ºç”Ÿäº§ä¸­çš„å­—æ®µæ˜¯éå¸¸éå¸¸å¤šçš„ï¼Œæ˜¯ä¸å¯èƒ½åªæœ‰20æ¥ä¸ªå­—æ®µçš„ã€‚ç¤ºä¾‹ï¼š12345678910111213141516171819202122/** * convert rdd to dataframe 1 * @param spark */private def runInferSchemaExample(spark:SparkSession): Unit =&#123; import spark.implicits._ val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;) val df = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; People(x(0), x(1).trim.toInt)) //å°†rddçš„æ¯ä¸€è¡Œéƒ½è½¬æ¢æˆäº†ä¸€ä¸ªpeople .toDF //å¿…é¡»å…ˆå¯¼å…¥import spark.implicits._ ä¸ç„¶è¿™ä¸ªæ–¹æ³•ä¼šæŠ¥é”™ df.show() df.createOrReplaceTempView(&quot;people&quot;) // è¿™ä¸ªDFåŒ…å«äº†ä¸¤ä¸ªå­—æ®µnameå’Œage val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;) // teenager(0)ä»£è¡¨ç¬¬ä¸€ä¸ªå­—æ®µ // å–å€¼çš„ç¬¬ä¸€ç§æ–¹å¼ï¼šindex from zero teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show() // å–å€¼çš„ç¬¬äºŒç§æ–¹å¼ï¼šbyName teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()&#125;// æ³¨æ„ï¼šcase classå¿…é¡»å®šä¹‰åœ¨mainæ–¹æ³•ä¹‹å¤–ï¼›å¦åˆ™ä¼šæŠ¥é”™case class People(name:String, age:Int)æ–¹æ³•äºŒåˆ›å»ºä¸€ä¸ªDataFrameï¼Œä½¿ç”¨ç¼–ç¨‹çš„æ–¹å¼ è¿™ä¸ªæ–¹å¼ç”¨çš„éå¸¸å¤šã€‚é€šè¿‡ç¼–ç¨‹æ–¹å¼æŒ‡å®šschema ï¼Œå¯¹äºç¬¬ä¸€ç§æ–¹å¼çš„schemaå…¶å®å®šä¹‰åœ¨äº†case classé‡Œé¢äº†ã€‚å®˜ç½‘è§£è¯»ï¼šå½“æˆ‘ä»¬çš„case classä¸èƒ½æå‰å®šä¹‰(å› ä¸ºä¸šåŠ¡å¤„ç†çš„è¿‡ç¨‹å½“ä¸­ï¼Œä½ çš„å­—æ®µå¯èƒ½æ˜¯åœ¨å˜åŒ–çš„),å› æ­¤ä½¿ç”¨case classå¾ˆéš¾å»æå‰å®šä¹‰ã€‚ä½¿ç”¨è¯¥æ–¹å¼åˆ›å»ºDFçš„ä¸‰å¤§æ­¥éª¤ï¼šCreate an RDD of Rows from the original RDD;Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.ç¤ºä¾‹ï¼š1234567891011121314151617181920212223242526/** * convert rdd to dataframe 2 * @param spark */private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123; // 1.è½¬æˆRDD val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;) // 2.å®šä¹‰schemaï¼Œå¸¦æœ‰StructTypeçš„ // å®šä¹‰schemaä¿¡æ¯ val schemaString = &quot;name age&quot; // å¯¹schemaä¿¡æ¯æŒ‰ç©ºæ ¼è¿›è¡Œåˆ†å‰² // æœ€ç»ˆfiledsé‡ŒåŒ…å«äº†2ä¸ªStructField val fields = schemaString.split(&quot; &quot;) // å­—æ®µç±»å‹ï¼Œå­—æ®µåç§°åˆ¤æ–­æ˜¯ä¸æ˜¯ä¸ºç©º .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // 3.æŠŠæˆ‘ä»¬çš„schemaä¿¡æ¯ä½œç”¨åˆ°RDDä¸Š // è¿™ä¸ªRDDé‡Œé¢åŒ…å«äº†ä¸€äº›è¡Œ // å½¢æˆRowç±»å‹çš„RDD val rowRDD = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; Row(x(0), x(1).trim)) // é€šè¿‡SparkSessionåˆ›å»ºä¸€ä¸ªDataFrame // ä¼ è¿›æ¥ä¸€ä¸ªrowRDDå’Œschemaï¼Œå°†schemaä½œç”¨åˆ°rowRDDä¸Š val peopleDF = spark.createDataFrame(rowRDD, schema) peopleDF.show()&#125;[æ‰©å±•]ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å…¶å®é€‰æ‹©çš„æ˜¯æ–¹å¼äºŒè¿™ç§è¿›è¡Œåˆ›å»ºDataFrameçš„ï¼Œè¿™é‡Œå°†å±•ç¤ºéƒ¨åˆ†ä»£ç ï¼šSchemaçš„å®šä¹‰1234567891011121314151617181920212223242526272829303132333435363738394041object AccessConvertUtil &#123; val struct = StructType( Array( StructField(&quot;url&quot;,StringType), StructField(&quot;cmsType&quot;,StringType), StructField(&quot;cmsId&quot;,LongType), StructField(&quot;traffic&quot;,LongType), StructField(&quot;ip&quot;,StringType), StructField(&quot;city&quot;,StringType), StructField(&quot;time&quot;,StringType), StructField(&quot;day&quot;,StringType) ) ) /** * æ ¹æ®è¾“å…¥çš„æ¯ä¸€è¡Œä¿¡æ¯è½¬æ¢æˆè¾“å‡ºçš„æ ·å¼ */ def parseLog(log:String) = &#123; try &#123; val splits = log.split(&quot;\t&quot;) val url = splits(1) val traffic = splits(2).toLong val ip = splits(3) val domain = &quot;http://www.imooc.com/&quot; val cms = url.substring(url.indexOf(domain) + domain.length) val cmsTypeId = cms.split(&quot;/&quot;) var cmsType = &quot;&quot; var cmsId = 0l if (cmsTypeId.length &gt; 1) &#123; cmsType = cmsTypeId(0) cmsId = cmsTypeId(1).toLong &#125; val city = IpUtils.getCity(ip) val time = splits(0) val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;) //è¿™ä¸ªRowé‡Œé¢çš„å­—æ®µè¦å’Œstructä¸­çš„å­—æ®µå¯¹åº”ä¸Š Row(url, cmsType, cmsId, traffic, ip, city, time, day) &#125; catch &#123; case e: Exception =&gt; Row(0) &#125; &#125;&#125;åˆ›å»ºDataFrame1234567891011121314object SparkStatCleanJob &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;) .master(&quot;local[2]&quot;).getOrCreate() val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;) //accessRDD.take(10).foreach(println) //RDD ==&gt; DFï¼Œåˆ›å»ºç”ŸæˆDataFrame val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct) accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite) .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;) spark.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœ€å‰æ²¿ï¼å¸¦ä½ è¯»Structured Streamingé‡é‡çº§è®ºæ–‡ï¼]]></title>
    <url>%2F2018%2F06%2F14%2F%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81%2F</url>
    <content type="text"><![CDATA[1.è®ºæ–‡ä¸‹è½½åœ°å€https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf2.å‰è¨€å»ºè®®é¦–å…ˆé˜…è¯»Structured Streamingå®˜ç½‘ï¼šhttp://spark.apache.org/docs/latest/structured-streaming-programming-guide.htmlä»¥åŠè¿™ä¸¤ç¯‡Databricksåœ¨2016å¹´å…³äºStructured Streamingçš„æ–‡ç« ï¼šhttps://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.htmlhttps://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.htmlè¨€å½’æ­£ä¼ è¯¥è®ºæ–‡æ”¶å½•è‡ª2018å¹´ACM SIGMODä¼šè®®ï¼Œæ˜¯ç”±ç¾å›½è®¡ç®—æœºåä¼šï¼ˆACMï¼‰å‘èµ·çš„ã€åœ¨æ•°æ®åº“é¢†åŸŸå…·æœ‰æœ€é«˜å­¦æœ¯åœ°ä½çš„å›½é™…æ€§å­¦æœ¯ä¼šè®®ã€‚è®ºæ–‡çš„ä½œè€…ä¸ºDatabricksçš„å·¥ç¨‹å¸ˆåŠSparkçš„å¼€å‘è€…ï¼Œå…¶æƒå¨æ€§ã€é‡è¦ç¨‹åº¦ä¸è¨€è€Œå–»ã€‚æ–‡ç« å¼€å¤´ä¸ºè¯¥è®ºæ–‡çš„ä¸‹è½½åœ°å€ï¼Œä¾›è¯»è€…é˜…è¯»äº¤æµã€‚æœ¬æ–‡å¯¹è¯¥è®ºæ–‡è¿›è¡Œç®€è¦çš„æ€»ç»“ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿä¸‹è½½åŸæ–‡ç»†ç»†å“è¯»ï¼Œäº†è§£æœ€å‰æ²¿çš„å¤§æ•°æ®æŠ€æœ¯ã€‚3.è®ºæ–‡ç®€è¦æ€»ç»“é¢˜ç›®ï¼šStructured Streaming: A Declarative API for Real-Time Applications in Apache Spark3.1 æ‘˜è¦æ‘˜è¦æ˜¯ä¸€ç¯‡è®ºæ–‡çš„ç²¾é«“ï¼Œè¿™é‡Œç»™å‡ºæ‘˜è¦å®Œæ•´çš„ç¿»è¯‘ã€‚éšç€å®æ—¶æ•°æ®çš„æ™®éå­˜åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å¯æ‰©å±•çš„ã€æ˜“ç”¨çš„ã€æ˜“äºé›†æˆçš„æµå¼å¤„ç†ç³»ç»Ÿã€‚ç»“æ„åŒ–æµæ˜¯åŸºäºæˆ‘ä»¬å¯¹Spark Streamingçš„ç»éªŒå¼€å‘å‡ºæ¥çš„é«˜çº§åˆ«çš„Sparkæµå¼APIã€‚ç»“æ„åŒ–æµä¸å…¶ä»–ç°æœ‰çš„æµå¼APIï¼Œå¦‚è°·æ­Œçš„Dataflowï¼Œä¸»è¦æœ‰ä¸¤ç‚¹ä¸åŒã€‚ç¬¬ä¸€ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºè‡ªåŠ¨å¢é‡åŒ–çš„å…³ç³»å‹æŸ¥è¯¢APIï¼Œæ— éœ€ç”¨æˆ·è‡ªå·±æ„å»ºDAGï¼›ç¬¬äºŒï¼Œç»“æ„åŒ–æµæ—¨åœ¨äºæ”¯æŒç«¯åˆ°ç«¯çš„å®æ—¶åº”ç”¨å¹¶æ•´åˆæµä¸æ‰¹å¤„ç†çš„äº¤äº’åˆ†æã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ•´åˆæ˜¯ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ã€‚ç»“æ„åŒ–æµé€šè¿‡Spark SQLçš„ä»£ç ç”Ÿæˆå¼•æ“å®ç°äº†å¾ˆé«˜çš„æ€§èƒ½ï¼Œæ˜¯Apache Flinkçš„ä¸¤å€ä»¥åŠApache Kafkaçš„90å€ã€‚å®ƒè¿˜æä¾›äº†ä¸°å¯Œçš„è¿è¡Œç‰¹æ€§ï¼Œå¦‚å›æ»šã€ä»£ç æ›´æ–°ä»¥åŠæµ/æ‰¹æ··åˆæ‰§è¡Œã€‚æœ€åæˆ‘ä»¬æè¿°äº†ç³»ç»Ÿçš„è®¾è®¡ä»¥åŠéƒ¨ç½²åœ¨Databrickså‡ ç™¾ä¸ªç”Ÿäº§èŠ‚ç‚¹çš„ä¸€ä¸ªç”¨ä¾‹ã€‚3.2 æµå¼å¤„ç†é¢ä¸´çš„æŒ‘æˆ˜(1) å¤æ‚ã€ä½çº§åˆ«çš„API(2) ç«¯åˆ°ç«¯åº”ç”¨çš„é›†æˆ(3) è¿è¡Œæ—¶æŒ‘æˆ˜ï¼šå®¹ç¾ï¼Œä»£ç æ›´æ–°ï¼Œç›‘æ§ç­‰(4) æˆæœ¬å’Œæ€§èƒ½æŒ‘æˆ˜3.3 ç»“æ„åŒ–æµåŸºæœ¬æ¦‚å¿µå›¾1 ç»“æ„åŒ–æµçš„ç»„æˆéƒ¨åˆ†(1) Input and OutputInput sources å¿…é¡»æ˜¯ replayable çš„ï¼Œæ”¯æŒèŠ‚ç‚¹å®•æœºåä»å½“å‰è¾“å…¥ç»§ç»­è¯»å–ã€‚ä¾‹å¦‚ï¼šApache Kinesiså’ŒApache Kafkaã€‚Output sinks å¿…é¡»æ”¯æŒ idempotent ï¼ˆå¹‚ç­‰ï¼‰ï¼Œç¡®ä¿åœ¨èŠ‚ç‚¹å®•æœºæ—¶å¯é çš„æ¢å¤ã€‚(2) APIsç¼–å†™ç»“æ„åŒ–æµç¨‹åºæ—¶ï¼Œå¯ä»¥ä½¿ç”¨Spark SQLçš„APIsï¼šDataFrameå’ŒSQLæ¥æŸ¥è¯¢streamså’Œtablesï¼Œè¯¥æŸ¥è¯¢å®šä¹‰äº†ä¸€ä¸ªoutput tableï¼ˆè¾“å‡ºè¡¨ï¼‰ï¼Œç”¨æ¥æ¥æ”¶æ¥è‡ªsteamçš„æ•°æ®ã€‚engineå†³å®šå¦‚ä½•è®¡ç®—å¹¶å°†è¾“å‡ºè¡¨ incrementallyï¼ˆå¢é‡åœ°ï¼‰å†™å…¥sinkã€‚ä¸åŒçš„sinksæ”¯æŒä¸åŒçš„output modesï¼ˆè¾“å‡ºæ¨¡å¼ï¼Œåé¢ä¼šæåˆ°ï¼‰ã€‚ä¸ºäº†å¤„ç†æµå¼æ•°æ®ï¼Œç»“æ„åŒ–æµè¿˜å¢åŠ äº†ä¸€äº›APIsä¸å·²æœ‰çš„Spark SQL APIç›¸é…åˆï¼ša. Triggers æ§åˆ¶engineå¤šä¹…æ‰§è¡Œä¸€æ¬¡è®¡ç®—b. event time æ˜¯æ•°æ®æºçš„æ—¶é—´æˆ³ï¼›watermark ç­–ç•¥ï¼Œä¸event time ç›¸å·®ä¸€æ®µæ—¶é—´åä¸å†æ¥æ”¶æ•°æ®ã€‚c.Stateful operatorï¼ˆçŠ¶æ€ç®—å­ï¼‰ï¼Œç±»ä¼¼äºSpark Streaming çš„updateStateByKeyã€‚(3) æ‰§è¡Œä¸€æ—¦æ¥æ”¶åˆ°äº†æŸ¥è¯¢ï¼Œç»“æ„åŒ–æµå°±ä¼šè¿›è¡Œä¼˜åŒ–é€’å¢ï¼Œå¹¶å¼€å§‹æ‰§è¡Œã€‚ç»“æ„åŒ–æµä½¿ç”¨ä¸¤ç§æŒä¹…åŒ–å­˜å‚¨çš„æ–¹å¼å®ç°å®¹é”™ï¼ša.write-ahead log ï¼ˆWALï¼šé¢„å†™æ—¥å¿—ï¼‰æŒç»­è¿½è¸ªå“ªäº›æ•°æ®å·²è¢«æ‰§è¡Œï¼Œç¡®ä¿æ•°æ®çš„å¯é å†™å…¥ã€‚b.ç³»ç»Ÿé‡‡ç”¨å¤§è§„æ¨¡çš„ state storeï¼ˆçŠ¶æ€å­˜å‚¨ï¼‰æ¥ä¿å­˜é•¿æ—¶é—´è¿è¡Œçš„èšåˆç®—å­çš„ç®—å­çŠ¶æ€å¿«ç…§ã€‚3.4 ç¼–ç¨‹æ¨¡å‹ç»“æ„åŒ–æµå°†è°·æ­Œçš„Dataflowã€å¢é‡æŸ¥è¯¢å’ŒSpark Streaming ç»“åˆèµ·æ¥ï¼Œä»¥ä¾¿åœ¨Spark SQLä¸‹å®ç°æµå¼å¤„ç†ã€‚a. A Short Exampleé¦–å…ˆä»ä¸€ä¸ªæ‰¹å¤„ç†ä½œä¸šå¼€å§‹ï¼Œç»Ÿè®¡ä¸€ä¸ªwebåº”ç”¨åœ¨ä¸åŒå›½å®¶çš„ç‚¹å‡»æ•°ã€‚å‡è®¾è¾“å…¥æ•°æ®æ˜¯ä¸€ä¸ªJSONæ–‡ä»¶ï¼Œè¾“å‡ºä¸€ä¸ªParquetæ–‡ä»¶ï¼Œè¯¥ä½œä¸šå¯ä»¥é€šè¿‡DataFrameæ¥å®Œæˆï¼š 1234561// Define a DataFrame to read from static data2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a static data sink6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)æŠŠè¯¥ä½œä¸šå˜æˆä½¿ç”¨ç»“æ„åŒ–æµä»…ä»…éœ€è¦æ”¹å˜è¾“å…¥å’Œè¾“å‡ºæºï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ–°çš„JSONæ–‡ä»¶continuallyï¼ˆæŒç»­åœ°ï¼‰ä¸Šä¼ ï¼Œæˆ‘ä»¬åªéœ€è¦æ”¹å˜ç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œã€‚12345671// Define a DataFrame to read streaming data2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a streaming data sink6counts . writeStream . format (&quot; parquet &quot;)7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)ç»“æ„åŒ–æµä¹Ÿæ”¯æŒ windowingï¼ˆçª—å£ï¼‰å’Œé€šè¿‡Spark SQLå·²å­˜åœ¨çš„èšåˆç®—å­å¤„ç†event timeã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹ä¸­é—´çš„ä»£ç ï¼Œè®¡ç®—1å°æ—¶çš„æ»‘åŠ¨çª—å£ï¼Œæ¯äº”åˆ†é’Ÿå‰è¿›ä¸€æ¬¡ï¼š121// Count events by windows on the &quot; time &quot; field2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()b. ç¼–ç¨‹æ¨¡å‹è¯­ä¹‰å›¾ 2 ä¸¤ç§è¾“å‡ºæ¨¡å¼i. æ¯ä¸€ä¸ªè¾“å…¥æºæä¾›äº†ä¸€ä¸ªåŸºäºæ—¶é—´çš„éƒ¨åˆ†æœ‰åºçš„è®°å½•é›†ï¼ˆset of recordsï¼‰ï¼Œä¾‹å¦‚ï¼ŒKafkaå°†æµå¼æ•°æ®åˆ†ä¸ºå„è‡ªæœ‰åºçš„partitionsã€‚ii. ç”¨æˆ·æä¾›è·¨è¾“å…¥æ•°æ®æ‰§è¡Œçš„æŸ¥è¯¢ï¼Œè¯¥è¾“å…¥æ•°æ®å¯ä»¥åœ¨ä»»æ„ç»™å®šçš„å¤„ç†æ—¶é—´ç‚¹è¾“å‡ºä¸€ä¸ª result tableï¼ˆç»“æœè¡¨ï¼‰ã€‚ç»“æ„åŒ–æµæ€»ä¼šäº§ç”Ÿä¸æ‰€æœ‰è¾“å…¥æºçš„æ•°æ®çš„å‰ç¼€ä¸Šï¼ˆprefix of the data in all input sourcesï¼‰æŸ¥è¯¢ç›¸ä¸€è‡´çš„ç»“æœã€‚iii. Triggers å‘Šè¯‰ç³»ç»Ÿä½•æ—¶å»è¿è¡Œä¸€ä¸ªæ–°çš„å¢é‡è®¡ç®—ï¼Œä½•æ—¶æ›´æ–°result tableã€‚ä¾‹å¦‚ï¼Œåœ¨å¾®æ‰¹å¤„ç†æ¨¡å¼ï¼Œç”¨æˆ·å¸Œæœ›ä¼šæ¯åˆ†é’Ÿè§¦å‘ä¸€æ¬¡å¢é‡è®¡ç®—ã€‚iiii. engineæ”¯æŒä¸‰ç§output modeï¼š Completeï¼šengineä¸€æ¬¡å†™æ‰€æœ‰result tableã€‚ Appendï¼šengineä»…ä»…å‘sinkå¢åŠ è®°å½•ã€‚ Updateï¼šengineåŸºäºkeyæ›´æ–°æ¯ä¸€ä¸ªrecordï¼Œæ›´æ–°å€¼æ”¹å˜çš„keysã€‚ è¯¥æ¨¡å‹æœ‰ä¸¤ä¸ªç‰¹æ€§ï¼šç¬¬ä¸€ï¼Œç»“æœè¡¨çš„å†…å®¹ç‹¬ç«‹äºè¾“å‡ºæ¨¡å¼ã€‚ç¬¬äºŒï¼Œè¯¥æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè¢«ç§°ä¸ºprefix consistencyã€‚ c.æµå¼ç®—å­åŠ å…¥äº†ä¸¤ç§ç±»å‹çš„ç®—å­ï¼šwatermarkingç®—å­å‘Šè¯‰ç³»ç»Ÿä½•æ—¶å…³é—­event time windowå’Œè¾“å‡ºç»“æœï¼›ç»“æ„åŒ–æµå…è®¸ç”¨æˆ·é€šè¿‡withWatermarkç®—å­æ¥è®¾ç½®ä¸€ä¸ªwatermarkï¼Œè¯¥ç®—å­ç»™ç³»ç»Ÿè®¾ç½®ä¸€ä¸ªç»™å®šæ—¶é—´æˆ³Cçš„å»¶è¿Ÿé˜ˆå€¼tCï¼Œåœ¨ä»»æ„æ—¶é—´ç‚¹ï¼ŒCçš„watermarkæ˜¯maxï¼ˆCï¼‰-tCã€‚ stateful operatorså…è®¸ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰é€»è¾‘æ¥å®ç°å¤æ‚çš„åŠŸèƒ½ã€‚ 1234567891011121314 1// Define an update function that simply tracks the 2// number of events for each key as its state , returns 3// that as its result , and times out keys after 30 min. 4def updateFunc (key: UserId , newValues : Iterator [ Event ], 5state : GroupState [Int ]): Int = &#123; 6val totalEvents = state .get () + newValues . size () 7state . update ( totalEvents ) 8state . setTimeoutDuration (&quot;30 min&quot;) 9return totalEvents10&#125;11// Use this update function on a stream , returning a12// new table lens that contains the session lengths .13lens = events . groupByKey ( event =&gt; event . userId )14. mapGroupsWithState ( updateFunc )ç”¨mapGroupWithStateç®—å­æ¥è¿½è¸ªæ¯ä¸ªä¼šè¯çš„äº‹ä»¶æ•°é‡ï¼Œ30åˆ†é’Ÿåå…³é—­ä¼šè¯ã€‚3.5 è¿è¡Œç‰¹æ€§(1) ä»£ç æ›´æ–°ï¼ˆcode updateï¼‰å¼€å‘è€…èƒ½å¤Ÿåœ¨ç¼–ç¨‹è¿‡ç¨‹ä¸­æ›´æ–°UDFï¼Œå¹¶ä¸”å¯ä»¥ç®€å•çš„é‡å¯ä»¥ä½¿ç”¨æ–°ç‰ˆæœ¬çš„ä»£ç ã€‚(2) æ‰‹åŠ¨å›æ»šï¼ˆmanual rollbackï¼‰æœ‰æ—¶åœ¨ç”¨æˆ·å‘ç°ä¹‹å‰ï¼Œç¨‹åºä¼šè¾“å‡ºé”™è¯¯çš„ç»“æœï¼Œå› æ­¤å›æ»šè‡³å…³é‡è¦ã€‚ç»“æ„åŒ–æµå¾ˆå®¹æ˜“å®šä½é—®é¢˜æ‰€åœ¨ã€‚åŒæ—¶æ‰‹åŠ¨å›æ»šä¸å‰é¢æåˆ°çš„prefix consistencyæœ‰å¾ˆå¥½çš„äº¤äº’ã€‚(3) æµå¼å’Œæ‰¹æ¬¡æ··åˆå¤„ç†è¿™æ˜¯ç»“æ„åŒ–æµæœ€æ˜¾è€Œæ˜“è§çš„å¥½å¤„ï¼Œç”¨æˆ·èƒ½å¤Ÿå…±ç”¨æµå¼å¤„ç†å’Œæ‰¹å¤„ç†ä½œä¸šçš„ä»£ç ã€‚(4) ç›‘æ§ç»“æ„åŒ–æµä½¿ç”¨Sparkå·²æœ‰çš„APIå’Œç»“æ„åŒ–æ—¥å¿—æ¥æŠ¥å‘Šä¿¡æ¯ï¼Œä¾‹å¦‚å¤„ç†è¿‡çš„è®°å½•æ•°é‡ï¼Œè·¨ç½‘ç»œçš„å­—èŠ‚æ•°ç­‰ã€‚è¿™äº›æ¥å£è¢«Sparkå¼€å‘è€…æ‰€ç†ŸçŸ¥ï¼Œå¹¶æ˜“äºè¿æ¥åˆ°ä¸åŒçš„UIå·¥å…·ã€‚3.6 ç”Ÿäº§ç”¨ä¾‹ä¸æ€»ç»“ç»™å‡ºç®€è¦æ¶æ„å›¾ï¼Œç¯‡å¹…åŸå› ä¸å†èµ˜è¿°ï¼Œå¸Œæœ›è¯¦ç»†äº†è§£çš„ä¸‹è½½è®ºæ–‡è‡ªè¡Œé˜…è¯»ã€‚æœ¬æ–‡åªæŒ‘é€‰äº†éƒ¨åˆ†å…³é”®ç‚¹è¿›è¡Œäº†æµ…å±‚æ¬¡çš„å™è¿°ï¼Œå¸Œæœ›è¯»è€…èƒ½å¤Ÿå°†è®ºæ–‡ä¸‹è½½ä¸‹æ¥è®¤çœŸå“è¯»ï¼Œææ‡‚å¼€å‘è€…çš„å¼€å‘æ€è·¯ï¼Œè·Ÿä¸Šå¤§æ•°æ®çš„å‰æ²¿æ­¥ä¼ã€‚]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Javaå¯æ‰©å±•çº¿ç¨‹æ± ä¹‹ThreadPoolExecutor]]></title>
    <url>%2F2018%2F06%2F13%2FJava%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[1ã€ThreadPoolExecutoræˆ‘ä»¬çŸ¥é“ThreadPoolExecutoræ˜¯å¯æ‰©å±•çš„,å®ƒæä¾›äº†å‡ ä¸ªå¯ä»¥åœ¨å­ç±»ä¸­æ”¹å†™çš„ç©ºæ–¹æ³•å¦‚ä¸‹ï¼š123protected void beforeExecute(Thread t, Runnable r) &#123; &#125;protected void beforeExecute(Thread t, Runnable r) &#123; &#125; protected void terminated() &#123; &#125;2ã€ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰©å±•ï¼Ÿå› ä¸ºåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å¯¹çº¿ç¨‹æ± è¿è¡ŒçŠ¶æ€è¿›è¡Œè·Ÿè¸ªï¼Œè¾“å‡ºä¸€äº›æœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯ï¼Œä»¥å¸®åŠ©æ•…éšœè¯Šæ–­ã€‚3ã€ThreadPoolExecutor.Workerçš„runæ–¹æ³•å®ç°é€šè¿‡çœ‹æºç æˆ‘ä»¬å‘ç° ThreadPoolExecutorçš„å·¥ä½œçº¿ç¨‹å…¶å®å°±æ˜¯Workerå®ä¾‹ï¼ŒWorker.runTask()ä¼šè¢«çº¿ç¨‹æ± ä»¥å¤šçº¿ç¨‹æ¨¡å¼å¼‚æ­¥è°ƒç”¨ï¼Œåˆ™ä»¥ä¸Šä¸‰ä¸ªæ–¹æ³•ä¹Ÿå°†è¢«å¤šçº¿ç¨‹åŒæ—¶è®¿é—®ã€‚1234567891011121314151617181920212223242526272829303132333435363738391// åŸºäºjdk1.8.0_161final void runWorker(Worker w) &#123; 2 Thread wt = Thread.currentThread(); 3 Runnable task = w.firstTask; 4 w.firstTask = null; 5 w.unlock(); // allow interrupts 6 boolean completedAbruptly = true; 7 try &#123; 8 while (task != null || (task = getTask()) != null) &#123; 9 w.lock(); 10 if ((runStateAtLeast(ctl.get(), STOP) ||11 (Thread.interrupted() &amp;&amp;12 runStateAtLeast(ctl.get(), STOP))) &amp;&amp;13 !wt.isInterrupted())14 wt.interrupt(); 15 try &#123;16 beforeExecute(wt, task);17 Throwable thrown = null; 18 try &#123;19 task.run();20 &#125; catch (RuntimeException x) &#123;21 thrown = x; throw x;22 &#125; catch (Error x) &#123;23 thrown = x; throw x;24 &#125; catch (Throwable x) &#123;25 thrown = x; throw new Error(x);26 &#125; finally &#123;27 afterExecute(task, thrown);28 &#125;29 &#125; finally &#123;30 task = null;31 w.completedTasks++;32 w.unlock();33 &#125;34 &#125;35 completedAbruptly = false;36 &#125; finally &#123;37 processWorkerExit(w, completedAbruptly);38 &#125;39 &#125;4ã€æ‰©å±•çº¿ç¨‹æ± å®ç°1234567891011121314151617181920212223242526272829303132333435 1public class ExtThreadPool &#123; 2 public static class MyTask implements Runnable &#123; 3 public String name; 4 public MyTask(String name) &#123; 5 this.name = name; 6 &#125; 7 public void run() &#123; 8 System.out.println(&quot;æ­£åœ¨æ‰§è¡Œ:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name); try &#123; 9 Thread.sleep(100);10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;14 &#125; 15public static void main(String args[]) throws InterruptedException &#123;16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123; 18protected void beforeExecute(Thread t, Runnable r) &#123;19 System.out.println(&quot;å‡†å¤‡æ‰§è¡Œï¼š&quot; + ((MyTask) r).name);20&#125; 21protected void afterExecute(Thread t, Runnable r) &#123;22 System.out.println(&quot;æ‰§è¡Œå®Œæˆ&quot; + ((MyTask) r).name);23&#125; 24protected void terminated() &#123;25 System.out.println(&quot;çº¿ç¨‹æ± é€€å‡ºï¼&quot;);26&#125;27&#125;; 28for (int i = 0; i &lt; 5; i++) &#123;29 MyTask task = new MyTask(&quot;TASK--&quot; + i);30 executorService.execute(task);31 Thread.sleep(10);32 &#125;33 executorService.shutdown();34 &#125;35&#125;è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š1234567891011å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“0 æ­£åœ¨æ‰§è¡Œ:Thread ID:10,Task Name:TASKâ€“0 å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“1 æ­£åœ¨æ‰§è¡Œ:Thread ID:11,Task Name:TASKâ€“1 å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“2 æ­£åœ¨æ‰§è¡Œ:Thread ID:12,Task Name:TASKâ€“2 å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“3 æ­£åœ¨æ‰§è¡Œ:Thread ID:13,Task Name:TASKâ€“3 å‡†å¤‡æ‰§è¡Œï¼šTASKâ€“4 æ­£åœ¨æ‰§è¡Œ:Thread ID:14,Task Name:TASKâ€“4 çº¿ç¨‹æ± é€€å‡ºï¼è¿™æ ·å°±å®ç°äº†åœ¨æ‰§è¡Œå‰åè¿›è¡Œçš„ä¸€äº›æ§åˆ¶ï¼Œé™¤æ­¤ä¹‹å¤–æˆ‘ä»¬è¿˜å¯ä»¥è¾“å‡ºæ¯ä¸ªçº¿ç¨‹çš„æ‰§è¡Œæ—¶é—´ï¼Œæˆ–è€…ä¸€äº›å…¶ä»–å¢å¼ºæ“ä½œã€‚5ã€æ€è€ƒï¼Ÿè¯·è¯»è€…æ€è€ƒshutdownNowå’Œshutdownæ–¹æ³•çš„åŒºåˆ«ï¼Ÿå¦‚ä½•ä¼˜é›…çš„å…³é—­çº¿ç¨‹æ± ï¼Ÿ]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½ å¤§çˆ·æ°¸è¿œæ˜¯ä½ å¤§çˆ·ï¼ŒRDDè¡€ç¼˜å…³ç³»æºç è¯¦è§£ï¼]]></title>
    <url>%2F2018%2F06%2F13%2F%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81%2F</url>
    <content type="text"><![CDATA[ä¸€ã€RDDçš„ä¾èµ–å…³ç³»RDDçš„ä¾èµ–å…³ç³»åˆ†ä¸ºä¸¤ç±»ï¼šå®½ä¾èµ–å’Œçª„ä¾èµ–ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¤ä¸ºï¼šï¼ˆ1ï¼‰çª„ä¾èµ–ï¼šæ¯ä¸ªparent RDD çš„ partition æœ€å¤šè¢« child RDD çš„ä¸€ä¸ªpartition ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰å®½ä¾èµ–ï¼šæ¯ä¸ªparent RDD partition è¢«å¤šä¸ª child RDD çš„partition ä½¿ç”¨ã€‚çª„ä¾èµ–æ¯ä¸ª child RDD çš„ partition çš„ç”Ÿæˆæ“ä½œéƒ½æ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œè€Œå®½ä¾èµ–åˆ™éœ€è¦æ‰€æœ‰çš„ parent RDD partition shuffle ç»“æœå¾—åˆ°åå†è¿›è¡Œã€‚äºŒã€org.apache.spark.Dependency.scala æºç è§£æDependencyæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼š1234// Denpendency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125;å®ƒæœ‰ä¸¤ä¸ªå­ç±»ï¼šNarrowDependency å’Œ ShuffleDenpendencyï¼Œåˆ†åˆ«å¯¹åº”çª„ä¾èµ–å’Œå®½ä¾èµ–ã€‚ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»å®šä¹‰äº†æŠ½è±¡æ–¹æ³•getParentsï¼Œè¾“å…¥partitionIdï¼Œç”¨äºè·å¾—child RDD çš„æŸä¸ªpartitionä¾èµ–çš„parent RDDçš„æ‰€æœ‰ partitionsã€‚1234567891011// Denpendency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125;çª„ä¾èµ–åˆæœ‰ä¸¤ä¸ªå…·ä½“çš„å®ç°ï¼šOneToOneDependencyå’ŒRangeDependencyã€‚ï¼ˆaï¼‰OneToOneDependencyæŒ‡child RDDçš„partitionåªä¾èµ–äºparent RDD çš„ä¸€ä¸ªpartitionï¼Œäº§ç”ŸOneToOneDependencyçš„ç®—å­æœ‰mapï¼Œfilterï¼ŒflatMapç­‰ã€‚å¯ä»¥çœ‹åˆ°getParentså®ç°å¾ˆç®€å•ï¼Œå°±æ˜¯ä¼ è¿›å»ä¸€ä¸ªpartitionIdï¼Œå†æŠŠpartitionIdæ”¾åœ¨Listé‡Œé¢ä¼ å‡ºå»ã€‚1234567891011121314151617// Denpendency.scalaclass OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; ï¼ˆbï¼‰RangeDependencyæŒ‡child RDD partitionåœ¨ä¸€å®šçš„èŒƒå›´å†…ä¸€å¯¹ä¸€çš„ä¾èµ–äºparent RDD partitionï¼Œä¸»è¦ç”¨äºunionã€‚// Denpendency.scalaclass RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123;//inStartè¡¨ç¤ºparent RDDçš„å¼€å§‹ç´¢å¼•ï¼ŒoutStartè¡¨ç¤ºchild RDD çš„å¼€å§‹ç´¢å¼• override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart)//è¡¨ç¤ºäºå½“å‰ç´¢å¼•çš„ç›¸å¯¹ä½ç½® &#125; else &#123; Nil &#125; &#125;&#125;ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–è¡¨ç¤ºä¸€ä¸ªparent RDDçš„partitionä¼šè¢«child RDDçš„partitionä½¿ç”¨å¤šæ¬¡ã€‚éœ€è¦ç»è¿‡shuffleæ‰èƒ½å½¢æˆã€‚123456789101112131415161718192021// Denpendency.scalaclass ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123; //shuffleéƒ½æ˜¯åŸºäºPairRDDè¿›è¡Œçš„ï¼Œæ‰€ä»¥ä¼ å…¥çš„RDDè¦æ˜¯key-valueç±»å‹çš„ override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) //è·å–shuffleId val shuffleId: Int = _rdd.context.newShuffleId() //å‘shuffleManageræ³¨å†Œshuffleä¿¡æ¯ val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;ç”±äºshuffleæ¶‰åŠåˆ°ç½‘ç»œä¼ è¾“ï¼Œæ‰€ä»¥è¦æœ‰åºåˆ—åŒ–serializerï¼Œä¸ºäº†å‡å°‘ç½‘ç»œä¼ è¾“ï¼Œå¯ä»¥mapç«¯èšåˆï¼Œé€šè¿‡mapSideCombineå’Œaggregatoræ§åˆ¶ï¼Œè¿˜æœ‰keyæ’åºç›¸å…³çš„keyOrderingï¼Œä»¥åŠé‡è¾“å‡ºçš„æ•°æ®å¦‚ä½•åˆ†åŒºçš„partitionerï¼Œè¿˜æœ‰ä¸€äº›classä¿¡æ¯ã€‚Partitionä¹‹é—´çš„å…³ç³»åœ¨shuffleå¤„æˆ›ç„¶è€Œæ­¢ï¼Œå› æ­¤shuffleæ˜¯åˆ’åˆ†stageçš„ä¾æ®ã€‚ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†é¦–å…ˆï¼Œçª„ä¾èµ–å…è®¸åœ¨ä¸€ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Šä»¥æµæ°´çº¿çš„æ–¹å¼ï¼ˆpipelineï¼‰è®¡ç®—æ‰€æœ‰çˆ¶åˆ†åŒºã€‚ä¾‹å¦‚ï¼Œé€ä¸ªå…ƒç´ åœ°æ‰§è¡Œmapã€ç„¶åfilteræ“ä½œï¼›è€Œå®½ä¾èµ–åˆ™éœ€è¦é¦–å…ˆè®¡ç®—å¥½æ‰€æœ‰çˆ¶åˆ†åŒºæ•°æ®ï¼Œç„¶ååœ¨èŠ‚ç‚¹ä¹‹é—´è¿›è¡ŒShuffleï¼Œè¿™ä¸MapReduceç±»ä¼¼ã€‚ç¬¬äºŒï¼Œçª„ä¾èµ–èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤±æ•ˆèŠ‚ç‚¹çš„æ¢å¤ï¼Œå³åªéœ€é‡æ–°è®¡ç®—ä¸¢å¤±RDDåˆ†åŒºçš„çˆ¶åˆ†åŒºï¼Œè€Œä¸”ä¸åŒèŠ‚ç‚¹ä¹‹é—´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼›è€Œå¯¹äºä¸€ä¸ªå®½ä¾èµ–å…³ç³»çš„Lineageå›¾ï¼Œå•ä¸ªèŠ‚ç‚¹å¤±æ•ˆå¯èƒ½å¯¼è‡´è¿™ä¸ªRDDçš„æ‰€æœ‰ç¥–å…ˆä¸¢å¤±éƒ¨åˆ†åˆ†åŒºï¼Œå› è€Œéœ€è¦æ•´ä½“é‡æ–°è®¡ç®—ã€‚]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark æŠ€æœ¯å›¢é˜Ÿå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow]]></title>
    <url>%2F2018%2F06%2F12%2FApache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow%2F</url>
    <content type="text"><![CDATA[è¿‘æ—¥ï¼Œæ¥è‡ª Databricks çš„ Matei Zaharia å®£å¸ƒæ¨å‡ºå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow ã€‚Matei Zaharia æ˜¯ Apache Spark å’Œ Apache Mesos çš„æ ¸å¿ƒä½œè€…ï¼Œä¹Ÿæ˜¯ Databrick çš„é¦–å¸­æŠ€æœ¯ä¸“å®¶ã€‚Databrick æ˜¯ç”± Apache Spark æŠ€æœ¯å›¢é˜Ÿæ‰€åˆ›ç«‹çš„å•†ä¸šåŒ–å…¬å¸ã€‚MLflow ç›®å‰å·²å¤„äºæ—©æœŸæµ‹è¯•é˜¶æ®µï¼Œå¼€å‘è€…å¯ä¸‹è½½æºç ä½“éªŒã€‚Matei Zaharia è¡¨ç¤ºå½“å‰åœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ çš„å…¬å¸æ™®éå­˜åœ¨å·¥å…·è¿‡å¤šã€éš¾ä»¥è·Ÿè¸ªå®éªŒã€éš¾ä»¥é‡ç°ç»“æœã€éš¾ä»¥éƒ¨ç½²ç­‰é—®é¢˜ã€‚ä¸ºè®©æœºå™¨å­¦ä¹ å¼€å‘å˜å¾—ä¸ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸€æ ·å¼ºå¤§ã€å¯é¢„æµ‹å’Œæ™®åŠï¼Œè®¸å¤šä¼ä¸šå·²å¼€å§‹æ„å»ºå†…éƒ¨æœºå™¨å­¦ä¹ å¹³å°æ¥ç®¡ç† MLç”Ÿå‘½å‘¨æœŸã€‚åƒæ˜¯ Facebookã€Google å’Œ Uber å°±å·²åˆ†åˆ«æ„å»ºäº† FBLearner Flowã€TFX å’Œ Michelangelo æ¥ç®¡ç†æ•°æ®ã€æ¨¡å‹åŸ¹è®­å’Œéƒ¨ç½²ã€‚ä¸è¿‡ç”±äºè¿™äº›å†…éƒ¨å¹³å°å­˜åœ¨å±€é™æ€§å’Œç»‘å®šæ€§ï¼Œæ— æ³•å¾ˆå¥½åœ°ä¸ç¤¾åŒºå…±äº«æˆæœï¼Œå…¶ä»–ç”¨æˆ·ä¹Ÿæ— æ³•è½»æ˜“ä½¿ç”¨ã€‚MLflow æ­£æ˜¯å—ç°æœ‰çš„ ML å¹³å°å¯å‘ï¼Œä¸»æ‰“å¼€æ”¾æ€§ï¼šå¼€æ”¾æ¥å£ï¼šå¯ä¸ä»»æ„ ML åº“ã€ç®—æ³•ã€éƒ¨ç½²å·¥å…·æˆ–ç¼–ç¨‹è¯­è¨€ä¸€èµ·ä½¿ç”¨ã€‚å¼€æºï¼šå¼€å‘è€…å¯è½»æ¾åœ°å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œå¹¶è·¨ç»„ç»‡å…±äº«å·¥ä½œæµæ­¥éª¤å’Œæ¨¡å‹ã€‚MLflow ç›®å‰çš„ alpha ç‰ˆæœ¬åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šå…¶ä¸­ï¼ŒMLflow Trackingï¼ˆè·Ÿè¸ªç»„ä»¶ï¼‰æä¾›äº†ä¸€ç»„ API å’Œç”¨æˆ·ç•Œé¢ï¼Œç”¨äºåœ¨è¿è¡Œæœºå™¨å­¦ä¹ ä»£ç æ—¶è®°å½•å’ŒæŸ¥è¯¢å‚æ•°ã€ä»£ç ç‰ˆæœ¬ã€æŒ‡æ ‡å’Œè¾“å‡ºæ–‡ä»¶ï¼Œä»¥ä¾¿ä»¥åå¯è§†åŒ–å®ƒä»¬ã€‚1234567891011121314import mlflow# Log parameters (key-value pairs)mlflow.log_param(&quot;num_dimensions&quot;, 8)mlflow.log_param(&quot;regularization&quot;, 0.1)# Log a metric; metrics can be updated throughout the runmlflow.log_metric(&quot;accuracy&quot;, 0.1)...mlflow.log_metric(&quot;accuracy&quot;, 0.45)# Log artifacts (output files)mlflow.log_artifact(&quot;roc.png&quot;)mlflow.log_artifact(&quot;model.pkl&quot;)MLflow Projectsï¼ˆé¡¹ç›®ç»„ä»¶ï¼‰æä¾›äº†æ‰“åŒ…å¯é‡ç”¨æ•°æ®ç§‘å­¦ä»£ç çš„æ ‡å‡†æ ¼å¼ã€‚æ¯ä¸ªé¡¹ç›®éƒ½åªæ˜¯ä¸€ä¸ªåŒ…å«ä»£ç æˆ– Git å­˜å‚¨åº“çš„ç›®å½•ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªæè¿°ç¬¦æ–‡ä»¶æ¥æŒ‡å®šå®ƒçš„ä¾èµ–å…³ç³»ä»¥åŠå¦‚ä½•è¿è¡Œä»£ç ã€‚æ¯ä¸ª MLflow é¡¹ç›®éƒ½æ˜¯ç”±ä¸€ä¸ªç®€å•çš„åä¸º MLproject çš„ YAML æ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰ã€‚123456789101112name: My Projectconda_env: conda.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot; validate: parameters: data_file: path command: &quot;python validate.py &#123;data_file&#125;&quot;MLflow Modelsï¼ˆæ¨¡å‹ç»„ä»¶ï¼‰æä¾›äº†ä¸€ç§ç”¨å¤šç§æ ¼å¼æ‰“åŒ…æœºå™¨å­¦ä¹ æ¨¡å‹çš„è§„èŒƒï¼Œè¿™äº›æ ¼å¼è¢«ç§°ä¸º â€œflavorâ€ ã€‚MLflow æä¾›äº†å¤šç§å·¥å…·æ¥éƒ¨ç½²ä¸åŒ flavor çš„æ¨¡å‹ã€‚æ¯ä¸ª MLflow æ¨¡å‹è¢«ä¿å­˜æˆä¸€ä¸ªç›®å½•ï¼Œç›®å½•ä¸­åŒ…å«äº†ä»»æ„æ¨¡å‹æ–‡ä»¶å’Œä¸€ä¸ª MLmodel æè¿°ç¬¦æ–‡ä»¶ï¼Œæ–‡ä»¶ä¸­åˆ—å‡ºäº†ç›¸åº”çš„ flavor ã€‚12345678time_created: 2018-02-21T13:21:34.12flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn pickled_model: model.pkl]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming çŠ¶æ€ç®¡ç†å‡½æ•°çš„é€‰æ‹©æ¯”è¾ƒ]]></title>
    <url>%2F2018%2F06%2F06%2FSparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[ä¸€ã€updateStateByKeyå®˜ç½‘åŸè¯ï¼š1In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.ä¹Ÿå³æ˜¯è¯´å®ƒä¼šç»Ÿè®¡å…¨å±€çš„keyçš„çŠ¶æ€ï¼Œå°±ç®—æ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œå®ƒä¹Ÿä¼šåœ¨æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶å€™è¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ã€‚ç¼ºç‚¹ï¼šè‹¥æ•°æ®é‡å¤ªå¤§çš„è¯ï¼Œéœ€è¦checkpointçš„æ•°æ®ä¼šå ç”¨è¾ƒå¤§çš„å­˜å‚¨ï¼Œæ•ˆç‡ä½ä¸‹ã€‚ç¨‹åºç¤ºä¾‹å¦‚ä¸‹ï¼š12345678910111213141516171819202122232425262728object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //æ³¨æ„ï¼šè¦ä½¿ç”¨updateStateByKeyå¿…é¡»è®¾ç½®checkpointç›®å½• ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; /*çŠ¶æ€æ›´æ–°å‡½æ•° * @param currentValues keyç›¸åŒvalueå½¢æˆçš„åˆ—è¡¨ * @param preValues keyå¯¹åº”çš„valueï¼Œå‰ä¸€çŠ¶æ€ * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seqåˆ—è¡¨ä¸­æ‰€æœ‰valueæ±‚å’Œ val pre = preValues.getOrElse(0) //è·å–ä¸Šä¸€çŠ¶æ€å€¼ Some(curr + pre) &#125; &#125;äºŒã€mapWithStatemapWithStateï¼šä¹Ÿæ˜¯ç”¨äºå…¨å±€ç»Ÿè®¡keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å®ƒå¦‚æœæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä¾¿ä¸ä¼šè¿”å›ä¹‹å‰çš„keyçš„çŠ¶æ€ï¼Œæœ‰ä¸€ç‚¹å¢é‡çš„æ„Ÿè§‰ã€‚æ•ˆç‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨å®˜æ–¹ä»£ç å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulNetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) val initialRDD = ssc.sparkContext .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1))) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;ä¸‰ã€æºç åˆ†æupateStateByKeyï¼šmapè¿”å›çš„æ˜¯MappedDStreamï¼Œè€ŒMappedDStreamå¹¶æ²¡æœ‰updateStateByKeyæ–¹æ³•ï¼Œå¹¶ä¸”å®ƒçš„çˆ¶ç±»DStreamä¸­ä¹Ÿæ²¡æœ‰è¯¥æ–¹æ³•ã€‚ä½†æ˜¯DStreamçš„ä¼´ç”Ÿå¯¹è±¡ä¸­æœ‰ä¸€ä¸ªéšå¼è½¬æ¢å‡½æ•°ï¼š123456object DStream &#123; implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairDStreamFunctions[K, V] = &#123; new PairDStreamFunctions[K, V](stream) &#125;è·Ÿè¿›å» PairDStreamFunctions ï¼Œå‘ç°æœ€ç»ˆè°ƒç”¨çš„æ˜¯è‡ªå·±çš„updateStateByKeyã€‚å…¶ä¸­updateFuncå°±è¦ä¼ å…¥çš„å‚æ•°ï¼Œä»–æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒSeq[V]è¡¨ç¤ºå½“å‰keyå¯¹åº”çš„æ‰€æœ‰å€¼ï¼Œ123456Option[S] æ˜¯å½“å‰keyçš„å†å²çŠ¶æ€ï¼Œè¿”å›çš„æ˜¯æ–°çš„çŠ¶æ€ã€‚def updateStateByKey[S: ClassTag]( updateFunc: (Seq[V], Option[S]) =&gt; Option[S] ): DStream[(K, S)] = ssc.withScope &#123; updateStateByKey(updateFunc, defaultPartitioner())&#125;æœ€ç»ˆè°ƒç”¨ï¼š12345678910def updateStateByKey[S: ClassTag]( updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)], partitioner: Partitioner, rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123; val cleanedFunc = ssc.sc.clean(updateFunc) val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123; cleanedFunc(it) &#125; new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)&#125;å†è·Ÿè¿›å» new StateDStream:åœ¨è¿™é‡Œé¢newå‡ºäº†ä¸€ä¸ªStateDStreamå¯¹è±¡ã€‚åœ¨å…¶computeæ–¹æ³•ä¸­ï¼Œä¼šå…ˆè·å–ä¸Šä¸€ä¸ªbatchè®¡ç®—å‡ºçš„RDDï¼ˆåŒ…å«äº†è‡³ç¨‹åºå¼€å§‹åˆ°ä¸Šä¸€ä¸ªbatchå•è¯çš„ç´¯è®¡è®¡æ•°ï¼‰ï¼Œç„¶ååœ¨è·å–æœ¬æ¬¡batchä¸­StateDStreamçš„çˆ¶ç±»è®¡ç®—å‡ºçš„RDDï¼ˆæœ¬æ¬¡batchçš„å•è¯è®¡æ•°ï¼‰åˆ†åˆ«æ˜¯prevStateRDDå’ŒparentRDDï¼Œç„¶ååœ¨è°ƒç”¨ computeUsingPreviousRDD æ–¹æ³•ï¼š1234567891011121314151617181920private [this] def computeUsingPreviousRDD( batchTime: Time, parentRDD: RDD[(K, V)], prevStateRDD: RDD[(K, S)]) = &#123; // Define the function for the mapPartition operation on cogrouped RDD; // first map the cogrouped tuple to tuples of required type, // and then apply the update function val updateFuncLocal = updateFunc val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123; val i = iterator.map &#123; t =&gt; val itr = t._2._2.iterator val headOption = if (itr.hasNext) Some(itr.next()) else None (t._1, t._2._1.toSeq, headOption) &#125; updateFuncLocal(batchTime, i) &#125; val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner) val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning) Some(stateRDD)&#125;åœ¨è¿™é‡Œä¸¤ä¸ªRDDè¿›è¡Œcogroupç„¶ååº”ç”¨updateStateByKeyä¼ å…¥çš„å‡½æ•°ã€‚æˆ‘ä»¬çŸ¥é“cogroupçš„æ€§èƒ½æ˜¯æ¯”è¾ƒä½ä¸‹ï¼Œå‚è€ƒhttp://lxw1234.com/archives/2015/07/384.htmã€‚mapWithState:123456789@Experimentaldef mapWithState[StateType: ClassTag, MappedType: ClassTag]( spec: StateSpec[K, V, StateType, MappedType] ): MapWithStateDStream[K, V, StateType, MappedType] = &#123; new MapWithStateDStreamImpl[K, V, StateType, MappedType]( self, spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]] )&#125;è¯´æ˜ï¼šStateSpec å°è£…äº†çŠ¶æ€ç®¡ç†å‡½æ•°ï¼Œå¹¶åœ¨è¯¥æ–¹æ³•ä¸­åˆ›å»ºäº†MapWithStateDStreamImplå¯¹è±¡ã€‚MapWithStateDStreamImpl ä¸­åˆ›å»ºäº†ä¸€ä¸ªInternalMapWithStateDStreamç±»å‹å¯¹è±¡internalStreamï¼Œåœ¨MapWithStateDStreamImplçš„computeæ–¹æ³•ä¸­è°ƒç”¨äº†internalStreamçš„getOrComputeæ–¹æ³•ã€‚12345678910111213141516private[streaming] class MapWithStateDStreamImpl[ KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag]( dataStream: DStream[(KeyType, ValueType)], spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType]) extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123; private val internalStream = new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec) override def slideDuration: Duration = internalStream.slideDuration override def dependencies: List[DStream[_]] = List(internalStream) override def compute(validTime: Time): Option[RDD[MappedType]] = &#123; internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125; &#125;InternalMapWithStateDStreamä¸­æ²¡æœ‰getOrComputeæ–¹æ³•ï¼Œè¿™é‡Œè°ƒç”¨çš„æ˜¯å…¶çˆ¶ç±» DStream çš„getOrCpmputeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸­æœ€ç»ˆä¼šè°ƒç”¨InternalMapWithStateDStreamçš„Computeæ–¹æ³•ï¼š12345678910111213141516171819202122232425262728293031323334/** Method that generates an RDD for the given time */override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123; // Get the previous state or create a new empty state RDD val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123; case Some(rdd) =&gt; if (rdd.partitioner != Some(partitioner)) &#123; // If the RDD is not partitioned the right way, let us repartition it using the // partition index as the key. This is to ensure that state RDD is always partitioned // before creating another state RDD using it MapWithStateRDD.createFromRDD[K, V, S, E]( rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime) &#125; else &#123; rdd &#125; case None =&gt; MapWithStateRDD.createFromPairRDD[K, V, S, E]( spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)), partitioner, validTime ) &#125; // Compute the new state RDD with previous state RDD and partitioned data RDD // Even if there is no data RDD, use an empty one to create a new state RDD val dataRDD = parent.getOrCompute(validTime).getOrElse &#123; context.sparkContext.emptyRDD[(K, V)] &#125; val partitionedDataRDD = dataRDD.partitionBy(partitioner) val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt; (validTime - interval).milliseconds &#125; Some(new MapWithStateRDD( prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))&#125;æ ¹æ®ç»™å®šçš„æ—¶é—´ç”Ÿæˆä¸€ä¸ªMapWithStateRDDï¼Œé¦–å…ˆè·å–äº†å…ˆå‰çŠ¶æ€çš„RDDï¼špreStateRDDå’Œå½“å‰æ—¶é—´çš„RDD:dataRDDï¼Œç„¶åå¯¹dataRDDåŸºäºå…ˆå‰çŠ¶æ€RDDçš„åˆ†åŒºå™¨è¿›è¡Œé‡æ–°åˆ†åŒºè·å–partitionedDataRDDã€‚æœ€åå°†preStateRDDï¼ŒpartitionedDataRDDå’Œç”¨æˆ·å®šä¹‰çš„å‡½æ•°mappingFunctionä¼ ç»™æ–°ç”Ÿæˆçš„MapWithStateRDDå¯¹è±¡è¿”å›ã€‚åç»­è‹¥æœ‰å…´è¶£å¯ä»¥ç»§ç»­è·Ÿè¿›MapWithStateRDDçš„computeæ–¹æ³•ï¼Œé™äºç¯‡å¹…ä¸å†å±•ç¤ºã€‚]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL ä¹‹å¤–éƒ¨æ•°æ®æºå¦‚ä½•æˆä¸ºåœ¨ä¼ä¸šå¼€å‘ä¸­çš„ä¸€æŠŠåˆ©å™¨]]></title>
    <url>%2F2018%2F06%2F06%2FSpark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 æ¦‚è¿°1.Spark1.2ä¸­ï¼ŒSpark SQLå¼€å§‹æ­£å¼æ”¯æŒå¤–éƒ¨æ•°æ®æºã€‚Spark SQLå¼€æ”¾äº†ä¸€ç³»åˆ—æ¥å…¥å¤–éƒ¨æ•°æ®æºçš„æ¥å£ï¼Œæ¥è®©å¼€å‘è€…å¯ä»¥å®ç°ã€‚ä½¿å¾—Spark SQLå¯ä»¥åŠ è½½ä»»ä½•åœ°æ–¹çš„æ•°æ®ï¼Œä¾‹å¦‚mysqlï¼Œhiveï¼Œhdfsï¼Œhbaseç­‰ï¼Œè€Œä¸”æ”¯æŒå¾ˆå¤šç§æ ¼å¼å¦‚json, parquet, avro, csvæ ¼å¼ã€‚æˆ‘ä»¬å¯ä»¥å¼€å‘å‡ºä»»æ„çš„å¤–éƒ¨æ•°æ®æºæ¥è¿æ¥åˆ°Spark SQLï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIæ¥è¿›è¡Œæ“ä½œã€‚2.æˆ‘ä»¬é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIè¯»å–å„ç§æ ¼å¼çš„æ•°æ®ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªDataFrameï¼Œè¿™æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„æ–¹å¼å•Šï¼Œå°±å¯ä»¥ä½¿ç”¨DataFrameçš„APIæˆ–è€…SQLçš„APIè¿›è¡Œæ“ä½œå“ˆã€‚3.å¤–éƒ¨æ•°æ®æºçš„APIå¯ä»¥è‡ªåŠ¨åšä¸€äº›åˆ—çš„è£å‰ªï¼Œä»€ä¹ˆå«åˆ—çš„è£å‰ªï¼Œå‡å¦‚ä¸€ä¸ªuserè¡¨æœ‰id,name,age,gender4ä¸ªåˆ—ï¼Œåœ¨åšselectçš„æ—¶å€™ä½ åªéœ€è¦id,nameè¿™ä¸¤åˆ—ï¼Œé‚£ä¹ˆå…¶ä»–åˆ—ä¼šé€šè¿‡åº•å±‚çš„ä¼˜åŒ–å»ç»™æˆ‘ä»¬è£å‰ªæ‰ã€‚4.ä¿å­˜æ“ä½œå¯ä»¥é€‰æ‹©ä½¿ç”¨SaveModeï¼ŒæŒ‡å®šå¦‚ä½•ä¿å­˜ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚2.è¯»å–jsonæ–‡ä»¶å¯åŠ¨shellè¿›è¡Œæµ‹è¯•1234567891011121314151617181920//æ ‡å‡†å†™æ³•val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)//å¦å¤–ä¸€ç§å†™æ³•spark.read.json(&quot;path&quot;)çœ‹çœ‹æºç è¿™ä¸¤è€…ä¹‹é—´åˆ°åº•æœ‰å•¥ä¸åŒå‘¢ï¼Ÿ/** * Loads a JSON file and returns the results as a `DataFrame`. * * See the documentation on the overloaded `json()` method with varargs for more details. * * @since 1.4.0 */ def json(path: String): DataFrame = &#123; // This method ensures that calls that explicit need single argument works, see SPARK-16009 json(Seq(path): _*) &#125;æˆ‘ä»¬è°ƒç”¨josn() æ–¹æ³•å…¶å®è¿›è¡Œäº† overloaded ï¼Œæˆ‘ä»¬ç»§ç»­æŸ¥çœ‹ def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*) è¿™å¥è¯æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œå…¶å®å°±æ˜¯æˆ‘ä»¬çš„æ ‡å‡†å†™æ³•1234567891011121314151617 scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+3 è¯»å–parquetæ•°æ®12345678910val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+4 è¯»å–hiveä¸­çš„æ•°æ®1234567891011121314151617181920212223242526272829303132spark.sql(&quot;show tables&quot;).show+--------+----------+-----------+|database| tableName|isTemporary|+--------+----------+-----------+| default|states_raw| false|| default|states_seq| false|| default| t1| false|+--------+----------+-----------+spark.table(&quot;states_raw&quot;).show+-----+------+| code| name|+-----+------+|hello| java||hello|hadoop||hello| hive||hello| sqoop||hello| hdfs||hello| spark|+-----+------+scala&gt; spark.sql(&quot;select name from states_raw &quot;).show+------+| name|+------+| java||hadoop|| hive|| sqoop|| hdfs|| spark|+------+5 ä¿å­˜æ•°æ®æ³¨æ„ï¼šä¿å­˜çš„æ–‡ä»¶å¤¹ä¸èƒ½å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™(é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å¼)ï¼šorg.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;ä¿å­˜æˆæ–‡æœ¬æ ¼å¼ï¼Œåªèƒ½ä¿å­˜ä¸€åˆ—ï¼Œå¦åˆ™æŠ¥é”™ï¼šorg.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;123456789101112131415161718192021222324252627282930val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)//ä¿å­˜df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)ç»“æœï¼š[hadoop@hadoop out]$ pwd/home/hadoop/data/out[hadoop@hadoop out]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:34 _SUCCESS[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt MichaelAndyJustin//ä¿å­˜ä¸ºjsonæ ¼å¼df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)ç»“æœ[hadoop@hadoop data]$ cd out1[hadoop@hadoop out1]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:35 _SUCCESS[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;ä¸Šé¢è¯´äº†åœ¨ä¿å­˜æ•°æ®æ—¶å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåœ¨é»˜è®¤æ¨¡å¼ä¸‹ä¼šæŠ¥é”™ï¼Œé‚£æˆ‘ä»¬ä¸‹é¢è®²è§£ä¿å­˜çš„å‡ ç§æ¨¡å¼ï¼š6 è¯»å–mysqlä¸­çš„æ•°æ®1234567891011121314151617181920212223val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;).option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123456&quot;).load()scala&gt; jdbcDF.printSchemaroot |-- TBL_ID: long (nullable = false) |-- CREATE_TIME: integer (nullable = false) |-- DB_ID: long (nullable = true) |-- LAST_ACCESS_TIME: integer (nullable = false) |-- OWNER: string (nullable = true) |-- RETENTION: integer (nullable = false) |-- SD_ID: long (nullable = true) |-- TBL_NAME: string (nullable = true) |-- TBL_TYPE: string (nullable = true) |-- VIEW_EXPANDED_TEXT: string (nullable = true) |-- VIEW_ORIGINAL_TEXT: string (nullable = true)jdbcDF.show7 spark SQLæ“ä½œmysqlè¡¨æ•°æ®123456789101112131415161718192021222324252627282930313233CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url &quot;jdbc:mysql://localhost:3306&quot;, dbtable &quot;basic01.tbls&quot;, user &apos;root&apos;, password &apos;123456&apos;, driver &quot;com.mysql.jdbc.Driver&quot;);æŸ¥çœ‹ï¼šshow tables;default states_raw falsedefault states_seq falsedefault t1 falsejdbctable trueselect * from jdbctable;1 1519944170 6 0 hadoop 0 1 page_views MANAGED_TABLE NULL NULL2 1519944313 6 0 hadoop 0 2 page_views_bzip2 MANAGED_TABLE NULL NULL3 1519944819 6 0 hadoop 0 3 page_views_snappy MANAGED_TABLE NULL NULL21 1520067771 6 0 hadoop 0 21 tt MANAGED_TABLE NULL NULL22 1520069148 6 0 hadoop 0 22 page_views_seq MANAGED_TABLE NULL NULL23 1520071381 6 0 hadoop 0 23 page_views_rcfile MANAGED_TABLE NULL NULL24 1520074675 6 0 hadoop 0 24 page_views_orc_zlib MANAGED_TABLE NULL NULL27 1520078184 6 0 hadoop 0 27 page_views_lzo_index MANAGED_TABLE NULL NULL30 1520083461 6 0 hadoop 0 30 page_views_lzo_index1 MANAGED_TABLE NULL NULL31 1524370014 1 0 hadoop 0 31 t1 EXTERNAL_TABLE NULL NULL37 1524468636 1 0 hadoop 0 37 states_raw MANAGED_TABLE NULL NULL38 1524468678 1 0 hadoop 0 38 states_seq MANAGED_TABLE NULL NULLmysqlä¸­çš„tblsçš„æ•°æ®å·²ç»å­˜åœ¨jdbctableè¡¨ä¸­äº†ã€‚jdbcDF.show8 åˆ†åŒºæ¨æµ‹ï¼ˆPartition Discoveryï¼‰è¡¨åˆ†åŒºæ˜¯åœ¨åƒHiveè¿™æ ·çš„ç³»ç»Ÿä¸­ä½¿ç”¨çš„å¸¸è§ä¼˜åŒ–æ–¹æ³•ã€‚ åœ¨åˆ†åŒºè¡¨ä¸­ï¼Œæ•°æ®é€šå¸¸å­˜å‚¨åœ¨ä¸åŒçš„ç›®å½•ä¸­ï¼Œåˆ†åŒºåˆ—å€¼åœ¨æ¯ä¸ªåˆ†åŒºç›®å½•çš„è·¯å¾„ä¸­ç¼–ç ã€‚ æ‰€æœ‰å†…ç½®çš„æ–‡ä»¶æºï¼ˆåŒ…æ‹¬Text / CSV / JSON / ORC / Parquetï¼‰éƒ½èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å’Œæ¨æ–­åˆ†åŒºä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ›å»ºå¦‚ä¸‹çš„ç›®å½•ç»“æ„;123456789hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CNæ·»åŠ jsonæ–‡ä»¶ï¼špeople.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CNæˆ‘ä»¬ä½¿ç”¨spark sqlè¯»å–å¤–éƒ¨æ•°æ®æºï¼š1234567891011121314151617val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+æˆ‘ä»¬æ”¹å˜è¯»å–çš„ç›®å½•12345678910111213141516val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) |-- country: string (nullable = true)scala&gt; df.show+----+-------+-------+| age| name|country|+----+-------+-------+|null|Michael| CN|| 30| Andy| CN|| 19| Justin| CN|+----+-------+-------+å¤§å®¶æœ‰æ²¡æœ‰å‘ç°ä»€ä¹ˆå‘¢ï¼ŸSpark SQLå°†è‡ªåŠ¨ä»è·¯å¾„ä¸­æå–åˆ†åŒºä¿¡æ¯ã€‚æ³¨æ„ï¼Œåˆ†åŒºåˆ—çš„æ•°æ®ç±»å‹æ˜¯è‡ªåŠ¨æ¨æ–­çš„ã€‚ç›®å‰æ”¯æŒæ•°å­—æ•°æ®ç±»å‹ï¼Œæ—¥æœŸï¼Œæ—¶é—´æˆ³å’Œå­—ç¬¦ä¸²ç±»å‹ã€‚æœ‰æ—¶ç”¨æˆ·å¯èƒ½ä¸æƒ³è‡ªåŠ¨æ¨æ–­åˆ†åŒºåˆ—çš„æ•°æ®ç±»å‹ã€‚å¯¹äºè¿™äº›ç”¨ä¾‹ï¼Œè‡ªåŠ¨ç±»å‹æ¨æ–­å¯ä»¥é€šè¿‡spark.sql.sources.partitionColumnTypeInference.enabledè¿›è¡Œé…ç½®ï¼Œé»˜è®¤ä¸ºtrueã€‚å½“ç¦ç”¨ç±»å‹æ¨æ–­æ—¶ï¼Œå­—ç¬¦ä¸²ç±»å‹å°†ç”¨äºåˆ†åŒºåˆ—ã€‚ä»Spark 1.6.0å¼€å§‹ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ†åŒºå‘ç°ä»…åœ¨ç»™å®šè·¯å¾„ä¸‹æ‰¾åˆ°åˆ†åŒºã€‚å¯¹äºä¸Šé¢çš„ç¤ºä¾‹ï¼Œå¦‚æœç”¨æˆ·å°†è·¯å¾„/table/gender=maleä¼ é€’ç»™SparkSession.read.parquetæˆ–SparkSession.read.loadï¼Œåˆ™ä¸ä¼šå°†æ€§åˆ«è§†ä¸ºåˆ†åŒºåˆ—ã€‚å¦‚æœç”¨æˆ·éœ€è¦æŒ‡å®šå¯åŠ¨åˆ†åŒºå‘ç°çš„åŸºæœ¬è·¯å¾„ï¼Œåˆ™å¯ä»¥basePathåœ¨æ•°æ®æºé€‰é¡¹ä¸­è¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼Œå½“path/to/table/gender=maleæ˜¯æ•°æ®è·¯å¾„å¹¶ä¸”ç”¨æˆ·å°†basePathè®¾ç½®ä¸ºpath/to/table/æ—¶ï¼Œæ€§åˆ«å°†æ˜¯åˆ†åŒºåˆ—ã€‚]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxç³»ç»Ÿé‡è¦å‚æ•°è°ƒä¼˜ï¼Œä½ çŸ¥é“å—]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%2F</url>
    <content type="text"><![CDATA[å½“å‰ä¼šè¯ç”Ÿæ•ˆulimit -u -&gt; æŸ¥çœ‹å½“å‰æœ€å¤§è¿›ç¨‹æ•°ulimit -n -&gt;æŸ¥çœ‹å½“å‰æœ€å¤§æ–‡ä»¶æ•°ulimit -u xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§è¿›ç¨‹æ•°ä¸ºxxxulimit -n xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§æ–‡ä»¶æ•°ä¸ºxxxæ°¸ä¹…ç”Ÿæ•ˆ1.vi /etc/security/limits.confï¼Œæ·»åŠ å¦‚ä¸‹çš„è¡Œsoft noproc 11000hard noproc 11000soft nofile 4100hard nofile 4100 è¯´æ˜ï¼šä»£è¡¨é’ˆå¯¹æ‰€æœ‰ç”¨æˆ·noproc æ˜¯ä»£è¡¨æœ€å¤§è¿›ç¨‹æ•°nofile æ˜¯ä»£è¡¨æœ€å¤§æ–‡ä»¶æ‰“å¼€æ•°2.è®© SSH æ¥å— Login ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨ ssh å®¢æˆ·ç«¯æŸ¥çœ‹ ulimit -a èµ„æºé™åˆ¶ï¼š1)ã€vi /etc/ssh/sshd_configæŠŠ UserLogin çš„å€¼æ”¹ä¸º yesï¼Œå¹¶æŠŠ # æ³¨é‡Šå»æ‰2)ã€é‡å¯ sshd æœåŠ¡/etc/init.d/sshd restart3)ã€ä¿®æ”¹æ‰€æœ‰ linux ç”¨æˆ·çš„ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼švi /etc/profileulimit -u 10000ulimit -n 4096ulimit -d unlimitedulimit -m unlimitedulimit -s unlimitedulimit -t unlimitedulimit -v unlimited4)ã€ç”Ÿæ•ˆsource /etc/profile]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkåŠ¨æ€å†…å­˜ç®¡ç†æºç è§£æï¼]]></title>
    <url>%2F2018%2F06%2F03%2FSpark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81%2F</url>
    <content type="text"><![CDATA[ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼Sparkæœ‰ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œé™æ€å†…å­˜ç®¡ç†(Static MemoryManager)å’ŒåŠ¨æ€ï¼ˆç»Ÿä¸€ï¼‰å†…å­˜ç®¡ç†ï¼ˆUnified MemoryManagerï¼‰ã€‚åŠ¨æ€å†…å­˜ç®¡ç†ä»Spark1.6å¼€å§‹å¼•å…¥ï¼Œåœ¨SparkEnv.scalaä¸­çš„æºç å¯ä»¥çœ‹åˆ°ï¼ŒSparkç›®å‰é»˜è®¤é‡‡ç”¨åŠ¨æ€å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œè‹¥å°†spark.memory.useLegacyModeè®¾ç½®ä¸ºtrueï¼Œåˆ™ä¼šæ”¹ä¸ºé‡‡ç”¨é™æ€å†…å­˜ç®¡ç†ã€‚12345678// SparkEnv.scala val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false) val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) &#125;äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…ç›¸æ¯”äºStatic MemoryManageræ¨¡å¼ï¼ŒUnified MemoryManageræ¨¡å‹æ‰“ç ´äº†å­˜å‚¨å†…å­˜å’Œè¿è¡Œå†…å­˜çš„ç•Œé™ï¼Œä½¿æ¯ä¸€ä¸ªå†…å­˜åŒºèƒ½å¤ŸåŠ¨æ€ä¼¸ç¼©ï¼Œé™ä½OOMçš„æ¦‚ç‡ã€‚ç”±ä¸Šå›¾å¯çŸ¥ï¼Œexecutor JVMå†…å­˜ä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªåŒºåŸŸç»„æˆï¼šï¼ˆ1ï¼‰Reserved Memoryï¼ˆé¢„ç•™å†…å­˜ï¼‰ï¼šè¿™éƒ¨åˆ†å†…å­˜é¢„ç•™ç»™ç³»ç»Ÿä½¿ç”¨ï¼Œé»˜è®¤ä¸º300MBï¼Œå¯é€šè¿‡spark.testing.reservedMemoryè¿›è¡Œè®¾ç½®ã€‚12// UnifiedMemoryManager.scalaprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024å¦å¤–ï¼ŒJVMå†…å­˜çš„æœ€å°å€¼ä¹Ÿä¸reserved Memoryæœ‰å…³ï¼Œå³minSystemMemory = reserved Memory1.5ï¼Œå³é»˜è®¤æƒ…å†µä¸‹JVMå†…å­˜æœ€å°å€¼ä¸º300MB1.5=450MBã€‚12// UnifiedMemoryManager.scala val minSystemMemory = (reservedMemory * 1.5).ceil.toLongï¼ˆ2ï¼‰Spark Memeoy:åˆ†ä¸ºexecution Memoryå’Œstorage Memoryã€‚å»é™¤æ‰reserved Memoryï¼Œå‰©ä¸‹usableMemoryçš„ä¸€éƒ¨åˆ†ç”¨äºexecutionå’Œstorageè¿™ä¸¤ç±»å †å†…å­˜ï¼Œé»˜è®¤æ˜¯0.6ï¼Œå¯é€šè¿‡spark.memory.fractionè¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼šJVMå†…å­˜æ˜¯1Gï¼Œé‚£ä¹ˆç”¨äºexecutionå’Œstorageçš„é»˜è®¤å†…å­˜ä¸ºï¼ˆ1024-300ï¼‰*0.6=434MBã€‚1234// UnifiedMemoryManager.scala val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6) (usableMemory * memoryFraction).toLongä»–ä»¬çš„è¾¹ç•Œç”±spark.memory.storageFractionè®¾å®šï¼Œé»˜è®¤ä¸º0.5ã€‚å³é»˜è®¤çŠ¶æ€ä¸‹storage Memoryå’Œexecution Memoryä¸º1ï¼š1.1234// UnifiedMemoryManager.scala onHeapStorageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)ï¼ˆ3ï¼‰user Memory:å‰©ä½™å†…å­˜ï¼Œç”¨æˆ·æ ¹æ®éœ€è¦ä½¿ç”¨ï¼Œé»˜è®¤å usableMemoryçš„ï¼ˆ1-0.6ï¼‰=0.4.ä¸‰ã€å†…å­˜æ§åˆ¶è¯¦è§£é¦–å…ˆæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹Sparkå†…å­˜ç®¡ç†å®ç°ç±»ä¹‹å‰çš„å…³ç³»ã€‚1.MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼šï¼ˆ1ï¼‰è®°å½•ç”¨äº†å¤šå°‘StorageMemoryå’ŒExecutionMemoryï¼›ï¼ˆ2ï¼‰ç”³è¯·Storageã€Executionå’ŒUnroll Memoryï¼›ï¼ˆ3ï¼‰é‡Šæ”¾Stroageå’ŒExecution Memoryã€‚Executionå†…å­˜ç”¨æ¥æ‰§è¡Œshuffleã€joinsã€sortså’Œaggegationsæ“ä½œï¼ŒStorageå†…å­˜ç”¨äºç¼“å­˜å’Œå¹¿æ’­æ•°æ®ï¼Œæ¯ä¸€ä¸ªJVMä¸­éƒ½å­˜åœ¨ç€ä¸€ä¸ªMemoryManagerã€‚æ„é€ MemoryManageréœ€è¦æŒ‡å®šonHeapStorageMemoryå’ŒonHeapExecutionMemoryå‚æ•°ã€‚123456 // MemoryManager.scalaprivate[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging &#123;åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPoolå¯¹è±¡ï¼Œç”¨æ¥åˆ›å»ºå †å†…æˆ–å †å¤–çš„Storageå’ŒExecutionå†…å­˜æ± ï¼Œç®¡ç†Storageå’ŒExecutionçš„å†…å­˜åˆ†é…ã€‚123456789// MemoryManager.scala @GuardedBy(&quot;this&quot;) protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(&quot;this&quot;) protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ä½¿ç”¨å †å¤–å†…å­˜ï¼Œå¯é€šè¿‡saprk.memory.offHeap.enabledè®¾ç½®ï¼Œé»˜è®¤å †å¤–å†…å­˜ä¸º0ï¼Œå¯ä½¿ç”¨spark.memory.offHeap.sizeå‚æ•°è®¾ç½®ã€‚123456789101112// All the code you will ever need final val tungstenMemoryMode: MemoryMode = &#123; if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123; require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0, &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;) require(Platform.unaligned(), &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;) MemoryMode.OFF_HEAP &#125; else &#123; MemoryMode.ON_HEAP &#125; &#125;12// MemoryManager.scala protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)é‡Šæ”¾numByteså­—èŠ‚çš„Executionå†…å­˜æ–¹æ³•12345678910// MemoryManager.scaladef releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) &#125; &#125;é‡Šæ”¾æŒ‡å®štaskçš„æ‰€æœ‰Executionå†…å­˜å¹¶å°†è¯¥taskæ ‡è®°ä¸ºinactiveã€‚12345// MemoryManager.scala private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123; onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) &#125;é‡Šæ”¾numByteså­—èŠ‚çš„Stoargeå†…å­˜æ–¹æ³•1234567// MemoryManager.scaladef releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes) &#125; &#125;é‡Šæ”¾æ‰€æœ‰Storageå†…å­˜æ–¹æ³•12345// MemoryManager.scalafinal def releaseAllStorageMemory(): Unit = synchronized &#123; onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() &#125;2.æ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡Œæ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼ŸUnifiedMemoryManageç»§æ‰¿äº†MemoryManager1234567891011// UnifiedMemoryManage.scalaprivate[spark] class UnifiedMemoryManager private[memory] ( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) &#123;é‡å†™äº†maxOnHeapStorageMemoryæ–¹æ³•ï¼Œæœ€å¤§Storageå†…å­˜=æœ€å¤§å†…å­˜-æœ€å¤§Executionå†…å­˜ã€‚1234// UnifiedMemoryManage.scala override def maxOnHeapStorageMemory: Long = synchronized &#123; maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed &#125;æ ¸å¿ƒæ–¹æ³•acquireStorageMemoryï¼šç”³è¯·Storageå†…å­˜ã€‚12345678910111213141516171819202122232425262728293031// UnifiedMemoryManage.scalaoverride def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized &#123; assertInvariants() assert(numBytes &gt;= 0) val (executionPool, storagePool, maxMemory) = memoryMode match &#123; //æ ¹æ®ä¸åŒçš„å†…å­˜æ¨¡å¼å»åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPool case MemoryMode.ON_HEAP =&gt; ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP =&gt; ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapMemory) &#125; if (numBytes &gt; maxMemory) &#123; // è‹¥ç”³è¯·å†…å­˜å¤§äºæœ€å¤§å†…å­˜ï¼Œåˆ™ç”³è¯·å¤±è´¥ logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; + s&quot;memory limit ($maxMemory bytes)&quot;) return false &#125; if (numBytes &gt; storagePool.memoryFree) &#123; // å¦‚æœStorageå†…å­˜æ± æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œåˆ™å‘Executionå†…å­˜æ± å€Ÿç”¨ val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//å½“Executionå†…å­˜æœ‰ç©ºé—²æ—¶ï¼ŒStorageæ‰èƒ½å€Ÿåˆ°å†…å­˜ executionPool.decrementPoolSize(memoryBorrowedFromExecution)//ç¼©å°Executionå†…å­˜ storagePool.incrementPoolSize(memoryBorrowedFromExecution)//å¢åŠ Storageå†…å­˜ &#125; storagePool.acquireMemory(blockId, numBytes)æ ¸å¿ƒæ–¹æ³•acquireExecutionMemoryï¼šç”³è¯·Executionå†…å­˜ã€‚123456789// UnifiedMemoryManage.scalaoverride private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized &#123;//ä½¿ç”¨äº†synchronizedå…³é”®å­—ï¼Œè°ƒç”¨acquireExecutionMemoryæ–¹æ³•å¯èƒ½ä¼šé˜»å¡ï¼Œç›´åˆ°Executionå†…å­˜æ± æœ‰è¶³å¤Ÿçš„å†…å­˜ã€‚ ... executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize) &#125;æ–¹æ³•æœ€åè°ƒç”¨äº†ExecutionMemoryPoolçš„acquireMemoryæ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„å‚æ•°éœ€è¦ä¸¤ä¸ªå‡½æ•°ï¼šmaybeGrowExecutionPool()å’ŒcomputeMaxExecutionPoolSize()ã€‚æ¯ä¸ªTaskèƒ½å¤Ÿä½¿ç”¨çš„å†…å­˜è¢«é™åˆ¶åœ¨pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasksã€‚å…¶ä¸­maxPoolSizeä»£è¡¨äº†execution poolçš„æœ€å¤§å†…å­˜ï¼ŒpoolSizeè¡¨ç¤ºå½“å‰è¿™ä¸ªpoolçš„å¤§å°ã€‚1234// ExecutionMemoryPool.scala val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks)maybeGrowExecutionPool()æ–¹æ³•å®ç°äº†å¦‚ä½•åŠ¨æ€å¢åŠ Executionå†…å­˜åŒºçš„å¤§å°ã€‚åœ¨æ¯æ¬¡ç”³è¯·executionå†…å­˜çš„åŒæ—¶ï¼Œexecutionå†…å­˜æ± ä¼šè¿›è¡Œå¤šæ¬¡å°è¯•ï¼Œæ¯æ¬¡å°è¯•éƒ½å¯èƒ½ä¼šå›æ”¶ä¸€äº›å­˜å‚¨å†…å­˜ã€‚123456789101112131415// UnifiedMemoryManage.scala def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123; if (extraMemoryNeeded &gt; 0) &#123;//å¦‚æœç”³è¯·çš„å†…å­˜å¤§äº0 //è®¡ç®—executionå¯å€Ÿåˆ°çš„storageå†…å­˜ï¼Œæ˜¯storageå‰©ä½™å†…å­˜å’Œå¯å€Ÿå‡ºå†…å­˜çš„æœ€å¤§å€¼ val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage &gt; 0) &#123;//å¦‚æœå¯ä»¥ç”³è¯·åˆ°å†…å­˜ val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//å®é™…éœ€è¦çš„å†…å­˜ï¼Œå–å®é™…éœ€è¦çš„å†…å­˜å’Œstorageå†…å­˜åŒºåŸŸå…¨éƒ¨å¯ç”¨å†…å­˜å¤§å°çš„æœ€å°å€¼ storagePool.decrementPoolSize(spaceToReclaim)//storageå†…å­˜åŒºåŸŸå‡å°‘ executionPool.incrementPoolSize(spaceToReclaim)//executionå†…å­˜åŒºåŸŸå¢åŠ  &#125; &#125; &#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>æºç é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½å¤§æ•°æ®-é›¶åŸºç¡€å­¦å‘˜æ·±åœ³æŸå¸é«˜è–ªé¢è¯•é¢˜]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[å•¥ä¹Ÿä¸è¯´ï¼ç›´æ¥ä¸Šé¢˜é¢è¯•æ—¶é—´ï¼š20180531ç®€å•è¯´ä¸‹hdfsè¯»æ–‡ä»¶å’Œå†™æ–‡ä»¶çš„æµç¨‹æ¯å¤©æ•°æ®é‡æœ‰å¤šå¤§ï¼Ÿç”Ÿäº§é›†ç¾¤è§„æ¨¡æœ‰å¤šå¤§ï¼Ÿè¯´å‡ ä¸ªsparkå¼€å‘ä¸­é‡åˆ°çš„é—®é¢˜ï¼Œå’Œè§£å†³çš„æ–¹æ¡ˆé˜è¿°ä¸€ä¸‹æœ€è¿‘å¼€å‘çš„é¡¹ç›®ï¼Œä»¥åŠæ‹…ä»»çš„è§’è‰²ä½ç½®kafkaæœ‰åšè¿‡å“ªäº›è°ƒä¼˜æˆ‘ä»¬é¡¹ç›®ä¸­æ•°æ®å€¾æ–œçš„åœºæ™¯å’Œè§£å†³æ–¹æ¡ˆé›¶åŸºç¡€â•å››ä¸ªæœˆç´§è·Ÿè‹¥æ³½å¤§æ•°æ®å­¦ä¹ ä¹‹åæ˜¯è¿™æ ·]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»Hiveä¸­çš„stored as file_foramtçœ‹hiveè°ƒä¼˜]]></title>
    <url>%2F2018%2F05%2F30%2F%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”1ã€å­˜å‚¨æ¯”è¾ƒè¡Œå¼æ•°æ®åº“å­˜å‚¨åœ¨hdfsä¸Šå¼æŒ‰è¡Œè¿›è¡Œå­˜å‚¨çš„ï¼Œä¸€ä¸ªblockå­˜å‚¨ä¸€æˆ–å¤šè¡Œæ•°æ®ã€‚è€Œåˆ—å¼æ•°æ®åº“åœ¨hdfsä¸Šåˆ™æ˜¯æŒ‰ç…§åˆ—è¿›è¡Œå­˜å‚¨ï¼Œä¸€ä¸ªblockå¯èƒ½æœ‰ä¸€åˆ—æˆ–å¤šåˆ—æ•°æ®ã€‚2ã€å‹ç¼©æ¯”è¾ƒå¯¹äºè¡Œå¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰è¡Œå‹ç¼©ï¼Œå½“ä¸€è¡Œä¸­æœ‰å¤šä¸ªå­—æ®µï¼Œå„ä¸ªå­—æ®µå¯¹åº”çš„æ•°æ®ç±»å‹å¯èƒ½ä¸ä¸€è‡´ï¼Œå‹ç¼©æ€§èƒ½å‹ç¼©æ¯”å°±æ¯”è¾ƒå·®ã€‚å¯¹äºåˆ—å¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰åˆ—å‹ç¼©ï¼Œæ¯ä¸€åˆ—å¯¹åº”çš„æ˜¯ç›¸åŒæ•°æ®ç±»å‹çš„æ•°æ®ï¼Œæ•…åˆ—å¼æ•°æ®åº“çš„å‹ç¼©æ€§èƒ½è¦å¼ºäºè¡Œå¼æ•°æ®åº“ã€‚3ã€æŸ¥è¯¢æ¯”è¾ƒå‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect id,name from table_emp;å¯¹äºè¡Œå¼æ•°æ®åº“ï¼Œå®ƒè¦éå†ä¸€æ•´å¼ è¡¨å°†æ¯ä¸€è¡Œä¸­çš„id,nameå­—æ®µæ‹¼æ¥å†å±•ç°å‡ºæ¥ï¼Œè¿™æ ·éœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°±æ¯”è¾ƒå¤§ï¼Œæ•ˆç‡ä½ã€‚å¯¹äºåˆ—å¼æ•°æ®åº“ï¼Œå®ƒåªéœ€æ‰¾åˆ°å¯¹åº”çš„id,nameå­—æ®µçš„åˆ—å±•ç°å‡ºæ¥å³å¯ï¼Œéœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°ï¼Œæ•ˆç‡é«˜ã€‚å‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect * from table_emp;å¯¹äºè¿™ç§æŸ¥è¯¢æ•´ä¸ªè¡¨å…¨éƒ¨ä¿¡æ¯çš„æ“ä½œï¼Œç”±äºåˆ—å¼æ•°æ®åº“éœ€è¦å°†åˆ†æ•£çš„è¡Œè¿›è¡Œé‡æ–°ç»„åˆï¼Œè¡Œå¼æ•°æ®åº“æ•ˆç‡å°±é«˜äºåˆ—å¼æ•°æ®åº“ã€‚ä½†æ˜¯ï¼Œåœ¨å¤§æ•°æ®é¢†åŸŸï¼Œè¿›è¡Œå…¨è¡¨æŸ¥è¯¢çš„åœºæ™¯å°‘ä¹‹åˆå°‘ï¼Œè¿›è€Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå¤šçš„è¿˜æ˜¯åˆ—å¼æ•°æ®åº“åŠåˆ—å¼å‚¨å­˜ã€‚äºŒã€stored as file_format è¯¦è§£1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored as file_formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚123456789101112131415161718192021CREATE TABLE tt (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;CREATE TABLE tt2 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;CREATE TABLE tt3 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;#ä»¥ä¸Šä¸‰ç§æ–¹å¼å­˜å‚¨çš„æ ¼å¼éƒ½æ˜¯TEXTFILEã€‚2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ12345678file_format: | SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameTEXTFILE: åªæ˜¯hiveä¸­è¡¨æ•°æ®é»˜è®¤çš„å­˜å‚¨æ ¼å¼ï¼Œå®ƒå°†æ‰€æœ‰ç±»å‹çš„æ•°æ®éƒ½å­˜å‚¨ä¸ºStringç±»å‹ï¼Œä¸ä¾¿äºæ•°æ®çš„è§£æï¼Œä½†å®ƒå´æ¯”è¾ƒé€šç”¨ã€‚ä¸å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒå‹ç¼©ã€‚SEQUENCEFILE: è¿™ç§å‚¨å­˜æ ¼å¼æ¯”TEXTFILEæ ¼å¼å¤šäº†å¤´éƒ¨ã€æ ‡è¯†ã€ä¿¡æ¯é•¿åº¦ç­‰ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä½¿å¾—å…¶å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒå‹ç¼©ï¼Œä½†å‹ç¼©çš„æ˜¯valueã€‚ï¼ˆå­˜å‚¨ç›¸åŒçš„æ•°æ®ï¼ŒSEQUENCEFILEæ¯”TEXTFILEç•¥å¤§ï¼‰RCFILEï¼ˆRecord Columnar Fileï¼‰: ç°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¾ˆå¤šä¸ªRow Group,æ¯ä¸ªRow Groupé»˜è®¤å¤§å°4MBï¼ŒRow Groupå†…éƒ¨å†æŒ‰åˆ—å­˜å‚¨ä¿¡æ¯ã€‚ç”±facebookå¼€æºï¼Œæ¯”æ ‡å‡†è¡Œå¼å­˜å‚¨èŠ‚çº¦10%çš„ç©ºé—´ã€‚ORC: ä¼˜åŒ–è¿‡åçš„RCFile,ç°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¤šä¸ªStripes,å†åœ¨Stripeä¸­æŒ‰åˆ—å­˜å‚¨ã€‚æ¯ä¸ªStripeç”±ä¸€ä¸ªIndex Dataã€ä¸€ä¸ªRow Dataã€ä¸€ä¸ªStripe Footerç»„æˆã€‚æ¯ä¸ªStripesçš„å¤§å°ä¸º250MBï¼Œæ¯ä¸ªIndex Dataè®°å½•çš„æ˜¯æ•´å‹æ•°æ®æœ€å¤§å€¼æœ€å°å€¼ã€å­—ç¬¦ä¸²æ•°æ®å‰åç¼€ä¿¡æ¯ï¼Œæ¯ä¸ªåˆ—çš„ä½ç½®ç­‰ç­‰è¯¸å¦‚æ­¤ç±»çš„ä¿¡æ¯ã€‚è¿™å°±ä½¿å¾—æŸ¥è¯¢ååˆ†å¾—é«˜æ•ˆï¼Œé»˜è®¤æ¯ä¸€ä¸‡è¡Œæ•°æ®å»ºç«‹ä¸€ä¸ªIndex Dataã€‚ORCå­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„40%å·¦å³ï¼Œä½¿ç”¨å‹ç¼©åˆ™å¯ä»¥è¿›ä¸€æ­¥å°†è¿™ä¸ªæ•°å­—é™åˆ°10%~20%ã€‚ORCè¿™ç§æ–‡ä»¶æ ¼å¼å¯ä»¥ä½œç”¨äºè¡¨æˆ–è€…è¡¨çš„åˆ†åŒºï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç§æ–¹å¼è¿›è¡ŒæŒ‡å®šï¼š123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=OrcThe parameters are all placed in the TBLPROPERTIES (see Create Table). They are:Key|Default|Notes|-|-|-|orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)|orc.compress.size|262,144|number of bytes in each compression chunk|orc.stripe.size|67,108,864|number of bytes in each stripe|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)|orc.create.index|true|whether to create row indexes|orc.bloom.filter.columns |â€â€| comma separated list of column names for which bloom filter should be created|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)ç¤ºä¾‹ï¼šåˆ›å»ºå¸¦å‹ç¼©çš„ORCå­˜å‚¨è¡¨1234567create table Addresses ( name string, street string, city string, state string, zip int) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);PARQUET: å­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„60%~70%ï¼Œå‹ç¼©ååœ¨20%~30%ä¹‹é—´ã€‚æ³¨æ„ï¼šä¸åŒçš„å­˜å‚¨æ ¼å¼ä¸ä»…è¡¨ç°åœ¨å­˜å‚¨ç©ºé—´ä¸Šçš„ä¸åŒï¼Œå¯¹äºæ•°æ®çš„æŸ¥è¯¢ï¼Œæ•ˆç‡ä¹Ÿä¸ä¸€æ ·ã€‚å› ä¸ºå¯¹äºä¸åŒçš„å­˜å‚¨æ ¼å¼ï¼Œæ‰§è¡Œç›¸åŒçš„æŸ¥è¯¢æ“ä½œï¼Œä»–ä»¬è®¿é—®çš„æ•°æ®é‡å¤§å°æ˜¯ä¸ä¸€æ ·çš„ã€‚å¦‚æœè¦ä½¿ç”¨TEXTFILEä½œä¸ºhiveè¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œåˆ™å¿…é¡»å…ˆå­˜åœ¨ä¸€å¼ ç›¸åŒæ•°æ®çš„å­˜å‚¨æ ¼å¼ä¸ºTEXTFILEçš„è¡¨table_t0,ç„¶ååœ¨å»ºè¡¨æ—¶ä½¿ç”¨â€œinsert into table table_stored_file_ORC select from table_t0;â€åˆ›å»ºã€‚æˆ–è€…ä½¿ç”¨â€create table as select from table_t0;â€åˆ›å»ºã€‚]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkä¹‹åºåˆ—åŒ–åœ¨ç”Ÿäº§ä¸­çš„åº”ç”¨]]></title>
    <url>%2F2018%2F05%2F29%2FSpark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[åºåˆ—åŒ–åœ¨åˆ†å¸ƒå¼åº”ç”¨çš„æ€§èƒ½ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ ¼å¼åŒ–å¯¹è±¡ç¼“æ…¢ï¼Œæˆ–è€…æ¶ˆè€—å¤§é‡çš„å­—èŠ‚æ ¼å¼åŒ–ï¼Œä¼šå¤§å¤§é™ä½è®¡ç®—æ€§èƒ½ã€‚åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåˆ›å»ºå¤§é‡çš„è‡ªå®šä¹‰å®ä½“å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡åœ¨ç½‘ç»œä¼ è¾“æ—¶éœ€è¦åºåˆ—åŒ–ï¼Œè€Œä¸€ç§å¥½çš„åºåˆ—åŒ–æ–¹å¼å¯ä»¥è®©æ•°æ®æœ‰æ›´å¥½çš„å‹ç¼©æ¯”ï¼Œä»è€Œæå‡ç½‘ç»œä¼ è¾“é€Ÿç‡ï¼Œæé«˜sparkä½œä¸šçš„è¿è¡Œé€Ÿåº¦ã€‚é€šå¸¸è¿™æ˜¯åœ¨sparkåº”ç”¨ä¸­ç¬¬ä¸€ä»¶éœ€è¦ä¼˜åŒ–çš„äº‹æƒ…ã€‚Sparkçš„ç›®æ ‡æ˜¯åœ¨ä¾¿åˆ©ä¸æ€§èƒ½ä¸­å–å¾—å¹³è¡¡ï¼Œæ‰€ä»¥æä¾›2ç§åºåˆ—åŒ–çš„é€‰æ‹©ã€‚Java serializationåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä¼šä½¿ç”¨Javaçš„ObjectOutputStreamæ¡†æ¶å¯¹å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä»»ä½•å®ç°java.io.Serializableçš„ç±»ä¸€èµ·å·¥ä½œã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æ‰©å±•java.io.Externalizableæ¥æ›´ç´§å¯†åœ°æ§åˆ¶åºåˆ—åŒ–çš„æ€§èƒ½ã€‚Javaåºåˆ—åŒ–æ˜¯çµæ´»çš„ï¼Œä½†é€šå¸¸ç›¸å½“æ…¢ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´è®¸å¤šç±»çš„å¤§å‹åºåˆ—åŒ–æ ¼å¼ã€‚æµ‹è¯•ä»£ç ï¼šæµ‹è¯•ç»“æœï¼šKryo serializationSparkè¿˜å¯ä»¥ä½¿ç”¨Kryoåº“ï¼ˆç‰ˆæœ¬2ï¼‰æ¥æ›´å¿«åœ°åºåˆ—åŒ–å¯¹è±¡ã€‚Kryoæ¯”Javaä¸²è¡ŒåŒ–ï¼ˆé€šå¸¸å¤šè¾¾10å€ï¼‰è¦å¿«å¾—å¤šï¼Œä¹Ÿæ›´ç´§å‡‘ï¼Œä½†æ˜¯ä¸æ”¯æŒæ‰€æœ‰å¯ä¸²è¡ŒåŒ–ç±»å‹ï¼Œå¹¶ä¸”è¦æ±‚æ‚¨æå‰æ³¨å†Œæ‚¨å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„ç±»ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚æµ‹è¯•ä»£ç ï¼šæµ‹è¯•ç»“æœï¼šæµ‹è¯•ç»“æœä¸­å‘ç°ï¼Œä½¿ç”¨ Kryo serialization çš„åºåˆ—åŒ–å¯¹è±¡ æ¯”ä½¿ç”¨ Java serializationçš„åºåˆ—åŒ–å¯¹è±¡è¦å¤§ï¼Œä¸æè¿°çš„ä¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼ŸæŸ¥æ‰¾å®˜ç½‘ï¼Œå‘ç°è¿™ä¹ˆä¸€å¥è¯ Finally, if you donâ€™t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.ã€‚ä¿®æ”¹ä»£ç ååœ¨æµ‹è¯•ä¸€æ¬¡ã€‚æµ‹è¯•ç»“æœï¼šæ€»ç»“ï¼šKryo serialization æ€§èƒ½å’Œåºåˆ—åŒ–å¤§å°éƒ½æ¯”é»˜è®¤æä¾›çš„ Java serialization è¦å¥½ï¼Œä½†æ˜¯ä½¿ç”¨Kryoéœ€è¦å°†è‡ªå®šä¹‰çš„ç±»å…ˆæ³¨å†Œè¿›å»ï¼Œä½¿ç”¨èµ·æ¥æ¯”Java serializationéº»çƒ¦ã€‚è‡ªä»Spark 2.0.0ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ç®€å•ç±»å‹ã€ç®€å•ç±»å‹æ•°ç»„æˆ–å­—ç¬¦ä¸²ç±»å‹çš„ç®€å•ç±»å‹æ¥è°ƒæ•´RDDsæ—¶ï¼Œåœ¨å†…éƒ¨ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨ã€‚é€šè¿‡æŸ¥æ‰¾sparkcontextåˆå§‹åŒ–çš„æºç ï¼Œå¯ä»¥å‘ç°æŸäº›ç±»å‹å·²ç»åœ¨sparkcontextåˆå§‹åŒ–çš„æ—¶å€™è¢«æ³¨å†Œè¿›å»ã€‚]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®å¸¦ä½ éšæ—¶äº†è§£ä¸šç•Œé¢è¯•é¢˜ï¼Œéšæ—¶è·³é«˜è–ª]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA%2F</url>
    <content type="text"><![CDATA[é“¾å®¶(ä¸€é¢ï¼ŒäºŒé¢)0.è‡ªæˆ‘ä»‹ç»1.å°è£…ç»§æ‰¿å¤šæ€æ¦‚å¿µ2.mvcè®¾è®¡æ€æƒ³3.çº¿ç¨‹æ± ,çœ‹è¿‡æºç å—4.sshæ¡†æ¶ä¸­åˆ†åˆ«å¯¹åº”mvcä¸­é‚£ä¸€å±‚5.shellå‘½ä»¤ï¼ˆæŸ¥è¯¢ä¸€ä¸ªæ–‡ä»¶æœ‰å¤šå°‘è¡Œã€‚ chown ä¿®æ”¹æ–‡ä»¶æƒé™ï¼Œ åªè®°å¾—é‚£ä¹ˆå¤šäº† ï¼‰6.spring ioc aop åŸç†7.å•åˆ©æ¨¡å¼8.SQLé¢˜ï¼Œæƒ³ä¸èµ·æ¥äº†ã€‚ã€‚9.jvm è¿è¡Œæ—¶æ•°æ®åŒºåŸŸ10.spring mvcçŸ¥é“å—ã€‚ã€‚11.å·¥å‚æ¨¡å¼12.mr è®¡ç®—æµç¨‹13.hiveæŸ¥è¯¢è¯­å¥ï¼ˆè¡¨1ï¼šæ—¶é—´ é£Ÿå ‚æ¶ˆè´¹ è¡¨äºŒï¼šå„ä¸ªæ—¶é—´æ®µ ç”¨æˆ· æ¯ä¸ªé£Ÿå ‚æ¶ˆè´¹ æŸ¥è¯¢ç”¨æˆ·åœ¨æ¯ä¸ªæ—¶é—´å‡ºç°åœ¨é‚£ä¸ªé£Ÿå ‚ç»Ÿè®¡æ¶ˆè´¹è®°å½• ï¼Œå¤§æ¦‚æ˜¯è¿™æ ·çš„ã€‚ã€‚ï¼‰14.gitçš„ä½¿ç”¨15.hadoopçš„ç†è§£16.hiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„åŒºåˆ«17.hiveå­˜å‚¨æ ¼å¼å’Œå‹ç¼©æ ¼å¼18.å¯¹sparkäº†è§£å—ï¼Ÿ å½“æ—¶é«˜çº§ç­è¿˜æ²¡å­¦ã€‚ã€‚19.hiveäºå…³ç³»å‹æ•°æ®åº“çš„åŒºåˆ«20.å„ç§æ’åº æ‰‹å†™å †æ’åº,è¯´è¯´åŸç†21.é“¾è¡¨é—®é¢˜ï¼Œæµè§ˆå™¨è®¿é—®è®°å½•ï¼Œå‰è¿›åé€€å½¢æˆé“¾è¡¨ï¼Œæ–°åŠ ä¸€ä¸ªè®°å½•ï¼Œå¤šå‡ºä¸€ä¸ªåˆ†æ”¯ï¼Œåˆ é™¤ä»¥å‰çš„åˆ†æ”¯ã€‚è®¾è®¡ç»“æ„ï¼Œå¦‚æœè¿™ä¸ªç»“æ„å†™åœ¨å‡½æ•°ä¸­æ€ä¹ˆç»´æŠ¤ã€‚22ä¸­é—´ä¹Ÿç©¿æ’äº†é¡¹ç›®ã€‚æ— è®ºæ˜¯å·²ç»æ‰¾åˆ°å·¥ä½œçš„è¿˜æ˜¯æ­£åœ¨å·¥ä½œçš„ï¼Œæˆ‘çš„è§‰çš„é¢è¯•é¢˜éƒ½å¯ä»¥ç»™æ‚¨ä»¬å¸¦æ¥ä¸€äº›å¯å‘ã€‚å¯ä»¥äº†è§£å¤§æ•°æ®è¡Œä¸šéœ€è¦ä»€ä¹ˆæ ·çš„äººæ‰ï¼Œä»€ä¹ˆæŠ€èƒ½ï¼Œå¯¹åº”å»è¡¥å……è‡ªå·±çš„ä¸è¶³ä¹‹å¤„ï¼Œä¸ºä¸‹ä¸€ä¸ªé«˜è–ªå·¥ä½œåšå‡†å¤‡ã€‚è‹¥æ³½å¤§æ•°æ®åé¢ä¼šéšæ—¶æ›´æ–°å­¦å‘˜é¢è¯•é¢˜ï¼Œè®©å¤§å®¶äº†è§£å¤§æ•°æ®è¡Œä¸šçš„å‘å±•è¶‹åŠ¿ï¼Œæ—¨åœ¨å¸®åŠ©æ­£åœ¨è‰°è¾›æ‰“æ‹¼çš„æ‚¨æŒ‡å‡ºä¸€æ¡åŒºç›´çš„æœªæ¥ä¹‹è·¯ï¼ï¼ˆå°‘èµ°å¼¯è·¯å™¢å™¢ã€‚ã€‚ï¼‰]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸€æ¬¡è·³æ§½ç»å†ï¼ˆé˜¿é‡Œ/ç¾å›¢/å¤´æ¡/ç½‘æ˜“/æœ‰èµ...)]]></title>
    <url>%2F2018%2F05%2F24%2F%E6%9C%89%E8%B5%9E...)%2F</url>
    <content type="text"><![CDATA[ä¸ºå•¥è·³æ§½æ¯æ¬¡è¯´å› ä¸ºç”Ÿæ´»æˆæœ¬çš„æ—¶å€™é¢è¯•å®˜éƒ½ä¼šå¾ˆæƒŠå¥‡ï¼Œéš¾é“æœ‰æˆ‘ä»¬è¿™é‡Œè´µï¼Ÿå¥½æƒ³ç›´æ¥ç»™å‡ºä¸‹é¢è¿™å¼ å›¾ï¼Œå¦é—¨çš„æˆ¿ä»·çœŸçš„å¥½è´µå¥½è´µå¥½è´µã€‚ã€‚ã€‚é¢è¯•è¿‡ç¨‹ï¼ˆå…ˆæ‰“ä¸ªå¹¿å‘Šï¼Œæœ‰å…´è¶£åŠ å…¥é˜¿é‡Œçš„æ¬¢è¿å‘ç®€å†è‡³ zhangzb2007@gmail.comï¼Œæˆ–ç®€ä¹¦ä¸Šç»™æˆ‘å‘ä¿¡æ¯ï¼‰é¢çš„æ˜¯Javaå²—ï¼Œæ€»å…±é¢äº†7å®¶å…¬å¸ï¼Œé€šè¿‡äº†6å®¶ã€‚æŒ‰è‡ªå·±çš„ä¿¡å¿ƒæå‡åº¦æˆ‘æŠŠé¢è¯•è¿‡ç¨‹åˆ†ä¸ºä¸ŠåŠåœºå’Œä¸‹åŠåœºã€‚ä¸ŠåŠåœºæ›¹æ“ä¸“è½¦è¿™æ˜¯å‰åˆ©é›†å›¢ä¸‹å±å­å…¬å¸ï¼Œå·²ç»æ˜¯ä¸€å®¶ç‹¬è§’å…½ã€‚ä¸€é¢ä¸­è§„ä¸­çŸ©ï¼Œæ²¡å•¥ç‰¹åˆ«çš„ã€‚äºŒé¢å¥½åƒæ˜¯ä¸ªä¸»ç®¡ï¼Œéš”äº†å¥½å‡ å¤©ï¼ŒåŸºæœ¬æ²¡é—®æŠ€æœ¯é—®é¢˜ï¼Œåè€Œæ˜¯é—®èŒä¸šè§„åˆ’ï¼Œå¯¹åŠ ç­æœ‰å•¥çœ‹æ³•ï¼Œæœ‰ç‚¹æªæ‰‹ä¸åŠï¼Œæ„Ÿè§‰å›ç­”çš„ä¸å¥½ã€‚ä½†æ˜¯è¿‡å‡ å¤©è¿˜æ˜¯æ”¶åˆ°HRçš„ç°åœºé¢è¯•é€šçŸ¥ã€‚ç°åœºæ˜¯æŠ€æœ¯é¢åŠ HRé¢ï¼ŒæŠ€æœ¯é¢è¢«é—®äº†å‡ ä¸ªé—®é¢˜æœ‰ç‚¹æ‡µé€¼ï¼ša. zookeeperçš„watcherä¹è§‚é”æ€ä¹ˆå®ç° b. ä¸€ä¸ªé¡¹ç›®çš„æ•´ä¸ªæµç¨‹ c. è¯´å‡ºä¸€ä¸ªç©ºé—´æ¢æ—¶é—´çš„åœºæ™¯ d. centos7çš„å†…å­˜åˆ†é…æ–¹å¼å’Œ6æœ‰å•¥ä¸åŒ f. ä½ å¯¹å…¬å¸æœ‰ä»€ä¹ˆä»·å€¼ã€‚HRè·Ÿæˆ‘è¯´èŠ‚åï¼ˆé‚£ä¼šå†è¿‡ä¸¤å¤©å°±æ˜¯æ¸…æ˜ï¼‰ä¼šç»™æˆ‘æ¶ˆæ¯ï¼Œç»“æœè¿‡äº†åŠä¸ªæœˆçªç„¶æ¥åˆ°ä»–ä»¬çš„ç”µè¯ï¼Œè¯´æˆ‘é€šè¿‡äº†ï¼Œç»™æˆ‘è®²äº†ä»–ä»¬çš„è–ªèµ„æ–¹æ¡ˆï¼Œæ²¡å¤ªå¤§å¸å¼•åŠ›ï¼Œå†åŠ ä¸Šè¿™ç§è«åå…¶å¦™çš„æ—¶é—´ç­‰å¾…ï¼Œç›´æ¥æ‹’äº†ã€‚ç¾äºšæŸç§‘ä¼°è®¡å¾ˆå¤šäººæ²¡å¬è¯´è¿‡è¿™å®¶å…¬å¸ï¼Œè¿™æ˜¯ä¸€å®¶å¦é—¨æœ¬åœŸå…¬å¸ï¼Œåšæ”¿åºœå®‰é˜²é¡¹ç›®çš„ï¼Œåœ¨å¦é—¨ä¹Ÿè¿˜æ˜¯å°æœ‰åæ°”ã€‚ä½†æ˜¯é¢è¯•å®Œç›´æ¥é¢ è¦†äº†æˆ‘å¯¹è¿™å®¶å…¬å¸çš„è®¤çŸ¥ã€‚è¿›é—¨æœ€æ˜¾çœ¼çš„åœ°æ–¹æ˜¯å…šæ´»åŠ¨å®¤ï¼Œåœ¨ç­‰é¢è¯•å®˜çš„ä¸€å°æ®µæ—¶é—´é‡Œæœ‰å¥½å‡ æ‹¨äººåˆ°é‡Œé¢å‚è§‚ã€‚é¢è¯•å‰åšäº†ä¸€ä»½ç¬”è¯•é¢˜ï¼ŒåŸºæœ¬éƒ½æ˜¯web/æ•°æ®åº“æ–¹é¢çš„ã€‚ç¬¬ä¸€é¢ç®€å•é—®äº†å‡ ä¸ªredisçš„é—®é¢˜ä¹‹åé¢è¯•å®˜ä»‹ç»äº†ä»–ä»¬çš„é¡¹ç›®ï¼Œä»–ä»¬éƒ½æ˜¯åšCå’ŒC++çš„ï¼Œæƒ³æ‰¾ä¸€ä¸ªäººæ­ä¸€å¥—å¤§æ•°æ®é›†ç¾¤ï¼Œå¤„ç†ä»–ä»¬æ¯å¤©å‡ ç™¾Gçš„æ•°æ®ï¼Œç„¶åæœåŠ¡å™¨å…¨éƒ¨æ˜¯windowsï¼äºŒé¢æ˜¯å¦ä¸€ä¸ªéƒ¨é—¨çš„ï¼Œå°è±¡ä¸­å°±é—®äº†kafkaä¸ºä»€ä¹ˆæ€§èƒ½è¿™ä¹ˆå¥½ï¼Œç„¶åå°±å¼€å§‹é—®ä¹°æˆ¿äº†æ²¡æœ‰ï¼Œç»“å©šäº†æ²¡æœ‰ï¼Œä»–å¯¹æˆ‘ç°åœ¨çš„å…¬å¸æ¯”è¾ƒäº†è§£ï¼Œåˆæ‰¯äº†æŒºä¹…ã€‚ä¸‰é¢åº”è¯¥æ˜¯ä¸ªéƒ¨é—¨è€å¤§äº†ï¼Œæ²¡æœ‰é—®æŠ€æœ¯é—®é¢˜ï¼Œä¹Ÿæ˜¯é—®ä¹°æˆ¿äº†æ²¡ï¼Œç»“å©šæ²¡ï¼Œé—®å„ç§ç”Ÿæ´»é—®é¢˜ï¼Œæœ‰ç‚¹åƒäººå£æ™®æŸ¥ã€‚æˆ‘æœ‰ç‚¹å¥½å¥‡ï¼Œé—®ä»–ä»¬ä¸ºå•¥è¿™ä¹ˆå…³å¿ƒè¿™äº›é—®é¢˜ï¼Œä»–ç›´æ¥è¯´ä»–ä»¬æ›´å¼ºè°ƒå‘˜å·¥çš„ç¨³å®šæ€§ï¼Œé¡¹ç›®æ¯”è¾ƒç®€å•ï¼Œèƒ½åŠ›ä¸ç”¨è¦æ±‚å¤ªé«˜ï¼Œä¸è¦å¤ªå·®å°±è¡Œã€‚æ±—ï¼Œç›´æ¥æ‹’äº†ã€‚æœ‰èµç»å¯¹æ¨èçš„ä¸€å®¶å…¬å¸ï¼Œæ•ˆç‡è¶…é«˜ã€‚ä¸­åˆæ‰¾äº†ä¸€ä¸ªç½‘å‹å¸®å¿™å†…æ¨ï¼Œæ™šä¸Šå°±å¼€å§‹ä¸€é¢ï¼Œç¬¬äºŒå¤©æ—©ä¸ŠäºŒé¢ï¼Œç¬¬ä¸‰å¤©HRå°±çº¦ç°åœºé¢è¯•æ—¶é—´ï¼Œå¿«çš„è¶…ä¹æƒ³è±¡ã€‚ç°åœºé¢ä¹Ÿæ˜¯å…ˆä¸€ä¸ªæŠ€æœ¯é¢ï¼Œæœ€åæ‰HRé¢ã€‚é¢è¯•çš„æ•´ä½“éš¾åº¦ä¸­ç­‰ã€‚ç°åœ¨å°±è®°å¾—å‡ ä¸ªé—®é¢˜ï¼šG1å’ŒCMSçš„åŒºåˆ«ï¼ŒG1æœ‰å•¥åŠ£åŠ¿ï¼›Kafkaçš„æ•´ä½“æ¶æ„ï¼›Nettyçš„ä¸€æ¬¡è¯·æ±‚è¿‡ç¨‹ï¼›è‡ªæ—‹é”/åå‘é”/è½»é‡çº§é”ï¼ˆè¿™ä¸ªé—®é¢˜åœ¨å¤´æ¡çš„é¢è¯•é‡Œä¹Ÿå‡ºç°äº†ä¸€æ¬¡ï¼‰ã€hbaseçº¿ä¸Šé—®é¢˜æ’æŸ¥ï¼ˆåˆšå¥½é‡åˆ°è¿‡NUMAæ¶æ„ä¸‹çš„ä¸€ä¸ªé—®é¢˜ï¼Œå€Ÿæ­¤æŠŠhbaseçš„å†…æ ¸ä»‹ç»äº†ä¸‹ï¼‰ã€‚è¿™é‡Œä¸å¾—ä¸è¯´ä¸‹æœ‰èµçš„äººï¼ŒçœŸçš„å¾ˆèµã€‚ç»ˆé¢çš„é¢è¯•å®˜æ˜¯ä¸€ä¸ªç ”å‘å›¢é˜Ÿçš„è´Ÿè´£äººï¼Œå…¨ç¨‹ä¸€ç›´å¾®ç¬‘ï¼Œä¸­é—´ç”µè¯å“äº†ä¸€æ¬¡ï¼Œä¸€ç›´è·Ÿæˆ‘é“æ­‰ã€‚é¢å®Œä¹‹åè¿˜æä¾›äº†å›¢é˜Ÿçš„ä¸‰ä¸ªç ”å‘æ–¹å‘è®©æˆ‘è‡ªå·±é€‰æ‹©ã€‚åé¢çœ‹ä»–çš„æœ‹å‹åœˆçŠ¶æ€ï¼Œä»–é‚£å¤©é«˜çƒ§ï¼Œé¢å®Œæˆ‘å°±å»æ‰“ç‚¹æ»´äº†ï¼Œä½†æ˜¯æ•´ä¸ªè¿‡ç¨‹å®Œå…¨çœ‹ä¸å‡ºæ¥ã€‚å¸®æˆ‘å†…æ¨çš„ç½‘å‹æ˜¯åœ¨å¾®ä¿¡ç¾¤é‡Œæ‰¾åˆ°çš„ï¼ŒçŸ¥é“æˆ‘è¿‡äº†ä¹‹åä¸»åŠ¨æ‰¾æˆ‘ï¼Œè®©æˆ‘è¿‡å»æ­å·æœ‰å•¥é—®é¢˜éšæ—¶æ‰¾ä»–ã€‚è™½ç„¶æœ€ç»ˆæ²¡æœ‰å»ï¼Œä½†è¿˜æ˜¯å¯ä»¥æ˜æ˜¾æ„Ÿå—åˆ°ä»–ä»¬çš„çƒ­æƒ…ã€‚å­—èŠ‚è·³åŠ¨(ä»Šæ—¥å¤´æ¡)HRç¾çœ‰æ‰“ç”µè¯è¿‡æ¥è¯´æ˜¯å­—èŠ‚è·³åŠ¨å…¬å¸ï¼Œæƒ³çº¦ä¸‹è§†é¢‘é¢è¯•æ—¶é—´ã€‚é‚£ä¼šæ˜¯æœ‰ç‚¹æ‡µçš„ï¼Œæˆ‘åªçŸ¥é“ä»Šæ—¥å¤´æ¡å’ŒæŠ–éŸ³ã€‚åé¢æƒ³åˆ°åŒ—äº¬çš„å·ç æ‰æƒ³èµ·æ¥ã€‚å¤´æ¡å¯ä»¥è¯´æ˜¯è¿™æ¬¡æ‰€æœ‰é¢è¯•é‡Œæµç¨‹æœ€è§„èŒƒçš„ï¼Œæ”¶åˆ°ç®€å†åæœ‰é‚®ä»¶é€šçŸ¥ï¼Œé¢„çº¦é¢è¯•æ—¶é—´åé‚®ä»¶çŸ­ä¿¡é€šçŸ¥ï¼Œé¢è¯•å®Œåä¸è¶…è¿‡ä¸€å¤©é€šçŸ¥é¢è¯•ç»“æœï¼Œæ¯æ¬¡é¢è¯•æœ‰é¢è¯•åé¦ˆã€‚è¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒç‰¹åˆ«çš„ï¼Œå¤§éƒ¨åˆ†å…¬å¸çš„ç”µè¯æˆ–è€…è§†é¢‘é¢è¯•åŸºæœ¬æ˜¯ä¸‹ç­åï¼Œå¤´æ¡éƒ½æ˜¯ä¸Šç­æ—¶é—´ï¼Œè¿˜ä¸ç»™çº¦ä¸‹ç­æ—¶é—´ï¼ˆéš¾é“ä»–ä»¬ä¸åŠ ç­ï¼Ÿï¼‰ã€‚ä¸€é¢é¢è¯•å®˜åˆšä¸Šæ¥å°±è¯´ä»–ä»¬æ˜¯åšgoçš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰å…´è¶£ï¼Œä»–è‡ªå·±ä¹Ÿæ˜¯Javaè½¬çš„ã€‚æˆ‘è¯´æ²¡é—®é¢˜ï¼Œä»–å…ˆé—®äº†ä¸€äº›JavaåŸºç¡€é—®é¢˜ï¼Œç„¶åæœ‰ä¸€é“ç¼–ç¨‹é¢˜ï¼Œæ±‚ä¸€æ£µæ ‘ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€è¿‘çš„å…¬å…±çˆ¶èŠ‚ç‚¹ã€‚æ€è·¯åŸºæœ¬æ˜¯å¯¹çš„ï¼Œä½†æ˜¯æœ‰äº›ç»†èŠ‚æœ‰é—®é¢˜ï¼Œé¢è¯•å®˜äººå¾ˆå¥½ï¼Œè¾¹çœ‹è¾¹è·Ÿæˆ‘è®¨è®ºï¼Œæˆ‘è¾¹æ”¹è¿›ï¼Œå‰å‰ååä¼°è®¡ç”¨æ¥å¿«åŠå°æ—¶ã€‚ç„¶ååˆç»§ç»­é—®é—®é¢˜ï¼ŒHTTP 301 302æœ‰å•¥åŒºåˆ«ï¼Ÿè®¾è®¡ä¸€ä¸ªçŸ­é“¾æ¥ç®—æ³•ï¼›md5é•¿åº¦æ˜¯å¤šå°‘ï¼Ÿæ•´ä¸ªé¢è¯•è¿‡ç¨‹ä¸€ä¸ªå¤šå°æ—¶ï¼Œè‡ªæˆ‘æ„Ÿè§‰ä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä»¥ä¸ºè¿™æ¬¡åº”è¯¥æŒ‚äº†ï¼Œç»“æœæ™šä¸Šæ”¶åˆ°é¢è¯•é€šè¿‡çš„é€šçŸ¥ã€‚äºŒé¢æ˜¯åœ¨ä¸€ä¸ªä¸Šåˆè¿›è¡Œçš„ï¼Œæˆ‘ä»¥ä¸ºzoomè§†é¢‘ç³»ç»Ÿä¼šè‡ªåŠ¨è¿ä¸Šï¼ˆä¸€é¢å°±æ˜¯è‡ªåŠ¨è¿ä¸Šï¼‰ï¼Œå°±åœ¨é‚£è¾¹ç­‰ï¼Œè¿‡äº†5åˆ†é’Ÿè¿˜æ˜¯ä¸è¡Œï¼Œæˆ‘å°±è”ç³»HRï¼ŒåŸæ¥è¦æ”¹idï¼Œç»ˆäºè¿ä¸Šåé¢è¯•å®˜çš„è¡¨æƒ…ä¸æ˜¯å¾ˆå¥½çœ‹ï¼Œæœ‰ç‚¹ä¸è€çƒ¦çš„æ ·å­ï¼Œä¸æ‡‚æ˜¯ä¸æ˜¯å› ä¸ºæˆ‘è€½è¯¯äº†å‡ åˆ†é’Ÿï¼Œè¿™ç§è¡¨æƒ…å»¶ç»­äº†æ•´ä¸ªé¢è¯•è¿‡ç¨‹ï¼Œå…¨ç¨‹æœ‰ç‚¹å‹æŠ‘ã€‚é—®çš„é—®é¢˜å¤§éƒ¨åˆ†å¿˜äº†ï¼Œåªè®°å¾—é—®äº†ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„é—®é¢˜ï¼ŒThreadLocalå¦‚æœå¼•ç”¨ä¸€ä¸ªstaticå˜é‡æ˜¯ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Ÿé—®ç€é—®ç€çªç„¶è¯´ä»Šå¤©é¢è¯•åˆ°æ­¤ä¸ºæ­¢ï¼Œä¸€çœ‹æ—¶é—´æ‰è¿‡å»äºŒåå‡ åˆ†é’Ÿã€‚ç¬¬äºŒå¤©å°±æ”¶åˆ°é¢è¯•æ²¡è¿‡çš„é€šçŸ¥ï¼Œæ„Ÿè§‰è‡ªå·±äºŒé¢ç­”çš„æ¯”ä¸€é¢å¥½å¤šäº†ï¼Œå®åœ¨æƒ³ä¸é€šã€‚ä¸‹åŠåœºä¸€ç›´æ„Ÿè§‰è‡ªå·±å¤ªæ°´äº†ï¼Œä»£ç é‡ä¸å¤§ï¼Œä¸‰å¹´åŠçš„ITç»éªŒè¿˜æœ‰ä¸€å¹´å»åšäº†äº§å“ï¼Œéƒ½ä¸æ•¢æŠ•å¤§å‚ã€‚ä¸ŠåŠåœºçš„æŠ€æœ¯é¢åŸºæœ¬è¿‡äº†ä¹‹åè‡ªä¿¡å¿ƒå¤§å¤§æå‡ï¼Œå¼€å§‹æŒ‘æˆ˜æ›´é«˜éš¾åº¦çš„ã€‚ç¾å›¢è¿™ä¸ªæ˜¯å¦é—¨ç¾å›¢ï¼Œä»–ä»¬åœ¨è¿™è¾¹åšäº†ä¸€ä¸ªå«æ¦›æœæ°‘å®¿çš„APPï¼ŒåŠå…¬åœ°ç‚¹åœ¨JFCé«˜æ¡£å†™å­—æ¥¼ï¼Œä¼‘æ¯åŒºå¯ä»¥é¢æœå¤§æµ·ï¼Œç¯å¢ƒæ˜¯å¾ˆä¸é”™ï¼Œé¢è¯•å°±æœ‰ç‚¹è™å¿ƒäº†ã€‚ä¸¤ç‚¹åŠè¿›å»ã€‚ä¸€é¢ã€‚æˆ‘çš„ç®€å†å¤§éƒ¨åˆ†æ˜¯å¤§æ•°æ®ç›¸å…³çš„ï¼Œä»–ä¸æ˜¯å¾ˆäº†è§£ï¼Œé—®äº†ä¸€äº›åŸºç¡€é—®é¢˜å’Œnettyçš„å†™æµç¨‹ï¼Œè¿˜é—®äº†ä¸€ä¸ªredisæ•°æ®ç»“æ„çš„å®ç°ï¼Œç»“æ„ä»–é—®äº†é‡Œé¢å­—ç¬¦ä¸²æ˜¯æ€ä¹ˆå®ç°çš„ï¼Œæœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚ä¸€ç›´æ„Ÿè§‰è¿™ä¸ªå¤ªç®€å•ï¼Œæ²¡å¥½å¥½çœ‹ï¼Œåªè®°å¾—æœ‰æ ‡è®°é•¿åº¦ï¼Œå¯ä»¥ç›´æ¥å–ã€‚ç„¶åå°±æ¥ä¸¤é“ç¼–ç¨‹é¢˜ã€‚ç¬¬ä¸€é¢˜æ˜¯æ±‚ä¸€æ£µæ ‘æ‰€æœ‰å·¦å¶å­èŠ‚ç‚¹çš„å’Œï¼Œæ¯”è¾ƒç®€å•ï¼Œä¸€ä¸ªæ·±åº¦ä¼˜å…ˆå°±å¯ä»¥æå®šã€‚ç¬¬äºŒé¢˜æ˜¯ç»™å®šä¸€ä¸ªå€¼Kï¼Œä¸€ä¸ªæ•°åˆ—ï¼Œæ±‚æ•°åˆ—ä¸­ä¸¤ä¸ªå€¼aå’Œbï¼Œä½¿å¾—a+b=kã€‚æˆ‘æƒ³åˆ°äº†ä¸€ä¸ªä½¿ç”¨æ•°ç»„ä¸‹æ ‡çš„æ–¹æ³•ï¼ˆæ„Ÿè§‰æ˜¯åœ¨å“ªé‡Œæœ‰è§è¿‡ï¼Œä¸ç„¶ä¼°è®¡æ˜¯æƒ³ä¸å‡ºæ¥ï¼‰ï¼Œè¿™ç§å¯æ˜¯è¾¾åˆ°O(n)çš„å¤æ‚åº¦ï¼›ä»–åˆåŠ äº†ä¸ªé™åˆ¶æ¡ä»¶ï¼Œä¸èƒ½ä½¿ç”¨æ›´å¤šå†…å­˜ï¼Œæˆ‘æƒ³åˆ°äº†å¿«æ’+éå†ï¼Œä»–é—®æœ‰æ²¡æœ‰æ›´ä¼˜çš„ï¼Œå®åœ¨æƒ³ä¸å‡ºæ¥ï¼Œä»–æäº†ä¸€ä¸ªå¯ä»¥ä¸¤ç«¯é€¼è¿‘ï¼Œæ„Ÿè§‰å¾ˆå·§å¦™ã€‚äºŒé¢ã€‚é¢è¯•å®˜é«˜é«˜ç˜¦ç˜¦çš„ï¼Œæˆ‘å¯¹è¿™ç§äººçš„å°è±¡éƒ½æ˜¯è‚¯å®šå¾ˆç‰›é€¼ï¼Œå¯èƒ½æ˜¯æºäºå¤§å­¦æ—¶ä»£é‚£äº›å¤§ç‰›éƒ½é•¿è¿™æ ·ã€‚å…ˆè®©æˆ‘è®²ä¸‹kafkaçš„ç»“æ„ï¼Œç„¶åæ€ä¹ˆé˜²æ­¢è®¢å•é‡å¤æäº¤ï¼Œç„¶åå¼€å§‹å›´ç»•ç¼“å­˜åŒæ­¥é—®é¢˜å±•å¼€äº†é•¿è¾¾åŠå°æ—¶çš„è®¨è®ºï¼šå…ˆå†™æ•°æ®åº“ï¼Œå†å†™ç¼“å­˜æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå…ˆå†™ç¼“å­˜å†å†™æ•°æ®åº“æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå†™åº“æˆåŠŸç¼“å­˜æ›´æ–°å¤±è´¥æ€ä¹ˆåŠï¼Ÿç¼“å­˜æ›´æ–°æˆåŠŸå†™åº“å¤±è´¥æ€ä¹ˆåŠï¼Ÿä»–å’Œæˆ‘ä¸€èµ·åœ¨ä¸€å¼ çº¸ä¸Šå„ç§ç”»ï¼Œæ„Ÿè§‰ä¸æ˜¯é¢è¯•ï¼Œè€Œæ˜¯åœ¨è®¾è®¡æ–¹æ¡ˆã€‚ä¸‰é¢ã€‚è¿™æ˜¯åç«¯å›¢é˜Ÿè´Ÿè´£äººäº†ï¼Œå¾ˆå’Œè”¼ï¼Œä¸€ç›´ç¬‘å‘µå‘µã€‚é—®äº†æˆ‘ä¸€äº›å¾®æœåŠ¡çš„é—®é¢˜ï¼Œæˆ‘æåˆ°äº†istioï¼Œä»‹ç»äº†è®¾è®¡ç†å¿µï¼Œæ„Ÿè§‰ä»–æœ‰ç‚¹æ„å¤–ã€‚ç„¶åä»–é—®java8çš„æ–°ç‰¹æ€§ï¼Œé—®æˆ‘çŸ¥ä¸çŸ¥é“lambdaè¡¨è¾¾å¼æ€ä¹ˆæ¥çš„ï¼Œæˆ‘ä»lambdaæ¼”ç®—è¯´åˆ°lispè¯´åˆ°scalaï¼Œæ„Ÿè§‰ä»–æ›´æ„å¤–ã€‚æ­¤å¤„æœ‰ç‚¹å¹ç‰›äº†ã€‚æˆ‘é—®äº†ä¸€äº›å›¢é˜Ÿçš„é—®é¢˜ï¼Œé¡¹ç›®æœªæ¥è§„åˆ’ç­‰ï¼Œæ„Ÿè§‰æ¦›æœè¿˜æ˜¯æŒºä¸é”™çš„ã€‚å››é¢ã€‚è¿™ä¸ªåº”è¯¥æ˜¯æ¦›æœå¦é—¨çš„è´Ÿè´£äººäº†ï¼ŒæŠ€æœ¯é—®é¢˜é—®çš„ä¸å¤šï¼Œæ›´å¤šæ˜¯ä¸€äº›èŒä¸šè§„åˆ’ï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ç­‰ã€‚é¢è¯•ç»“æŸçš„æ—¶å€™ä»–å…ˆå‡ºå»ï¼Œæˆ‘æ”¶æ‹¾ä¸‹ä¸œè¥¿ï¼Œå‡ºå»çš„æ—¶å€™å‘ç°ä»–åœ¨ç”µæ¢¯æ—å¸®æˆ‘å¼€ç”µæ¢¯ï¼Œå¯¹å¾…é¢è¯•è€…çš„è¿™ç§æ€åº¦å®åœ¨è®©äººå¾ˆæœ‰å¥½æ„Ÿã€‚å‡ºæ¥çš„æ—¶å€™å·²ç»æ˜¯å…­ç‚¹åŠã€‚ç½‘æ˜“é¢çš„æ˜¯ç½‘æ˜“äº‘éŸ³ä¹ï¼Œå¹³æ—¶ç»å¸¸ç”¨ï¼Œæ„Ÿè§‰å¦‚æœå¯ä»¥å‚ä¸ç ”å‘åº”è¯¥æ˜¯ç§æŒºç¾å¦™çš„æ„Ÿè§‰ã€‚ä¸€é¢ã€‚ä¸‹åˆæ‰“è¿‡æ¥çš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç©ºï¼Œæˆ‘è¯´æœ‰ï¼Œä»–è¯´ä½ ä¸ç”¨ä¸Šç­å—ï¼Ÿæœ‰æ€åº¦çœŸçš„å¯ä»¥ä¸ºæ‰€æ¬²ä¸ºï¼ˆè‹¦ç¬‘ï¼‰ã€‚ç„¶åé—®äº†ä¸ºä»€ä¹ˆç¦»èŒï¼ŒèŠäº†ä¼šæˆ¿ä»·ï¼Œé—®äº†å‡ ä¸ªnettyçš„é—®é¢˜ï¼Œgcçš„é—®é¢˜ï¼Œæœ€åé—®ä¸‹å¯¹ä¸šåŠ¡çš„çœ‹æ³•ã€‚ç„¶åçº¦äº†ä¸ªäºŒé¢çš„æ—¶é—´ï¼Œç»“æœæ—¶é—´åˆ°äº†æ²¡äººè”ç³»æˆ‘ï¼Œç¬¬äºŒå¤©æ‰“ç”µè¯è·Ÿæˆ‘é“æ­‰é‡æ–°çº¦äº†æ—¶é—´ï¼Œä¸å¾—ä¸è¯´æ€åº¦è¿˜æ˜¯å¾ˆå¥½çš„ã€‚äºŒé¢é—®çš„åè€Œå¾ˆåŸºç¡€ï¼Œæ²¡å¤ªå¤šç‰¹åˆ«çš„ã€‚è®©æˆ‘æé—®çš„æ—¶å€™æˆ‘æŠŠç¾å›¢äºŒé¢é‡Œçš„ç¼“å­˜é—®é¢˜æ‹¿å‡ºæ¥é—®ä»–ï¼Œå¾ˆè€å¿ƒçš„ç»™æˆ‘è§£ç­”äº†å¥½å‡ åˆ†é’Ÿï¼Œäººå¾ˆå¥½ã€‚é˜¿é‡Œè¿™ä¸ªå…¶å®ä¸æ˜¯æœ€åé¢è¯•çš„ï¼Œä½†æ˜¯æ˜¯æœ€åç»“æŸçš„ï¼Œä¸å¾—ä¸è¯´é˜¿é‡ŒäººçœŸçš„å¥½å¿™ï¼Œå‘¨ä¸‰è·Ÿæˆ‘é¢„çº¦æ—¶é—´ï¼Œç„¶åå·²ç»æ’åˆ°ä¸‹ä¸€å‘¨çš„å‘¨ä¸€ã€‚æ€»ä½“ä¸Šæ„Ÿè§‰é˜¿é‡Œçš„é¢è¯•é£æ ¼æ˜¯å–œæ¬¢åœ¨æŸä¸ªç‚¹ä¸Šä¸æ–­æ·±å…¥ï¼Œç›´åˆ°ä½ è¯´ä¸çŸ¥é“ã€‚ä¸€é¢ã€‚è‡ªæˆ‘ä»‹ç»ï¼Œç„¶åä»‹ç»ç°åœ¨çš„é¡¹ç›®æ¶æ„ï¼Œç¬¬ä¸€éƒ¨åˆ†å°±æ˜¯æ—¥å¿—ä¸Šä¼ å’Œæ¥æ”¶ï¼Œç„¶åå°±å¦‚ä½•ä¿è¯æ—¥å¿—ä¸Šä¼ çš„å¹‚ç­‰æ€§å¼€å§‹ä¸æ–­æ·±å…¥ï¼Œå…ˆè®©æˆ‘è®¾è®¡ä¸€ä¸ªæ–¹æ¡ˆï¼Œç„¶åé—®æœ‰æ²¡æœ‰ä»€ä¹ˆæ”¹è¿›çš„ï¼Œç„¶åå¦‚ä½•åœ¨ä¿è¯å¹‚ç­‰çš„å‰æä¸‹æé«˜æ€§èƒ½ï¼Œä¸­é—´ç©¿æ’åˆ†å¸ƒå¼é”ã€redisã€mqã€æ•°æ®åº“é”ç­‰å„ç§é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜è®¨è®ºäº†å·®ä¸å¤šåŠå°æ—¶ã€‚ç„¶åå°±é—®æˆ‘æœ‰æ²¡æœ‰ä»€ä¹ˆè¦äº†è§£çš„ï¼ŒèŠ±äº†åå‡ åˆ†é’Ÿä»‹ç»ä»–ä»¬ç°åœ¨åšçš„äº‹æƒ…ã€æŠ€æœ¯æ ˆã€æœªæ¥çš„ä¸€äº›è®¡åˆ’ï¼Œéå¸¸è€å¿ƒã€‚äºŒé¢ã€‚ä¹Ÿæ˜¯ä»ä»‹ç»é¡¹ç›®å¼€å§‹ï¼Œç„¶åæŠ“ä½ä¸€ä¸ªç‚¹ï¼Œç»“åˆç§’æ€çš„åœºæ™¯æ·±å…¥ï¼Œå¦‚ä½•å®ç°åˆ†å¸ƒå¼é”ã€å¦‚ä½•ä¿è¯å¹‚ç­‰æ€§ã€åˆ†å¸ƒå¼äº‹åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚é—®æˆ‘åˆ†å¸ƒå¼é”çš„ç¼ºç‚¹ï¼Œæˆ‘è¯´æ€§èƒ½ä¼šå‡ºç°ç“¶é¢ˆï¼Œä»–é—®æ€ä¹ˆè§£å†³ï¼Œæˆ‘æƒ³äº†æ¯”è¾ƒä¹…ï¼Œä»–æç¤ºè¯´å‘æ•£ä¸‹æ€ç»´ï¼Œæˆ‘æœ€åæƒ³äº†ä¸ªç®€å•çš„æ–¹æ¡ˆï¼Œç›´æ¥ä¸ä½¿ç”¨åˆ†å¸ƒå¼é”ï¼Œä»–å¥½åƒæŒºæ»¡æ„ã€‚æ„Ÿè§‰ä»–ä»¬æ›´çœ‹é‡æ€è€ƒçš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯å…·ä½“æ–¹æ¡ˆã€‚è¿˜é—®äº†ä¸€è‡´æ€§hashå¦‚ä½•ä¿è¯è´Ÿè½½å‡è¡¡ï¼Œkafkaå’Œrocketmqå„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼Œdubboçš„ä¸€ä¸ªè¯·æ±‚è¿‡ç¨‹ã€åºåˆ—åŒ–æ–¹å¼ï¼Œåºåˆ—åŒ–æ¡†æ¶ã€PBçš„ç¼ºç‚¹ã€å¦‚ä½•ä»æ•°æ®åº“å¤§æ‰¹é‡å¯¼å…¥æ•°æ®åˆ°hbaseã€‚ä¸‰é¢ã€‚æ˜¯HRå’Œä¸»ç®¡çš„è”åˆè§†é¢‘é¢è¯•ã€‚è¿™ç§é¢è¯•è¿˜ç¬¬ä¸€æ¬¡é‡åˆ°ï¼Œæœ‰ç‚¹ç´§å¼ ã€‚ä¸»ç®¡å…ˆé¢ï¼Œä¹Ÿæ˜¯è®©æˆ‘å…ˆä»‹ç»é¡¹ç›®ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç”¨è¿‡mqï¼Œå¦‚ä½•ä¿è¯æ¶ˆæ¯å¹‚ç­‰æ€§ã€‚æˆ‘å°±æŠŠkafka0.11ç‰ˆæœ¬çš„å¹‚ç­‰æ€§æ–¹æ¡ˆè¯´äº†ä¸‹ï¼Œå°±æ²¡å†é—®æŠ€æœ¯é—®é¢˜äº†ã€‚åé¢åˆé—®äº†ä¸ºå•¥ç¦»èŒï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ä¹‹ç±»çš„ã€‚ç„¶åå°±äº¤ç»™HRï¼Œåªé—®äº†å‡ ä¸ªé—®é¢˜ï¼Œç„¶åå°±ç»“æŸäº†ï¼Œå…¨ç¨‹ä¸åˆ°åŠå°æ—¶ã€‚ä¸æ‡‚æ˜¯ä¸æ˜¯è·Ÿé¢è¯•çš„éƒ¨é—¨æœ‰å…³ï¼Œé˜¿é‡Œå¯¹å¹‚ç­‰æ€§è¿™ä¸ªé—®é¢˜å¾ˆæ‰§ç€ï¼Œä¸‰æ¬¡éƒ½é—®åˆ°ï¼Œè€Œä¸”è¿˜æ˜¯ä»ä¸åŒè§’åº¦ã€‚æ€»ç»“ä»é¢è¯•çš„éš¾æ˜“ç¨‹åº¦çœ‹é˜¿é‡Œ &gt; ç¾å›¢ &gt; å¤´æ¡ &gt; æœ‰èµ &gt; ç½‘æ˜“ &gt; æ›¹æ“ä¸“è½¦ &gt; ç¾äºšæŸç§‘ã€‚æ•´ä¸ªè¿‡ç¨‹çš„ä½“ä¼šæ˜¯åŸºç¡€çœŸçš„å¾ˆé‡è¦ï¼ŒåŸºç¡€å¥½äº†å¾ˆå¤šé—®é¢˜å³ä½¿æ²¡é‡åˆ°è¿‡ä¹Ÿå¯ä»¥ä¸¾ä¸€åä¸‰ã€‚ å¦å¤–å¯¹ä¸€æ ·æŠ€æœ¯ä¸€å®šè¦æ‡‚åŸç†ï¼Œè€Œä¸ä»…ä»…æ˜¯æ€ä¹ˆä½¿ç”¨ï¼Œå°¤å…¶æ˜¯ç¼ºç‚¹ï¼Œå¯¹é€‰å‹å¾ˆå…³é”®ï¼Œå¯ä»¥å¾ˆå¥½çš„ç”¨æ¥å›ç­”ä¸ºä»€ä¹ˆä¸é€‰xxxã€‚å¦å¤–å¯¹ä¸€äº›æ¯”è¾ƒæ–°çš„æŠ€æœ¯æœ‰æ‰€äº†è§£ä¹Ÿæ˜¯ä¸€ä¸ªåŠ åˆ†é¡¹ã€‚]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveä¸­è‡ªå®šä¹‰UDAFå‡½æ•°ç”Ÿäº§å°æ¡ˆä¾‹]]></title>
    <url>%2F2018%2F05%2F23%2FHive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[ä¸€ã€UDAF å›é¡¾1.å®šä¹‰ï¼šUDAF(User Defined Aggregation Funcation ) ç”¨æˆ·è‡ªå®šä¹‰èšç±»æ–¹æ³•ï¼Œå’Œgroup byè”åˆä½¿ç”¨ï¼Œæ¥å—å¤šä¸ªè¾“å…¥æ•°æ®è¡Œï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªè¾“å‡ºæ•°æ®è¡Œã€‚2.Hiveæœ‰ä¸¤ç§UDAFï¼šç®€å•å’Œé€šç”¨ç®€å•ï¼šåˆ©ç”¨æŠ½è±¡ç±»UDAFå’ŒUDAFEvaluatorï¼Œä½¿ç”¨Javaåå°„å¯¼è‡´æ€§èƒ½æŸå¤±ï¼Œä¸”æœ‰äº›ç‰¹æ€§ä¸èƒ½ä½¿ç”¨ï¼Œå¦‚å¯å˜é•¿åº¦å‚æ•°åˆ—è¡¨ ã€‚é€šç”¨ï¼šåˆ©ç”¨æ¥å£GenericUDAFResolver2ï¼ˆæˆ–æŠ½è±¡ç±»AbstractGenericUDAFResolverï¼‰å’ŒæŠ½è±¡ç±»GenericUDAFEvaluatorï¼Œå¯ä»¥ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½ï¼Œä½†æ¯”è¾ƒå¤æ‚ï¼Œä¸ç›´è§‚ã€‚3.ä¸€ä¸ªè®¡ç®—å‡½æ•°å¿…é¡»å®ç°çš„5ä¸ªæ–¹æ³•çš„å…·ä½“å«ä¹‰å¦‚ä¸‹ï¼šinit()ï¼šä¸»è¦æ˜¯è´Ÿè´£åˆå§‹åŒ–è®¡ç®—å‡½æ•°å¹¶ä¸”é‡è®¾å…¶å†…éƒ¨çŠ¶æ€ï¼Œä¸€èˆ¬å°±æ˜¯é‡è®¾å…¶å†…éƒ¨å­—æ®µã€‚ä¸€èˆ¬åœ¨é™æ€ç±»ä¸­å®šä¹‰ä¸€ä¸ªå†…éƒ¨å­—æ®µæ¥å­˜æ”¾æœ€ç»ˆçš„ç»“æœã€‚iterate()ï¼šæ¯ä¸€æ¬¡å¯¹ä¸€ä¸ªæ–°å€¼è¿›è¡Œèšé›†è®¡ç®—æ—¶å€™éƒ½ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œè®¡ç®—å‡½æ•°ä¼šæ ¹æ®èšé›†è®¡ç®—ç»“æœæ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚å½“è¾“ å…¥å€¼åˆæ³•æˆ–è€…æ­£ç¡®è®¡ç®—äº†ï¼Œåˆ™å°±è¿”å›trueã€‚terminatePartial()ï¼šHiveéœ€è¦éƒ¨åˆ†èšé›†ç»“æœçš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œå¿…é¡»è¦è¿”å›ä¸€ä¸ªå°è£…äº†èšé›†è®¡ç®—å½“å‰çŠ¶æ€çš„å¯¹è±¡ã€‚merge()ï¼šHiveè¿›è¡Œåˆå¹¶ä¸€ä¸ªéƒ¨åˆ†èšé›†å’Œå¦ä¸€ä¸ªéƒ¨åˆ†èšé›†çš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚terminate()ï¼šHiveæœ€ç»ˆèšé›†ç»“æœçš„æ—¶å€™å°±ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚è®¡ç®—å‡½æ•°éœ€è¦æŠŠçŠ¶æ€ä½œä¸ºä¸€ä¸ªå€¼è¿”å›ç»™ç”¨æˆ·ã€‚äºŒã€éœ€æ±‚ä½¿ç”¨UDAFç®€å•æ–¹å¼å®ç°ç»Ÿè®¡åŒºåŸŸäº§å“ç”¨æˆ·è®¿é—®æ’åä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®ç°12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hive.org.ruozedata;import java.util.*;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import org.apache.log4j.Logger;public class UserClickUDAF extends UDAF &#123; // æ—¥å¿—å¯¹è±¡åˆå§‹åŒ– public static Logger logger = Logger.getLogger(UserClickUDAF.class); // é™æ€ç±»å®ç°UDAFEvaluator public static class Evaluator implements UDAFEvaluator &#123; // è®¾ç½®æˆå‘˜å˜é‡ï¼Œå­˜å‚¨æ¯ä¸ªç»Ÿè®¡èŒƒå›´å†…çš„æ€»è®°å½•æ•° private static Map&lt;String, String&gt; courseScoreMap; private static Map&lt;String, String&gt; city_info; private static Map&lt;String, String&gt; product_info; private static Map&lt;String, String&gt; user_click; //åˆå§‹åŒ–å‡½æ•°,mapå’Œreduceå‡ä¼šæ‰§è¡Œè¯¥å‡½æ•°,èµ·åˆ°åˆå§‹åŒ–æ‰€éœ€è¦çš„å˜é‡çš„ä½œç”¨ public Evaluator() &#123; init(); &#125; // åˆå§‹åŒ–å‡½æ•°é—´ä¼ é€’çš„ä¸­é—´å˜é‡ public void init() &#123; courseScoreMap = new HashMap&lt;String, String&gt;(); city_info = new HashMap&lt;String, String&gt;(); product_info = new HashMap&lt;String, String&gt;(); &#125; //mapé˜¶æ®µï¼Œè¿”å›å€¼ä¸ºbooleanç±»å‹ï¼Œå½“ä¸ºtrueåˆ™ç¨‹åºç»§ç»­æ‰§è¡Œï¼Œå½“ä¸ºfalseåˆ™ç¨‹åºé€€å‡º public boolean iterate(String pcid, String pcname, String pccount) &#123; if (pcid == null || pcname == null || pccount == null) &#123; return true; &#125; if (pccount.equals(&quot;-1&quot;)) &#123; // åŸå¸‚è¡¨ city_info.put(pcid, pcname); &#125; else if (pccount.equals(&quot;-2&quot;)) &#123; // äº§å“è¡¨ product_info.put(pcid, pcname); &#125; else &#123; // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è” unionCity_Prod_UserClic1(pcid, pcname, pccount); &#125; return true; &#125; // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è” private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123; if (product_info.containsKey(pcid)) &#123; if (city_info.containsKey(pcname)) &#123; String city_name = city_info.get(pcname); String prod_name = product_info.get(pcid); String cp_name = city_name + prod_name; // å¦‚æœä¹‹å‰å·²ç»Putè¿‡Keyå€¼ä¸ºåŒºåŸŸä¿¡æ¯ï¼Œåˆ™æŠŠè®°å½•ç›¸åŠ å¤„ç† if (courseScoreMap.containsKey(cp_name)) &#123; int pcrn = 0; String strTemp = courseScoreMap.get(cp_name); String courseScoreMap_pn = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim(); pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn); courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn)); &#125; else &#123; courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount); &#125; &#125; &#125; &#125; /** * ç±»ä¼¼äºcombiner,åœ¨mapèŒƒå›´å†…åšéƒ¨åˆ†èšåˆï¼Œå°†ç»“æœä¼ ç»™mergeå‡½æ•°ä¸­çš„å½¢å‚mapOutput * å¦‚æœéœ€è¦èšåˆï¼Œåˆ™å¯¹iteratorè¿”å›çš„ç»“æœå¤„ç†ï¼Œå¦åˆ™ç›´æ¥è¿”å›iteratorçš„ç»“æœå³å¯ */ public Map&lt;String, String&gt; terminatePartial() &#123; return courseScoreMap; &#125; // reduce é˜¶æ®µï¼Œç”¨äºé€ä¸ªè¿­ä»£å¤„ç†mapå½“ä¸­æ¯ä¸ªä¸åŒkeyå¯¹åº”çš„ terminatePartialçš„ç»“æœ public boolean merge(Map&lt;String, String&gt; mapOutput) &#123; this.courseScoreMap.putAll(mapOutput); return true; &#125; // å¤„ç†mergeè®¡ç®—å®Œæˆåçš„ç»“æœï¼Œå³å¯¹mergeå®Œæˆåçš„ç»“æœåšæœ€åçš„ä¸šåŠ¡å¤„ç† public String terminate() &#123; return courseScoreMap.toString(); &#125; &#125;&#125;å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°123DROP TEMPORARY FUNCTION user_click;add jar /data/hive_udf-1.0.jar;CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®1234567891011121314151617insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from ( select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from ( select * from ( select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info union all select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info union all select count(1) as type, product_id as pcid, city_id as pcname from user_click where action_time=&apos;2016-05-05&apos; group by product_id,city_id ) a order by type) b) c ;å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨1234567create external table tmp1(city_name string,product_name string,rn string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;location &apos;/works/tmp1&apos;;ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æ’å123456789select * from (select city_name, product_name, floor(sum(rn)) visit_num, row_number()over(partition by city_name order by sum(rn) desc) rn, &apos;2016-05-05&apos; action_time from tmp1 group by city_name,product_name) a where rn &lt;=3 ;å…«ã€æœ€ç»ˆç»“æœ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark åŸºæœ¬æ¦‚å¿µ]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[åŸºäº Spark æ„å»ºçš„ç”¨æˆ·ç¨‹åºï¼ŒåŒ…å«äº† ä¸€ä¸ªdriver ç¨‹åºå’Œé›†ç¾¤ä¸Šçš„ executorsï¼›ï¼ˆèµ·äº†ä¸€ä¸ªä½œä¸šï¼Œå°±æ˜¯ä¸€ä¸ªApplicationï¼‰sparkåè¯è§£é‡ŠApplication jarï¼šåº”ç”¨ç¨‹åºjaråŒ…åŒ…å«äº†ç”¨æˆ·çš„ Spark ç¨‹åºçš„ä¸€ä¸ª jar åŒ…. åœ¨æŸäº›æƒ…å†µä¸‹ç”¨æˆ·å¯èƒ½æƒ³è¦åˆ›å»ºä¸€ä¸ªå›Šæ‹¬äº†åº”ç”¨åŠå…¶ä¾èµ–çš„ â€œèƒ–â€ jar åŒ…. ä½†å®é™…ä¸Š, ç”¨æˆ·çš„ jar ä¸åº”è¯¥åŒ…æ‹¬ Hadoop æˆ–æ˜¯ Spark çš„åº“, è¿™äº›åº“ä¼šåœ¨è¿è¡Œæ—¶è¢«è¿›è¡ŒåŠ è½½ï¼›Driver Programï¼šè¿™ä¸ªè¿›ç¨‹è¿è¡Œåº”ç”¨ç¨‹åºçš„ main æ–¹æ³•å¹¶ä¸”æ–°å»º SparkContext ï¼›Cluster Managerï¼šé›†ç¾¤ç®¡ç†è€…åœ¨é›†ç¾¤ä¸Šè·å–èµ„æºçš„å¤–éƒ¨æœåŠ¡ (ä¾‹å¦‚:standalone,Mesos,Yarn)ï¼›ï¼ˆâ€“masterï¼‰Deploy modeï¼šéƒ¨ç½²æ¨¡å¼å‘Šè¯‰ä½ åœ¨å“ªé‡Œå¯åŠ¨driver program. åœ¨ â€œclusterâ€ æ¨¡å¼ä¸‹, æ¡†æ¶åœ¨é›†ç¾¤å†…éƒ¨è¿è¡Œ driver. åœ¨ â€œclientâ€ æ¨¡å¼ä¸‹, æäº¤è€…åœ¨é›†ç¾¤å¤–éƒ¨è¿è¡Œ driver.ï¼›Worker Nodeï¼šå·¥ä½œèŠ‚ç‚¹é›†ç¾¤ä¸­ä»»ä½•å¯ä»¥è¿è¡Œåº”ç”¨ä»£ç çš„èŠ‚ç‚¹ï¼›ï¼ˆyarnä¸Šå°±æ˜¯node managerï¼‰Executorï¼šåœ¨ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šä¸ºæŸåº”ç”¨å¯åŠ¨çš„ä¸€ä¸ªè¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹è´Ÿè´£è¿è¡Œä»»åŠ¡ï¼Œå¹¶ä¸”è´Ÿè´£å°†æ•°æ®å­˜åœ¨å†…å­˜æˆ–è€…ç£ç›˜ä¸Šã€‚æ¯ä¸ªåº”ç”¨éƒ½æœ‰å„è‡ªç‹¬ç«‹çš„ executorsï¼›Taskï¼šä»»åŠ¡è¢«é€åˆ°æŸä¸ª executor ä¸Šæ‰§è¡Œçš„å·¥ä½œå•å…ƒï¼›Jobï¼šåŒ…å«å¾ˆå¤šå¹¶è¡Œè®¡ç®—çš„taskã€‚ä¸€ä¸ª action å°±ä¼šäº§ç”Ÿä¸€ä¸ªjobï¼›Stageï¼šä¸€ä¸ª Job ä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªtaskçš„é›†åˆï¼Œæ¯ä¸ªtaské›†åˆè¢«ç§°ä¸º stageï¼Œstageä¹‹é—´æ˜¯ç›¸äº’ä¾èµ–çš„(å°±åƒ Mapreduce åˆ† mapå’Œ reduce stagesä¸€æ ·)ï¼Œå¯ä»¥åœ¨Driver çš„æ—¥å¿—ä¸Šçœ‹åˆ°ã€‚sparkå·¥ä½œæµç¨‹1ä¸ªactionä¼šè§¦å‘1ä¸ªjobï¼Œ1ä¸ªjobåŒ…å«nä¸ªstageï¼Œæ¯ä¸ªstageåŒ…å«nä¸ªtaskï¼Œnä¸ªtaskä¼šé€åˆ°nä¸ªexecutorä¸Šæ‰§è¡Œï¼Œä¸€ä¸ªApplicationæ˜¯ç”±ä¸€ä¸ªdriver ç¨‹åºå’Œnä¸ª executorç»„æˆã€‚æäº¤çš„æ—¶å€™ï¼Œé€šè¿‡Cluster Managerå’ŒDeploy modeæ§åˆ¶ã€‚sparkåº”ç”¨ç¨‹åºåœ¨é›†ç¾¤ä¸Šè¿è¡Œä¸€ç»„ç‹¬ç«‹çš„è¿›ç¨‹ï¼Œé€šè¿‡SparkContextåè°ƒçš„åœ¨mainæ–¹æ³•é‡Œé¢ã€‚å¦‚æœè¿è¡Œåœ¨ä¸€ä¸ªé›†ç¾¤ä¹‹ä¸Šï¼ŒSparkContextèƒ½å¤Ÿè¿æ¥å„ç§çš„é›†ç¾¤ç®¡ç†è€…ï¼Œå»è·å–åˆ°ä½œä¸šæ‰€éœ€è¦çš„èµ„æºã€‚ä¸€æ—¦è¿æ¥æˆåŠŸï¼Œsparkåœ¨é›†ç¾¤èŠ‚ç‚¹ä¹‹ä¸Šè¿è¡Œexecutorè¿›ç¨‹ï¼Œæ¥ç»™ä½ çš„åº”ç”¨ç¨‹åºè¿è¡Œè®¡ç®—å’Œå­˜å‚¨æ•°æ®ã€‚å®ƒä¼šå‘é€ä½ çš„åº”ç”¨ç¨‹åºä»£ç åˆ°executorsä¸Šã€‚æœ€åï¼ŒSparkContextå‘é€tasksåˆ°executorsä¸Šå»è¿è¡Œ1ã€æ¯ä¸ªApplicationéƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„executorè¿›ç¨‹ï¼Œè¿™äº›è¿›ç¨‹åœ¨è¿è¡Œå‘¨æœŸå†…éƒ½æ˜¯å¸¸é©»çš„ä»¥å¤šçº¿ç¨‹çš„æ–¹å¼è¿è¡Œtasksã€‚å¥½å¤„æ˜¯æ¯ä¸ªè¿›ç¨‹æ— è®ºæ˜¯åœ¨è°ƒåº¦è¿˜æ˜¯æ‰§è¡Œéƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ„å‘³ç€æ•°æ®ä¸èƒ½è·¨åº”ç”¨ç¨‹åºè¿›è¡Œå…±äº«ï¼Œé™¤éå†™åˆ°å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿï¼ˆAlluxioï¼‰ã€‚2ã€sparkå¹¶ä¸å…³å¿ƒåº•å±‚çš„é›†ç¾¤ç®¡ç†ã€‚3ã€driver ç¨‹åºä¼šç›‘å¬å¹¶ä¸”æ¥æ”¶å¤–é¢çš„ä¸€äº›executorè¯·æ±‚ï¼Œåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸé‡Œé¢ã€‚æ‰€ä»¥ï¼Œdriver ç¨‹åºåº”è¯¥èƒ½è¢«Worker Nodeé€šè¿‡ç½‘ç»œè®¿é—®ã€‚4ã€å› ä¸ºdriver åœ¨é›†ç¾¤ä¸Šè°ƒåº¦Tasksï¼Œdriver å°±åº”è¯¥é è¿‘Worker Nodeã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark History Server Web UIé…ç½®]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶12[root@hadoop000 ~]# cd /opt/app/spark/conf[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf2.åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š(å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š)12345678[root@hadoop000 conf]# hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - root root 0 2017-02-14 12:43 /sparkdrwxrwx--- - root root 0 2017-02-14 12:58 /tmpdrwxr-xr-x - root root 0 2017-02-14 12:58 /userYou have new mail in /var/spool/mail/root[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 itemsdrwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylogåœ¨HDFSä¸­åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œç”¨äºä¿å­˜Sparkè¿è¡Œæ—¥å¿—ä¿¡æ¯ã€‚Spark History Serverä»æ­¤ç›®å½•ä¸­è¯»å–æ—¥å¿—ä¿¡æ¯3.é…ç½®12345[root@hadoop000 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080spark.eventLog.dirä¿å­˜æ—¥å¿—ç›¸å…³ä¿¡æ¯çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯hdfs://å¼€å¤´çš„HDFSè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯file://å¼€å¤´çš„æœ¬åœ°è·¯å¾„ï¼Œéƒ½éœ€è¦æå‰åˆ›å»ºspark.yarn.historyServer.address : Spark history serverçš„åœ°å€(ä¸åŠ http://).è¿™ä¸ªåœ°å€ä¼šåœ¨Sparkåº”ç”¨ç¨‹åºå®Œæˆåæäº¤ç»™YARN RMï¼Œç„¶åå¯ä»¥åœ¨RM UIä¸Šç‚¹å‡»é“¾æ¥è·³è½¬åˆ°history server UIä¸Š.4.æ·»åŠ SPARK_HISTORY_OPTSå‚æ•°12345[root@hadoop01 conf]# vi spark-env.sh#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹123456[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01 ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer[root@hadoop01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@hadoop01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/javaä»¥ä¸Šé…ç½®æ˜¯é’ˆå¯¹ä½¿ç”¨è‡ªå·±ç¼–è¯‘çš„Sparkéƒ¨ç½²åˆ°é›†ç¾¤ä¸­ä¸€åˆ°ä¸¤å°æœºå™¨ä¸Šä½œä¸ºæäº¤ä½œä¸šå®¢æˆ·ç«¯çš„ï¼Œå¦‚æœä½ æ˜¯CDHé›†ç¾¤ä¸­é›†æˆçš„Sparké‚£ä¹ˆå¯ä»¥åœ¨ç®¡ç†ç•Œé¢ç›´æ¥æŸ¥çœ‹ï¼]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkä¸å¾—ä¸ç†è§£çš„é‡è¦æ¦‚å¿µâ€”â€”ä»æºç è§’åº¦çœ‹RDD]]></title>
    <url>%2F2018%2F05%2F20%2FSpark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD%2F</url>
    <content type="text"><![CDATA[1.RDDæ˜¯ä»€ä¹ˆResilient Distributed Datasetï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ï¼Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¹¶è¡Œæ“ä½œä¸å¯å˜çš„åˆ†åŒºå…ƒç´ çš„é›†åˆ2.RDDäº”å¤§ç‰¹æ€§A list of partitionsæ¯ä¸ªrddæœ‰å¤šä¸ªåˆ†åŒºprotected def getPartitions: Array[Partition]A function for computing each splitè®¡ç®—ä½œç”¨åˆ°æ¯ä¸ªåˆ†åŒºdef compute(split: Partition, context: TaskContext): Iterator[T]A list of dependencies on other RDDsrddä¹‹é—´å­˜åœ¨ä¾èµ–ï¼ˆRDDçš„è¡€ç¼˜å…³ç³»ï¼‰å¦‚ï¼šRDDA=&gt;RDDB=&gt;RDDC=&gt;RDDDprotected def getDependencies: Seq[Dependency[_]] = depsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)å¯é€‰ï¼Œé»˜è®¤å“ˆå¸Œçš„åˆ†åŒº@transient val partitioner: Option[Partitioner] = NoneOptionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„æœ€ä¼˜æ‰§è¡Œä½ç½®ï¼Œå°½é‡å®ç°æ•°æ®æœ¬åœ°åŒ–ï¼Œå‡å°‘IOï¼ˆè¿™å¾€å¾€æ˜¯ç†æƒ³çŠ¶æ€ï¼‰protected def getPreferredLocations(split: Partition): Seq[String] = Nilæºç æ¥è‡ªgithubã€‚3.å¦‚ä½•åˆ›å»ºRDDåˆ›å»ºRDDæœ‰ä¸¤ç§æ–¹å¼ parallelize() å’Œtextfile()ï¼Œå…¶ä¸­parallelizeå¯æ¥æ”¶é›†åˆç±»ï¼Œä¸»è¦ä½œä¸ºæµ‹è¯•ç”¨ã€‚textfileå¯è¯»å–æ–‡ä»¶ç³»ç»Ÿï¼Œæ˜¯å¸¸ç”¨çš„ä¸€ç§æ–¹å¼1234567891011121314151617parallelize() def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) &#125;textfileï¼ˆï¼‰ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;æºç æ€»ç»“ï¼š1ï¼‰.å–_2æ˜¯å› ä¸ºæ•°æ®ä¸ºï¼ˆkeyï¼ˆåç§»é‡ï¼‰ï¼Œvalueï¼ˆæ•°æ®ï¼‰ï¼‰4.å¸¸è§çš„transformationå’Œactionç”±äºæ¯”è¾ƒç®€å•ï¼Œå¤§æ¦‚è¯´ä¸€ä¸‹å¸¸ç”¨çš„ç”¨å¤„ï¼Œä¸åšä»£ç æµ‹è¯•transformationMapï¼šå¯¹æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œFlatMapï¼šå…ˆå¯¹æ•°æ®é›†è¿›è¡Œæ‰å¹³åŒ–å¤„ç†ï¼Œç„¶åå†MapFilterï¼šå¯¹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œä¸ºtrueåˆ™é€šè¿‡destinctï¼šå»é‡æ“ä½œactionreduceï¼šå¯¹æ•°æ®è¿›è¡Œèšé›†reduceBykeyï¼šå¯¹keyå€¼ç›¸åŒçš„è¿›è¡Œæ“ä½œcollectï¼šæ²¡æœ‰æ•ˆæœçš„actionï¼Œä½†æ˜¯å¾ˆæœ‰ç”¨saveAstextFileï¼šæ•°æ®å­˜å…¥æ–‡ä»¶ç³»ç»Ÿforeachï¼šå¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œfuncçš„æ“ä½œ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç¾å‘³ä¸ç”¨ç­‰å¤§æ•°æ®é¢è¯•é¢˜(201804æœˆ)]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)%2F</url>
    <content type="text"><![CDATA[1.è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´ç°åœºé¢è¯•é¢˜æˆªå›¾:2.åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š]]></content>
      <categories>
        <category>é¢è¯•çœŸé¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDDã€DataFrameå’ŒDataSetçš„åŒºåˆ«]]></title>
    <url>%2F2018%2F05%2F19%2FSpark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[åœ¨Sparkä¸­ï¼ŒRDDã€DataFrameã€Datasetæ˜¯æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼Œä»Šå¤©è°ˆè°ˆä»–ä»¬çš„åŒºåˆ«ï¼ä¸€ ã€å…±æ€§1ã€RDDã€DataFrameã€Datasetå…¨éƒ½æ˜¯sparkå¹³å°ä¸‹çš„åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†ï¼Œä¸ºå¤„ç†è¶…å¤§å‹æ•°æ®æä¾›ä¾¿åˆ©2ã€ä¸‰è€…éƒ½æœ‰æƒ°æ€§æœºåˆ¶ï¼Œåœ¨è¿›è¡Œåˆ›å»ºã€è½¬æ¢ï¼Œå¦‚mapæ–¹æ³•æ—¶ï¼Œä¸ä¼šç«‹å³æ‰§è¡Œï¼Œåªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶ï¼Œä¸‰è€…æ‰ä¼šå¼€å§‹éå†è¿ç®—ã€‚3ã€ä¸‰è€…éƒ½ä¼šæ ¹æ®sparkçš„å†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—ï¼Œè¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º4ã€ä¸‰è€…éƒ½æœ‰partitionçš„æ¦‚å¿µã€‚äºŒã€RDDä¼˜ç¼ºç‚¹ä¼˜ç‚¹ï¼š1ã€ç›¸æ¯”äºä¼ ç»Ÿçš„MapReduceæ¡†æ¶ï¼ŒSparkåœ¨RDDä¸­å†…ç½®å¾ˆå¤šå‡½æ•°æ“ä½œï¼Œgroupï¼Œmapï¼Œfilterç­‰ï¼Œæ–¹ä¾¿å¤„ç†ç»“æ„åŒ–æˆ–éç»“æ„åŒ–æ•°æ®ã€‚2ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹é£æ ¼3ã€ç¼–è¯‘æ—¶ç±»å‹å®‰å…¨ï¼Œç¼–è¯‘æ—¶å°±èƒ½æ£€æŸ¥å‡ºç±»å‹é”™è¯¯ç¼ºç‚¹ï¼š1ã€åºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„æ€§èƒ½å¼€é”€2ã€GCçš„æ€§èƒ½å¼€é”€ï¼Œé¢‘ç¹çš„åˆ›å»ºå’Œé”€æ¯å¯¹è±¡, åŠ¿å¿…ä¼šå¢åŠ GCä¸‰ã€DataFrame1ã€ä¸RDDå’ŒDatasetä¸åŒï¼ŒDataFrameæ¯ä¸€è¡Œçš„ç±»å‹å›ºå®šä¸ºRowï¼Œåªæœ‰é€šè¿‡è§£ææ‰èƒ½è·å–å„ä¸ªå­—æ®µçš„å€¼ã€‚å¦‚12345df.foreach&#123; x =&gt; val v1=x.getAs[String](&quot;v1&quot;) val v2=x.getAs[String](&quot;v2&quot;)&#125;2ã€DataFrameå¼•å…¥äº†schemaå’Œoff-heapschema : RDDæ¯ä¸€è¡Œçš„æ•°æ®, ç»“æ„éƒ½æ˜¯ä¸€æ ·çš„. è¿™ä¸ªç»“æ„å°±å­˜å‚¨åœ¨schemaä¸­. Sparké€šè¿‡schameå°±èƒ½å¤Ÿè¯»æ‡‚æ•°æ®, å› æ­¤åœ¨é€šä¿¡å’ŒIOæ—¶å°±åªéœ€è¦åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ•°æ®, è€Œç»“æ„çš„éƒ¨åˆ†å°±å¯ä»¥çœç•¥äº†.off-heap : æ„å‘³ç€JVMå †ä»¥å¤–çš„å†…å­˜, è¿™äº›å†…å­˜ç›´æ¥å—æ“ä½œç³»ç»Ÿç®¡ç†ï¼ˆè€Œä¸æ˜¯JVMï¼‰ã€‚Sparkèƒ½å¤Ÿä»¥äºŒè¿›åˆ¶çš„å½¢å¼åºåˆ—åŒ–æ•°æ®(ä¸åŒ…æ‹¬ç»“æ„)åˆ°off-heapä¸­, å½“è¦æ“ä½œæ•°æ®æ—¶, å°±ç›´æ¥æ“ä½œoff-heapå†…å­˜. ç”±äºSparkç†è§£schema, æ‰€ä»¥çŸ¥é“è¯¥å¦‚ä½•æ“ä½œ.off-heapå°±åƒåœ°ç›˜, schemaå°±åƒåœ°å›¾, Sparkæœ‰åœ°å›¾åˆæœ‰è‡ªå·±åœ°ç›˜äº†, å°±å¯ä»¥è‡ªå·±è¯´äº†ç®—äº†, ä¸å†å—JVMçš„é™åˆ¶, ä¹Ÿå°±ä¸å†æ”¶GCçš„å›°æ‰°äº†.3ã€ç»“æ„åŒ–æ•°æ®å¤„ç†éå¸¸æ–¹ä¾¿ï¼Œæ”¯æŒAvro, CSV, Elasticsearchæ•°æ®ç­‰ï¼Œä¹Ÿæ”¯æŒHive, MySQLç­‰ä¼ ç»Ÿæ•°æ®è¡¨4ã€å…¼å®¹Hiveï¼Œæ”¯æŒHqlã€UDFæœ‰schemaå’Œoff-heapæ¦‚å¿µï¼ŒDataFrameè§£å†³äº†RDDçš„ç¼ºç‚¹, ä½†æ˜¯å´ä¸¢äº†RDDçš„ä¼˜ç‚¹. DataFrameä¸æ˜¯ç±»å‹å®‰å…¨çš„ï¼ˆåªæœ‰ç¼–è¯‘åæ‰èƒ½çŸ¥é“ç±»å‹é”™è¯¯ï¼‰, APIä¹Ÿä¸æ˜¯é¢å‘å¯¹è±¡é£æ ¼çš„.å››ã€DataSet1ã€DataSetæ˜¯åˆ†å¸ƒå¼çš„æ•°æ®é›†åˆã€‚DataSetæ˜¯åœ¨Spark1.6ä¸­æ·»åŠ çš„æ–°çš„æ¥å£ã€‚å®ƒé›†ä¸­äº†RDDçš„ä¼˜ç‚¹ï¼ˆå¼ºç±»å‹ å’Œå¯ä»¥ç”¨å¼ºå¤§lambdaå‡½æ•°ï¼‰ä»¥åŠSpark SQLä¼˜åŒ–çš„æ‰§è¡Œå¼•æ“ã€‚DataSetå¯ä»¥é€šè¿‡JVMçš„å¯¹è±¡è¿›è¡Œæ„å»ºï¼Œå¯ä»¥ç”¨å‡½æ•°å¼çš„è½¬æ¢ï¼ˆmap/flatmap/filterï¼‰è¿›è¡Œå¤šç§æ“ä½œã€‚2ã€DataSetç»“åˆäº†RDDå’ŒDataFrameçš„ä¼˜ç‚¹, å¹¶å¸¦æ¥çš„ä¸€ä¸ªæ–°çš„æ¦‚å¿µEncoderã€‚DataSet é€šè¿‡Encoderå®ç°äº†è‡ªå®šä¹‰çš„åºåˆ—åŒ–æ ¼å¼ï¼Œä½¿å¾—æŸäº›æ“ä½œå¯ä»¥åœ¨æ— éœ€åºåˆ—åŒ–æƒ…å†µä¸‹è¿›è¡Œã€‚å¦å¤–Datasetè¿˜è¿›è¡Œäº†åŒ…æ‹¬Tungstenä¼˜åŒ–åœ¨å†…çš„å¾ˆå¤šæ€§èƒ½æ–¹é¢çš„ä¼˜åŒ–ã€‚3ã€Datasetç­‰åŒäºDataFrameï¼ˆSpark 2.Xï¼‰]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¤§æ•°æ®ä¹‹å®æ—¶æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶--ç”Ÿäº§ä¸ŠCanalä¸Maxwellé¢ å³°å¯¹å†³]]></title>
    <url>%2F2018%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3%2F</url>
    <content type="text"><![CDATA[ä¸€.æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼šCanalhttps://github.com/alibaba/canalhttps://github.com/Hackeruncle/syncClientMaxwellhttps://github.com/zendesk/maxwelläºŒ.æ¶æ„ä½¿ç”¨MySQL â€”- ä¸­é—´ä»¶ mcp â€”&gt;KAFKAâ€”&gt;?â€”&gt;å­˜å‚¨HBASE/KUDU/Cassandra å¢é‡çš„a.å…¨é‡ bootstrapb.å¢é‡1.å¯¹æ¯”Canal(æœåŠ¡ç«¯)Maxwell(æœåŠ¡ç«¯+å®¢æˆ·ç«¯)è¯­è¨€JavaJavaæ´»è·ƒåº¦æ´»è·ƒæ´»è·ƒHAæ”¯æŒå®šåˆ¶ ä½†æ˜¯æ”¯æŒæ–­ç‚¹è¿˜åŸåŠŸèƒ½æ•°æ®è½åœ°å®šåˆ¶è½åœ°åˆ°kafkaåˆ†åŒºæ”¯æŒæ”¯æŒbootstrap(å¼•å¯¼)ä¸æ”¯æŒæ”¯æŒæ•°æ®æ ¼å¼æ ¼å¼è‡ªç”±json(æ ¼å¼å›ºå®š) spark jsonâ€“&gt;DFæ–‡æ¡£è¾ƒè¯¦ç»†è¾ƒè¯¦ç»†éšæœºè¯»æ”¯æŒæ”¯æŒä¸ªäººé€‰æ‹©Maxwella.æœåŠ¡ç«¯+å®¢æˆ·ç«¯ä¸€ä½“ï¼Œè½»é‡çº§çš„b.æ”¯æŒæ–­ç‚¹è¿˜åŸåŠŸèƒ½+bootstrap+jsonCan do SELECT * from table (bootstrapping) initial loads of a table.supports automatic position recover on master promotionflexible partitioning schemes for Kakfa - by database, table, primary key, or columnMaxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).2.å®˜ç½‘è§£è¯»Bç«™è§†é¢‘3.éƒ¨ç½²3.1 MySQL Installhttps://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txthttps://ke.qq.com/course/262452?tuin=11cffd503.2 ä¿®æ”¹12345678910111213141516171819202122$ vi /etc/my.cnf[mysqld]binlog_format=row$ service mysql start3.3 åˆ›å»ºMaxwellçš„dbå’Œç”¨æˆ·mysql&gt; create database maxwell;Query OK, 1 row affected (0.03 sec)mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt;3.4è§£å‹1[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz3.5æµ‹è¯•STDOUT:123bin/maxwell --user=&apos;maxwell&apos; \--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \--producer=stdoutæµ‹è¯•1ï¼šinsert sqlï¼š12mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);Query OK, 1 row affected (0.03 sec)maxwellè¾“å‡ºï¼š123456789101112131415161718&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;insert&quot;, &quot;ts&quot;: 1525959044, &quot;xid&quot;: 201, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 18, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;updateuser&quot;: null &#125;&#125;æµ‹è¯•1ï¼šupdate sql:1mysql&gt; update ruozedata set age=29 where id=999;é—®é¢˜: ROWï¼Œä½ è§‰å¾—binlogæ›´æ–°å‡ ä¸ªå­—æ®µï¼Ÿmaxwellè¾“å‡ºï¼š12345678910111213141516171819202122&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;update&quot;, &quot;ts&quot;: 1525959208, &quot;xid&quot;: 255, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 29, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;, &quot;updateuser&quot;: null &#125;, &quot;old&quot;: &#123; &quot;age&quot;: 18, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot; &#125;&#125;4.å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§4.1 kafka_version ç‰ˆæœ¬Using kafka version: 0.11.0.1 0.10jar:12345678[root@hadoop000 kafka-clients]# lltotal 4000-rw-r--r--. 1 ruoze games 746207 May 8 06:34 kafka-clients-0.10.0.1.jar-rw-r--r--. 1 ruoze games 951041 May 8 06:35 kafka-clients-0.10.2.1.jar-rw-r--r--. 1 ruoze games 1419544 May 8 06:35 kafka-clients-0.11.0.1.jar-rw-r--r--. 1 ruoze games 324016 May 8 06:34 kafka-clients-0.8.2.2.jar-rw-r--r--. 1 ruoze games 641408 May 8 06:34 kafka-clients-0.9.0.1.jar[root@hadoop000 kafka-clients]#]]></content>
      <categories>
        <category>å…¶ä»–ç»„ä»¶</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>maxwell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«]]></title>
    <url>%2F2018%2F05%2F12%2FSpark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[ä¸€. YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«ï¼ˆ1ï¼‰SparkContextåˆå§‹åŒ–ä¸åŒï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Driveræ‰€åœ¨ä½ç½®çš„ä¸åŒï¼ŒYarnClusterçš„Driveræ˜¯åœ¨é›†ç¾¤çš„æŸä¸€å°NMä¸Šï¼Œä½†æ˜¯Yarn-Clientå°±æ˜¯åœ¨driveræ‰€åœ¨çš„æœºå™¨ä¸Šï¼›ï¼ˆ2ï¼‰è€ŒDriverä¼šå’ŒExecutorsè¿›è¡Œé€šä¿¡ï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Yarn_clusteråœ¨æäº¤Appä¹‹åå¯ä»¥å…³é—­Clientï¼Œè€ŒYarn-Clientä¸å¯ä»¥ï¼›ï¼ˆ3ï¼‰æœ€åå†æ¥è¯´åº”ç”¨åœºæ™¯ï¼ŒYarn-Clusteré€‚åˆç”Ÿäº§ç¯å¢ƒï¼ŒYarn-Clienté€‚åˆäº¤äº’å’Œè°ƒè¯•ã€‚äºŒ. yarn client æ¨¡å¼yarn-client æ¨¡å¼çš„è¯ ï¼ŒæŠŠ å®¢æˆ·ç«¯å…³æ‰çš„è¯ ï¼Œæ˜¯ä¸èƒ½æäº¤ä»»åŠ¡çš„ ã€‚ä¸‰.yarn cluster æ¨¡å¼yarn-cluster æ¨¡å¼çš„è¯ï¼Œ client å…³é—­æ˜¯å¯ä»¥æäº¤ä»»åŠ¡çš„ ï¼Œæ€»ç»“:1.spark-shell/spark-sql åªæ”¯æŒ yarn-clientæ¨¡å¼ï¼›2.spark-submitå¯¹äºä¸¤ç§æ¨¡å¼éƒ½æ”¯æŒã€‚]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>æ¶æ„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§æ”¹é€ Spark1.6æºä»£ç ï¼Œcreate tableè¯­æ³•æ”¯æŒOracleåˆ—è¡¨åˆ†åŒº]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1.éœ€æ±‚é€šè¿‡Spark SQL JDBC æ–¹æ³•ï¼ŒæŠ½å–Oracleè¡¨æ•°æ®ã€‚2.é—®é¢˜å¤§æ•°æ®å¼€å‘äººå‘˜åæ˜ ï¼Œä½¿ç”¨æ•ˆæœä¸Šåˆ—è¡¨åˆ†åŒºä¼˜äºæ•£åˆ—åˆ†åŒºã€‚ä½†Spark SQL JDBCæ–¹æ³•åªæ”¯æŒæ•°å­—ç±»å‹åˆ†åŒºï¼Œè€Œä¸šåŠ¡è¡¨çš„åˆ—è¡¨åˆ†åŒºå­—æ®µæ˜¯ä¸ªå­—ç¬¦ä¸²ã€‚ç›®å‰Oracleè¡¨ä½¿ç”¨åˆ—è¡¨åˆ†åŒºï¼Œå¯¹çœçº§ä»£ç åˆ† åŒºã€‚å‚è€ƒ http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases3.Oracleçš„åˆ†åŒº3.1åˆ—è¡¨åˆ†åŒº:è¯¥åˆ†åŒºçš„ç‰¹ç‚¹æ˜¯æŸåˆ—çš„å€¼åªæœ‰å‡ ä¸ªï¼ŒåŸºäºè¿™æ ·çš„ç‰¹ç‚¹æˆ‘ä»¬å¯ä»¥é‡‡ç”¨åˆ—è¡¨åˆ†åŒºã€‚ä¾‹ä¸€:1234567891011CREATE TABLE PROBLEM_TICKETS(PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,STATUS VARCHAR2(20))PARTITION BY LIST (STATUS)(PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02)3.2æ•£åˆ—åˆ†åŒº:è¿™ç±»åˆ†åŒºæ˜¯åœ¨åˆ—å€¼ä¸Šä½¿ç”¨æ•£åˆ—ç®—æ³•ï¼Œä»¥ç¡®å®šå°†è¡Œæ”¾å…¥å“ªä¸ªåˆ†åŒºä¸­ã€‚å½“åˆ—çš„å€¼æ²¡æœ‰åˆé€‚çš„æ¡ä»¶æ—¶ï¼Œå»ºè®®ä½¿ç”¨æ•£åˆ—åˆ†åŒºã€‚ æ•£åˆ—åˆ†åŒºä¸ºé€šè¿‡æŒ‡å®šåˆ†åŒºç¼–å·æ¥å‡åŒ€åˆ†å¸ƒæ•°æ®çš„ä¸€ç§åˆ†åŒºç±»å‹ï¼Œå› ä¸ºé€šè¿‡åœ¨I/Oè®¾å¤‡ä¸Šè¿›è¡Œæ•£åˆ—åˆ†åŒºï¼Œä½¿å¾—è¿™äº›åˆ†åŒºå¤§å°ä¸€è‡´ã€‚ä¾‹ä¸€:1234567891011CREATE TABLE HASH_TABLE(COL NUMBER(8),INF VARCHAR2(100) )PARTITION BY HASH (COL)(PARTITION PART01 TABLESPACE HASH_TS01, PARTITION PART02 TABLESPACE HASH_TS02, PARTITION PART03 TABLESPACE HASH_TS03)4.æ”¹é€ è“è‰²ä»£ç æ˜¯æ”¹é€ Sparkæºä»£ç ,åŠ è¯¾ç¨‹é¡¾é—®é¢†å–PDFã€‚1) Spark SQL JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚12345678910111213CREATE TEMPORARY TABLE TBLS_INUSING org.apache.spark.sql.jdbc OPTIONS (driver &quot;com.mysql.jdbc.Driver&quot;,url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,fetchSize &quot;1000&quot;,partitionColumn &quot;TBL_ID&quot;,numPartitions &quot;null&quot;,lowerBound &quot;null&quot;,upperBound &quot;null&quot;,user &quot;hive2user&quot;,password &quot;hive2user&quot;,partitionInRule &quot;1|15,16,18,19|20,21&quot;);2)ç¨‹åºå…¥å£org.apache.spark.sql.execution.datasources.jdbc.DefaultSourceï¼Œæ–¹æ³•createRelation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748override def createRelation(sqlContext: SQLContext,parameters: Map[String, String]): BaseRelation = &#123;val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)// add partition in ruleval partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)// validind all the partition in rule if (partitionColumn != null&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)&amp;&amp; partitionInRule == null )&#123; sys.error(&quot;Partitioning incompletely specified&quot;) &#125;val partitionInfo = if (partitionColumn == null) &#123; null&#125; else &#123; val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123; val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot; upperBound = &quot;0&quot; inGroups &#125; else&#123; Array[String]() &#125; JDBCPartitioningInfo( partitionColumn, lowerBound.toLong, upperBound.toLong, numPartitions.toInt, inPartitions)&#125;val parts = JDBCRelation.columnPartition(partitionInfo)val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))// parameters is immutableif(numPartitions != null)&#123;properties.put(&quot;numPartitions&quot; , numPartitions) &#125;JDBCRelation(url, table, parts, properties)(sqlContext) &#125; &#125;3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelationï¼Œæ–¹æ³•columnPartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))val column = partitioning.columnvar i: Int = 0var ans = new ArrayBuffer[Partition]()// partition by long if(partitioning.inPartitions.length == 0)&#123;val numPartitions = partitioning.numPartitionsif (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.// Here we get a little roundoff, but that&apos;s (hopefully) OK.val stride: Long = (partitioning.upperBound / numPartitions- partitioning.lowerBound / numPartitions)var currentValue: Long = partitioning.lowerBoundwhile (i &lt; numPartitions) &#123;val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else nullcurrentValue += strideval upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =if (upperBound == null) &#123; lowerBound&#125; else if (lowerBound == null) &#123; upperBound&#125; else &#123; s&quot;$lowerBound AND $upperBound&quot; &#125; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125;&#125;// partition by in else&#123; while(i &lt; partitioning.inPartitions.length)&#123; val inContent = partitioning.inPartitions(i) val whereClause = s&quot;$column in ($inContent)&quot; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125; &#125; ans.toArray &#125;4)å¯¹å¤–æ–¹æ³•org.apache.spark.sql.SQLContext , æ–¹æ³•jdbc123456789101112def jdbc(url: String,table: String,columnName: String,lowerBound: Long,upperBound: Long,numPartitions: Int,inPartitions: Array[String] = Array[String]()): DataFrame = &#123;read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)&#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>æºç é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§ä¸­Hiveé™æ€å’ŒåŠ¨æ€åˆ†åŒºè¡¨ï¼Œè¯¥æ€æ ·æŠ‰æ‹©å‘¢ï¼Ÿ]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[ä¸€.éœ€æ±‚æŒ‰ç…§ä¸åŒéƒ¨é—¨ä½œä¸ºåˆ†åŒºï¼Œå¯¼æ•°æ®åˆ°ç›®æ ‡è¡¨äºŒ.ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ71.åˆ›å»ºé™æ€åˆ†åŒºè¡¨ï¼š12345678910create table emp_static_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;2.æ’å…¥æ•°æ®ï¼š12hive&gt;insert into table emp_static_partition partition(deptno=10) select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;3.æŸ¥è¯¢æ•°æ®ï¼š1hive&gt;select * from emp_static_partition;ä¸‰.ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ1.åˆ›å»ºåŠ¨æ€åˆ†åŒºè¡¨ï¼š123456789create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;ã€æ³¨æ„ã€‘åŠ¨æ€åˆ†åŒºè¡¨ä¸é™æ€åˆ†åŒºè¡¨çš„åˆ›å»ºï¼Œåœ¨è¯­æ³•ä¸Šæ˜¯æ²¡æœ‰ä»»ä½•åŒºåˆ«çš„2.æ’å…¥æ•°æ®ï¼š12hive&gt;insert into table emp_dynamic_partition partition(deptno) select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;ã€æ³¨æ„ã€‘åˆ†åŒºçš„å­—æ®µåç§°ï¼Œå†™åœ¨æœ€åï¼Œæœ‰å‡ ä¸ªå°±å†™å‡ ä¸ª ä¸é™æ€åˆ†åŒºç›¸æ¯”ï¼Œä¸éœ€è¦whereéœ€è¦è®¾ç½®å±æ€§çš„å€¼ï¼š1hive&gt;set hive.exec.dynamic.partition.mode=nonstrictï¼›å‡å¦‚ä¸è®¾ç½®ï¼ŒæŠ¥é”™å¦‚ä¸‹:3.æŸ¥è¯¢æ•°æ®ï¼š1hive&gt;select * from emp_dynamic_partition;åˆ†åŒºåˆ—ä¸ºdeptnoï¼Œå®ç°äº†åŠ¨æ€åˆ†åŒºå››.æ€»ç»“åœ¨ç”Ÿäº§ä¸Šæˆ‘ä»¬æ›´å€¾å‘æ˜¯é€‰æ‹©åŠ¨æ€åˆ†åŒºï¼Œæ— éœ€æ‰‹å·¥æŒ‡å®šæ•°æ®å¯¼å…¥çš„å…·ä½“åˆ†åŒºï¼Œè€Œæ˜¯ç”±selectçš„å­—æ®µ(å­—æ®µå†™åœ¨æœ€åï¼Œæœ‰å‡ ä¸ªå†™å‡ ä¸ª)è‡ªè¡Œå†³å®šå¯¼å‡ºåˆ°å“ªä¸€ä¸ªåˆ†åŒºä¸­ï¼Œ å¹¶è‡ªåŠ¨åˆ›å»ºç›¸åº”çš„åˆ†åŒºï¼Œä½¿ç”¨ä¸Šæ›´åŠ æ–¹ä¾¿å¿«æ· ï¼Œåœ¨ç”Ÿäº§å·¥ä½œä¸­ç”¨çš„éå¸¸å¤šå¤šã€‚]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5minæŒæ¡ï¼ŒHiveçš„HiveServer2 å’ŒJDBCå®¢æˆ·ç«¯&ä»£ç çš„ç”Ÿäº§ä½¿ç”¨]]></title>
    <url>%2F2018%2F05%2F04%2F5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF%26%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. ä»‹ç»ï¼šä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œé€šè¿‡HiveServeræˆ–è€…HiveServer2ï¼Œå®¢æˆ·ç«¯å¯ä»¥åœ¨ä¸å¯åŠ¨CLIçš„æƒ…å†µä¸‹å¯¹Hiveä¸­çš„æ•°æ®è¿›è¡Œæ“ä½œï¼Œä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€å¦‚javaï¼Œpythonç­‰å‘hiveæäº¤è¯·æ±‚ï¼Œå–å›ç»“æœï¼ˆä»hive0.15èµ·å°±ä¸å†æ”¯æŒhiveserveräº†ï¼‰ï¼Œä½†æ˜¯åœ¨è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯è¦è¯´ä¸€ä¸‹HiveServerã€‚HiveServeræˆ–è€…HiveServer2éƒ½æ˜¯åŸºäºThriftçš„ï¼Œä½†HiveSeveræœ‰æ—¶è¢«ç§°ä¸ºThrift serverï¼Œè€ŒHiveServer2å´ä¸ä¼šã€‚æ—¢ç„¶å·²ç»å­˜åœ¨HiveServerï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦HiveServer2å‘¢ï¼Ÿè¿™æ˜¯å› ä¸ºHiveServerä¸èƒ½å¤„ç†å¤šäºä¸€ä¸ªå®¢æˆ·ç«¯çš„å¹¶å‘è¯·æ±‚ï¼Œè¿™æ˜¯ç”±äºHiveServerä½¿ç”¨çš„Thriftæ¥å£æ‰€å¯¼è‡´çš„é™åˆ¶ï¼Œä¸èƒ½é€šè¿‡ä¿®æ”¹HiveServerçš„ä»£ç ä¿®æ­£ã€‚å› æ­¤åœ¨Hive-0.11.0ç‰ˆæœ¬ä¸­é‡å†™äº†HiveServerä»£ç å¾—åˆ°äº†HiveServer2ï¼Œè¿›è€Œè§£å†³äº†è¯¥é—®é¢˜ã€‚HiveServer2æ”¯æŒå¤šå®¢æˆ·ç«¯çš„å¹¶å‘å’Œè®¤è¯ï¼Œä¸ºå¼€æ”¾APIå®¢æˆ·ç«¯å¦‚é‡‡ç”¨jdbcã€odbcã€beelineçš„æ–¹å¼è¿›è¡Œè¿æ¥ã€‚2.é…ç½®å‚æ•°Hiveserver2å…è®¸åœ¨é…ç½®æ–‡ä»¶hive-site.xmlä¸­è¿›è¡Œé…ç½®ç®¡ç†ï¼Œå…·ä½“çš„å‚æ•°ä¸ºï¼šå‚æ•° | å«ä¹‰ |-|-|hive.server2.thrift.min.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º5ã€‚hive.server2.thrift.max.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º500ã€‚hive.server2.thrift.port| TCP çš„ç›‘å¬ç«¯å£ï¼Œé»˜è®¤ä¸º10000ã€‚hive.server2.thrift.bind.host| TCPç»‘å®šçš„ä¸»æœºï¼Œé»˜è®¤ä¸ºlocalhosté…ç½®ç›‘å¬ç«¯å£å’Œè·¯å¾„123456789vi hive-site.xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.48.130&lt;/value&gt;&lt;/property&gt;3. å¯åŠ¨hiveserver2ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨123[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/[hadoop@hadoop001 bin]$ hiveserver2 which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)4. é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿æ¥-n æŒ‡å®šæœºå™¨ç™»é™†çš„åå­—ï¼Œå½“å‰æœºå™¨çš„ç™»é™†ç”¨æˆ·å-u æŒ‡å®šä¸€ä¸ªè¿æ¥ä¸²æ¯æˆåŠŸè¿è¡Œä¸€ä¸ªå‘½ä»¤ï¼Œhiveserver2å¯åŠ¨çš„é‚£ä¸ªçª—å£ï¼Œåªè¦åœ¨å¯åŠ¨beelineçš„çª—å£ä¸­æ‰§è¡ŒæˆåŠŸä¸€æ¡å‘½ä»¤ï¼Œå¦å¤–ä¸ªçª—å£éšå³æ‰“å°ä¸€ä¸ªOKå¦‚æœå‘½ä»¤é”™è¯¯ï¼Œhiveserver2é‚£ä¸ªçª—å£å°±ä¼šæŠ›å‡ºå¼‚å¸¸ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨123456789[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoopwhich: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)scan complete in 4msConnecting to jdbc:hive2://localhost:10000/defaultConnected to: Apache Hive (version 1.1.0-cdh5.7.0)Driver: Hive JDBC (version 1.1.0-cdh5.7.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.7.0 by Apache Hive0: jdbc:hive2://localhost:10000/default&gt;ä½¿ç”¨SQL123456789101112131415160: jdbc:hive2://localhost:10000/default&gt; show databases;INFO : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)INFO : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 secondsINFO : Concurrency mode is disabled, not creating a lock managerINFO : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Starting task [Stage-0:DDL] in serial modeINFO : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 secondsINFO : OK+----------------+--+| database_name |+----------------+--+| default |+----------------+--+1 row selected5.ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿æ¥5.1ä½¿ç”¨mavenæ„å»ºé¡¹ç›®ï¼Œpom.xmlæ–‡ä»¶å¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;5.2JdbcApp.javaæ–‡ä»¶ä»£ç :123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class JdbcApp &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws Exception &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); //select table:ename String tableName = &quot;emp&quot;; String sql = &quot;select ename from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); while(res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2minå¿«é€Ÿäº†è§£ï¼ŒHiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨]]></title>
    <url>%2F2018%2F05%2F01%2F2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[åœ¨äº†è§£å†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨åŒºåˆ«å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸‹Hiveæ¶æ„ ï¼šå¤§å®¶å¯ä»¥ç®€å•çœ‹ä¸€ä¸‹è¿™ä¸ªæ¶æ„å›¾ï¼Œæˆ‘ä»‹ç»å…¶ä¸­è¦ç‚¹ï¼šHiveçš„æ•°æ®åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§ä¸ºæ™®é€šæ•°æ®ï¼Œä¸€ç§ä¸ºå…ƒæ•°æ®ã€‚å…ƒæ•°æ®å­˜å‚¨ç€è¡¨çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¢åˆ æ”¹æŸ¥è®°å½•ï¼Œç±»ä¼¼äºHadoopæ¶æ„ä¸­çš„namespaceã€‚æ™®é€šæ•°æ®å°±æ˜¯è¡¨ä¸­çš„è¯¦ç»†æ•°æ®ã€‚Hiveçš„å…ƒæ•°æ®é»˜è®¤å­˜å‚¨åœ¨derbyä¸­ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹å­˜å‚¨åœ¨MySQLä¸­ã€‚æ™®é€šæ•°æ®å¦‚æ¶æ„å›¾æ‰€ç¤ºå­˜å‚¨åœ¨hdfsä¸­ã€‚ä¸‹é¢æˆ‘ä»¬æ¥ä»‹ç»è¡¨çš„ä¸¤ç§ç±»å‹ï¼šå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨å†…éƒ¨è¡¨ï¼ˆMANAGEDï¼‰ï¼šhiveåœ¨hdfsä¸­å­˜åœ¨é»˜è®¤çš„å­˜å‚¨è·¯å¾„ï¼Œå³defaultæ•°æ®åº“ã€‚ä¹‹ååˆ›å»ºçš„æ•°æ®åº“åŠè¡¨ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„åº”éƒ½åœ¨/user/hive/warehouseä¸‹ï¼Œæ‰€ä»¥åœ¨è¯¥è·¯å¾„ä¸‹çš„è¡¨ä¸ºå†…éƒ¨è¡¨ã€‚å¤–éƒ¨è¡¨ï¼ˆEXTERNALï¼‰ï¼šæŒ‡å®šäº†/user/hive/warehouseä»¥å¤–è·¯å¾„æ‰€åˆ›å»ºçš„è¡¨è€Œå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„ä¸»è¦åŒºåˆ«å°±æ˜¯å†…éƒ¨è¡¨ï¼šå½“åˆ é™¤å†…éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®å’ŒHDFSä¸Šçš„æ™®é€šæ•°æ®éƒ½ä¼šåˆ é™¤ ï¼›å¤–éƒ¨è¡¨ï¼šå½“åˆ é™¤å¤–éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®ä¼šè¢«åˆ é™¤ï¼ŒHDFSä¸Šçš„æ•°æ®ä¸ä¼šè¢«åˆ é™¤ï¼›1.å‡†å¤‡æ•°æ®: æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦cat /tmp/ruozedata.txt 1 jepson 32 110 2 ruoze 22 112 3 www.ruozedata.com 18 120 2.å†…éƒ¨è¡¨æµ‹è¯•ï¼šåœ¨Hiveé‡Œé¢åˆ›å»ºä¸€ä¸ªè¡¨ï¼š123456789hive&gt; create table ruozedata(id int, &gt; name string, &gt; age int, &gt; tele string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\t&apos; &gt; STORED AS TEXTFILE;OKTime taken: 0.759 secondsè¿™æ ·æˆ‘ä»¬å°±åœ¨Hiveé‡Œé¢åˆ›å»ºäº†ä¸€å¼ æ™®é€šçš„è¡¨ï¼Œç°åœ¨ç»™è¿™ä¸ªè¡¨å¯¼å…¥æ•°æ®ï¼š1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;å†…éƒ¨è¡¨åˆ é™¤1hive&gt; drop table ruozedata;3.å¤–éƒ¨è¡¨æµ‹è¯•:åˆ›å»ºå¤–éƒ¨è¡¨å¤šäº†externalå…³é”®å­—è¯´æ˜ä»¥åŠhdfsä¸Šlocation â€˜/hive/externalâ€™12345678hive&gt; create external table exter_ruozedata( &gt; id int, &gt; name string, &gt; age int, &gt; tel string) &gt; location &apos;/hive/external&apos;;OKTime taken: 0.098 secondsåˆ›å»ºå¤–éƒ¨è¡¨ï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨çš„æ—¶å€™åŠ ä¸Šexternalå…³é”®å­—ï¼ŒåŒæ—¶æŒ‡å®šå¤–éƒ¨è¡¨å­˜æ”¾æ•°æ®çš„è·¯å¾„ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä¸æŒ‡å®šå¤–éƒ¨è¡¨çš„å­˜æ”¾è·¯å¾„ï¼Œè¿™æ ·Hiveå°† åœ¨HDFSä¸Šçš„/user/hive/warehouse/æ–‡ä»¶å¤¹ä¸‹ä»¥å¤–éƒ¨è¡¨çš„è¡¨ååˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå¹¶å°†å±äºè¿™ä¸ªè¡¨çš„æ•°æ®å­˜æ”¾åœ¨è¿™é‡Œï¼‰å¤–éƒ¨è¡¨å¯¼å…¥æ•°æ®å’Œå†…éƒ¨è¡¨ä¸€æ ·ï¼š1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;åˆ é™¤å¤–éƒ¨è¡¨1hive&gt; drop table exter_ruozedata;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è°ˆè°ˆæˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜åŠå…¥é—¨]]></title>
    <url>%2F2018%2F05%2F01%2F%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[&#8195;å½“å¹´æˆ‘æ˜¯åšC#+Javaè½¯ä»¶å¼€å‘ï¼Œç„¶åè€ƒå–OCPæ¥äº†ä¸Šæµ·ï¼Œç«‹å¿—è¦åšä¸€åDBAã€‚åªè®°å¾—å½“å¹´è¯•ç”¨æœŸåˆšè¿‡æ—¶ï¼Œé˜´å·®é˜³é”™è½®åˆ°æˆ‘è´Ÿè´£å…¬å¸çš„å¤§æ•°æ®å¹³å°è¿™å—ï¼Œåˆšå¼€å§‹å¾ˆç—›è‹¦ï¼Œä¸€ä¸ªé™Œç”Ÿçš„è¡Œä¸šï¼Œä¸€ä¸ªè®¨è®ºçš„å°ä¼™ä¼´éƒ½æ²¡æœ‰ï¼Œä¸€ä»½ç°æˆèµ„æ–™éƒ½æ²¡æœ‰ï¼Œå¿ƒæƒ…ç„¦è™‘ã€‚åæ¥æˆ‘è°ƒæ•´å¿ƒæ€ï¼Œä»DBè½¬ç§»åˆ°å¯¹å¤§æ•°æ®çš„ç ”ç©¶ï¼Œå†³å®šå•ƒä¸‹è¿™å—ç¡¬éª¨å¤´ï¼ŒæŠŠå®ƒåš¼ç¢ï¼ŒæŠŠå®ƒæ¶ˆåŒ–å¸æ”¶ã€‚&#8195;ç”±äºå½“æ—¶å…¬å¸éƒ½æ˜¯CDHç¯å¢ƒï¼Œåˆšå¼€å§‹å®‰è£…å¡äº†å¾ˆä¹…éƒ½è¿‡ä¸å»ï¼Œåé¢é€‰æ‹©åœ¨çº¿å®‰è£…ï¼Œå¾ˆæ…¢ï¼Œæœ‰æ—¶éœ€è¦1å¤©ã€‚åæ¥å®‰è£…HDFS ,YARN,HIVEç»„ä»¶ï¼Œä¸è¿‡å¯¹å®ƒä»¬ä¸ç†è§£ï¼Œä¸æ˜ç™½ï¼Œæœ‰æ—¶å¾ˆå›°æƒ‘ã€‚è¿™æ ·çš„è¿‡ç¨‹å¤§æ¦‚æŒç»­ä¸‰ä¸ªæœˆäº†ã€‚&#8195;åæ¥çœ‹äº†å¾ˆå¤šåšæ–‡ï¼Œéƒ½æ˜¯Apache Hadoopç‰ˆæœ¬æ­å»ºï¼Œäºæ˜¯æˆ‘å…ˆè¯•è¯•ç”¨Apache Hadoopæ­å»ºéƒ¨ç½²å•èŠ‚ç‚¹å’Œé›†ç¾¤ï¼Œç„¶åé…ç½®HAï¼Œæœ€åæˆ‘å‘ç°è‡ªå·±æ¯”è¾ƒå–œæ¬¢è¿™ç§æ–¹å¼ï¼Œå› ä¸ºæˆ‘èƒ½äº†è§£å…¶é…ç½®å‚æ•°ï¼Œé…ç½®æ–‡ä»¶å’Œå¸¸è§„å‘½ä»¤ç­‰ç­‰ï¼Œå†å›å¤´å»å¯¹æ¯”CDHå®‰è£…HDFSæœåŠ¡ï¼ŒçœŸæ˜¯å¤ªçˆ½äº†ï¼Œå› ä¸ºApache Hadoopç‰ˆæœ¬æœ‰çœŸæ­£ä½“éªŒæ„Ÿï¼Œè¿™æ—¶æˆ‘å°±è¿…é€Ÿè°ƒæ•´æ–¹å‘ : å…ˆApacheç‰ˆæœ¬ï¼Œå†CDHã€‚&#8195;ç”±äºå…¬å¸é¡¹ç›®ç¯å¢ƒï¼Œæ¨è¿›è‡ªå·±å®åœ¨å¤ªæ…¢ï¼Œäºæ˜¯æˆ‘åœ¨ç½‘ä¸Šçœ‹å„ç§ç›¸å…³è§†é¢‘æ•™ç¨‹ï¼›åŠ nç§ç¾¤ï¼Œåœ¨ç¾¤é‡Œæ½œæ°´ï¼Œçœ‹æ°´å‹ä»¬æçš„é—®é¢˜è‡ªå·±ä¼šä¸ä¼šï¼Œä¸ä¼šå°±å»æŸ¥èµ„æ–™ï¼Œä¼šå°±å¸®åŠ©ä»–ä»¬ä¸€èµ·ç ”ç©¶å­¦ä¹ è¿›æ­¥ã€‚&#8195;åæ¥è¿™æ ·çš„è¿›åº¦å¤ªæ…¢äº†ï¼Œå› ä¸ºå¾ˆå¤šç¾¤éƒ½æ˜¯æ‰“å¹¿å‘Šï¼Œæ½œæ°´ï¼Œæ²¡æœ‰çœŸæ­£çš„æŠ€æœ¯è®¨è®ºæ°›å›´ï¼Œäºæ˜¯æˆ‘è¿…é€Ÿè°ƒæ•´æ–¹å‘ï¼Œè‡ªå·±å»ºä¸ªQQç¾¤ï¼Œæ…¢æ…¢æ‹›å…µä¹°é©¬ï¼Œå’Œç®¡ç†å‘˜ä»¬ä¸€èµ·å»ç®¡ç†ï¼Œåœ¨è¿‡å»çš„ä¸¤å¹´é‡Œæˆ‘ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šçŸ¥è¯†å’Œè®¤è¯†å’Œæˆ‘ä¸€æ ·å‰è¿›çš„å°ä¼™ä¼´ä»¬ï¼Œç°åœ¨ä¹Ÿæœ‰å¾ˆå¤šå·²æˆä¸ºfriendsã€‚&#8195;æ¯å½“å¤œæ™šï¼Œæˆ‘å°±ä¼šæ·±æ·±æ€è€ƒä»…å‡­å…¬å¸é¡¹ç›®,ç½‘ä¸Šå…è´¹è¯¾ç¨‹è§†é¢‘ï¼ŒQQç¾¤ç­‰ï¼Œè¿˜æ˜¯ä¸å¤Ÿçš„ï¼Œäºæ˜¯æˆ‘å¼€å§‹å’¨è¯¢åŸ¹è®­æœºæ„çš„è¯¾ç¨‹ï¼Œåœ¨è¿™é‡Œæé†’å„ä½å°ä¼™ä¼´ä»¬ï¼ŒæŠ¥ç­ä¸€å®šè¦æ“¦äº®çœ¼ç›ï¼Œé€‰æ‹©è€å¸ˆå¾ˆé‡è¦ï¼ŒçœŸå¿ƒå¾ˆé‡è¦ï¼Œè®¸å¤šåŸ¹è®­æœºæ„çš„è€å¸ˆéƒ½æ˜¯Javaè½¬çš„ï¼Œè®²çš„æ˜¯å…¨æ˜¯åŸºç¡€ï¼Œæ ¹æœ¬æ²¡æœ‰ä¼ä¸šé¡¹ç›®å®æˆ˜ç»éªŒï¼›è¿˜æœ‰ä¸è¦è·Ÿé£ï¼Œä¸€å®šçœ‹ä»”ç»†çœ‹æ¸…æ¥šè¯¾ç¨‹æ˜¯å¦ç¬¦åˆå½“å‰çš„ä½ ã€‚&#8195;è¿™æ—¶è¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œäºæ˜¯æˆ‘å¼€å§‹æ¯å¤©ä¸Šä¸‹ç­åœ°é“ä¸Šçœ‹æŠ€æœ¯åšå®¢ï¼Œç§¯æåˆ†äº«ã€‚ç„¶åå†ç”³è¯·åšå®¢ï¼Œå†™åšæ–‡ï¼Œå†™æ€»ç»“ï¼ŒåšæŒæ¯æ¬¡åšå®Œä¸€æ¬¡å®éªŒå°±å°†åšæ–‡ï¼Œæ¢³ç†å¥½ï¼Œå†™å¥½ï¼Œè¿™æ ·ä¹…è€Œä¹…ä¹‹ï¼ŒçŸ¥è¯†ç‚¹å°±æ…¢æ…¢å¤¯å®ç§¯ç´¯äº†ã€‚&#8195;å†ç€åé¢å°±å¼€å§‹å—é‚€å‡ å¤§åŸ¹è®­æœºæ„åšå…¬å¼€è¯¾ï¼Œå†ä¸€æ¬¡å°†çŸ¥è¯†ç‚¹æ¢³ç†äº†ï¼Œä¹Ÿè®¤è¯†äº†æ–°çš„å°ä¼™ä¼´ä»¬ï¼Œæˆ‘ä»¬æœ‰ç€ç›¸åŒçš„æ–¹å‘å’Œç›®æ ‡ï¼Œæˆ‘ä»¬å°½æƒ…çš„è®¨è®ºç€å¤§æ•°æ®çš„çŸ¥è¯†ç‚¹ï¼Œæ…¢æ…¢æœç€æˆ‘ä»¬å¿ƒç›®ä¸­çš„ç›®æ ‡è€ŒåŠªåŠ›ç€ï¼ä»¥ä¸ŠåŸºæœ¬å°±æ˜¯æˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜ï¼Œä¸‹é¢æˆ‘æ¥è°ˆè°ˆæˆ‘å¯¹å¤§æ•°æ®å…¥é—¨çš„æ„Ÿæ‚Ÿã€‚1. å¿ƒæ€è¦ç«¯æ­£ã€‚æ—¢ç„¶æƒ³è¦ä»äº‹è¿™è¡Œï¼Œé‚£ä¹ˆä¸€å®šè¦ä¸‹å®šå†³å¿ƒï¼Œå½“ç„¶ä»˜å‡ºæ˜¯è‚¯å®šå¤§å¤§çš„ï¼Œä¸å…‰å…‰æ˜¯æ¯›çˆ·çˆ·ï¼Œè€Œæ›´å¤šçš„ä»˜å‡ºæ˜¯è‡ªå·±çš„é‚£ä¸€ä»½åšæŒï¼Œå‡¡äº‹è´µåœ¨åšæŒï¼ŒçœŸçœŸä½“ç°åœ¨è¿™é‡Œã€‚åæ¥æˆ‘å°†æˆ‘è€å©†ä»åŒ–å·¥å®éªŒå®¤åˆ†æå‘˜è½¬è¡Œï¼ŒåšPythonçˆ¬è™«å’Œæ•°æ®åˆ†æï¼Œå½“ç„¶è¿™ä¸ªä¸»è¦è¿˜æ˜¯é å¥¹çš„é‚£ä»½åšæŒã€‚2. å¿ƒç›®ä¸­è¦æœ‰è®¡åˆ’ã€‚å…ˆå­¦ä¹ Linuxå’ŒShellï¼Œå†å­¦ä¹ æ•°æ®åº“å’ŒSQLï¼Œå†å­¦ä¹ Javaå’ŒScalaï¼Œç„¶åå­¦ä¹ Apache Haoopã€Hiveã€Kafkaã€Sparkï¼Œæœå¤§æ•°æ®ç ”å‘æˆ–å¼€å‘è€ŒåŠªåŠ›ç€ã€‚3. å„ç§æ–¹å¼å­¦ä¹ ã€‚QQç¾¤ï¼Œåšå®¢ï¼Œä¸Šä¸‹ç­çœ‹æŠ€æœ¯æ–‡ç« ï¼Œé€‰æ‹©å¥½çš„è€å¸ˆå’Œè¯¾ç¨‹åŸ¹è®­ï¼Œ(æ“¦äº®çœ¼ç›ï¼Œå¾ˆå¤šè§†é¢‘ï¼Œå¾ˆå¤šå¤§æ•°æ®è€å¸ˆéƒ½æ˜¯çæ‰¯çš„ï¼Œæœ€ç»ˆæ€»ç»“ä¸€å¥è¯ï¼Œä¸åœ¨ä¼ä¸šä¸Šç­çš„æ•™å¤§æ•°æ®éƒ½æ˜¯è€æµæ°“çš„ã€‚)å¯ä»¥åŠ é€Ÿè‡ªå·±å‰è¿›çš„é©¬æ‹‰æ¾é‡Œç¨‹ï¼Œå…¶å®ä¸€èˆ¬éƒ½è¦çœ‹å¤§å®¶æ€ä¹ˆè¡¡é‡åŸ¹è®­è¿™ä¸ªäº‹çš„ï¼Œtimeå’Œmoneyçš„æŠ‰æ‹©ï¼Œä»¥åŠå¿«é€Ÿjumpåçš„é«˜è–ªã€‚4. é¡¹ç›®ç»éªŒã€‚å¾ˆå¤šå°ç™½éƒ½æ²¡æœ‰é¡¹ç›®ç»éªŒä¹Ÿæ²¡æœ‰é¢è¯•ç»éªŒå’ŒæŠ€å·§ï¼Œå±¡å±¡é¢è¯•ä»¥å¤±è´¥å‘Šç»ˆï¼Œè¿™æ—¶å¤§å®¶å¯ä»¥æ‰¾ä½ ä»¬ç†Ÿæ‚‰çš„å°ä¼™ä¼´ä»¬çš„ï¼Œè®©ä»–ç»™ä½ åŸ¹è®­ä»–çš„é¡¹ç›®ï¼Œè¿™æ ·å°±æœ‰äº†ï¼Œå½“ç„¶å¯ä»¥ç›´æ¥äº’è”ç½‘æœç´¢ä¸€ä¸ªå°±è¡Œï¼Œä¸è¿‡ä¸€èˆ¬å¾ˆéš¾æœ‰å®Œæ•´çš„ã€‚è€Œé¢è¯•ï¼Œå°±çœ‹çœ‹å…¶ä»–äººé¢è¯•åˆ†äº«ï¼Œå­¦ä¹ ä»–äººã€‚æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚æœ€åï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚]]></content>
      <categories>
        <category>æœ‰ç¼˜å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>äººç”Ÿæ„Ÿæ‚Ÿ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F27%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Œä¸‰ç§æ–¹å¼ï¼ä¸€.ä¸´æ—¶å‡½æ•°ideaç¼–å†™udfæ‰“åŒ…Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Buildrzä¸Šä¼ è‡³æœåŠ¡å™¨æ·»åŠ jaråŒ…hive&gt;add xxx.jar jar_filepath;æŸ¥çœ‹jaråŒ…hive&gt;list jars;åˆ›å»ºä¸´æ—¶å‡½æ•°hive&gt;create temporary function my_lower as â€˜com.example.hive.udf.Lowerâ€™;äºŒ.æŒä¹…å‡½æ•°ideaç¼–å†™udfæ‰“åŒ…Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Buildrzä¸Šä¼ è‡³æœåŠ¡å™¨ä¸Šä¼ åˆ°HDFS$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jaråˆ›å»ºæŒä¹…å‡½æ•°hive&gt;CREATE FUNCTION myfunc AS â€˜myclassâ€™ USING JAR â€˜hdfs:///path/to/xxx.jarâ€™;æ³¨æ„ç‚¹ï¼šæ­¤æ–¹æ³•åœ¨show functionsæ—¶æ˜¯çœ‹ä¸åˆ°çš„ï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨éœ€è¦ä¸Šä¼ è‡³hdfsä¸‰.æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†Œç¯å¢ƒä»‹ç»ï¼šCentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9ä¸‹è½½æºç hive-1.1.0-cdh5.7.0-src.tar.gzhttp://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gzè§£å‹æºç tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/cd /home/hadoop/hive-1.1.0-cdh5.7.0å°†HelloUDF.javaæ–‡ä»¶å¢åŠ åˆ°HIVEæºç ä¸­cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/ä¿®æ”¹FunctionRegistry.java æ–‡ä»¶1234cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/vi FunctionRegistry.javaåœ¨importä¸­å¢åŠ ï¼šimport org.apache.hadoop.hive.ql.udf.HelloUDF;åœ¨æ–‡ä»¶å¤´éƒ¨ static å—ä¸­æ·»åŠ ï¼šsystem.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);é‡æ–°ç¼–è¯‘cd /home/hadoop/hive-1.1.0-cdh5.7.0mvn clean package -DskipTests -Phadoop-2 -Pdistç¼–è¯‘ç»“æœå…¨éƒ¨ä¸ºï¼šBUILD SUCCESSæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼š/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/targeté…ç½®hiveç¯å¢ƒé…ç½®hiveç¯å¢ƒæ—¶ï¼Œå¯ä»¥å…¨æ–°é…ç½®æˆ–å°†ç¼–è¯‘åå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveç¯å¢ƒä¸­ï¼š7.1. å…¨éƒ¨é…ç½®ï¼šå‚ç…§ä¹‹å‰æ–‡æ¡£ Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²7.2. å°†ç¼–è¯‘åå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveç¯å¢ƒåˆ°/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libä¸‹ï¼Œæ‰¾åˆ°hive-exec-1.1.0-cdh5.7.0.jaråŒ…ï¼Œå¹¶å°†æ—§ç¯å¢ƒä¸­å¯¹ç…§çš„åŒ…æ›¿æ¢æ‰å‘½ä»¤ï¼š1234cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/libmv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bakcd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libcp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/libæœ€ç»ˆå¯åŠ¨hiveæµ‹è¯•ï¼š123hivehive (default)&gt; show functions ; -- èƒ½æŸ¥çœ‹åˆ°æœ‰ helloudfhive(default)&gt;select deptno,dname,helloudf(dname) from dept; -- helloudfå‡½æ•°ç”Ÿæ•ˆ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„ç¼–ç¨‹å¼€å‘ï¼Œä½ ä¼šå—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šIntelliJ IDEA+Maven3.3.91. åˆ›å»ºå·¥ç¨‹æ‰“å¼€IntelliJ IDEAFileâ€“&gt;Newâ€“&gt;Projectâ€¦â€“&gt;Mavené€‰æ‹©Create from archetyeâ€“&gt;org.apache.maven.archety:maven-archetype-quitkstart2. é…ç½®åœ¨å·¥ç¨‹ä¸­æ‰¾åˆ°pom.xmlæ–‡ä»¶ï¼Œæ·»åŠ hadoopã€hiveä¾èµ–3. åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF.javaï¼Œä»£ç å¦‚ä¸‹ï¼šé¦–å…ˆä¸€ä¸ªUDFå¿…é¡»æ»¡è¶³ä¸‹é¢ä¸¤ä¸ªæ¡ä»¶:ä¸€ä¸ªUDFå¿…é¡»æ˜¯org.apache.hadoop.hive.ql.exec.UDFçš„å­ç±»ï¼ˆæ¢å¥è¯è¯´å°±æ˜¯æˆ‘ä»¬ä¸€èˆ¬éƒ½æ˜¯å»ç»§æ‰¿è¿™ä¸ªç±»ï¼‰ä¸€ä¸ªUDFå¿…é¡»è‡³å°‘å®ç°äº†evaluate()æ–¹æ³•4. æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun â€˜HelloUDF.main()â€™5. æ‰“åŒ…åœ¨IDEAèœå•ä¸­é€‰æ‹©viewâ€“&gt;Tool Windowsâ€“&gt;Maven Projectsï¼Œç„¶ååœ¨Maven Projectsçª—å£ä¸­é€‰æ‹©ã€å·¥ç¨‹åã€‘â€“&gt;Lifecycleâ€“&gt;packageï¼Œåœ¨packageä¸­å³é”®é€‰æ‹©Run Maven Buildå¼€å§‹æ‰“åŒ…æ‰§è¡ŒæˆåŠŸååœ¨æ—¥å¿—ä¸­æ‰¾ï¼š[INFO] Building jar: (è·¯å¾„)/hive-1.0.jar]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDLï¼Œä½ çœŸçš„äº†è§£å—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ å…¨é¢å‰–æHive DDLï¼æ¦‚å¿µDatabaseHiveä¸­åŒ…å«äº†å¤šä¸ªæ•°æ®åº“ï¼Œé»˜è®¤çš„æ•°æ®åº“ä¸ºdefaultï¼Œå¯¹åº”äºHDFSç›®å½•æ˜¯/user/hadoop/hive/warehouseï¼Œå¯ä»¥é€šè¿‡hive.metastore.warehouse.dirå‚æ•°è¿›è¡Œé…ç½®ï¼ˆhive-site.xmlä¸­é…ç½®ï¼‰TableHiveä¸­çš„è¡¨åˆåˆ†ä¸ºå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨ ,Hive ä¸­çš„æ¯å¼ è¡¨å¯¹åº”äºHDFSä¸Šçš„ä¸€ä¸ªç›®å½•ï¼ŒHDFSç›®å½•ä¸ºï¼š/user/hadoop/hive/warehouse/[databasename.db]/tablePartitionåˆ†åŒºï¼Œæ¯å¼ è¡¨ä¸­å¯ä»¥åŠ å…¥ä¸€ä¸ªåˆ†åŒºæˆ–è€…å¤šä¸ªï¼Œæ–¹ä¾¿æŸ¥è¯¢ï¼Œæé«˜æ•ˆç‡ï¼›å¹¶ä¸”HDFSä¸Šä¼šæœ‰å¯¹åº”çš„åˆ†åŒºç›®å½•ï¼š/user/hadoop/hive/warehouse/[databasename.db]/tableDDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];IF NOT EXISTSï¼šåŠ ä¸Šè¿™å¥è¯ä»£è¡¨åˆ¤æ–­æ•°æ®åº“æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±ä¼šåˆ›å»ºï¼Œå­˜åœ¨å°±ä¸ä¼šåˆ›å»ºã€‚COMMENTï¼šæ•°æ®åº“çš„æè¿°LOCATIONï¼šåˆ›å»ºæ•°æ®åº“çš„åœ°å€ï¼Œä¸åŠ é»˜è®¤åœ¨/user/hive/warehouse/è·¯å¾„ä¸‹WITH DBPROPERTIESï¼šæ•°æ®åº“çš„å±æ€§Drop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICTï¼šé»˜è®¤æ˜¯restrictï¼Œå¦‚æœè¯¥æ•°æ®åº“è¿˜æœ‰è¡¨å­˜åœ¨åˆ™æŠ¥é”™ï¼›CASCADEï¼šçº§è”åˆ é™¤æ•°æ®åº“(å½“æ•°æ®åº“è¿˜æœ‰è¡¨æ—¶ï¼Œçº§è”åˆ é™¤è¡¨ååœ¨åˆ é™¤æ•°æ®åº“)ã€‚Alter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)Use Database12USE database_name;USE DEFAULT;Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;â€œ | â€ï¼šå¯ä»¥é€‰æ‹©å…¶ä¸­ä¸€ç§â€œ[ ]â€ï¼šå¯é€‰é¡¹LIKE â€˜identifier_with_wildcardsâ€™ï¼šæ¨¡ç³ŠæŸ¥è¯¢æ•°æ®åº“Describe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_nameï¼šæŸ¥çœ‹æ•°æ®åº“çš„æè¿°ä¿¡æ¯å’Œæ–‡ä»¶ç›®å½•ä½ç½®è·¯å¾„ä¿¡æ¯ï¼›EXTENDEDï¼šåŠ ä¸Šæ•°æ®åº“é”®å€¼å¯¹çš„å±æ€§ä¿¡æ¯ã€‚hive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s)Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type1: ARRAY &lt; data_type &gt;map_type1: MAP &lt; primitive_type, data_type &gt;struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameconstraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARYï¼ˆä¸´æ—¶è¡¨ï¼‰Hiveä»0.14.0å¼€å§‹æä¾›åˆ›å»ºä¸´æ—¶è¡¨çš„åŠŸèƒ½ï¼Œè¡¨åªå¯¹å½“å‰sessionæœ‰æ•ˆï¼Œsessioné€€å‡ºåï¼Œè¡¨è‡ªåŠ¨åˆ é™¤ã€‚è¯­æ³•ï¼šCREATE TEMPORARY TABLE â€¦æ³¨æ„ï¼šå¦‚æœåˆ›å»ºçš„ä¸´æ—¶è¡¨è¡¨åå·²å­˜åœ¨ï¼Œé‚£ä¹ˆå½“å‰sessionå¼•ç”¨åˆ°è¯¥è¡¨åæ—¶å®é™…ç”¨çš„æ˜¯ä¸´æ—¶è¡¨ï¼Œåªæœ‰dropæˆ–renameä¸´æ—¶è¡¨åæ‰èƒ½ä½¿ç”¨åŸå§‹è¡¨ä¸´æ—¶è¡¨é™åˆ¶ï¼šä¸æ”¯æŒåˆ†åŒºå­—æ®µå’Œåˆ›å»ºç´¢å¼•EXTERNALï¼ˆå¤–éƒ¨è¡¨ï¼‰Hiveä¸Šæœ‰ä¸¤ç§ç±»å‹çš„è¡¨ï¼Œä¸€ç§æ˜¯Managed Table(é»˜è®¤çš„)ï¼Œå¦ä¸€ç§æ˜¯External Tableï¼ˆåŠ ä¸ŠEXTERNALå…³é”®å­—ï¼‰ã€‚å®ƒä¿©çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼šå½“æˆ‘ä»¬dropè¡¨æ—¶ï¼ŒManaged Tableä¼šåŒæ—¶åˆ å»dataï¼ˆå­˜å‚¨åœ¨HDFSä¸Šï¼‰å’Œmeta dataï¼ˆå­˜å‚¨åœ¨MySQLï¼‰ï¼Œè€ŒExternal Tableåªä¼šåˆ meta dataã€‚1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; );PARTITIONED BYï¼ˆåˆ†åŒºè¡¨ï¼‰äº§ç”ŸèƒŒæ™¯ï¼šå¦‚æœä¸€ä¸ªè¡¨ä¸­æ•°æ®å¾ˆå¤šï¼Œæˆ‘ä»¬æŸ¥è¯¢æ—¶å°±å¾ˆæ…¢ï¼Œè€—è´¹å¤§é‡æ—¶é—´ï¼Œå¦‚æœè¦æŸ¥è¯¢å…¶ä¸­éƒ¨åˆ†æ•°æ®è¯¥æ€ä¹ˆåŠå‘¢ï¼Œè¿™æ˜¯æˆ‘ä»¬å¼•å…¥åˆ†åŒºçš„æ¦‚å¿µã€‚å¯ä»¥æ ¹æ®PARTITIONED BYåˆ›å»ºåˆ†åŒºè¡¨ï¼Œä¸€ä¸ªè¡¨å¯ä»¥æ‹¥æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºä»¥æ–‡ä»¶å¤¹çš„å½¢å¼å•ç‹¬å­˜åœ¨è¡¨æ–‡ä»¶å¤¹çš„ç›®å½•ä¸‹ï¼›åˆ†åŒºæ˜¯ä»¥å­—æ®µçš„å½¢å¼åœ¨è¡¨ç»“æ„ä¸­å­˜åœ¨ï¼Œé€šè¿‡describe tableå‘½ä»¤å¯ä»¥æŸ¥çœ‹åˆ°å­—æ®µå­˜åœ¨ï¼Œä½†æ˜¯è¯¥å­—æ®µä¸å­˜æ”¾å®é™…çš„æ•°æ®å†…å®¹ï¼Œä»…ä»…æ˜¯åˆ†åŒºçš„è¡¨ç¤ºã€‚åˆ†åŒºå»ºè¡¨åˆ†ä¸º2ç§ï¼Œä¸€ç§æ˜¯å•åˆ†åŒºï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨è¡¨æ–‡ä»¶å¤¹ç›®å½•ä¸‹åªæœ‰ä¸€çº§æ–‡ä»¶å¤¹ç›®å½•ã€‚å¦å¤–ä¸€ç§æ˜¯å¤šåˆ†åŒºï¼Œè¡¨æ–‡ä»¶å¤¹ä¸‹å‡ºç°å¤šæ–‡ä»¶å¤¹åµŒå¥—æ¨¡å¼ã€‚å•åˆ†åŒºï¼š123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OKå¤šåˆ†åŒºï¼š123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMATå®˜ç½‘è§£é‡Šï¼š1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]DELIMITEDï¼šåˆ†éš”ç¬¦ï¼ˆå¯ä»¥è‡ªå®šä¹‰åˆ†éš”ç¬¦ï¼‰ï¼›FIELDS TERMINATED BY char:æ¯ä¸ªå­—æ®µä¹‹é—´ä½¿ç”¨çš„åˆ†å‰²ï¼›ä¾‹ï¼š-FIELDS TERMINATED BY â€˜\nâ€™ å­—æ®µä¹‹é—´çš„åˆ†éš”ç¬¦ä¸º\n;COLLECTION ITEMS TERMINATED BY char:é›†åˆä¸­å…ƒç´ ä¸å…ƒç´ ï¼ˆarrayï¼‰ä¹‹é—´ä½¿ç”¨çš„åˆ†éš”ç¬¦ï¼ˆcollectionå•ä¾‹é›†åˆçš„è·Ÿæ¥å£ï¼‰ï¼›MAP KEYS TERMINATED BY charï¼šå­—æ®µæ˜¯K-Vå½¢å¼æŒ‡å®šçš„åˆ†éš”ç¬¦ï¼›LINES TERMINATED BY charï¼šæ¯æ¡æ•°æ®ä¹‹é—´ç”±æ¢è¡Œç¬¦åˆ†å‰²ï¼ˆé»˜è®¤[ \n ]ï¼‰ä¸€èˆ¬æƒ…å†µä¸‹LINES TERMINATED BY charæˆ‘ä»¬å°±ä½¿ç”¨é»˜è®¤çš„æ¢è¡Œç¬¦\nï¼Œåªéœ€è¦æŒ‡å®šFIELDS TERMINATED BY charã€‚åˆ›å»ºdemo1è¡¨ï¼Œå­—æ®µä¸å­—æ®µä¹‹é—´ä½¿ç”¨\tåˆ†å¼€ï¼Œæ¢è¡Œç¬¦ä½¿ç”¨é»˜è®¤\nï¼š123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OKåˆ›å»ºdemo2è¡¨ï¼Œå¹¶æŒ‡å®šå…¶ä»–å­—æ®µï¼š123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED ASï¼ˆå­˜å‚¨æ ¼å¼ï¼‰Create Table As Selectåˆ›å»ºè¡¨ï¼ˆæ‹·è´è¡¨ç»“æ„åŠæ•°æ®ï¼Œå¹¶ä¸”ä¼šè¿è¡ŒMapReduceä½œä¸šï¼‰12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;åŠ è½½æ•°æ®1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;å¤åˆ¶æ•´å¼ è¡¨12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt;å¤åˆ¶è¡¨ä¸­çš„ä¸€äº›å­—æ®µ1create table emp3 as select empno,ename from emp;LIKEä½¿ç”¨likeåˆ›å»ºè¡¨æ—¶ï¼Œåªä¼šå¤åˆ¶è¡¨çš„ç»“æ„ï¼Œä¸ä¼šå¤åˆ¶è¡¨çš„æ•°æ®1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt;å¹¶æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®desc formatted table_nameæŸ¥è¯¢è¡¨çš„è¯¦ç»†ä¿¡æ¯12hive&gt; desc formatted emp;OKcol_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno intDetailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt;é€šè¿‡æŸ¥è¯¢å¯ä»¥åˆ—å‡ºåˆ›å»ºè¡¨æ—¶çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥åœ¨mysqlä¸­æŸ¥è¯¢å‡ºè¿™äº›ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰select * from table_params;æŸ¥è¯¢æ•°æ®åº“ä¸‹çš„æ‰€æœ‰è¡¨1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt;æŸ¥è¯¢åˆ›å»ºè¡¨çš„è¯­æ³•123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later)æŒ‡å®šPURGEåï¼Œæ•°æ®ä¸ä¼šæ”¾åˆ°å›æ”¶ç®±ï¼Œä¼šç›´æ¥åˆ é™¤DROP TABLEåˆ é™¤æ­¤è¡¨çš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚å¦‚æœé…ç½®äº†åƒåœ¾ç®±ï¼ˆå¹¶ä¸”æœªæŒ‡å®šPURGEï¼‰ï¼Œåˆ™å®é™…å°†æ•°æ®ç§»è‡³.Trash / Currentç›®å½•ã€‚å…ƒæ•°æ®å®Œå…¨ä¸¢å¤±åˆ é™¤EXTERNALè¡¨æ—¶ï¼Œè¡¨ä¸­çš„æ•°æ®ä¸ä¼šä»æ–‡ä»¶ç³»ç»Ÿä¸­åˆ é™¤Alter Tableé‡å‘½å1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)ç”¨æˆ·å¯ä»¥ç”¨ ALTER TABLE ADD PARTITION æ¥å‘ä¸€ä¸ªè¡¨ä¸­å¢åŠ åˆ†åŒºã€‚åˆ†åŒºåæ˜¯å­—ç¬¦ä¸²æ—¶åŠ å¼•å·ã€‚æ³¨ï¼šæ·»åŠ åˆ†åŒºæ—¶å¯èƒ½å‡ºç°FAILED: SemanticException table is not partitioned but partition spec existsé”™è¯¯ã€‚åŸå› æ˜¯ï¼Œä½ åœ¨åˆ›å»ºè¡¨æ—¶å¹¶æ²¡æœ‰æ·»åŠ åˆ†åŒºï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨æ—¶åˆ›å»ºåˆ†åŒºï¼Œå†æ·»åŠ åˆ†åŒºã€‚hive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 secondsæŸ¥è¯¢ç»“æœ123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);æŸ¥çœ‹åˆ†åŒºè¯­å¥12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s)æŒ‰åˆ†åŒºæŸ¥è¯¢1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åˆåˆåˆæ˜¯æºç ï¼RDD ä½œä¸šçš„DAGæ˜¯å¦‚ä½•åˆ‡åˆ†çš„ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F23%2F%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[æˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRDDå­˜åœ¨ç€ä¾èµ–å…³ç³»ï¼Œè¿™äº›ä¾èµ–å…³ç³»å½¢æˆäº†æœ‰å‘æ— ç¯å›¾DAGï¼ŒDAGé€šè¿‡DAGSchedulerè¿›è¡ŒStageçš„åˆ’åˆ†ï¼Œå¹¶åŸºäºæ¯ä¸ªStageç”Ÿæˆäº†TaskSetï¼Œæäº¤ç»™TaskSchedulerã€‚é‚£ä¹ˆè¿™æ•´ä¸ªè¿‡ç¨‹åœ¨æºç ä¸­æ˜¯å¦‚ä½•ä½“ç°çš„å‘¢ï¼Ÿ1.ä½œä¸šçš„æäº¤123// SparkContext.scala dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll())123// DAGScheduler.scala def runJob[T, U]( val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)å¯ä»¥çœ‹åˆ°ï¼ŒSparkContextçš„runjobæ–¹æ³•è°ƒç”¨äº†DAGSchedulerçš„runjobæ–¹æ³•æ­£å¼å‘é›†ç¾¤æäº¤ä»»åŠ¡ï¼Œæœ€ç»ˆè°ƒç”¨äº†submitJobæ–¹æ³•ã€‚12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2 def submitJob[T, U]( 3 rdd: RDD[T], 4 func: (TaskContext, Iterator[T]) =&gt; U, 5 partitions: Seq[Int], 6 callSite: CallSite, 7 resultHandler: (Int, U) =&gt; Unit, 8 properties: Properties): JobWaiter[U] = &#123; 9 // Check to make sure we are not launching a task on a partition that does not exist.10 val maxPartitions = rdd.partitions.length11 partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;12 throw new IllegalArgumentException(13 &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +14 &quot;Total number of partitions: &quot; + maxPartitions)15 &#125;1617 val jobId = nextJobId.getAndIncrement()18 if (partitions.size == 0) &#123;19 // Return immediately if the job is running 0 tasks20 return new JobWaiter[U](this, jobId, 0, resultHandler)21 &#125;2223 assert(partitions.size &gt; 0)24 val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]25 val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)26 //ç»™eventProcessLoopå‘é€JobSubmittedæ¶ˆæ¯27 eventProcessLoop.post(JobSubmitted(28 jobId, rdd, func2, partitions.toArray, callSite, waiter,29 SerializationUtils.clone(properties)))30 waiter31 &#125;è¿™é‡Œå‘eventProcessLoopå¯¹è±¡å‘é€äº†JobSubmittedæ¶ˆæ¯ã€‚1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481// DAGScheduler.scala2 private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) eventProcessLoopæ˜¯DAGSchedulerEventProcessLoopç±»çš„ä¸€ä¸ªå¯¹è±¡ã€‚ 1// DAGScheduler.scala 2 private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; 3 case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; 4 dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) 5 6 case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; 7 dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) 8 9 case StageCancelled(stageId) =&gt;10 dagScheduler.handleStageCancellation(stageId)1112 case JobCancelled(jobId) =&gt;13 dagScheduler.handleJobCancellation(jobId)1415 case JobGroupCancelled(groupId) =&gt;16 dagScheduler.handleJobGroupCancelled(groupId)1718 case AllJobsCancelled =&gt;19 dagScheduler.doCancelAllJobs()2021 case ExecutorAdded(execId, host) =&gt;22 dagScheduler.handleExecutorAdded(execId, host)2324 case ExecutorLost(execId, reason) =&gt;25 val filesLost = reason match &#123;26 case SlaveLost(_, true) =&gt; true27 case _ =&gt; false28 &#125;29 dagScheduler.handleExecutorLost(execId, filesLost)3031 case BeginEvent(task, taskInfo) =&gt;32 dagScheduler.handleBeginEvent(task, taskInfo)3334 case GettingResultEvent(taskInfo) =&gt;35 dagScheduler.handleGetTaskResult(taskInfo)3637 case completion: CompletionEvent =&gt;38 dagScheduler.handleTaskCompletion(completion)3940 case TaskSetFailed(taskSet, reason, exception) =&gt;41 dagScheduler.handleTaskSetFailed(taskSet, reason, exception)4243 case ResubmitFailedStages =&gt;44 dagScheduler.resubmitFailedStages()45 &#125;DAGSchedulerEventProcessLoopå¯¹æ¥æ”¶åˆ°çš„æ¶ˆæ¯è¿›è¡Œå¤„ç†ï¼Œåœ¨doOnReceiveæ–¹æ³•ä¸­å½¢æˆä¸€ä¸ªevent loopã€‚æ¥ä¸‹æ¥å°†è°ƒç”¨submitStage()æ–¹æ³•è¿›è¡Œstageçš„åˆ’åˆ†ã€‚2.stageçš„åˆ’åˆ†12345678910111213141516171819202122 1// DAGScheduler.scala 2 private def submitStage(stage: Stage) &#123; 3 val jobId = activeJobForStage(stage)//æŸ¥æ‰¾è¯¥Stageçš„æ‰€æœ‰æ¿€æ´»çš„job 4 if (jobId.isDefined) &#123; 5 logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;) 6 if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; 7 val missing = getMissingParentStages(stage).sortBy(_.id)//å¾—åˆ°Stageçš„çˆ¶Stageï¼Œå¹¶æ’åº 8 logDebug(&quot;missing: &quot; + missing) 9 if (missing.isEmpty) &#123;10 logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)11 submitMissingTasks(stage, jobId.get)//å¦‚æœStageæ²¡æœ‰çˆ¶Stageï¼Œåˆ™æäº¤ä»»åŠ¡é›†12 &#125; else &#123;13 for (parent &lt;- missing) &#123;//å¦‚æœæœ‰çˆ¶Stageï¼Œé€’å½’è°ƒç”¨submiStage14 submitStage(parent)15 &#125;16 waitingStages += stage//å°†å…¶æ ‡è®°ä¸ºç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…ä¸‹æ¬¡æäº¤17 &#125;18 &#125;19 &#125; else &#123;20 abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//å¦‚æœè¯¥Stageæ²¡æœ‰æ¿€æ´»çš„jobï¼Œåˆ™ä¸¢å¼ƒè¯¥Stage21 &#125;22 &#125;åœ¨submitStageæ–¹æ³•ä¸­åˆ¤æ–­Stageçš„çˆ¶Stageæœ‰æ²¡æœ‰è¢«æäº¤ï¼Œç›´åˆ°æ‰€æœ‰çˆ¶Stageéƒ½è¢«æäº¤ï¼Œåªæœ‰ç­‰çˆ¶Stageå®Œæˆåæ‰èƒ½è°ƒåº¦å­Stageã€‚12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123; 3 val missing = new HashSet[Stage] //ç”¨äºå­˜æ”¾çˆ¶Stage 4 val visited = new HashSet[RDD[_]] //ç”¨äºå­˜æ”¾å·²è®¿é—®è¿‡çš„RDD 5 6 val waitingForVisit = new Stack[RDD[_]] 7 def visit(rdd: RDD[_]) &#123; 8 if (!visited(rdd)) &#123; //å¦‚æœRDDæ²¡æœ‰è¢«è®¿é—®è¿‡ï¼Œåˆ™è¿›è¡Œè®¿é—® 9 visited += rdd //æ·»åŠ åˆ°å·²è®¿é—®RDDçš„HashSetä¸­10 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)11 if (rddHasUncachedPartitions) &#123;12 for (dep &lt;- rdd.dependencies) &#123; //è·å–è¯¥RDDçš„ä¾èµ–13 dep match &#123;14 case shufDep: ShuffleDependency[_, _, _] =&gt;//è‹¥ä¸ºå®½ä¾èµ–ï¼Œåˆ™è¯¥RDDä¾èµ–çš„RDDæ‰€åœ¨çš„stageä¸ºçˆ¶stage15 val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//ç”Ÿæˆçˆ¶Stage16 if (!mapStage.isAvailable) &#123;//è‹¥çˆ¶Stageä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ åˆ°çˆ¶Stageçš„HashSETä¸­17 missing += mapStage18 &#125;19 case narrowDep: NarrowDependency[_] =&gt;//è‹¥ä¸ºçª„ä¾èµ–ï¼Œåˆ™ç»§ç»­è®¿é—®çˆ¶RDD20 waitingForVisit.push(narrowDep.rdd)21 &#125;22 &#125;23 &#125;24 &#125;25 &#125;26 waitingForVisit.push(stage.rdd)27 while (waitingForVisit.nonEmpty) &#123;//å¾ªç¯éå†æ‰€æœ‰RDD28 visit(waitingForVisit.pop())29 &#125;30 missing.toList31 &#125;getmissingParentStages()æ–¹æ³•ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚è¿™é‡Œæˆ‘ä»¬è¦æ‡‚å¾—è¿™æ ·ä¸€ä¸ªé€»è¾‘ï¼šæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒStageæ˜¯é€šè¿‡shuffleåˆ’åˆ†çš„ï¼Œæ‰€ä»¥ï¼Œæ¯ä¸€Stageéƒ½æ˜¯ä»¥shuffleå¼€å§‹çš„ï¼Œè‹¥ä¸€ä¸ªRDDæ˜¯å®½ä¾èµ–ï¼Œåˆ™å¿…ç„¶è¯´æ˜è¯¥RDDçš„çˆ¶RDDåœ¨å¦ä¸€ä¸ªStageä¸­ï¼Œè‹¥ä¸€ä¸ªRDDæ˜¯çª„ä¾èµ–ï¼Œåˆ™è¯¥RDDæ‰€ä¾èµ–çš„çˆ¶RDDè¿˜åœ¨åŒä¸€ä¸ªStageä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªé€»è¾‘ï¼Œæ‰¾åˆ°è¯¥Stageçš„çˆ¶Stageã€‚]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>æºç é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveç”Ÿäº§ä¸Šï¼Œå‹ç¼©å’Œå­˜å‚¨ç»“åˆä½¿ç”¨æ¡ˆä¾‹]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[ä½ ä»¬Hiveç”Ÿäº§ä¸Šï¼Œå‹ç¼©å’Œå­˜å‚¨ï¼Œç»“åˆä½¿ç”¨äº†å—ï¼Ÿæ¡ˆä¾‹ï¼šåŸæ–‡ä»¶å¤§å°ï¼š19M1. ORC+Zlipç»“åˆ12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views;ç”¨ORC+Zlipä¹‹åçš„æ–‡ä»¶ä¸º2.8Mç”¨ORC+Zlipä¹‹åçš„æ–‡ä»¶ä¸º2.8M###### 2. Parquet+gzipç»“åˆ12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;ç”¨Parquet+gzipä¹‹åçš„æ–‡ä»¶ä¸º3.9M3. Parquet+Lzoç»“åˆ3.1 å®‰è£…Lzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile3.2 å®‰è£…Lzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile3.3 è½¯è¿æ¥1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop3.4 æµ‹è¯•lzoplzop xxx.logè‹¥ç”Ÿæˆxxx.log.lzoæ–‡ä»¶ï¼Œåˆ™è¯´æ˜æˆåŠŸ3.5 å®‰è£…Hadoop-LZO12345 gitæˆ–svn ä¸‹è½½https://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/3.6 é…ç½®åœ¨core-site.xmlé…ç½®1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;åœ¨mapred-site.xmlä¸­é…ç½® &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;åœ¨hadoop-env.shä¸­é…ç½®export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib3.7 æµ‹è¯•12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views;ç”¨Parquet+Lzo(æœªå»ºç«‹ç´¢å¼•)ä¹‹åçš„æ–‡ä»¶ä¸º5.9M]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>å‹ç¼©æ ¼å¼</tag>
        <tag>æ¡ˆä¾‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveå­˜å‚¨æ ¼å¼çš„ç”Ÿäº§åº”ç”¨]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ç›¸åŒæ•°æ®ï¼Œåˆ†åˆ«ä»¥TextFileã€SequenceFileã€RcFileã€ORCå­˜å‚¨çš„æ¯”è¾ƒã€‚åŸå§‹å¤§å°: 19M1. TextFile(é»˜è®¤) æ–‡ä»¶å¤§å°ä¸º18.1M2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY â€œ\tâ€ STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views;ç”¨SequenceFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º19.6M3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views;ç”¨RcFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º17.9M4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views;ç”¨ORCFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º7.7M5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; ç”¨ORCFileå­˜å‚¨åçš„æ–‡ä»¶ä¸º13.1Mæ€»ç»“ï¼šç£ç›˜ç©ºé—´å ç”¨å¤§å°æ¯”è¾ƒORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>å‹ç¼©æ ¼å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¤§æ•°æ®å‹ç¼©ï¼Œä½ ä»¬çœŸçš„äº†è§£å—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ ä»¬å‰–æå¤§æ•°æ®ä¹‹å‹ç¼©ï¼1. å‹ç¼©çš„å¥½å¤„å’Œåå¤„å¥½å¤„å‡å°‘å­˜å‚¨ç£ç›˜ç©ºé—´é™ä½IO(ç½‘ç»œçš„IOå’Œç£ç›˜çš„IO)åŠ å¿«æ•°æ®åœ¨ç£ç›˜å’Œç½‘ç»œä¸­çš„ä¼ è¾“é€Ÿåº¦ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦åå¤„ç”±äºä½¿ç”¨æ•°æ®æ—¶ï¼Œéœ€è¦å…ˆå°†æ•°æ®è§£å‹ï¼ŒåŠ é‡CPUè´Ÿè·2. å‹ç¼©æ ¼å¼å‹ç¼©æ¯”å‹ç¼©æ—¶é—´å¯ä»¥çœ‹å‡ºï¼Œå‹ç¼©æ¯”è¶Šé«˜ï¼Œå‹ç¼©æ—¶é—´è¶Šé•¿ï¼Œå‹ç¼©æ¯”ï¼šSnappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2å‹ç¼©æ ¼å¼ä¼˜ç‚¹ç¼ºç‚¹gzipå‹ç¼©æ¯”åœ¨å››ç§å‹ç¼©æ–¹å¼ä¸­è¾ƒé«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œåœ¨åº”ç”¨ä¸­å¤„ç†gzipæ ¼å¼çš„æ–‡ä»¶å°±å’Œç›´æ¥å¤„ç†æ–‡æœ¬ä¸€æ ·ï¼›æœ‰hadoop nativeåº“ï¼›å¤§éƒ¨åˆ†linuxç³»ç»Ÿéƒ½è‡ªå¸¦gzipå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿ä¸æ”¯æŒsplitlzoå‹ç¼©/è§£å‹é€Ÿåº¦ä¹Ÿæ¯”è¾ƒå¿«ï¼Œåˆç†çš„å‹ç¼©ç‡ï¼›æ”¯æŒsplitï¼Œæ˜¯hadoopä¸­æœ€æµè¡Œçš„å‹ç¼©æ ¼å¼ï¼›æ”¯æŒhadoop nativeåº“ï¼›éœ€è¦åœ¨linuxç³»ç»Ÿä¸‹è‡ªè¡Œå®‰è£…lzopå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿å‹ç¼©ç‡æ¯”gzipè¦ä½ï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›lzoè™½ç„¶æ”¯æŒsplitï¼Œä½†éœ€è¦å¯¹lzoæ–‡ä»¶å»ºç´¢å¼•ï¼Œå¦åˆ™hadoopä¹Ÿæ˜¯ä¼šæŠŠlzoæ–‡ä»¶çœ‹æˆä¸€ä¸ªæ™®é€šæ–‡ä»¶ï¼ˆä¸ºäº†æ”¯æŒsplitéœ€è¦å»ºç´¢å¼•ï¼Œéœ€è¦æŒ‡å®šinputformatä¸ºlzoæ ¼å¼ï¼‰snappyå‹ç¼©é€Ÿåº¦å¿«ï¼›æ”¯æŒhadoop nativeåº“ä¸æ”¯æŒsplitï¼›å‹ç¼©æ¯”ä½ï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›linuxç³»ç»Ÿä¸‹æ²¡æœ‰å¯¹åº”çš„å‘½ä»¤d. bzip2bzip2æ”¯æŒsplitï¼›å…·æœ‰å¾ˆé«˜çš„å‹ç¼©ç‡ï¼Œæ¯”gzipå‹ç¼©ç‡éƒ½é«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œä½†ä¸æ”¯æŒnativeï¼›åœ¨linuxç³»ç»Ÿä¸‹è‡ªå¸¦bzip2å‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿å‹ç¼©/è§£å‹é€Ÿåº¦æ…¢ï¼›ä¸æ”¯æŒnativeæ€»ç»“ï¼šä¸åŒçš„åœºæ™¯é€‰æ‹©ä¸åŒçš„å‹ç¼©æ–¹å¼ï¼Œè‚¯å®šæ²¡æœ‰ä¸€ä¸ªä¸€åŠ³æ°¸é€¸çš„æ–¹æ³•ï¼Œå¦‚æœé€‰æ‹©é«˜å‹ç¼©æ¯”ï¼Œé‚£ä¹ˆå¯¹äºcpuçš„æ€§èƒ½è¦æ±‚è¦é«˜ï¼ŒåŒæ—¶å‹ç¼©ã€è§£å‹æ—¶é—´è€—è´¹ä¹Ÿå¤šï¼›é€‰æ‹©å‹ç¼©æ¯”ä½çš„ï¼Œå¯¹äºç£ç›˜ioã€ç½‘ç»œioçš„æ—¶é—´è¦å¤šï¼Œç©ºé—´å æ®è¦å¤šï¼›å¯¹äºæ”¯æŒåˆ†å‰²çš„ï¼Œå¯ä»¥å®ç°å¹¶è¡Œå¤„ç†ã€‚åº”ç”¨åœºæ™¯ï¼šä¸€èˆ¬åœ¨HDFS ã€Hiveã€HBaseä¸­ä¼šä½¿ç”¨ï¼›å½“ç„¶ä¸€èˆ¬è¾ƒå¤šçš„æ˜¯ç»“åˆSpark æ¥ä¸€èµ·ä½¿ç”¨ã€‚]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>å‹ç¼©æ ¼å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒSpark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘ç¯å¢ƒå‡†å¤‡JDKï¼š Spark 2.2.0åŠä»¥ä¸Šç‰ˆæœ¬åªæ”¯æŒJDK1.8Mavenï¼š3.3.9è®¾ç½®mavenç¯å¢ƒå˜é‡æ—¶ï¼Œéœ€è®¾ç½®mavenå†…å­˜ï¼šexport MAVEN_OPTS=â€-Xmx2g -XX:ReservedCodeCacheSize=512mâ€Scalaï¼š2.11.8Gitç¼–è¯‘ä¸‹è½½sparkçš„taråŒ…ï¼Œå¹¶è§£å‹12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgzç¼–è¾‘dev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.shæ³¨é‡Šä»¥ä¸‹å†…å®¹ï¼š#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n)æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1ç¼–è¾‘pom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xmlæ·»åŠ åœ¨repositoryså†…&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;å®‰è£…1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarnç¨å¾®ç­‰å¾…å‡ å°æ—¶ï¼Œç½‘ç»œè¾ƒå¥½çš„è¯ï¼Œéå¸¸å¿«ã€‚ä¹Ÿå¯ä»¥å‚è€ƒJå“¥åšå®¢ï¼šåŸºäºCentOS6.4ç¯å¢ƒç¼–è¯‘Spark-2.1.0æºç  http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨1. å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡åŠŸèƒ½å‘½ä»¤å¯åŠ¨åç§°èŠ‚ç‚¹hadoop-daemon.sh start namenodeå¯åŠ¨æ•°æ®èŠ‚ç‚¹hadoop-daemons.sh start datanode slaveå¯åŠ¨secondarynamenodehadoop-daemon.sh start secondarynamenodeå¯åŠ¨resourcemanageryarn-daemon.sh start resourcemanagerå¯åŠ¨nodemanagerbin/yarn-daemons.sh start nodemanageråœæ­¢æ•°æ®èŠ‚ç‚¹hadoop-daemons.sh stop datanode2. å¸¸ç”¨çš„å‘½ä»¤åŠŸèƒ½å‘½ä»¤åˆ›å»ºç›®å½•hdfs dfs -mkdir /inputæŸ¥çœ‹hdfs dfs -lsé€’å½’æŸ¥çœ‹hdfs dfs ls -Rä¸Šä¼ hdfs dfs -putä¸‹è½½hdfs dfs -getåˆ é™¤hdfs dfs -rmä»æœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ°hdfshdfs fs -moveFromLocal /input/xx.txt /input/xx.txtä»hdfså‰ªåˆ‡ç²˜è´´åˆ°æœ¬åœ°hdfs fs -moveToLocal /input/xx.txt /input/xx.txtè¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å¦ä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾hdfs fs -appedToFile ./hello.txt /input/hello.txtæŸ¥çœ‹æ–‡ä»¶å†…å®¹hdfs fs -cat /input/hello.txtæ˜¾ç¤ºä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾hdfs fs -tail /input/hello.txtä»¥å­—ç¬¦ä¸²çš„å½¢å¼æ‰“å°æ–‡ä»¶çš„å†…å®¹hdfs fs -text /input/hello.txtä¿®æ”¹æ–‡ä»¶æƒé™hdfs fs -chmod 666 /input/hello.txtä¿®æ”¹æ–‡ä»¶æ‰€å±hdfs fs -chown ruoze.ruoze /input/hello.txtä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‹·è´åˆ°hdfsé‡Œhdfs fs -copyFromLocal /input/hello.txt /input/ä»hdfsæ‹·è´åˆ°æœ¬åœ°hdfs fs -copyToLocal /input/hello.txt /input/ä»hdfsåˆ°ä¸€ä¸ªè·¯å¾„æ‹·è´åˆ°å¦ä¸€ä¸ªè·¯å¾„hdfs fs -cp /input/xx.txt /output/xx.txtä»hdfsåˆ°ä¸€ä¸ªè·¯å¾„ç§»åŠ¨åˆ°å¦ä¸€ä¸ªè·¯å¾„hdfs fs -mv /input/xx.txt /output/xx.txtç»Ÿè®¡æ–‡ä»¶ç³»ç»Ÿçš„å¯ç”¨ç©ºé—´ä¿¡æ¯hdfs fs -df -h /ç»Ÿè®¡æ–‡ä»¶å¤¹çš„å¤§å°ä¿¡æ¯hdfs fs -du -s -h /ç»Ÿè®¡ä¸€ä¸ªæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶èŠ‚ç‚¹æ•°é‡hadoop fs -count /aaaè®¾ç½®hdfsçš„æ–‡ä»¶å‰¯æœ¬æ•°é‡hadoop fs -setrep 3 /input/xx.txtæ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©1.hadoopå‘½ä»¤ç›´æ¥å›è½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©2.hdfså‘½ä»¤ã€hdfs dfså‘½ä»¤ç›´æ¥å›è½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©3.hadoop fs ç­‰ä»· hdfs dfså‘½ä»¤ï¼Œå’ŒLinuxçš„å‘½ä»¤å·®ä¸å¤šã€‚]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnæ¨¡å¼ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnï¼Ÿå¼€å‘ä¸Šæˆ‘ä»¬é€‰æ‹©local[2]æ¨¡å¼ç”Ÿäº§ä¸Šè·‘ä»»åŠ¡Jobï¼Œæˆ‘ä»¬é€‰æ‹©Spark On Yarnæ¨¡å¼ ï¼Œå°†Spark Applicationéƒ¨ç½²åˆ°yarnä¸­ï¼Œæœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š1.éƒ¨ç½²Applicationå’ŒæœåŠ¡æ›´åŠ æ–¹ä¾¿åªéœ€è¦yarnæœåŠ¡ï¼ŒåŒ…æ‹¬Sparkï¼ŒStormåœ¨å†…çš„å¤šç§åº”ç”¨ç¨‹åºä¸è¦è¦è‡ªå¸¦æœåŠ¡ï¼Œå®ƒä»¬ç»ç”±å®¢æˆ·ç«¯æäº¤åï¼Œç”±yarnæä¾›çš„åˆ†å¸ƒå¼ç¼“å­˜æœºåˆ¶åˆ†å‘åˆ°å„ä¸ªè®¡ç®—èŠ‚ç‚¹ä¸Šã€‚2.èµ„æºéš”ç¦»æœºåˆ¶yarnåªè´Ÿè´£èµ„æºçš„ç®¡ç†å’Œè°ƒåº¦ï¼Œå®Œå…¨ç”±ç”¨æˆ·å’Œè‡ªå·±å†³å®šåœ¨yarné›†ç¾¤ä¸Šè¿è¡Œå“ªç§æœåŠ¡å’ŒApplicatioinï¼Œæ‰€ä»¥åœ¨yarnä¸Šæœ‰å¯èƒ½åŒæ—¶è¿è¡Œå¤šä¸ªåŒç±»çš„æœåŠ¡å’ŒApplicationã€‚Yarnåˆ©ç”¨Cgroupså®ç°èµ„æºçš„éš”ç¦»ï¼Œç”¨æˆ·åœ¨å¼€å‘æ–°çš„æœåŠ¡æˆ–è€…Applicationæ—¶ï¼Œä¸ç”¨æ‹…å¿ƒèµ„æºéš”ç¦»æ–¹é¢çš„é—®é¢˜ã€‚3.èµ„æºå¼¹æ€§ç®¡ç†Yarnå¯ä»¥é€šè¿‡é˜Ÿåˆ—çš„æ–¹å¼ï¼Œç®¡ç†åŒæ—¶è¿è¡Œåœ¨yarné›†ç¾¤ç§çš„å¤šä¸ªæœåŠ¡ï¼Œå¯æ ¹æ®ä¸åŒç±»å‹çš„åº”ç”¨ç¨‹åºå‹åŠ›æƒ…å†µï¼Œè°ƒæ•´å¯¹åº”çš„èµ„æºä½¿ç”¨é‡ï¼Œå®ç°èµ„æºå¼¹æ€§ç®¡ç†ã€‚Spark On Yarnæœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯clusteræ¨¡å¼ï¼Œä¸€ç§æ˜¯clientæ¨¡å¼ã€‚è¿è¡Œclientæ¨¡å¼ï¼šâ€œ./spark-shell â€“master yarnâ€â€œ./spark-shell â€“master yarn-clientâ€â€œ./spark-shell â€“master yarn â€“deploy-mode clientâ€è¿è¡Œçš„æ˜¯clusteræ¨¡å¼â€œ./spark-shell â€“master yarn-clusterâ€â€œ./spark-shell â€“master yarn â€“deploy-mode clusterâ€clientå’Œclusteræ¨¡å¼çš„ä¸»è¦åŒºåˆ«ï¼ša. clientçš„driveræ˜¯è¿è¡Œåœ¨å®¢æˆ·ç«¯è¿›ç¨‹ä¸­b. clusterçš„driveræ˜¯è¿è¡Œåœ¨Application Masterä¹‹ä¸­]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>é«˜çº§</tag>
        <tag>æ¶æ„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶ç›¸å…³ç¯å¢ƒï¼šjdk-7u80hadoop-2.6.0-cdh5.7.1 ä¸æ”¯æŒjdk1.8ï¼Œå› æ­¤æ­¤å¤„ä¹Ÿå»¶ç»­jdk1.7apache-maven-3.3.9mysql5.1hadoopä¼ªåˆ†å¸ƒé›†ç¾¤å·²å¯åŠ¨äºŒã€å®‰è£…jdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profileä¸‰ã€å®‰è£…maven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profileå››ã€å®‰è£…mysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges;äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼šè¾“å…¥ï¼šhttp://archive.cloudera.com/cdh5/cdh/5/æ ¹æ®cdhç‰ˆæœ¬é€‰æ‹©å¯¹åº”hiveè½¯ä»¶åŒ…ï¼šhive-1.1.0-cdh5.7.1-src.tar.gzè§£å‹åä½¿ç”¨mavenå‘½ä»¤ç¼–è¯‘æˆå®‰è£…åŒ…å…­ã€ç¼–è¯‘:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# ç¼–è¯‘ç”Ÿæˆçš„åŒ…åœ¨ä»¥ä¸‹ä½ç½®ï¼š# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åæµ‹è¯•12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profileå…«ã€æ›´æ”¹ç¯å¢ƒå˜é‡12345su - hadoopcd /usr/local/hivecd conf1ã€hive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop2ã€hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°$HIVE_HOME/libä¸Šæ–¹çš„hive-site.xmlä½¿ç”¨äº†javaçš„mysqlé©±åŠ¨åŒ…éœ€è¦å°†è¿™ä¸ªåŒ…ä¸Šä¼ åˆ°hiveçš„libç›®å½•ä¹‹ä¸‹è§£å‹ mysql-connector-java-5.1.45.zip å¯¹åº”çš„æ–‡ä»¶åˆ°ç›®å½•å³å¯1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/æœªæ‹·è´æœ‰ç›¸å…³æŠ¥é”™ï¼šThe specified datastore driver (â€œcom.mysql.jdbc.Driverâ€) was not found in the CLASSPATH.Please check your CLASSPATH specification,and the name of the driver.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)ä¿®æ”¹mapred-site.xml1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ä¿®æ”¹yarn-site.xml12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;å¯åŠ¨123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.shå…³é—­1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)1.æ·»åŠ hadoopç”¨æˆ·123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# æ‰¾åˆ°root ALL=(ALL) ALLï¼Œæ·»åŠ hadoop ALL=(ALL) NOPASSWD:ALL2.ä¸Šä¼ å¹¶è§£å‹123[root@hadoop-01 software]# rz #ä¸Šä¼ hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz3.è½¯è¿æ¥1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop4.è®¾ç½®ç¯å¢ƒå˜é‡1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile5.è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt6.åˆ‡æ¢hadoopç”¨æˆ·1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: å¯æ‰§è¡Œæ–‡ä»¶# etc: é…ç½®æ–‡ä»¶# sbin: shellè„šæœ¬ï¼Œå¯åŠ¨å…³é—­hdfs,yarnç­‰7.é…ç½®æ–‡ä»¶12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # é…ç½®è‡ªå·±æœºå™¨çš„IP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;8.é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»8.1å…¬é’¥/å¯†é’¥ é…ç½®æ— å¯†ç ç™»å½•12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys8.2 æŸ¥çœ‹æ—¥æœŸï¼Œçœ‹æ˜¯å¦é…ç½®æˆåŠŸ1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # ç¬¬ä¸€æ¬¡å›è½¦è¾“å…¥yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #ä¸éœ€è¦å›è½¦è¾“å…¥yes,å³OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # ç¬¬ä¸€æ¬¡å›è½¦è¾“å…¥yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #ä¸éœ€è¦å›è½¦è¾“å…¥yes,å³OKSun Aug 20 14:22:29 CST 20179.æ ¼å¼åŒ–å’Œå¯åŠ¨123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found.9.1è§£å†³æ–¹æ³•:æ·»åŠ ç¯å¢ƒå˜é‡12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# å°†export JAVA_HOME=$&#123;JAVA_HOME&#125;æ”¹ä¸ºexport JAVA_HOME=/usr/java/jdk1.8.0_4512345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied9.2è§£å†³æ–¹æ³•:æ·»åŠ æƒé™123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop9.3 ç»§ç»­å¯åŠ¨1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh9.4æ£€æŸ¥æ˜¯å¦æˆåŠŸ123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode9.5è®¿é—®ï¼š http://192.168.137.130:500709.6ä¿®æ”¹dfså¯åŠ¨çš„è¿›ç¨‹ï¼Œä»¥hadoop-01å¯åŠ¨å¯åŠ¨çš„ä¸‰ä¸ªè¿›ç¨‹ï¼šnamenode: hadoop-01 bin/hdfs getconf -namenodesdatanode: localhost datanodes (using default slaves file) etc/hadoop/slavessecondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt;9.7é‡å¯123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxå¸¸ç”¨å‘½ä»¤ï¼ˆäºŒï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆäºŒï¼‰å®æ—¶æŸ¥çœ‹æ–‡ä»¶å†…å®¹ tail filenametail -f filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åï¼Œä¸èƒ½ç›‘è§†æ–‡ä»¶å†…å®¹tail -F filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åï¼Œä¾ç„¶å¯ä»¥ç›‘è§†æ–‡ä»¶å†…å®¹å¤åˆ¶ã€ç§»åŠ¨æ–‡ä»¶cp oldfilename newfilename å¤åˆ¶mv oldfilename newfilename ç§»åŠ¨/é‡å‘½åechoecho â€œxxxâ€ è¾“å‡ºecho â€œxxxâ€ &gt; filename è¦†ç›–echo â€œxxxâ€ &gt;&gt; filename è¿½åŠ åˆ é™¤ rmrm -f å¼ºåˆ¶åˆ é™¤rm -rf å¼ºåˆ¶åˆ é™¤æ–‡ä»¶å¤¹ï¼Œr è¡¨ç¤ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹åŠæ–‡ä»¶å¤¹é‡Œé¢æ–‡ä»¶åˆ«å aliasalias x=â€xxxxxxâ€ ä¸´æ—¶å¼•ç”¨åˆ«åalias x=â€xxxxxxâ€ é…ç½®åˆ°ç¯å¢ƒå˜é‡ä¸­å³ä¸ºæ°¸ä¹…ç”Ÿæ•ˆæŸ¥çœ‹å†å²å‘½ä»¤ historyhistory æ˜¾ç¤ºå‡ºæ‰€æœ‰å†å²è®°å½•history n æ˜¾ç¤ºå‡ºnæ¡è®°å½•!n æ‰§è¡Œç¬¬næ¡è®°å½•ç®¡é“å‘½ä»¤ ï¼ˆ | ï¼‰ç®¡é“çš„ä¸¤è¾¹éƒ½æ˜¯å‘½ä»¤ï¼Œå·¦è¾¹çš„å‘½ä»¤å…ˆæ‰§è¡Œï¼Œæ‰§è¡Œçš„ç»“æœä½œä¸ºå³è¾¹å‘½ä»¤çš„è¾“å…¥æŸ¥çœ‹è¿›ç¨‹ã€æŸ¥çœ‹idã€ç«¯å£ps -ef ï½œgrep è¿›ç¨‹å æŸ¥çœ‹è¿›ç¨‹åŸºæœ¬ä¿¡æ¯netstat -nplï½œgrep è¿›ç¨‹åæˆ–è¿›ç¨‹id æŸ¥çœ‹æœåŠ¡idå’Œç«¯å£æ€æ­»è¿›ç¨‹ killkill -9 è¿›ç¨‹å/pid å¼ºåˆ¶åˆ é™¤kill -9 $(pgrep è¿›ç¨‹å)ï¼šæ€æ­»ä¸è¯¥è¿›ç¨‹ç›¸å…³çš„æ‰€æœ‰è¿›ç¨‹rpm æœç´¢ã€å¸è½½rpm -qa | grep xxx æœç´¢xxxrpm â€“nodeps -e xxx åˆ é™¤xxxâ€“nodeps ä¸éªŒè¯åŒ…çš„ä¾èµ–æ€§æŸ¥è¯¢find è·¯å¾„ -name xxx (æ¨è)which xxxlocal xxxæŸ¥çœ‹ç£ç›˜ã€å†…å­˜ã€ç³»ç»Ÿçš„æƒ…å†µdf -h æŸ¥çœ‹ç£ç›˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µfree -m æŸ¥çœ‹å†…å­˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µtop æŸ¥çœ‹ç³»ç»Ÿæƒ…å†µè½¯è¿æ¥ln -s åŸå§‹ç›®å½• ç›®æ ‡ç›®å½•å‹ç¼©ã€è§£å‹tar -czf å‹ç¼© tar -xzvf è§£å‹zip å‹ç¼© unzip è§£å‹]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸‰ï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆä¸‰ï¼‰ç”¨æˆ·ã€ç”¨æˆ·ç»„ç”¨æˆ·useradd ç”¨æˆ·å æ·»åŠ ç”¨æˆ·userdel ç”¨æˆ·å åˆ é™¤ç”¨æˆ·id ç”¨æˆ·å æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯passwd ç”¨æˆ·å ä¿®æ”¹ç”¨æˆ·å¯†ç su - ç”¨æˆ·å åˆ‡æ¢ç”¨æˆ·ll /home/ æŸ¥çœ‹å·²æœ‰çš„ç”¨æˆ·ç”¨æˆ·ç»„groupadd ç”¨æˆ·ç»„ æ·»åŠ ç”¨æˆ·ç»„cat /etc/group ç”¨æˆ·ç»„çš„æ–‡ä»¶usermod -a -G ç”¨æˆ·ç»„ ç”¨æˆ· å°†ç”¨æˆ·æ·»åŠ åˆ°ç”¨æˆ·ç»„ä¸­ç»™ä¸€ä¸ªæ™®é€šç”¨æˆ·æ·»åŠ sudoæƒé™123vi /etc/sudoers #åœ¨root ALL=(ALL) ALL ä¸‹é¢æ·»åŠ ä¸€è¡Œ ç”¨æˆ· ALL=(ALL) NOPASSWD:ALLä¿®æ”¹æ–‡ä»¶æƒé™chown ä¿®æ”¹æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„æ‰€å±ç”¨æˆ·å’Œç”¨æˆ·ç»„chown -R ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶å¤¹å -R ä¸ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹chown ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶åchmod: ä¿®æ”¹æ–‡ä»¶å¤¹æˆ–è€…æ–‡ä»¶çš„æƒé™chmod -R 700 æ–‡ä»¶å¤¹åchmod 700 æ–‡ä»¶å¤¹år =&gt; 4 w =&gt; 2 x =&gt; 1 åå°æ‰§è¡Œå‘½ä»¤&amp;nohupscreenå¤šäººåˆä½œ screenscreen -list æŸ¥çœ‹ä¼šè¯screen -S å»ºç«‹ä¸€ä¸ªåå°çš„ä¼šè¯screen -r è¿›å…¥ä¼šè¯ctrl+a+d é€€å‡ºä¼šè¯]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸€ï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®æˆ˜å‘½ä»¤ï¼ˆä¸€ï¼‰æŸ¥çœ‹å½“å‰ç›®å½• pwdæŸ¥çœ‹IPifconfig æŸ¥çœ‹è™šæ‹Ÿæœºiphostname ä¸»æœºåå­—i æŸ¥çœ‹ä¸»æœºåæ˜ å°„çš„IPåˆ‡æ¢ç›®å½• cdcd ~ åˆ‡æ¢å®¶ç›®å½•ï¼ˆrootä¸º/rootï¼Œæ™®é€šç”¨æˆ·ä¸º/home/ç”¨æˆ·åï¼‰cd /filename ä»¥ç»å¯¹è·¯å¾„åˆ‡æ¢ç›®å½•cd - è¿”å›ä¸Šä¸€æ¬¡æ“ä½œè·¯å¾„ï¼Œå¹¶è¾“å‡ºè·¯å¾„cd ../ è¿”å›ä¸Šä¸€å±‚ç›®å½•æ¸…ç†æ¡Œé¢ clearæ˜¾ç¤ºå½“å‰ç›®å½•æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ lsls -l(ll) æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ls -la æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´ï¼Œä¾‹ï¼š.sshï¼‰ls -lh æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æ–‡ä»¶å¤§å°ls -lrt æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æŒ‰æ—¶é—´æ’åºæŸ¥çœ‹æ–‡ä»¶å¤¹å¤§å° du -shå‘½ä»¤å¸®åŠ©man å‘½ä»¤å‘½ä»¤ â€“helpåˆ›å»ºæ–‡ä»¶å¤¹ mkdirmkdir -p filename1/filename2 é€’å½’åˆ›å»ºæ–‡ä»¶å¤¹åˆ›å»ºæ–‡ä»¶ touch/vi/echo xx&gt;filenameæŸ¥çœ‹æ–‡ä»¶å†…å®¹cat filename ç›´æ¥æ‰“å°æ‰€æœ‰å†…å®¹more filename æ ¹æ®çª—å£å¤§å°è¿›è¡Œåˆ†é¡µæ˜¾ç¤ºæ–‡ä»¶ç¼–è¾‘ viviåˆ†ä¸ºå‘½ä»¤è¡Œæ¨¡å¼ï¼Œæ’å…¥æ¨¡å¼ï¼Œå°¾è¡Œæ¨¡å¼å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;æ’å…¥æ¨¡å¼ï¼šæŒ‰iæˆ–aé”®æ’å…¥æ¨¡å¼â€”&gt;å‘½ä»¤è¡Œæ¨¡å¼ï¼šæŒ‰Escé”®å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;å°¾è¡Œæ¨¡å¼ï¼šæŒ‰Shiftå’Œ:é”®æ’å…¥æ¨¡å¼dd åˆ é™¤å…‰æ ‡æ‰€åœ¨è¡Œn+dd åˆ é™¤å…‰æ ‡ä»¥ä¸‹çš„nè¡ŒdG åˆ é™¤å…‰æ ‡ä»¥ä¸‹è¡Œgg ç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯G æœ€åä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯shift+$ è¯¥è¡Œæœ€åä¸€ä¸ªå­—æ¯å°¾è¡Œæ¨¡å¼q! å¼ºåˆ¶é€€å‡ºqw å†™å…¥å¹¶é€€å‡ºqw! å¼ºåˆ¶å†™å…¥é€€å‡ºx é€€å‡ºï¼Œå¦‚æœå­˜åœ¨æ”¹åŠ¨ï¼Œåˆ™ä¿å­˜å†é€€å‡º]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFSæ¶æ„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFSæ¶æ„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥HDFSä¸»è¦ç”±3ä¸ªç»„ä»¶æ„æˆï¼Œåˆ†åˆ«æ˜¯NameNodeã€SecondaryNameNodeå’ŒDataNodeï¼ŒHSFSæ˜¯ä»¥master/slaveæ¨¡å¼è¿è¡Œçš„ï¼Œå…¶ä¸­NameNodeã€SecondaryNameNode è¿è¡Œåœ¨masterèŠ‚ç‚¹ï¼ŒDataNodeè¿è¡ŒslaveèŠ‚ç‚¹ã€‚NameNodeå’ŒDataNodeæ¶æ„å›¾NameNode(åç§°èŠ‚ç‚¹)å­˜å‚¨ï¼šå…ƒä¿¡æ¯çš„ç§ç±»ï¼ŒåŒ…å«:æ–‡ä»¶åç§°æ–‡ä»¶ç›®å½•ç»“æ„æ–‡ä»¶çš„å±æ€§[æƒé™,åˆ›å»ºæ—¶é—´,å‰¯æœ¬æ•°]æ–‡ä»¶å¯¹åº”å“ªäº›æ•°æ®å—â€“&gt;æ•°æ®å—å¯¹åº”å“ªäº›datanodeèŠ‚ç‚¹ä½œç”¨ï¼šç®¡ç†ç€æ–‡ä»¶ç³»ç»Ÿå‘½åç©ºé—´ç»´æŠ¤è¿™æ–‡ä»¶ç³»ç»Ÿæ ‘åŠæ ‘ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•ç»´æŠ¤æ‰€æœ‰è¿™äº›æ–‡ä»¶æˆ–ç›®å½•çš„æ‰“å¼€ã€å…³é—­ã€ç§»åŠ¨ã€é‡å‘½åç­‰æ“ä½œDataNode(æ•°æ®èŠ‚ç‚¹)å­˜å‚¨ï¼šæ•°æ®å—ã€æ•°æ®å—æ ¡éªŒã€ä¸NameNodeé€šä¿¡ä½œç”¨ï¼šè¯»å†™æ–‡ä»¶çš„æ•°æ®å—NameNodeçš„æŒ‡ç¤ºæ¥è¿›è¡Œåˆ›å»ºã€åˆ é™¤ã€å’Œå¤åˆ¶ç­‰æ“ä½œé€šè¿‡å¿ƒè·³å®šæœŸå‘NameNodeå‘é€æ‰€å­˜å‚¨æ–‡ä»¶å—åˆ—è¡¨ä¿¡æ¯Scondary NameNode(ç¬¬äºŒåç§°èŠ‚ç‚¹)å­˜å‚¨: å‘½åç©ºé—´é•œåƒæ–‡ä»¶fsimage+ç¼–è¾‘æ—¥å¿—editlogä½œç”¨: å®šæœŸåˆå¹¶fsimage+editlogæ–‡ä»¶ä¸ºæ–°çš„fsimageæ¨é€ç»™NamenNodeå‰¯æœ¬æ”¾ç½®ç­–ç•¥ç¬¬ä¸€å‰¯æœ¬ï¼šæ”¾ç½®åœ¨ä¸Šä¼ æ–‡ä»¶çš„DataNodeä¸Šï¼›å¦‚æœæ˜¯é›†ç¾¤å¤–æäº¤ï¼Œåˆ™éšæœºæŒ‘é€‰ä¸€å°ç£ç›˜ä¸å¤ªæ…¢ã€CPUä¸å¤ªå¿™çš„èŠ‚ç‚¹ä¸Šç¬¬äºŒå‰¯æœ¬ï¼šæ”¾ç½®åœ¨ä¸ç¬¬ä¸€ä¸ªå‰¯æœ¬ä¸åŒçš„æœºæ¶çš„èŠ‚ç‚¹ä¸Šç¬¬ä¸‰å‰¯æœ¬ï¼šä¸ç¬¬äºŒä¸ªå‰¯æœ¬ç›¸åŒæœºæ¶çš„ä¸åŒèŠ‚ç‚¹ä¸Šå¦‚æœè¿˜æœ‰æ›´å¤šçš„å‰¯æœ¬ï¼šéšæœºæ”¾åœ¨èŠ‚ç‚¹ä¸­]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>æ¶æ„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é…ç½®å¤šå°è™šæ‹Ÿæœºä¹‹é—´çš„SSHä¿¡ä»»]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[æœ¬æœºç¯å¢ƒ3å°æœºå™¨æ‰§è¡Œå‘½ä»¤ssh-keygené€‰å–ç¬¬ä¸€å°,ç”Ÿæˆauthorized_keysæ–‡ä»¶hadoop002 hadoop003ä¼ è¾“id_rsa.pubæ–‡ä»¶åˆ°hadoop001hadoop001æœºå™¨ åˆå¹¶id_rsa.pub2ã€id_rsa.pub3åˆ°authorized_keysè®¾ç½®æ¯å°æœºå™¨çš„æƒé™12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keyså°†authorized_keysåˆ†å‘åˆ°hadoop002ã€hadoop003æœºå™¨éªŒè¯(æ¯å°æœºå™¨ä¸Šæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œåªè¾“å…¥yesï¼Œä¸è¾“å…¥å¯†ç ï¼Œè¯´æ˜é…ç½®æˆåŠŸ)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ç¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
