<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[若泽数据课程一览]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[若泽数据课程系列基础班LiunxVM虚拟机安装Liunx常用命令（重点）开发环境搭MySQL源码安装&amp;yum安装CRUD编写权限控制Hadoop架构介绍&amp;&amp;源码编译伪分布式安装&amp;&amp;企业应用HDFS（重点）架构设计副本放置策略读写流程YARN（重点）架构设计工作流程调度管理&amp;&amp;常见参数配置（调优）MapReduce架构设计wordcount原理&amp;&amp;join原理和案例Hive架构设计Hive DDL&amp;DMLjoin在大数据中的使用使用自带UDF和开发自定义UDFSqoop架构设计RDBMS导入导出整合项目将所有组件合作使用。人工智能基础python基础常用库——pandas、numpy、sklearn、keras高级班scala编程（重点）Spark（五星重点）Hadoop高级Hive高级FlumeKafkaHBaseFlinkCDH容器调度平台线下班]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令以及安装mysql]]></title>
    <url>%2F2019%2F05%2F08%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.简介Docker是一个开源的应用容器引擎；是一个轻量级容器技术；Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；运行中的这个镜像称为容器，容器启动是非常快速的。2.核心概念docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；docker客户端(Client)：连接docker主机进行操作；docker仓库(Registry)：用来保存各种打包好的软件镜像；docker镜像(Images)：软件打包好的镜像；放在docker仓库中；docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用3.安装环境1234VM ware Workstation10CentOS-7-x86_64-DVD-1804.isouname -r3.10.0-862.el7.x86_64检查内核版本，必须是3.10及以上 查看命令：uname -r4.在linux虚拟机上安装docker步骤：1、检查内核版本，必须是3.10及以上uname -r2、安装dockeryum install docker3、输入y确认安装Dependency Updated:audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1Complete!(成功标志)4、启动docker123[root@hadoop000 ~]# systemctl start docker[root@hadoop000 ~]# docker -vDocker version 1.13.1, build 8633870/1.13.15、开机启动docker12[root@hadoop000 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6、停止docker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@hadoop000 ~]# systemctl stop docker``` ### 5.常用命令镜像操作|操作|命令|说明||---|---|---|检索 |docker search 关键字 eg：docker search redis| 我们经常去docker hub上检索镜像的详细信息，如镜像的TAG。|拉取 |docker pull 镜像名:tag| :tag是可选的，tag表示标签，多为软件的版本，默认是latest列表| docker images |查看所有本地镜像删除|docker rmi image-id |删除指定的本地镜像当然大家也可以在官网查找：https://hub.docker.com/容器操作软件镜像（QQ安装程序）----运行镜像----产生一个容器（正在运行的软件，运行的QQ）；步骤：- 1、搜索镜像[root@localhost ~]# docker search tomcat- 2、拉取镜像[root@localhost ~]# docker pull tomcat- 3、根据镜像启动容器docker run --name mytomcat -d tomcat:latest- 4、docker ps 查看运行中的容器- 5、 停止运行中的容器docker stop 容器的id- 6、查看所有的容器docker ps -a- 7、启动容器docker start 容器id- 8、删除一个容器 docker rm 容器id- 9、启动一个做了端口映射的tomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-d：后台运行-p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口- 10、为了演示简单关闭了linux的防火墙service firewalld status ；查看防火墙状态service firewalld stop：关闭防火墙systemctl disable firewalld.service #禁止firewall开机启动- 11、查看容器的日志docker logs container-name/container-id更多命令参看https://docs.docker.com/engine/reference/commandline/docker/可以参考镜像文档### 6.使用docker安装mysql- docker pull mysqldocker pull mysqlUsing default tag: latestTrying to pull repository docker.io/library/mysql …latest: Pulling from docker.io/library/mysqla5a6f2f73cd8: Pull complete936836019e67: Pull complete283fa4c95fb4: Pull complete1f212fb371f9: Pull completee2ae0d063e89: Pull complete5ed0ae805b65: Pull complete0283dc49ef4e: Pull completea7e1170b4fdb: Pull complete88918a9e4742: Pull complete241282fa67c2: Pull completeb0fecf619210: Pull completebebf9f901dcc: Pull completeDigest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8Status: Downloaded newer image for docker.io/mysql:latest[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB1- 启动[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB[root@hadoop000 ~]# docker run –name mysql01 -d mysql756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a[root@hadoop000 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES756620c8e583 mysql “docker-entrypoint…” 9 seconds ago Exited (1) 7 seconds ago mysql01[root@hadoop000 ~]# docker logs 756620c8e583error: database is uninitialized and password option is not specifiedYou need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD1可以看到上面启动的方式是错误的，提示我们要带上具体的密码[root@hadoop000 ~]# docker run -p 3306:3306 –name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqleae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc0612345```–name：给新创建的容器命名，此处命名为pwc-mysql-e：配置信息，此处配置mysql的root用户的登陆密码-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...查看是否启动成功123[root@hadoop000 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeae86796e132 mysql &quot;docker-entrypoint...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql02登陆MySQL12345678910111213141516docker exec -it mysql04 /bin/bashroot@e34aba02c0c3:/# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 80Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;其他的高级操作123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci指定mysql的一些配置参数]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2详细介绍]]></title>
    <url>%2F2019%2F04%2F23%2Fspark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Spark发布了最新的版本spark-2.4.2根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的版本介绍Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。 我们强烈建议所有2.4用户升级到此稳定版本。显著的变化SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。详细更改BUGissues内容摘要[ SPARK-26961 ]在Spark Driver中发现Java死锁[ SPARK-26998 ]在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文[ SPARK-27216 ]将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题[ SPARK-27244 ]使用选项logConf = true时密码将以conf的明文形式记录[ SPARK-27267 ]用Snappy 1.1.7.1解压、压缩空序列化数据时失败[ SPARK-27275 ]EncryptedMessage.transferTo中的潜在损坏[ SPARK-27301 ]DStreamCheckpointData因文件系统已缓存而无法清理[ SPARK-27338 ]TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁[ SPARK-27351 ]在仅使用空值列的AggregateEstimation之后的错误outputRows估计[ SPARK-27390 ]修复包名称不匹配[ SPARK-27394 ]当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时[ SPARK-27403 ]修复updateTableStats以使用新统计信息或无更新表统计信息[ SPARK-27406 ]当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断[ SPARK-27419 ]将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败[ SPARK-27453 ]DSV1静默删除DataFrameWriter.partitionBy改进issues内容摘要[ SPARK-27346 ]松开在ExpressionInfo的’examples’字段中换行断言条件[ SPARK-27358 ]将jquery更新为1.12.x以获取安全修复程序[ SPARK-27479 ]隐藏“org.apache.spark.util.kvstore”的API文档工作issues内容摘要[ SPARK-27382 ]在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我司Kafka+Flink+MySQL生产完整案例代码]]></title>
    <url>%2F2018%2F12%2F20%2F%E6%88%91%E5%8F%B8Kafka%2BFlink%2BMySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1.版本信息：Flink Version:1.6.2Kafka Version:0.9.0.0MySQL Version:5.6.212.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]11.74.103.143 2018-12-20 18:12:00 &quot;GET /class/130.html HTTP/1.1&quot; 404 https://search.yahoo.com/search?p=Flink实战3.工程pom.xml12345678910111213141516171819202122232425262728&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;flink.version&gt;1.6.2&lt;/flink.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Flink-Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt;4.sConf类 定义与MySQL连接的JDBC的参数1234567891011package com.soul.conf;/** * @author 若泽数据soulChun * @create 2018-12-20-15:11 */public class sConf &#123; public static final String USERNAME = &quot;root&quot;; public static final String PASSWORD = &quot;www.ruozedata.com&quot;; public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;&#125;5.MySQLSlink类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.soul.kafka;import com.soul.conf.sConf;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * @author 若泽数据soulChun * @create 2018-12-20-15:09 */public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123; try &#123; if (connection == null) &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setString(3, value.f2); preparedStatement.setString(4, value.f3); preparedStatement.setString(5, value.f4); System.out.println(&quot;Start insert&quot;); preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void open(Configuration parms) throws Exception &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; public void close() throws Exception &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;6.数据清洗日期工具类1234567891011121314151617181920212223package com.soul.utils;import org.apache.commons.lang3.time.FastDateFormat;import java.util.Date;/** * @author soulChun * @create 2018-12-19-18:44 */public class DateUtils &#123; private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;); private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;); public static Long getTime(String time) throws Exception&#123; return SOURCE_FORMAT.parse(time).getTime(); &#125; public static String parseMinute(String time) throws Exception&#123; return TARGET_FORMAT.format(new Date(getTime(time))); &#125; //测试一下 public static void main(String[] args) throws Exception&#123; String time = &quot;2018-12-19 18:55:00&quot;; System.out.println(parseMinute(time)); &#125;&#125;7.MySQL建表123456789create table log_info(ID INT NOT NULL AUTO_INCREMENT,IP VARCHAR(50),TIME VARCHAR(50),CourseID VARCHAR(10),Status_Code VARCHAR(10),Referer VARCHAR(100),PRIMARY KEY ( ID ))ENGINE=InnoDB DEFAULT CHARSET=utf8;8.主程序：主要是将time的格式转成yyyyMMddHHmmss,还有取URL中的课程ID，将不是/class开头的过滤掉。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.soul.kafka;import com.soul.utils.DateUtils;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;import java.util.Properties;/** * @author soulChun * @create 2018-12-19-17:23 */public class FlinkCleanKafka &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔 properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔 properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties); DataStream&lt;String&gt; stream = env.addSource(myConsumer);// stream.print().setParallelism(2); DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123; String[] data = value.split(&quot;\\\t&quot;); String CourseID = null; String url = data[2].split(&quot;\\ &quot;)[2]; if (url.startsWith(&quot;/class&quot;)) &#123; String CourseHTML = url.split(&quot;\\/&quot;)[2]; CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));// System.out.println(CourseID); &#125; return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]); &#125; &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123; return value.f2 != null; &#125; &#125;); CleanData.addSink(new MySQLSink()); env.execute(&quot;Flink kafka&quot;); &#125;&#125;9.启动主程序，查看MySQL表数据在递增123456mysql&gt; select count(*) from log_info;+----------+| count(*) |+----------+| 15137 |+----------+Kafka过来的消息是我模拟的，一分钟产生100条。以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)]]></title>
    <url>%2F2018%2F11%2F10%2F%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource%2BSinkToMySQL)%2F</url>
    <content type="text"><![CDATA[1.下载Flink安装包flink下载地址https://archive.apache.org/dist/flink/flink-1.5.0/因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可上传至机器的/opt目录下2.解压tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/3.配置master节点选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。jobmanager.rpc.address: node1(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)rest.port: 8088本次安装 master节点为node1，因为单机，slave节点也为node14.配置slaves将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。5.启动flink集群bin/start-cluster.sh打开 http://node1:8088 查看web页面Task Managers代表当前的flink只有一个节点，每个task还有两个slots6.测试依赖123456789101112131415161718192021222324252627&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;7.Socket测试代码12345678910111213141516171819202122232425262728293031public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; final String hostName; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(&quot;port&quot;); hostName = params.get(&quot;hostname&quot;); &#125; catch (Exception e) &#123; System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + &quot; : &quot; + count; &#125; &#125;&#125;打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)在命令行set MAVEN_OPTS= -Xms128m -Xmx512m继续执行mvn clean install生成FlinkTest.jar找到打成的jar，并upload，开始上传运行参数介绍提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个发送数据12345[root@hadoop000 flink-1.5.0]# nc -l 8099aaa bbbaaa cccaaa bbbbbb ccc点开running的job，你可以看见接收的字节数等信息到log目录下可以清楚的看见输出1234567891011[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.outaaa : 1ccc : 1ccc : 1bbb : 1ccc : 1bbb : 1bbb : 1ccc : 1bbb : 1ccc : 1除了可以在界面提交，还可以将jar上传的linux中进行提交任务运行flink上传的jar1bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1其他步骤一致。8.使用kafka作为source加上依赖1234&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;1234567891011121314151617public class KakfaSource010 &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); //可以通过正则表达式来匹配合适的topic FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties); //配置从最新的地方开始消费 kafkaSource.setStartFromLatest(); //使用addsource，将kafka的输入转变为datastream DataStream&lt;String&gt; consume = env.addSource(wordfre); ... //process and sink env.execute(&quot;KakfaSource010&quot;); &#125;&#125;9.使用mysql作为sinkflink本身并没有提供datastream输出到mysql，需要我们自己去实现首先，导入依赖12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt;自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下12345678910111213141516171819202122public class MysqlSink implements SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; String username = &quot;mysql.user&quot;; String password = &quot;mysql.password&quot;; String drivername = &quot;mysql.driver&quot;; String dburl = &quot;mysql.url&quot;; @Override public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下12345678910111213141516171819202122232425262728public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement preparedStatement = null; private String userName = null; private String password = null; private String driverName = null; private String DBUrl = null; public MysqlSink() &#123; userName = &quot;mysql.username&quot;; password = &quot;mysql.password&quot;; driverName = &quot;mysql.driverName&quot;; DBUrl = &quot;mysql.DBUrl&quot;; &#125; public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; if(connection==null)&#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; String sql =&quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,value.f0); preparedStatement.setString(2,value.f1); preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0 &#125; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; @Override public void close() throws Exception &#123; if(preparedStatement!=null)&#123; preparedStatement.close(); &#125; if(connection!=null)&#123; connection.close(); &#125; &#125;&#125;Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。使用这个mysqlsink也非常简单1//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());10.总结本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark 技术团队开源机器学习平台 MLflow]]></title>
    <url>%2F2018%2F06%2F12%2FApache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow%2F</url>
    <content type="text"><![CDATA[近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。MLflow 正是受现有的 ML 平台启发，主打开放性：开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。MLflow 目前的 alpha 版本包含三个组件：其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。1234567891011121314import mlflow# Log parameters (key-value pairs)mlflow.log_param(&quot;num_dimensions&quot;, 8)mlflow.log_param(&quot;regularization&quot;, 0.1)# Log a metric; metrics can be updated throughout the runmlflow.log_metric(&quot;accuracy&quot;, 0.1)...mlflow.log_metric(&quot;accuracy&quot;, 0.45)# Log artifacts (output files)mlflow.log_artifact(&quot;roc.png&quot;)mlflow.log_artifact(&quot;model.pkl&quot;)MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。123456789101112name: My Projectconda_env: conda.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot; validate: parameters: data_file: path command: &quot;python validate.py &#123;data_file&#125;&quot;MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。12345678time_created: 2018-02-21T13:21:34.12flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn pickled_model: model.pkl]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL 之外部数据源如何成为在企业开发中的一把利器]]></title>
    <url>%2F2018%2F06%2F06%2FSpark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 概述1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。2.读取json文件启动shell进行测试1234567891011121314151617181920//标准写法val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)//另外一种写法spark.read.json(&quot;path&quot;)看看源码这两者之间到底有啥不同呢？/** * Loads a JSON file and returns the results as a `DataFrame`. * * See the documentation on the overloaded `json()` method with varargs for more details. * * @since 1.4.0 */ def json(path: String): DataFrame = &#123; // This method ensures that calls that explicit need single argument works, see SPARK-16009 json(Seq(path): _*) &#125;我们调用josn() 方法其实进行了 overloaded ，我们继续查看 def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*) 这句话是不是很熟悉，其实就是我们的标准写法1234567891011121314151617 scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+3 读取parquet数据12345678910val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+4 读取hive中的数据1234567891011121314151617181920212223242526272829303132spark.sql(&quot;show tables&quot;).show+--------+----------+-----------+|database| tableName|isTemporary|+--------+----------+-----------+| default|states_raw| false|| default|states_seq| false|| default| t1| false|+--------+----------+-----------+spark.table(&quot;states_raw&quot;).show+-----+------+| code| name|+-----+------+|hello| java||hello|hadoop||hello| hive||hello| sqoop||hello| hdfs||hello| spark|+-----+------+scala&gt; spark.sql(&quot;select name from states_raw &quot;).show+------+| name|+------+| java||hadoop|| hive|| sqoop|| hdfs|| spark|+------+5 保存数据注意：保存的文件夹不能存在，否则报错(默认情况下，可以选择不同的模式)：org.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;保存成文本格式，只能保存一列，否则报错：org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;123456789101112131415161718192021222324252627282930val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)//保存df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)结果：[hadoop@hadoop out]$ pwd/home/hadoop/data/out[hadoop@hadoop out]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:34 _SUCCESS[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt MichaelAndyJustin//保存为json格式df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)结果[hadoop@hadoop data]$ cd out1[hadoop@hadoop out1]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:35 _SUCCESS[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;上面说了在保存数据时如果目录已经存在，在默认模式下会报错，那我们下面讲解保存的几种模式：6 读取mysql中的数据1234567891011121314151617181920212223val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;).option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123456&quot;).load()scala&gt; jdbcDF.printSchemaroot |-- TBL_ID: long (nullable = false) |-- CREATE_TIME: integer (nullable = false) |-- DB_ID: long (nullable = true) |-- LAST_ACCESS_TIME: integer (nullable = false) |-- OWNER: string (nullable = true) |-- RETENTION: integer (nullable = false) |-- SD_ID: long (nullable = true) |-- TBL_NAME: string (nullable = true) |-- TBL_TYPE: string (nullable = true) |-- VIEW_EXPANDED_TEXT: string (nullable = true) |-- VIEW_ORIGINAL_TEXT: string (nullable = true)jdbcDF.show7 spark SQL操作mysql表数据123456789101112131415161718192021222324252627282930313233CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url &quot;jdbc:mysql://localhost:3306&quot;, dbtable &quot;basic01.tbls&quot;, user &apos;root&apos;, password &apos;123456&apos;, driver &quot;com.mysql.jdbc.Driver&quot;);查看：show tables;default states_raw falsedefault states_seq falsedefault t1 falsejdbctable trueselect * from jdbctable;1 1519944170 6 0 hadoop 0 1 page_views MANAGED_TABLE NULL NULL2 1519944313 6 0 hadoop 0 2 page_views_bzip2 MANAGED_TABLE NULL NULL3 1519944819 6 0 hadoop 0 3 page_views_snappy MANAGED_TABLE NULL NULL21 1520067771 6 0 hadoop 0 21 tt MANAGED_TABLE NULL NULL22 1520069148 6 0 hadoop 0 22 page_views_seq MANAGED_TABLE NULL NULL23 1520071381 6 0 hadoop 0 23 page_views_rcfile MANAGED_TABLE NULL NULL24 1520074675 6 0 hadoop 0 24 page_views_orc_zlib MANAGED_TABLE NULL NULL27 1520078184 6 0 hadoop 0 27 page_views_lzo_index MANAGED_TABLE NULL NULL30 1520083461 6 0 hadoop 0 30 page_views_lzo_index1 MANAGED_TABLE NULL NULL31 1524370014 1 0 hadoop 0 31 t1 EXTERNAL_TABLE NULL NULL37 1524468636 1 0 hadoop 0 37 states_raw MANAGED_TABLE NULL NULL38 1524468678 1 0 hadoop 0 38 states_seq MANAGED_TABLE NULL NULLmysql中的tbls的数据已经存在jdbctable表中了。jdbcDF.show8 分区推测（Partition Discovery）表分区是在像Hive这样的系统中使用的常见优化方法。 在分区表中，数据通常存储在不同的目录中，分区列值在每个分区目录的路径中编码。 所有内置的文件源（包括Text / CSV / JSON / ORC / Parquet）都能够自动发现和推断分区信息。 例如，我们创建如下的目录结构;123456789hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN添加json文件：people.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN我们使用spark sql读取外部数据源：1234567891011121314151617val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+我们改变读取的目录12345678910111213141516val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) |-- country: string (nullable = true)scala&gt; df.show+----+-------+-------+| age| name|country|+----+-------+-------+|null|Michael| CN|| 30| Andy| CN|| 19| Justin| CN|+----+-------+-------+大家有没有发现什么呢？Spark SQL将自动从路径中提取分区信息。注意，分区列的数据类型是自动推断的。目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以通过spark.sql.sources.partitionColumnTypeInference.enabled进行配置，默认为true。当禁用类型推断时，字符串类型将用于分区列。从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下找到分区。对于上面的示例，如果用户将路径/table/gender=male传递给SparkSession.read.parquet或SparkSession.read.load，则不会将性别视为分区列。如果用户需要指定启动分区发现的基本路径，则可以basePath在数据源选项中进行设置。例如，当path/to/table/gender=male是数据路径并且用户将basePath设置为path/to/table/时，性别将是分区列。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming 状态管理函数的选择比较]]></title>
    <url>%2F2018%2F06%2F06%2FSparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[一、updateStateByKey官网原话：1In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.也即是说它会统计全局的key的状态，就算没有数据输入，它也会在每一个批次的时候返回之前的key的状态。缺点：若数据量太大的话，需要checkpoint的数据会占用较大的存储，效率低下。程序示例如下：12345678910111213141516171819202122232425262728object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //注意：要使用updateStateByKey必须设置checkpoint目录 ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; /*状态更新函数 * @param currentValues key相同value形成的列表 * @param preValues key对应的value，前一状态 * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seq列表中所有value求和 val pre = preValues.getOrElse(0) //获取上一状态值 Some(curr + pre) &#125; &#125;二、mapWithStatemapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。效率更高，生产中建议使用官方代码如下：1234567891011121314151617181920212223242526272829303132object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulNetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) val initialRDD = ssc.sparkContext .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1))) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;三、源码分析upateStateByKey：map返回的是MappedDStream，而MappedDStream并没有updateStateByKey方法，并且它的父类DStream中也没有该方法。但是DStream的伴生对象中有一个隐式转换函数：123456object DStream &#123; implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairDStreamFunctions[K, V] = &#123; new PairDStreamFunctions[K, V](stream) &#125;跟进去 PairDStreamFunctions ，发现最终调用的是自己的updateStateByKey。其中updateFunc就要传入的参数，他是一个函数，Seq[V]表示当前key对应的所有值，123456Option[S] 是当前key的历史状态，返回的是新的状态。def updateStateByKey[S: ClassTag]( updateFunc: (Seq[V], Option[S]) =&gt; Option[S] ): DStream[(K, S)] = ssc.withScope &#123; updateStateByKey(updateFunc, defaultPartitioner())&#125;最终调用：12345678910def updateStateByKey[S: ClassTag]( updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)], partitioner: Partitioner, rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123; val cleanedFunc = ssc.sc.clean(updateFunc) val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123; cleanedFunc(it) &#125; new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)&#125;再跟进去 new StateDStream:在这里面new出了一个StateDStream对象。在其compute方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中StateDStream的父类计算出的RDD（本次batch的单词计数）分别是prevStateRDD和parentRDD，然后在调用 computeUsingPreviousRDD 方法：1234567891011121314151617181920private [this] def computeUsingPreviousRDD( batchTime: Time, parentRDD: RDD[(K, V)], prevStateRDD: RDD[(K, S)]) = &#123; // Define the function for the mapPartition operation on cogrouped RDD; // first map the cogrouped tuple to tuples of required type, // and then apply the update function val updateFuncLocal = updateFunc val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123; val i = iterator.map &#123; t =&gt; val itr = t._2._2.iterator val headOption = if (itr.hasNext) Some(itr.next()) else None (t._1, t._2._1.toSeq, headOption) &#125; updateFuncLocal(batchTime, i) &#125; val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner) val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning) Some(stateRDD)&#125;在这里两个RDD进行cogroup然后应用updateStateByKey传入的函数。我们知道cogroup的性能是比较低下，参考http://lxw1234.com/archives/2015/07/384.htm。mapWithState:123456789@Experimentaldef mapWithState[StateType: ClassTag, MappedType: ClassTag]( spec: StateSpec[K, V, StateType, MappedType] ): MapWithStateDStream[K, V, StateType, MappedType] = &#123; new MapWithStateDStreamImpl[K, V, StateType, MappedType]( self, spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]] )&#125;说明：StateSpec 封装了状态管理函数，并在该方法中创建了MapWithStateDStreamImpl对象。MapWithStateDStreamImpl 中创建了一个InternalMapWithStateDStream类型对象internalStream，在MapWithStateDStreamImpl的compute方法中调用了internalStream的getOrCompute方法。12345678910111213141516private[streaming] class MapWithStateDStreamImpl[ KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag]( dataStream: DStream[(KeyType, ValueType)], spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType]) extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123; private val internalStream = new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec) override def slideDuration: Duration = internalStream.slideDuration override def dependencies: List[DStream[_]] = List(internalStream) override def compute(validTime: Time): Option[RDD[MappedType]] = &#123; internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125; &#125;InternalMapWithStateDStream中没有getOrCompute方法，这里调用的是其父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream的Compute方法：12345678910111213141516171819202122232425262728293031323334/** Method that generates an RDD for the given time */override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123; // Get the previous state or create a new empty state RDD val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123; case Some(rdd) =&gt; if (rdd.partitioner != Some(partitioner)) &#123; // If the RDD is not partitioned the right way, let us repartition it using the // partition index as the key. This is to ensure that state RDD is always partitioned // before creating another state RDD using it MapWithStateRDD.createFromRDD[K, V, S, E]( rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime) &#125; else &#123; rdd &#125; case None =&gt; MapWithStateRDD.createFromPairRDD[K, V, S, E]( spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)), partitioner, validTime ) &#125; // Compute the new state RDD with previous state RDD and partitioned data RDD // Even if there is no data RDD, use an empty one to create a new state RDD val dataRDD = parent.getOrCompute(validTime).getOrElse &#123; context.sparkContext.emptyRDD[(K, V)] &#125; val partitionedDataRDD = dataRDD.partitionBy(partitioner) val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt; (validTime - interval).milliseconds &#125; Some(new MapWithStateRDD( prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))&#125;根据给定的时间生成一个MapWithStateRDD，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD:dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取partitionedDataRDD。最后将preStateRDD，partitionedDataRDD和用户定义的函数mappingFunction传给新生成的MapWithStateRDD对象返回。后续若有兴趣可以继续跟进MapWithStateRDD的compute方法，限于篇幅不再展示。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统重要参数调优，你知道吗]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%2F</url>
    <content type="text"><![CDATA[当前会话生效ulimit -u -&gt; 查看当前最大进程数ulimit -n -&gt;查看当前最大文件数ulimit -u xxx -&gt; 修改当前最大进程数为xxxulimit -n xxx -&gt; 修改当前最大文件数为xxx永久生效1.vi /etc/security/limits.conf，添加如下的行soft noproc 11000hard noproc 11000soft nofile 4100hard nofile 4100 说明：代表针对所有用户noproc 是代表最大进程数nofile 是代表最大文件打开数2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制：1)、vi /etc/ssh/sshd_config把 UserLogin 的值改为 yes，并把 # 注释去掉2)、重启 sshd 服务/etc/init.d/sshd restart3)、修改所有 linux 用户的环境变量文件：vi /etc/profileulimit -u 10000ulimit -n 4096ulimit -d unlimitedulimit -m unlimitedulimit -s unlimitedulimit -t unlimitedulimit -v unlimited4)、生效source /etc/profile]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark动态内存管理源码解析！]]></title>
    <url>%2F2018%2F06%2F03%2FSpark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81%2F</url>
    <content type="text"><![CDATA[一、Spark内存管理模式Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。12345678// SparkEnv.scala val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false) val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) &#125;二、Spark动态内存管理空间分配相比于Static MemoryManager模式，Unified MemoryManager模型打破了存储内存和运行内存的界限，使每一个内存区能够动态伸缩，降低OOM的概率。由上图可知，executor JVM内存主要由以下几个区域组成：（1）Reserved Memory（预留内存）：这部分内存预留给系统使用，默认为300MB，可通过spark.testing.reservedMemory进行设置。12// UnifiedMemoryManager.scalaprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024另外，JVM内存的最小值也与reserved Memory有关，即minSystemMemory = reserved Memory1.5，即默认情况下JVM内存最小值为300MB1.5=450MB。12// UnifiedMemoryManager.scala val minSystemMemory = (reservedMemory * 1.5).ceil.toLong（2）Spark Memeoy:分为execution Memory和storage Memory。去除掉reserved Memory，剩下usableMemory的一部分用于execution和storage这两类堆内存，默认是0.6，可通过spark.memory.fraction进行设置。例如：JVM内存是1G，那么用于execution和storage的默认内存为（1024-300）*0.6=434MB。1234// UnifiedMemoryManager.scala val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6) (usableMemory * memoryFraction).toLong他们的边界由spark.memory.storageFraction设定，默认为0.5。即默认状态下storage Memory和execution Memory为1：1.1234// UnifiedMemoryManager.scala onHeapStorageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)（3）user Memory:剩余内存，用户根据需要使用，默认占usableMemory的（1-0.6）=0.4.三、内存控制详解首先我们先来了解一下Spark内存管理实现类之前的关系。1.MemoryManager主要功能是：（1）记录用了多少StorageMemory和ExecutionMemory；（2）申请Storage、Execution和Unroll Memory；（3）释放Stroage和Execution Memory。Execution内存用来执行shuffle、joins、sorts和aggegations操作，Storage内存用于缓存和广播数据，每一个JVM中都存在着一个MemoryManager。构造MemoryManager需要指定onHeapStorageMemory和onHeapExecutionMemory参数。123456 // MemoryManager.scalaprivate[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging &#123;创建StorageMemoryPool和ExecutionMemoryPool对象，用来创建堆内或堆外的Storage和Execution内存池，管理Storage和Execution的内存分配。123456789// MemoryManager.scala @GuardedBy(&quot;this&quot;) protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(&quot;this&quot;) protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)默认情况下，不使用堆外内存，可通过saprk.memory.offHeap.enabled设置，默认堆外内存为0，可使用spark.memory.offHeap.size参数设置。123456789101112// All the code you will ever need final val tungstenMemoryMode: MemoryMode = &#123; if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123; require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0, &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;) require(Platform.unaligned(), &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;) MemoryMode.OFF_HEAP &#125; else &#123; MemoryMode.ON_HEAP &#125; &#125;12// MemoryManager.scala protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)释放numBytes字节的Execution内存方法12345678910// MemoryManager.scaladef releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) &#125; &#125;释放指定task的所有Execution内存并将该task标记为inactive。12345// MemoryManager.scala private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123; onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) &#125;释放numBytes字节的Stoarge内存方法1234567// MemoryManager.scaladef releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes) &#125; &#125;释放所有Storage内存方法12345// MemoryManager.scalafinal def releaseAllStorageMemory(): Unit = synchronized &#123; onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() &#125;2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？UnifiedMemoryManage继承了MemoryManager1234567891011// UnifiedMemoryManage.scalaprivate[spark] class UnifiedMemoryManager private[memory] ( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) &#123;重写了maxOnHeapStorageMemory方法，最大Storage内存=最大内存-最大Execution内存。1234// UnifiedMemoryManage.scala override def maxOnHeapStorageMemory: Long = synchronized &#123; maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed &#125;核心方法acquireStorageMemory：申请Storage内存。12345678910111213141516171819202122232425262728293031// UnifiedMemoryManage.scalaoverride def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized &#123; assertInvariants() assert(numBytes &gt;= 0) val (executionPool, storagePool, maxMemory) = memoryMode match &#123; //根据不同的内存模式去创建StorageMemoryPool和ExecutionMemoryPool case MemoryMode.ON_HEAP =&gt; ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP =&gt; ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapMemory) &#125; if (numBytes &gt; maxMemory) &#123; // 若申请内存大于最大内存，则申请失败 logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; + s&quot;memory limit ($maxMemory bytes)&quot;) return false &#125; if (numBytes &gt; storagePool.memoryFree) &#123; // 如果Storage内存池没有足够的内存，则向Execution内存池借用 val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//当Execution内存有空闲时，Storage才能借到内存 executionPool.decrementPoolSize(memoryBorrowedFromExecution)//缩小Execution内存 storagePool.incrementPoolSize(memoryBorrowedFromExecution)//增加Storage内存 &#125; storagePool.acquireMemory(blockId, numBytes)核心方法acquireExecutionMemory：申请Execution内存。123456789// UnifiedMemoryManage.scalaoverride private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized &#123;//使用了synchronized关键字，调用acquireExecutionMemory方法可能会阻塞，直到Execution内存池有足够的内存。 ... executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize) &#125;方法最后调用了ExecutionMemoryPool的acquireMemory方法，该方法的参数需要两个函数：maybeGrowExecutionPool()和computeMaxExecutionPoolSize()。每个Task能够使用的内存被限制在pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks。其中maxPoolSize代表了execution pool的最大内存，poolSize表示当前这个pool的大小。1234// ExecutionMemoryPool.scala val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks)maybeGrowExecutionPool()方法实现了如何动态增加Execution内存区的大小。在每次申请execution内存的同时，execution内存池会进行多次尝试，每次尝试都可能会回收一些存储内存。123456789101112131415// UnifiedMemoryManage.scala def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123; if (extraMemoryNeeded &gt; 0) &#123;//如果申请的内存大于0 //计算execution可借到的storage内存，是storage剩余内存和可借出内存的最大值 val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage &gt; 0) &#123;//如果可以申请到内存 val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//实际需要的内存，取实际需要的内存和storage内存区域全部可用内存大小的最小值 storagePool.decrementPoolSize(spaceToReclaim)//storage内存区域减少 executionPool.incrementPoolSize(spaceToReclaim)//execution内存区域增加 &#125; &#125; &#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽大数据-零基础学员深圳某司高薪面试题]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[啥也不说！直接上题面试时间：20180531简单说下hdfs读文件和写文件的流程每天数据量有多大？生产集群规模有多大？说几个spark开发中遇到的问题，和解决的方案阐述一下最近开发的项目，以及担任的角色位置kafka有做过哪些调优我们项目中数据倾斜的场景和解决方案零基础➕四个月紧跟若泽大数据学习之后是这样]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从Hive中的stored as file_foramt看hive调优]]></title>
    <url>%2F2018%2F05%2F30%2F%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[一、行式数据库和列式数据库的对比1、存储比较行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。2、压缩比较对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。3、查询比较假设执行的查询操作是：select id,name from table_emp;对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。假设执行的查询操作是：select * from table_emp;对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。二、stored as file_format 详解1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。123456789101112131415161718192021CREATE TABLE tt (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;CREATE TABLE tt2 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;CREATE TABLE tt3 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;#以上三种方式存储的格式都是TEXTFILE。2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较12345678file_format: | SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameTEXTFILE: 只是hive中表数据默认的存储格式，它将所有类型的数据都存储为String类型，不便于数据的解析，但它却比较通用。不具备随机读写的能力。支持压缩。SEQUENCEFILE: 这种储存格式比TEXTFILE格式多了头部、标识、信息长度等信息，这些信息使得其具备随机读写的能力。支持压缩，但压缩的是value。（存储相同的数据，SEQUENCEFILE比TEXTFILE略大）RCFILE（Record Columnar File）: 现在水平上划分为很多个Row Group,每个Row Group默认大小4MB，Row Group内部再按列存储信息。由facebook开源，比标准行式存储节约10%的空间。ORC: 优化过后的RCFile,现在水平上划分为多个Stripes,再在Stripe中按列存储。每个Stripe由一个Index Data、一个Row Data、一个Stripe Footer组成。每个Stripes的大小为250MB，每个Index Data记录的是整型数据最大值最小值、字符串数据前后缀信息，每个列的位置等等诸如此类的信息。这就使得查询十分得高效，默认每一万行数据建立一个Index Data。ORC存储大小为TEXTFILE的40%左右，使用压缩则可以进一步将这个数字降到10%~20%。ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=OrcThe parameters are all placed in the TBLPROPERTIES (see Create Table). They are:Key|Default|Notes|-|-|-|orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)|orc.compress.size|262,144|number of bytes in each compression chunk|orc.stripe.size|67,108,864|number of bytes in each stripe|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)|orc.create.index|true|whether to create row indexes|orc.bloom.filter.columns |””| comma separated list of column names for which bloom filter should be created|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)示例：创建带压缩的ORC存储表1234567create table Addresses ( name string, street string, city string, state string, zip int) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);PARQUET: 存储大小为TEXTFILE的60%~70%，压缩后在20%~30%之间。注意：不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。如果要使用TEXTFILE作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“insert into table table_stored_file_ORC select from table_t0;”创建。或者使用”create table as select from table_t0;”创建。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark之序列化在生产中的应用]]></title>
    <url>%2F2018%2F05%2F29%2FSpark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。Java serialization在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。测试代码：测试结果：Kryo serializationSpark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。测试代码：测试结果：测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。修改代码后在测试一次。测试结果：总结：Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽数据带你随时了解业界面试题，随时跳高薪]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA%2F</url>
    <content type="text"><![CDATA[链家(一面，二面)0.自我介绍1.封装继承多态概念2.mvc设计思想3.线程池,看过源码吗4.ssh框架中分别对应mvc中那一层5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）6.spring ioc aop 原理7.单利模式8.SQL题，想不起来了。。9.jvm 运行时数据区域10.spring mvc知道吗。。11.工厂模式12.mr 计算流程13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费 查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）14.git的使用15.hadoop的理解16.hive内部表和外部表的区别17.hive存储格式和压缩格式18.对spark了解吗？ 当时高级班还没学。。19.hive于关系型数据库的区别20.各种排序 手写堆排序,说说原理21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。22中间也穿插了项目。无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次跳槽经历（阿里/美团/头条/网易/有赞...)]]></title>
    <url>%2F2018%2F05%2F24%2F%E6%9C%89%E8%B5%9E...)%2F</url>
    <content type="text"><![CDATA[为啥跳槽每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。面试过程（先打个广告，有兴趣加入阿里的欢迎发简历至 zhangzb2007@gmail.com，或简书上给我发信息）面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。上半场曹操专车这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。美亚柏科估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。有赞绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。字节跳动(今日头条)HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。下半场一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。美团这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。两点半进去。一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。出来的时候已经是六点半。网易面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。阿里这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。总结从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中自定义UDAF函数生产小案例]]></title>
    <url>%2F2018%2F05%2F23%2FHive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[一、UDAF 回顾1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。2.Hive有两种UDAF：简单和通用简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。3.一个计算函数必须实现的5个方法的具体含义如下：init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。二、需求使用UDAF简单方式实现统计区域产品用户访问排名三、自定义UDAF函数代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hive.org.ruozedata;import java.util.*;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import org.apache.log4j.Logger;public class UserClickUDAF extends UDAF &#123; // 日志对象初始化 public static Logger logger = Logger.getLogger(UserClickUDAF.class); // 静态类实现UDAFEvaluator public static class Evaluator implements UDAFEvaluator &#123; // 设置成员变量，存储每个统计范围内的总记录数 private static Map&lt;String, String&gt; courseScoreMap; private static Map&lt;String, String&gt; city_info; private static Map&lt;String, String&gt; product_info; private static Map&lt;String, String&gt; user_click; //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用 public Evaluator() &#123; init(); &#125; // 初始化函数间传递的中间变量 public void init() &#123; courseScoreMap = new HashMap&lt;String, String&gt;(); city_info = new HashMap&lt;String, String&gt;(); product_info = new HashMap&lt;String, String&gt;(); &#125; //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出 public boolean iterate(String pcid, String pcname, String pccount) &#123; if (pcid == null || pcname == null || pccount == null) &#123; return true; &#125; if (pccount.equals(&quot;-1&quot;)) &#123; // 城市表 city_info.put(pcid, pcname); &#125; else if (pccount.equals(&quot;-2&quot;)) &#123; // 产品表 product_info.put(pcid, pcname); &#125; else &#123; // 处理用户点击关联 unionCity_Prod_UserClic1(pcid, pcname, pccount); &#125; return true; &#125; // 处理用户点击关联 private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123; if (product_info.containsKey(pcid)) &#123; if (city_info.containsKey(pcname)) &#123; String city_name = city_info.get(pcname); String prod_name = product_info.get(pcid); String cp_name = city_name + prod_name; // 如果之前已经Put过Key值为区域信息，则把记录相加处理 if (courseScoreMap.containsKey(cp_name)) &#123; int pcrn = 0; String strTemp = courseScoreMap.get(cp_name); String courseScoreMap_pn = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim(); pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn); courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn)); &#125; else &#123; courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount); &#125; &#125; &#125; &#125; /** * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可 */ public Map&lt;String, String&gt; terminatePartial() &#123; return courseScoreMap; &#125; // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果 public boolean merge(Map&lt;String, String&gt; mapOutput) &#123; this.courseScoreMap.putAll(mapOutput); return true; &#125; // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理 public String terminate() &#123; return courseScoreMap.toString(); &#125; &#125;&#125;四、创建hive中的临时函数123DROP TEMPORARY FUNCTION user_click;add jar /data/hive_udf-1.0.jar;CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;五、调用自定义UDAF函数处理数据1234567891011121314151617insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from ( select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from ( select * from ( select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info union all select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info union all select count(1) as type, product_id as pcid, city_id as pcname from user_click where action_time=&apos;2016-05-05&apos; group by product_id,city_id ) a order by type) b) c ;六、创建Hive临时外部表1234567create external table tmp1(city_name string,product_name string,rn string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;location &apos;/works/tmp1&apos;;七、统计最终区域前3产品排名123456789select * from (select city_name, product_name, floor(sum(rn)) visit_num, row_number()over(partition by city_name order by sum(rn) desc) rn, &apos;2016-05-05&apos; action_time from tmp1 group by city_name,product_name) a where rn &lt;=3 ;八、最终结果]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 基本概念]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）spark名词解释Application jar：应用程序jar包包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；Driver Program：这个进程运行应用程序的 main 方法并且新建 SparkContext ；Cluster Manager：集群管理者在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）Deploy mode：部署模式告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；Worker Node：工作节点集群中任何可以运行应用代码的节点；（yarn上就是node manager）Executor：在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；Task：任务被送到某个 executor 上执行的工作单元；Job：包含很多并行计算的task。一个 action 就会产生一个job；Stage：一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。spark工作流程1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。2、spark并不关心底层的集群管理。3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark History Server Web UI配置]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.进入spark目录和配置文件12[root@hadoop000 ~]# cd /opt/app/spark/conf[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)12345678[root@hadoop000 conf]# hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - root root 0 2017-02-14 12:43 /sparkdrwxrwx--- - root root 0 2017-02-14 12:58 /tmpdrwxr-xr-x - root root 0 2017-02-14 12:58 /userYou have new mail in /var/spool/mail/root[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 itemsdrwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息3.配置12345[root@hadoop000 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建spark.yarn.historyServer.address : Spark history server的地址(不加http://).这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.4.添加SPARK_HISTORY_OPTS参数12345[root@hadoop01 conf]# vi spark-env.sh#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.启动服务和查看123456[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01 ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer[root@hadoop01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@hadoop01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/java以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark不得不理解的重要概念——从源码角度看RDD]]></title>
    <url>%2F2018%2F05%2F20%2FSpark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD%2F</url>
    <content type="text"><![CDATA[1.RDD是什么Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合2.RDD五大特性A list of partitions每个rdd有多个分区protected def getPartitions: Array[Partition]A function for computing each split计算作用到每个分区def compute(split: Partition, context: TaskContext): Iterator[T]A list of dependencies on other RDDsrdd之间存在依赖（RDD的血缘关系）如：RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDDprotected def getDependencies: Seq[Dependency[_]] = depsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)可选，默认哈希的分区@transient val partitioner: Option[Partitioner] = NoneOptionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）protected def getPreferredLocations(split: Partition): Seq[String] = Nil源码来自github。3.如何创建RDD创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式1234567891011121314151617parallelize() def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) &#125;textfile（） def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;源码总结：1）.取_2是因为数据为（key（偏移量），value（数据））4.常见的transformation和action由于比较简单，大概说一下常用的用处，不做代码测试transformationMap：对数据集的每一个元素进行操作FlatMap：先对数据集进行扁平化处理，然后再MapFilter：对数据进行过滤，为true则通过destinct：去重操作actionreduce：对数据进行聚集reduceBykey：对key值相同的进行操作collect：没有效果的action，但是很有用saveAstextFile：数据存入文件系统foreach：对每个元素进行func的操作]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美味不用等大数据面试题(201804月)]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)%2F</url>
    <content type="text"><![CDATA[1.若泽大数据线下班，某某某的小伙伴现场面试题截图:2.分享另外1家的忘记名字公司的大数据面试题：]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD、DataFrame和DataSet的区别]]></title>
    <url>%2F2018%2F05%2F19%2FSpark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！一 、共性1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出4、三者都有partition的概念。二、RDD优缺点优点：1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。2、面向对象的编程风格3、编译时类型安全，编译时就能检查出类型错误缺点：1、序列化和反序列化的性能开销2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC三、DataFrame1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如12345df.foreach&#123; x =&gt; val v1=x.getAs[String](&quot;v1&quot;) val v2=x.getAs[String](&quot;v2&quot;)&#125;2、DataFrame引入了schema和off-heapschema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表4、兼容Hive，支持Hql、UDF有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.四、DataSet1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。3、Dataset等同于DataFrame（Spark 2.X）]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决]]></title>
    <url>%2F2018%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3%2F</url>
    <content type="text"><![CDATA[一.数据源同步中间件：Canalhttps://github.com/alibaba/canalhttps://github.com/Hackeruncle/syncClientMaxwellhttps://github.com/zendesk/maxwell二.架构使用MySQL —- 中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra 增量的a.全量 bootstrapb.增量1.对比Canal(服务端)Maxwell(服务端+客户端)语言JavaJava活跃度活跃活跃HA支持定制 但是支持断点还原功能数据落地定制落地到kafka分区支持支持bootstrap(引导)不支持支持数据格式格式自由json(格式固定) spark json–&gt;DF文档较详细较详细随机读支持支持个人选择Maxwella.服务端+客户端一体，轻量级的b.支持断点还原功能+bootstrap+jsonCan do SELECT * from table (bootstrapping) initial loads of a table.supports automatic position recover on master promotionflexible partitioning schemes for Kakfa - by database, table, primary key, or columnMaxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).2.官网解读B站视频3.部署3.1 MySQL Installhttps://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txthttps://ke.qq.com/course/262452?tuin=11cffd503.2 修改12345678910111213141516171819202122$ vi /etc/my.cnf[mysqld]binlog_format=row$ service mysql start3.3 创建Maxwell的db和用户mysql&gt; create database maxwell;Query OK, 1 row affected (0.03 sec)mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt;3.4解压1[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz3.5测试STDOUT:123bin/maxwell --user=&apos;maxwell&apos; \--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \--producer=stdout测试1：insert sql：12mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);Query OK, 1 row affected (0.03 sec)maxwell输出：123456789101112131415161718&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;insert&quot;, &quot;ts&quot;: 1525959044, &quot;xid&quot;: 201, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 18, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;updateuser&quot;: null &#125;&#125;测试1：update sql:1mysql&gt; update ruozedata set age=29 where id=999;问题: ROW，你觉得binlog更新几个字段？maxwell输出：12345678910111213141516171819202122&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;update&quot;, &quot;ts&quot;: 1525959208, &quot;xid&quot;: 255, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 29, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;, &quot;updateuser&quot;: null &#125;, &quot;old&quot;: &#123; &quot;age&quot;: 18, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot; &#125;&#125;4.其他注意点和新特性4.1 kafka_version 版本Using kafka version: 0.11.0.1 0.10jar:12345678[root@hadoop000 kafka-clients]# lltotal 4000-rw-r--r--. 1 ruoze games 746207 May 8 06:34 kafka-clients-0.10.0.1.jar-rw-r--r--. 1 ruoze games 951041 May 8 06:35 kafka-clients-0.10.2.1.jar-rw-r--r--. 1 ruoze games 1419544 May 8 06:35 kafka-clients-0.11.0.1.jar-rw-r--r--. 1 ruoze games 324016 May 8 06:34 kafka-clients-0.8.2.2.jar-rw-r--r--. 1 ruoze games 641408 May 8 06:34 kafka-clients-0.9.0.1.jar[root@hadoop000 kafka-clients]#]]></content>
      <categories>
        <category>其他组件</category>
      </categories>
      <tags>
        <tag>maxwell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on YARN-Cluster和YARN-Client的区别]]></title>
    <url>%2F2018%2F05%2F12%2FSpark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[一. YARN-Cluster和YARN-Client的区别（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。二. yarn client 模式yarn-client 模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。三.yarn cluster 模式yarn-cluster 模式的话， client 关闭是可以提交任务的 ，总结:1.spark-shell/spark-sql 只支持 yarn-client模式；2.spark-submit对于两种模式都支持。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产改造Spark1.6源代码，create table语法支持Oracle列表分区]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1.需求通过Spark SQL JDBC 方法，抽取Oracle表数据。2.问题大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。参考 http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases3.Oracle的分区3.1列表分区:该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。例一:1234567891011CREATE TABLE PROBLEM_TICKETS(PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,STATUS VARCHAR2(20))PARTITION BY LIST (STATUS)(PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02)3.2散列分区:这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。例一:1234567891011CREATE TABLE HASH_TABLE(COL NUMBER(8),INF VARCHAR2(100) )PARTITION BY HASH (COL)(PARTITION PART01 TABLESPACE HASH_TS01, PARTITION PART02 TABLESPACE HASH_TS02, PARTITION PART03 TABLESPACE HASH_TS03)4.改造蓝色代码是改造Spark源代码,加课程顾问领取PDF。1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。12345678910111213CREATE TEMPORARY TABLE TBLS_INUSING org.apache.spark.sql.jdbc OPTIONS (driver &quot;com.mysql.jdbc.Driver&quot;,url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,fetchSize &quot;1000&quot;,partitionColumn &quot;TBL_ID&quot;,numPartitions &quot;null&quot;,lowerBound &quot;null&quot;,upperBound &quot;null&quot;,user &quot;hive2user&quot;,password &quot;hive2user&quot;,partitionInRule &quot;1|15,16,18,19|20,21&quot;);2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748override def createRelation(sqlContext: SQLContext,parameters: Map[String, String]): BaseRelation = &#123;val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)// add partition in ruleval partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)// validind all the partition in rule if (partitionColumn != null&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)&amp;&amp; partitionInRule == null )&#123; sys.error(&quot;Partitioning incompletely specified&quot;) &#125;val partitionInfo = if (partitionColumn == null) &#123; null&#125; else &#123; val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123; val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot; upperBound = &quot;0&quot; inGroups &#125; else&#123; Array[String]() &#125; JDBCPartitioningInfo( partitionColumn, lowerBound.toLong, upperBound.toLong, numPartitions.toInt, inPartitions)&#125;val parts = JDBCRelation.columnPartition(partitionInfo)val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))// parameters is immutableif(numPartitions != null)&#123;properties.put(&quot;numPartitions&quot; , numPartitions) &#125;JDBCRelation(url, table, parts, properties)(sqlContext) &#125; &#125;3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))val column = partitioning.columnvar i: Int = 0var ans = new ArrayBuffer[Partition]()// partition by long if(partitioning.inPartitions.length == 0)&#123;val numPartitions = partitioning.numPartitionsif (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.// Here we get a little roundoff, but that&apos;s (hopefully) OK.val stride: Long = (partitioning.upperBound / numPartitions- partitioning.lowerBound / numPartitions)var currentValue: Long = partitioning.lowerBoundwhile (i &lt; numPartitions) &#123;val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else nullcurrentValue += strideval upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =if (upperBound == null) &#123; lowerBound&#125; else if (lowerBound == null) &#123; upperBound&#125; else &#123; s&quot;$lowerBound AND $upperBound&quot; &#125; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125;&#125;// partition by in else&#123; while(i &lt; partitioning.inPartitions.length)&#123; val inContent = partitioning.inPartitions(i) val whereClause = s&quot;$column in ($inContent)&quot; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125; &#125; ans.toArray &#125;4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc123456789101112def jdbc(url: String,table: String,columnName: String,lowerBound: Long,upperBound: Long,numPartitions: Int,inPartitions: Array[String] = Array[String]()): DataFrame = &#123;read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产中Hive静态和动态分区表，该怎样抉择呢？]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[一.需求按照不同部门作为分区，导数据到目标表二.使用静态分区表来完成71.创建静态分区表：12345678910create table emp_static_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;2.插入数据：12hive&gt;insert into table emp_static_partition partition(deptno=10) select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;3.查询数据：1hive&gt;select * from emp_static_partition;三.使用动态分区表来完成1.创建动态分区表：123456789create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的2.插入数据：12hive&gt;insert into table emp_dynamic_partition partition(deptno) select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where需要设置属性的值：1hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；假如不设置，报错如下:3.查询数据：1hive&gt;select * from emp_dynamic_partition;分区列为deptno，实现了动态分区四.总结在生产上我们更倾向是选择动态分区，无需手工指定数据导入的具体分区，而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5min掌握，Hive的HiveServer2 和JDBC客户端&代码的生产使用]]></title>
    <url>%2F2018%2F05%2F04%2F5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF%26%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 介绍：两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，客户端可以在不启动CLI的情况下对Hive中的数据进行操作，两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，不能通过修改HiveServer的代码修正。因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。2.配置参数Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：参数 | 含义 |-|-|hive.server2.thrift.min.worker.threads| 最小工作线程数，默认为5。hive.server2.thrift.max.worker.threads| 最小工作线程数，默认为500。hive.server2.thrift.port| TCP 的监听端口，默认为10000。hive.server2.thrift.bind.host| TCP绑定的主机，默认为localhost配置监听端口和路径123456789vi hive-site.xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.48.130&lt;/value&gt;&lt;/property&gt;3. 启动hiveserver2使用hadoop用户启动123[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/[hadoop@hadoop001 bin]$ hiveserver2 which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)4. 重新开个窗口，使用beeline方式连接-n 指定机器登陆的名字，当前机器的登陆用户名-u 指定一个连接串每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK如果命令错误，hiveserver2那个窗口就会抛出异常使用hadoop用户启动123456789[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoopwhich: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)scan complete in 4msConnecting to jdbc:hive2://localhost:10000/defaultConnected to: Apache Hive (version 1.1.0-cdh5.7.0)Driver: Hive JDBC (version 1.1.0-cdh5.7.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.7.0 by Apache Hive0: jdbc:hive2://localhost:10000/default&gt;使用SQL123456789101112131415160: jdbc:hive2://localhost:10000/default&gt; show databases;INFO : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)INFO : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 secondsINFO : Concurrency mode is disabled, not creating a lock managerINFO : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Starting task [Stage-0:DDL] in serial modeINFO : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 secondsINFO : OK+----------------+--+| database_name |+----------------+--+| default |+----------------+--+1 row selected5.使用编写java代码方式连接5.1使用maven构建项目，pom.xml文件如下：123456789101112131415161718192021222324252627282930313233343536373839&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;5.2JdbcApp.java文件代码:123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class JdbcApp &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws Exception &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); //select table:ename String tableName = &quot;emp&quot;; String sql = &quot;select ename from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); while(res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2min快速了解，Hive内部表和外部表]]></title>
    <url>%2F2018%2F05%2F01%2F2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[在了解内部表和外部表区别前，我们需要先了解一下Hive架构 ：大家可以简单看一下这个架构图，我介绍其中要点：Hive的数据分为两种，一种为普通数据，一种为元数据。元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。下面我们来介绍表的两种类型：内部表和外部表内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表而内部表和外部表的主要区别就是内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；1.准备数据: 按tab键制表符作为字段分割符cat /tmp/ruozedata.txt 1 jepson 32 110 2 ruoze 22 112 3 www.ruozedata.com 18 120 2.内部表测试：在Hive里面创建一个表：123456789hive&gt; create table ruozedata(id int, &gt; name string, &gt; age int, &gt; tele string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\t&apos; &gt; STORED AS TEXTFILE;OKTime taken: 0.759 seconds这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;内部表删除1hive&gt; drop table ruozedata;3.外部表测试:创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’12345678hive&gt; create external table exter_ruozedata( &gt; id int, &gt; name string, &gt; age int, &gt; tel string) &gt; location &apos;/hive/external&apos;;OKTime taken: 0.098 seconds创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）外部表导入数据和内部表一样：1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;删除外部表1hive&gt; drop table exter_ruozedata;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈我和大数据的情缘及入门]]></title>
    <url>%2F2018%2F05%2F01%2F%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[&#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。&#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。&#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。&#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。&#8195;后来这样的进度太慢了，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。&#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。&#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。1. 心态要端正。既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。2. 心目中要有计划。先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。3. 各种方式学习。QQ群，博客，上下班看技术文章，选择好的老师和课程培训，(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。4. 项目经验。很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。而面试，就看看其他人面试分享，学习他人。最后，总结一句话，坚持才是最重要的。最后，总结一句话，坚持才是最重要的。最后，总结一句话，坚持才是最重要的。]]></content>
      <categories>
        <category>感想</category>
      </categories>
      <tags>
        <tag>人生感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive自定义函数(UDF)的部署使用，你会吗？]]></title>
    <url>%2F2018%2F04%2F27%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Hive自定义函数(UDF)的部署使用，你会吗，三种方式！一.临时函数idea编写udf打包Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Buildrz上传至服务器添加jar包hive&gt;add xxx.jar jar_filepath;查看jar包hive&gt;list jars;创建临时函数hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;二.持久函数idea编写udf打包Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Buildrz上传至服务器上传到HDFS$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jar创建持久函数hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;注意点：此方法在show functions时是看不到的，但是可以使用需要上传至hdfs三.持久函数，并注册环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9下载源码hive-1.1.0-cdh5.7.0-src.tar.gzhttp://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz解压源码tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/cd /home/hadoop/hive-1.1.0-cdh5.7.0将HelloUDF.java文件增加到HIVE源码中cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/修改FunctionRegistry.java 文件1234cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/vi FunctionRegistry.java在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);重新编译cd /home/hadoop/hive-1.1.0-cdh5.7.0mvn clean package -DskipTests -Phadoop-2 -Pdist编译结果全部为：BUILD SUCCESS文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target配置hive环境配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：7.1. 全部配置：参照之前文档 Hive全网最详细的编译及部署7.2. 将编译后带UDF函数的包复制到旧hive环境到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉命令：1234cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/libmv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bakcd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libcp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib最终启动hive测试：123hivehive (default)&gt; show functions ; -- 能查看到有 helloudfhive(default)&gt;select deptno,dname,helloudf(dname) from dept; -- helloudf函数生效]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive自定义函数(UDF)的编程开发，你会吗？]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本地开发环境：IntelliJ IDEA+Maven3.3.91. 创建工程打开IntelliJ IDEAFile–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart2. 配置在工程中找到pom.xml文件，添加hadoop、hive依赖3. 创建类、并编写一个HelloUDF.java，代码如下：首先一个UDF必须满足下面两个条件:一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）一个UDF必须至少实现了evaluate()方法4. 测试，右击运行run ‘HelloUDF.main()’5. 打包在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包执行成功后在日志中找：[INFO] Building jar: (路径)/hive-1.0.jar]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDL，你真的了解吗？]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你全面剖析Hive DDL！概念DatabaseHive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）TableHive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/tablePartition分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：/user/hadoop/hive/warehouse/[databasename.db]/tableDDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。COMMENT：数据库的描述LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下WITH DBPROPERTIES：数据库的属性Drop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICT：默认是restrict，如果该数据库还有表存在则报错；CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。Alter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)Use Database12USE database_name;USE DEFAULT;Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;“ | ”：可以选择其中一种“[ ]”：可选项LIKE ‘identifier_with_wildcards’：模糊查询数据库Describe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；EXTENDED：加上数据库键值对的属性信息。hive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s)Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type1: ARRAY &lt; data_type &gt;map_type1: MAP &lt; primitive_type, data_type &gt;struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameconstraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARY（临时表）Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。语法：CREATE TEMPORARY TABLE …注意：如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表临时表限制：不支持分区字段和创建索引EXTERNAL（外部表）Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; );PARTITIONED BY（分区表）产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。单分区：123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OK多分区：123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMAT官网解释：1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]DELIMITED：分隔符（可以自定义分隔符）；FIELDS TERMINATED BY char:每个字段之间使用的分割；例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OK创建demo2表，并指定其他字段：123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED AS（存储格式）Create Table As Select创建表（拷贝表结构及数据，并且会运行MapReduce作业）12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;加载数据1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;复制整张表12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt;复制表中的一些字段1create table emp3 as select empno,ename from emp;LIKE使用like创建表时，只会复制表的结构，不会复制表的数据1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt;并没有查询到数据desc formatted table_name查询表的详细信息12hive&gt; desc formatted emp;OKcol_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno intDetailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt;通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;查询数据库下的所有表1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt;查询创建表的语法123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later)指定PURGE后，数据不会放到回收箱，会直接删除DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失删除EXTERNAL表时，表中的数据不会从文件系统中删除Alter Table重命名1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。hive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 seconds查询结果123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);查看分区语句12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s)按分区查询1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive生产上，压缩和存储结合使用案例]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[你们Hive生产上，压缩和存储，结合使用了吗？案例：原文件大小：19M1. ORC+Zlip结合12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views;用ORC+Zlip之后的文件为2.8M用ORC+Zlip之后的文件为2.8M###### 2. Parquet+gzip结合12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;用Parquet+gzip之后的文件为3.9M3. Parquet+Lzo结合3.1 安装Lzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile3.2 安装Lzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile3.3 软连接1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop3.4 测试lzoplzop xxx.log若生成xxx.log.lzo文件，则说明成功3.5 安装Hadoop-LZO12345 git或svn 下载https://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/3.6 配置在core-site.xml配置1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;在mapred-site.xml中配置 &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;在hadoop-env.sh中配置export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib3.7 测试12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views;用Parquet+Lzo(未建立索引)之后的文件为5.9M]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive存储格式的生产应用]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。原始大小: 19M1. TextFile(默认) 文件大小为18.1M2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views;用SequenceFile存储后的文件为19.6M3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views;用RcFile存储后的文件为17.9M4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views;用ORCFile存储后的文件为7.7M5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; 用ORCFile存储后的文件为13.1M总结：磁盘空间占用大小比较ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据压缩，你们真的了解吗？]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你们剖析大数据之压缩！1. 压缩的好处和坏处好处减少存储磁盘空间降低IO(网络的IO和磁盘的IO)加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度坏处由于使用数据时，需要先将数据解压，加重CPU负荷2. 压缩格式压缩比压缩时间可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2压缩格式优点缺点gzip压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便不支持splitlzo压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）snappy压缩速度快；支持hadoop native库不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2bzip2支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便压缩/解压速度慢；不支持native总结：不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。应用场景：一般在HDFS 、Hive、HBase中会使用；当然一般较多的是结合Spark 来一起使用。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 全网最详细的源码编译]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[若泽大数据，Spark2.2.0 全网最详细的源码编译环境准备JDK： Spark 2.2.0及以上版本只支持JDK1.8Maven：3.3.9设置maven环境变量时，需设置maven内存：export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”Scala：2.11.8Git编译下载spark的tar包，并解压12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz编辑dev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh注释以下内容：#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n)添加以下内容：1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1编辑pom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml添加在repositorys内&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;安装1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn稍微等待几小时，网络较好的话，非常快。也可以参考J哥博客：基于CentOS6.4环境编译Spark-2.1.0源码 http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop常用命令大全]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop常用命令大全1. 单独启动和关闭hadoop服务功能命令启动名称节点hadoop-daemon.sh start namenode启动数据节点hadoop-daemons.sh start datanode slave启动secondarynamenodehadoop-daemon.sh start secondarynamenode启动resourcemanageryarn-daemon.sh start resourcemanager启动nodemanagerbin/yarn-daemons.sh start nodemanager停止数据节点hadoop-daemons.sh stop datanode2. 常用的命令功能命令创建目录hdfs dfs -mkdir /input查看hdfs dfs -ls递归查看hdfs dfs ls -R上传hdfs dfs -put下载hdfs dfs -get删除hdfs dfs -rm从本地剪切粘贴到hdfshdfs fs -moveFromLocal /input/xx.txt /input/xx.txt从hdfs剪切粘贴到本地hdfs fs -moveToLocal /input/xx.txt /input/xx.txt追加一个文件到另一个文件到末尾hdfs fs -appedToFile ./hello.txt /input/hello.txt查看文件内容hdfs fs -cat /input/hello.txt显示一个文件到末尾hdfs fs -tail /input/hello.txt以字符串的形式打印文件的内容hdfs fs -text /input/hello.txt修改文件权限hdfs fs -chmod 666 /input/hello.txt修改文件所属hdfs fs -chown ruoze.ruoze /input/hello.txt从本地文件系统拷贝到hdfs里hdfs fs -copyFromLocal /input/hello.txt /input/从hdfs拷贝到本地hdfs fs -copyToLocal /input/hello.txt /input/从hdfs到一个路径拷贝到另一个路径hdfs fs -cp /input/xx.txt /output/xx.txt从hdfs到一个路径移动到另一个路径hdfs fs -mv /input/xx.txt /output/xx.txt统计文件系统的可用空间信息hdfs fs -df -h /统计文件夹的大小信息hdfs fs -du -s -h /统计一个指定目录下的文件节点数量hadoop fs -count /aaa设置hdfs的文件副本数量hadoop fs -setrep 3 /input/xx.txt总结：一定要学会查看命令帮助1.hadoop命令直接回车查看命令帮助2.hdfs命令、hdfs dfs命令直接回车查看命令帮助3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我们生产上要选择Spark On Yarn模式？]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，为什么我们生产上要选择Spark On Yarn？开发上我们选择local[2]模式生产上跑任务Job，我们选择Spark On Yarn模式 ，将Spark Application部署到yarn中，有如下优点：1.部署Application和服务更加方便只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。2.资源隔离机制yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。3.资源弹性管理Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。运行client模式：“./spark-shell –master yarn”“./spark-shell –master yarn-client”“./spark-shell –master yarn –deploy-mode client”运行的是cluster模式“./spark-shell –master yarn-cluster”“./spark-shell –master yarn –deploy-mode cluster”client和cluster模式的主要区别：a. client的driver是运行在客户端进程中b. cluster的driver是运行在Application Master之中]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive全网最详细的编译及部署]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hive全网最详细的编译及部署一、需要安装的软件相关环境：jdk-7u80hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7apache-maven-3.3.9mysql5.1hadoop伪分布集群已启动二、安装jdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profile三、安装maven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profile四、安装mysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges;五、下载hive源码包：输入：http://archive.cloudera.com/cdh5/cdh/5/根据cdh版本选择对应hive软件包：hive-1.1.0-cdh5.7.1-src.tar.gz解压后使用maven命令编译成安装包六、编译:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# 编译生成的包在以下位置：# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz七、安装编译生成的Hive包，然后测试12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile八、更改环境变量12345su - hadoopcd /usr/local/hivecd conf1、hive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop2、hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;九、拷贝mysql驱动包到$HIVE_HOME/lib上方的hive-site.xml使用了java的mysql驱动包需要将这个包上传到hive的lib目录之下解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/未拷贝有相关报错：The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH.Please check your CLASSPATH specification,and the name of the driver.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)修改mapred-site.xml1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改yarn-site.xml12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;启动123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh关闭1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS)1.添加hadoop用户123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# 找到root ALL=(ALL) ALL，添加hadoop ALL=(ALL) NOPASSWD:ALL2.上传并解压123[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz3.软连接1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop4.设置环境变量1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile5.设置用户、用户组123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt6.切换hadoop用户1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: 可执行文件# etc: 配置文件# sbin: shell脚本，启动关闭hdfs,yarn等7.配置文件12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # 配置自己机器的IP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;8.配置hadoop用户的ssh信任关系8.1公钥/密钥 配置无密码登录12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys8.2 查看日期，看是否配置成功1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 20179.格式化和启动123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found.9.1解决方法:添加环境变量12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为export JAVA_HOME=/usr/java/jdk1.8.0_4512345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied9.2解决方法:添加权限123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop9.3 继续启动1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh9.4检查是否成功123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode9.5访问： http://192.168.137.130:500709.6修改dfs启动的进程，以hadoop-01启动启动的三个进程：namenode: hadoop-01 bin/hdfs getconf -namenodesdatanode: localhost datanodes (using default slaves file) etc/hadoop/slavessecondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt;9.7重启123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（二）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（二）实时查看文件内容 tail filenametail -f filename 当文件(名)被修改后，不能监视文件内容tail -F filename 当文件(名)被修改后，依然可以监视文件内容复制、移动文件cp oldfilename newfilename 复制mv oldfilename newfilename 移动/重命名echoecho “xxx” 输出echo “xxx” &gt; filename 覆盖echo “xxx” &gt;&gt; filename 追加删除 rmrm -f 强制删除rm -rf 强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件别名 aliasalias x=”xxxxxx” 临时引用别名alias x=”xxxxxx” 配置到环境变量中即为永久生效查看历史命令 historyhistory 显示出所有历史记录history n 显示出n条记录!n 执行第n条记录管道命令 （ | ）管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入查看进程、查看id、端口ps -ef ｜grep 进程名 查看进程基本信息netstat -npl｜grep 进程名或进程id 查看服务id和端口杀死进程 killkill -9 进程名/pid 强制删除kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程rpm 搜索、卸载rpm -qa | grep xxx 搜索xxxrpm –nodeps -e xxx 删除xxx–nodeps 不验证包的依赖性查询find 路径 -name xxx (推荐)which xxxlocal xxx查看磁盘、内存、系统的情况df -h 查看磁盘大小及其使用情况free -m 查看内存大小及其使用情况top 查看系统情况软连接ln -s 原始目录 目标目录压缩、解压tar -czf 压缩 tar -xzvf 解压zip 压缩 unzip 解压]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（一）]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（一）查看当前目录 pwd查看IPifconfig 查看虚拟机iphostname 主机名字i 查看主机名映射的IP切换目录 cdcd ~ 切换家目录（root为/root，普通用户为/home/用户名）cd /filename 以绝对路径切换目录cd - 返回上一次操作路径，并输出路径cd ../ 返回上一层目录清理桌面 clear显示当前目录文件和文件夹 lsls -l(ll) 显示详细信息ls -la 显示详细信息+隐藏文件（以 . 开头，例：.ssh）ls -lh 显示详细信息+文件大小ls -lrt 显示详细信息+按时间排序查看文件夹大小 du -sh命令帮助man 命令命令 –help创建文件夹 mkdirmkdir -p filename1/filename2 递归创建文件夹创建文件 touch/vi/echo xx&gt;filename查看文件内容cat filename 直接打印所有内容more filename 根据窗口大小进行分页显示文件编辑 vivi分为命令行模式，插入模式，尾行模式命令行模式—&gt;插入模式：按i或a键插入模式—&gt;命令行模式：按Esc键命令行模式—&gt;尾行模式：按Shift和:键插入模式dd 删除光标所在行n+dd 删除光标以下的n行dG 删除光标以下行gg 第一行第一个字母G 最后一行第一个字母shift+$ 该行最后一个字母尾行模式q! 强制退出qw 写入并退出qw! 强制写入退出x 退出，如果存在改动，则保存再退出]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令（三）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（三）用户、用户组用户useradd 用户名 添加用户userdel 用户名 删除用户id 用户名 查看用户信息passwd 用户名 修改用户密码su - 用户名 切换用户ll /home/ 查看已有的用户用户组groupadd 用户组 添加用户组cat /etc/group 用户组的文件usermod -a -G 用户组 用户 将用户添加到用户组中给一个普通用户添加sudo权限123vi /etc/sudoers #在root ALL=(ALL) ALL 下面添加一行 用户 ALL=(ALL) NOPASSWD:ALL修改文件权限chown 修改文件或文件夹的所属用户和用户组chown -R 用户:用户组 文件夹名 -R 为递归参数，指针对文件夹chown 用户:用户组 文件名chmod: 修改文件夹或者文件的权限chmod -R 700 文件夹名chmod 700 文件夹名r =&gt; 4 w =&gt; 2 x =&gt; 1 后台执行命令&amp;nohupscreen多人合作 screenscreen -list 查看会话screen -S 建立一个后台的会话screen -r 进入会话ctrl+a+d 退出会话]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构设计及副本放置策略]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFS架构设计及副本放置策略HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。NameNode和DataNode架构图NameNode(名称节点)存储：元信息的种类，包含:文件名称文件目录结构文件的属性[权限,创建时间,副本数]文件对应哪些数据块–&gt;数据块对应哪些datanode节点作用：管理着文件系统命名空间维护这文件系统树及树中的所有文件和目录维护所有这些文件或目录的打开、关闭、移动、重命名等操作DataNode(数据节点)存储：数据块、数据块校验、与NameNode通信作用：读写文件的数据块NameNode的指示来进行创建、删除、和复制等操作通过心跳定期向NameNode发送所存储文件块列表信息Scondary NameNode(第二名称节点)存储: 命名空间镜像文件fsimage+编辑日志editlog作用: 定期合并fsimage+editlog文件为新的fsimage推送给NamenNode副本放置策略第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上第二副本：放置在与第一个副本不同的机架的节点上第三副本：与第二个副本相同机架的不同节点上如果还有更多的副本：随机放在节点中]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置多台虚拟机之间的SSH信任]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[本机环境3台机器执行命令ssh-keygen选取第一台,生成authorized_keys文件hadoop002 hadoop003传输id_rsa.pub文件到hadoop001hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys设置每台机器的权限12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keys将authorized_keys分发到hadoop002、hadoop003机器验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
