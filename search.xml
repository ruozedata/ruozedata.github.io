<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[生产SparkStreaming数据零丢失最佳实践(含代码)]]></title>
    <url>%2F2019%2F06%2F14%2F%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)%2F</url>
    <content type="text"><![CDATA[MySQL创建存储offset的表格123456789mysql&gt; use testmysql&gt; create table hlw_offset( topic varchar(32), groupid varchar(50), partitions int, fromoffset bigint, untiloffset bigint, primary key(topic,groupid,partitions) );Maven依赖包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;--------------------------------------------------&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe&lt;/groupId&gt; &lt;artifactId&gt;config&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt;实现思路123451）StreamingContext2）从kafka中获取数据(从外部存储获取offset--&gt;根据offset获取kafka中的数据)3）根据业务进行逻辑处理4）将处理结果存到外部存储中--保存offset5）启动程序，等待程序结束代码实现SparkStreaming主体代码如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scalikejdbc._import scalikejdbc.config._object JDBCOffsetApp &#123; def main(args: Array[String]): Unit = &#123; //创建SparkStreaming入口 val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;) val ssc = new StreamingContext(conf,Seconds(5)) //kafka消费主题 val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet //kafka参数 //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改 val kafkaParams = Map[String,String]( &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;), &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;), &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;) ) //先使用scalikejdbc从MySQL数据库中读取offset信息 //+------------+------------------+------------+------------+-------------+ //| topic | groupid | partitions | fromoffset | untiloffset | //+------------+------------------+------------+------------+-------------+ //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来 //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到 DBs.setup() val fromOffset = DB.readOnly( implicit session =&gt; &#123; SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123; (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;)) &#125;).list().apply() &#125;).toMap //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费 val messages = if (fromOffset.isEmpty) &#123; println(&quot;从头开始消费...&quot;) KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics) &#125; else &#123; println(&quot;从已存在记录开始消费...&quot;) val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message()) KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler) &#125; messages.foreachRDD(rdd=&gt;&#123; if(!rdd.isEmpty())&#123; //输出rdd的数据量 println(&quot;数据统计记录为：&quot;+rdd.count()) //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组// trait HasOffsetRanges &#123;// def offsetRanges: Array[OffsetRange]// &#125; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges offsetRanges.foreach(x =&gt; &#123; //输出每次消费的主题，分区，开始偏移量和结束偏移量 println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;) //将最新的偏移量信息保存到MySQL表中 DB.autoCommit( implicit session =&gt; &#123; SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;) .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset) .update().apply() &#125;) &#125;) &#125; &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;自定义的ValueUtils工具类如下12345678910111213import com.typesafe.config.ConfigFactoryimport org.apache.commons.lang3.StringUtilsobject ValueUtils &#123;val load = ConfigFactory.load() def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;val value = load.getString(key) if(StringUtils.isNotEmpty(value)) &#123; value &#125; else &#123; defaultValue &#125; &#125;&#125;application.conf内容如下1234567891011metadata.broker.list = &quot;192.168.137.251:9092&quot;auto.offset.reset = &quot;smallest&quot;group.id = &quot;hlw_offset_group&quot;kafka.topics = &quot;hlw_offset&quot;serializer.class = &quot;kafka.serializer.StringEncoder&quot;request.required.acks = &quot;1&quot;# JDBC settingsdb.default.driver = &quot;com.mysql.jdbc.Driver&quot;db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;db.default.user=&quot;root&quot;db.default.password=&quot;123456&quot;自定义kafka producer123456789101112131415161718192021import java.util.&#123;Date, Properties&#125;import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;object KafkaProducer &#123; def main(args: Array[String]): Unit = &#123; val properties = new Properties() properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;)) properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;)) properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;)) val producerConfig = new ProducerConfig(properties) val producer = new Producer[String,String](producerConfig) val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;) //每次产生100条数据 var i = 0 for (i &lt;- 1 to 100) &#123; val runtimes = new Date().toString val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes) producer.send(messages) &#125; println(&quot;数据发送完毕...&quot;) &#125;&#125;测试启动kafka服务，并创建主题123[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset测试前查看MySQL中offset表，刚开始是个空表12mysql&gt; select * from hlw_offset;Empty set (0.00 sec)通过kafka producer产生500条数据启动SparkStreaming程序1234//控制台输出结果：从头开始消费...数据统计记录为：500---hlw_offset,0,0,500---查看MySQL表，offset记录成功 123456mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic | groupid | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group | 0 | 0 | 500 |+------------+------------------+------------+------------+-------------+ 关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）1234//控制台结果输出：从已存在记录开始消费...数据统计记录为：300---hlw_offset,0,500,800---查看更新后的offset MySQL数据123456mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic | groupid | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group | 0 | 500 | 800 |+------------+------------------+------------+------------+-------------+]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>spark streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019端午-线下项目第14期圆满结束]]></title>
    <url>%2F2019%2F06%2F11%2F2019%E7%AB%AF%E5%8D%88-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC14%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019年端午节4天3夜上海线下班圆满结束一句话，上海温度有点燥热但，会议室空调很给力小伙伴们来自11个城市北京、上海、深圳广州、杭州、合肥、徐州石家庄、大庆、天津、厦门大家为了一个真实目标学习真正企业级大数据生产项目3个生产项目+2个Topic分享一年我们只在节假日&amp;周末举办错过了就是错过了期待8月下旬线下项目班第15期]]></content>
      <categories>
        <category>线下实战班</category>
      </categories>
      <tags>
        <tag>线下实战班</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产常用Spark累加器剖析之四]]></title>
    <url>%2F2019%2F05%2F31%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[现象描述1234567891011val acc = sc.accumulator(0, “Error Accumulator”)val data = sc.parallelize(1 to 10)val newData = data.map(x =&gt; &#123; if (x % 2 == 0) &#123; accum += 1&#125;&#125;)newData.countacc.valuenewData.foreach(println)acc.value上述现象，会造成acc.value的最终值变为10原因分析Spark中的一系列transform操作都会构造成一长串的任务链，此时就需要通过一个action操作来触发（lazy的特性），accumulator也是如此。因此在一个action操作之后，调用value方法查看，是没有任何变化第一次action操作之后，调用value方法查看，变成了5第二次action操作之后，调用value方法查看，变成了10原因就在于第二次action操作的时候，又执行了一次累加器的操作，同个累加器，在原有的基础上又加了5，从而变成了10解决方案通过上述的现象描述，我们可以很快知道解决的方法：只进行一次action操作。基于此，我们只要切断任务之间的依赖关系就可以了，即使用cache、persist。这样操作之后，那么后续的累加器操作就不会受前面的transform操作影响了案例地址相关的工程案例地址在Github上：https://github.com/lemonahit/spark-train/tree/master/01-Accumulator123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * 使用Spark Accumulators完成Job的数据量处理 * 统计emp表中NULL出现的次数以及正常数据的条数 &amp; 打印正常数据的信息 * * 若泽数据学员-呼呼呼 on 2017/11/9. */object AccumulatorsApp &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;AccumulatorsApp&quot;) val sc = new SparkContext(conf) val lines = sc.textFile(&quot;E:/emp.txt&quot;) // long类型的累加器值 val nullNum = sc.longAccumulator(&quot;NullNumber&quot;) val normalData = lines.filter(line =&gt; &#123; var flag = true val splitLines = line.split(&quot;\t&quot;) for (splitLine &lt;- splitLines)&#123; if (&quot;&quot;.equals(splitLine))&#123; flag = false nullNum.add(1) &#125; &#125; flag &#125;) // 使用cache方法，将RDD的第一次计算结果进行缓存；防止后面RDD进行重复计算，导致累加器的值不准确 normalData.cache() // 打印每一条正常数据 normalData.foreach(println) // 打印正常数据的条数 println(&quot;NORMAL DATA NUMBER: &quot; + normalData.count()) // 打印emp表中NULL出现的次数 println(&quot;NULL: &quot; + nullNum.value) sc.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>累加器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上海某公司的生产MySQL灾难性挽救]]></title>
    <url>%2F2019%2F05%2F30%2F%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91%2F</url>
    <content type="text"><![CDATA[1.背景本人(若泽数据J哥)的媳妇，是个漂亮的妹子，同时也是一枚爬虫&amp;Spark开发工程师。前天，她的公司MySQL(阿里云ECS服务器)，由于磁盘爆了加上人为的修复，导致各种问题，然后经过2天的折腾，终于公司的大神修复不了了。于是就丢给她了，顺理成章的就丢给我了。我想说，难道J哥这么出名吗？那为了在妹子面前不能丢我们真正大佬的神技，于是乎我就很爽快接了这个MySQL故障恢复，此次故障的是一个数据盘，1T。这时的我，说真的并没有意识到，此事是如此的繁杂，特此写此博文记录一下，毕竟J哥我年纪也大了。PS:这里吐槽一下，并没有周日全备+周1~周6增量备份机制哟，不然恢复就爽歪歪了。2.故障现象12查看表结构、查询表数据都如下抛错:ERROR 1030 (HY000): Got error 122 from storage engine3.尝试修复第一次，失败3.1 使用repair命令修复表123mysql&gt; repair table wenshu.wenshu2018; 错误依旧:ERROR 1030 (HY000): Got error 122 from storage engine3.2 谷歌一篇有指导意义的https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine3.2.1 让其扩容数据磁盘为1.5T，试试，依旧这个错误；3.2.2 临时目录修改为大的磁盘空间，试试，依旧这个错误；3.2.3 取消磁盘限额，试试，依旧这个错误；3.2.4 就是一开始的repair命令修复，试试，依旧这个错误；这时的我，也无语了，什么鬼！谷歌一页页搜索验证，没有用！4.先部署相同系统的相同版本的机器和MySQL于是J哥，快速在【若泽数据】的阿里云账号上买了1台Ubuntu 16.04.6的按量付费机器迅速部署MySQL5.7.26。4.1 购买按量付费机器(假如不会购买，找J哥)4.2 部署MySQL123456789101112131415161718a.更新apt-get$ apt-get updateb.安装MySQL-Server$ apt-get install mysql-server之后会问你，是否要下载文件， 输入 y 就好了然后会出现让你设置 root 密码的界面输入密码: ruozedata123然后再重复一下，再次输入密码: ruozedata123c.安装MySQL-Client$ apt install mysql-clientd.我们可以使用$ mysql -uroot -pruozedata123来连接服务器本地的 MySQL5.尝试先通过frm文件恢复表结构，失败123456789101112131415161718192021222324252627a. 建立一个数据库，比如wenshu.b. 在ruozedata数据库下建立同名的数据表wenshu2018，表结构随意，这里只有一个id字段，操作过程片段如下：mysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;mysql&gt; show tables;+--------------+| Tables_in_aa |+--------------+| wenshu2018 |+--------------+1 rows in set (0.00 sec)mysql&gt; desc wenshu2018;+-------+------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+------------+------+-----+---------+-------+| id | bigint(20) | NO | | NULL | |+-------+------------+------+-----+---------+-------+1 row in set (0.00 sec)c.停止mysql服务器，将wenshu2018.frm文件scp远程拷贝到新的正常数据库的数据目录wenshu下，覆盖掉下边同名的frm文件：d.重新启动MYSQL服务e.测试下是否恢复成功，进入wenshu数据库，用desc命令测试下，错误为:mysql Tablespace is missing for table `wenshu`.`wenshu2018`.6.尝试有没有备份的表结构恢复数据，失败媳妇公司给出一个表结构,如下，经过测试无法恢复，原因就是无法和ibd文件匹配。123456789101112DROP TABLE IF EXISTS cpws_batch;CREATE TABLE cpws_batch ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL, crawl_time datetime NULL DEFAULT NULL, status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;, PRIMARY KEY (id) USING BTREE, INDEX ix_status(status) USING BTREE, INDEX ix_doc_id(doc_id) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;7.如何获取正确的表结构，这是【成功的第一步】1234567891011121314151617181920212223242526272829303132$ curl -s get.dbsake.net &gt; /tmp/dbsake$ chmod u+x /tmp/dbsake$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm ---- Table structure for table wenshu_0_1000-- Created with MySQL Version 5.7.25--CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;对比Step6的表结构，感觉就差分区设置而已，坑！这时，J哥有种信心，恢复应该小菜了。8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】1234567891011121314151617181920212223242526272829303132333435363738394041429.1首先需要一个跟要恢复的表结构完全一致的表，至关重要mysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;USE wenshu;CREATE TABLE wenshu2018 ( id int(11) NOT NULL AUTO_INCREMENT, doc_id varchar(255) DEFAULT NULL, source text, error_msg text, crawl_time datetime DEFAULT NULL, status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;, PRIMARY KEY (id), KEY ix_status (status), KEY ix_doc_id (doc_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 /*!50100 PARTITION BY RANGE (id)(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB, PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB, PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB, PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB, PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB, PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB, PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB, PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB, PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB, PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB, PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB, PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;9.2然后DISCARD TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;9.3把要恢复的ibd文件复制到mysql的data文件夹下，修改用户和用户组为mysql$ scp wenshu2018#P#p*.ibd 新建机器IP:/mnt/mysql_data/wenshu/$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd9.4然后执行IMPORT TABLESPACEmysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;9.5等待，有戏，耗时3h，这时我相信应该么问题的9.6查询数据，果然恢复有结果，心里暗暗自喜mysql&gt; select * from wenshu.wenshu2018 limit 1\G;10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择10.1 重新购买一台新的服务器，在初始化配置时，就加上1块1.5T的大磁盘。好处是无需挂盘操作，坏处是需要重新做第一个表，浪费3h；10.2 购买1.5T的大磁盘，挂载这个机器上。好处是无需再做一次第一个表，坏处是需要修改mysql的数据目录指向为这个大磁盘。系统盘扩容最大也就500G，所以必须外加一个数据盘1.5T容量。所以J哥是职场老手了！贼笑！11.服务器加数据磁盘，1.5T，购买、挂载、格式化接下来的操作是我媳妇独立完成的，这里表扬一下:11.1 先买云盘 https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n811.2 再挂载云盘 到对应机器 https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB11.3 最后Linux格式化数据盘 https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr图2所示，df -h命令查看，大磁盘/dev/vdb112.MySQL修改数据目录为大磁盘，重新启动失败，解决12345678910111213141516171819202122232425262728293031323312.1 修改数据目录为大磁盘$ mkdir -p /mnt/mysql_data$ chown mysql:mysql /mnt/mysql_data$ vi /etc/mysql/mysql.conf.d/mysqld.cnfdatadir = /mnt/mysql_data12.2 无法启动mysql$ service mysql restart无法启动成功，查看日志2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error12.3 百思不得其解，CentOS也没有这么麻烦，Ubuntu难道这么搞事吗？12.4 新增mysqld内容$ vi /etc/apparmor.d/local/usr.sbin.mysqld# Site-specific additions and overrides for usr.sbin.mysqld.# For more details, please see /etc/apparmor.d/local/README./mnt/mysql_data/ r,/mnt/mysql_data/** rwk,12.5 reload apparmor的配置并重启$ service apparmor reload $ service apparmor restart 12.6 重启mysql$ service mysql restart如果启动不了，查看/var/log/mysql/error.log如果出现：InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable 仔细核对目录权限12.7 进mysql查询数据验证，成功select * from wenshu.wenshu2018 limit 1\G;13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。最后@若泽数据J哥总结一下:表结构正确的获取；机器磁盘规划提前思考；ibd数据文件恢复；最后加上一个聪明的媳妇！(PS:老板会给媳妇涨薪水不🙅‍♂️)]]></content>
      <categories>
        <category>其他组件</category>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>环境搭建</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入门Impala只需此篇]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%85%A5%E9%97%A8Impala%E5%8F%AA%E9%9C%80%E6%AD%A4%E7%AF%87%20(1)%2F</url>
    <content type="text"><![CDATA[学习路径官网：http://impala.apache.org/使用手册：http://impala.apache.org/docs/build/html/index.htmlSql：http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html窗口函数：http://impala.apache.org/docs/build/html/topics/impala_functions.html基本操作：http://impala.apache.org/docs/build/html/topics/impala_tutorial.htmlimpala-shell：http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html概述Apache Impala是Apache Hadoop的开源原生分析数据库;Impala于2017年11月15日从Apache孵化成顶级项目。在以前称为“Cloudera Impala”的文档中，现在的官方名称是“Apache Impala”。Impala为Hadoop上的BI /分析查询提供低延迟和高并发性（不是由Apache Hive等批处理框架提供）。即使在多租户环境中，Impala也可以线性扩展。利用与Hadoop部署相同的文件和数据格式以及元数据，安全性和资源管理框架 - 无冗余基础架构或数据转换/复制。对于Apache Hive用户，Impala使用相同的元数据和ODBC驱动程序。与Hive一样，Impala支持SQLImpala与本机Hadoop安全性和Kerberos集成以进行身份验证，通过Sentry模块，您可以确保为正确的用户和应用程序授权使用正确的数据。使用Impala，无论是使用SQL查询还是BI应用程序，更多用户都可以通过单个存储库和元数据存储进行交互什么是ImpalaImpala是一种面向实时或者面向批处理的框架;Impala的数据可以存储在HDFS,HBase和Amazon Simple Storage Servive(S3)中;Impala和Hive使用了相同的元数据存储;可以通过SQL的语法,JDBC,ODBC和用户界面(Hue中的Impala进行查询);我们知道Hive底层是MapReduce,在这里就可以看出区别了,Impala并不是为了替换构建在MapReduce上的批处理框架,就像我们说的Hive,Hive适用于长时间运行的批处理作业,例如涉及到Extract,Transform和Load(ETL)类型的作业.而Impala是进行实时处理的.优势通过sql进行大量数据处理;可以进行分布式部署,进行分布式查询;可以和不同组件之间进行数据共享,不需要复制或者导入,导出等步骤,例如:可以先使用hive对数据进行ETL操作然后使用Impala进行查询.因为Impala和hive公用同一个元数据,这样就可以方便的对hive生成的数据进行分析.Impala如何与Apache Hadoop一起使用Impala解决方案由以下组件组成：客户端 - 包括Hue，ODBC客户端，JDBC客户端和Impala Shell的实体都可以与Impala进行交互。这些接口通常用于发出查询或完成管理任务，例如连接到Impala。Hive Metastore - 存储有关Impala可用数据的信息。例如，Metastore让Impala知道哪些数据库可用，以及这些数据库的结构是什么。在创建，删除和更改模式对象，将数据加载到表中等等时，通过Impala SQL语句，相关的元数据更改将通过Impala 1.2中引入的专用目录服务自动广播到所有Impala节点。Impala - 此过程在DataNodes上运行，协调并执行查询。Impala的每个实例都可以接收，计划和协调来自Impala客户端的查询。HBase和HDFS -数据的存储。下面这幅图应该说的很清楚了:使用Impala执行的查询流程如下：用户应用程序通过ODBC或JDBC向Impala发送SQL查询，这些查询提供标准化的查询接口。用户应用程序可以连接到impalad群集中的任何应用程序。这impalad将成为查询的协调者。Impala会解析查询并对其进行分析，以确定impalad整个群集中的实例需要执行哪些任务 。计划执行以实现最佳效率。本地impalad实例访问HDFS和HBase等服务以提供数据。每个都impalad将数据返回给协调impalad，协调将这些结果发送给客户端。impala-shell使用Impala shell工具（impala-shell）来设置数据库和表，插入数据和发出查询选项描述-B or –delimited导致使用分隔符分割的普通文本格式打印查询结果。当为其他 Hadoop 组件生成数据时有用。对于避免整齐打印所有输出的性能开销有用，特别是使用查询返回大量的结果集进行基准测试的时候。使用 –output_delimiter 选项指定分隔符。使用 -B 选项常用于保存所有查询结果到文件里而不是打印到屏幕上。在 Impala 1.0.1 中添加–print_header是否打印列名。整齐打印时是默认启用。同时使用 -B 选项时，在首行打印列名-o filename or –output_file filename保存所有查询结果到指定的文件。通常用于保存在命令行使用 -q 选项执行单个查询时的查询结果。对交互式会话同样生效；此时你只会看到获取了多少行数据，但看不到实际的数据集。当结合使用 -q 和 -o 选项时，会自动将错误信息输出到 /dev/null(To suppress these incidental messages when combining the -q and -o options, redirect stderr to /dev/null)。在 Impala 1.0.1 中添加–output_delimiter=character当使用 -B 选项以普通文件格式打印查询结果时，用于指定字段之间的分隔符(Specifies the character to use as a delimiter between fields when query results are printed in plain format by the -B option)。默认是制表符 tab (’\t’)。假如输出结果中包含了分隔符，该列会被引起且/或转义( If an output value contains the delimiter character, that field is quoted and/or escaped)。在 Impala 1.0.1 中添加-p or –show_profiles对 shell 中执行的每一个查询，显示其查询执行计划 (与 EXPLAIN 语句输出相同) 和发生低级故障(low-level breakdown)的执行步骤的更详细的信息-h or –help显示帮助信息-i hostname or –impalad=hostname指定连接运行 impalad 守护进程的主机。默认端口是 21000。你可以连接到集群中运行 impalad 的任意主机。假如你连接到 impalad 实例通过 –fe_port 标志使用了其他端口，则应当同时提供端口号，格式为 hostname:port-q query or –query=query从命令行中传递一个查询或其他 shell 命令。执行完这一语句后 shell 会立即退出。限制为单条语句，可以是 SELECT, CREATE TABLE, SHOW TABLES, 或其他 impala-shell 认可的语句。因为无法传递 USE 语句再加上其他查询，对于 default 数据库之外的表，应在表名前加上数据库标识符(或者使用 -f 选项传递一个包含 USE 语句和其他查询的文件)-f query_file or –query_file=query_file传递一个文件中的 SQL 查询。文件内容必须以分号分隔-k or –kerberos当连接到 impalad 时使用 Kerberos 认证。如果要连接的 impalad 实例不支持 Kerberos，将显示一个错误-s kerberos_service_name or –kerberos_service_name=nameInstructs impala-shell to authenticate to a particular impalad service principal. 如何没有设置 kerberos_service_name ，默认使用 impala。如何启用了本选项，而试图建立不支持Kerberos 的连接时，返回一个错误(If this option is used in conjunction with a connection in which Kerberos is not supported, errors are returned)-V or –verbose启用详细输出–quiet关闭详细输出-v or –version显示版本信息-c查询执行失败时继续执行-r or –refresh_after_connect建立连接后刷新 Impala 元数据，与建立连接后执行 REFRESH 语句效果相同-d default_db or –database=default_db指定启动后使用的数据库，与建立连接后使用 USE 语句选择数据库作用相同，如果没有指定，那么使用 default 数据库-l启用 LDAP 认证-u当使用 -l 选项启用 LDAP 认证时，提供用户名(使用短用户名，而不是完整的 LDAP 专有名称(distinguished name)) ，shell 会提示输入密码概念与架构Impala Server的组件Impala服务器是分布式，大规模并行处理（MPP）数据库引擎。它由在群集中的特定主机上运行的不同守护程序进程组成。The Impala DaemonImpala的核心组件是Impala daemon。Impala daemon执行的一些关键功能是：读取和写入数据文件。接受从impala-shell命令，Hue，JDBC或ODBC传输的查询。并行化查询并在群集中分配工作。将中间查询结果发送回中央协调器。可以通过以下方式之一部署Impala守护程序：HDFS和Impala位于同一位置，每个Impala守护程序与DataNode在同一主机上运行。Impala单独部署在计算群集中，可从HDFS，S3，ADLS等远程读取。Impala守护进程与StateStore保持持续通信，以确认哪些守护进程是健康的并且可以接受新工作。在Impala 2.9及更高版本中，您可以控制哪些主机充当查询协调器，哪些主机充当查询执行程序，以提高大型群集上高度并发工作负载的可伸缩性。Impala StatestoreImpala Statestore进程检查集群中所有Impala daemon的运行状况，并把信息反馈给Impala daemon进程。您只需要在群集中的一台主机上执行此类过程。如果Impala守护程序由于硬件故障，网络错误，软件问题或其他原因而脱机，则StateStore会通知所有其他Impala daemon程序，以便将来的查询可以避免向无法访问的Impala守护程序发出请求。因为StateStore的目的是在出现问题时提供帮助并向协调器广播元数据，因此对Impala集群的正常操作并不总是至关重要的。如果StateStore未运行或无法访问，则在处理Impala已知的数据时，Impala守护程序会像往常一样继续运行和分配工作。如果其他Impala守护程序失败，则群集变得不那么健壮，并且当StateStore脱机时，元数据变得不那么一致。当StateStore重新联机时，它会重新建立与Impala守护程序的通信并恢复其监视和广播功能。The Impala Catalog ServiceImpala Catalog Service进程可以把Impala SQL语句中的元数据更改信息反馈到集群中的所有Impala守护程序。只需要在群集中的一台主机上执行此类过程。因为请求是通过StateStore守护程序传递的，所以要在同一主机上运行statestored和catalogd服务。当通过Impala发出的语句执行元数据更改时，Impala Catalog Service进程避免了REFRESH和INVALIDATE METADATA语句的使用,该进程可以为我们更新元数据信息。使用–load_catalog_in_background选项控制何时加载表的元数据。如果设置为false，则在第一次引用表时会加载表的元数据。这意味着第一次运行可能比后续运行慢。在impala2.2开始，默认load_catalog_in_background是 false。如果设置为true，即使没有查询需要该元数据，目录服务也会尝试加载表的元数据。因此，当运行需要它的第一个查询时，可能已经加载了元数据。但是，由于以下原因，我们建议不要将选项设置为true。后台加载可能会干扰查询特定的元数据加载。这可能在启动时或在使元数据无效之后发生，持续时间取决于元数据的数量，并且可能导致看似随机的长时间运行的查询难以诊断。Impala可能会加载从未使用过的表的元数据，这会增加目录服务和Impala守护程序的目录大小，从而增加内存使用量。负载均衡和高可用性的大多数注意事项适用于impalad守护程序。该statestored和catalogd守护进程不具备高可用性的特殊要求，因为这些守护进程的问题不会造成数据丢失。如果这些守护程序由于特定主机上的中断而变得不可用，则可以停止Impala服务，删除Impala StateStore和Impala目录服务器角色，在其他主机上添加角色，然后重新启动Impala服务。数据类型Impala支持一组数据类型，可用于表列，表达式值，函数参数和返回值。注意： 目前，Impala仅支持标量类型，而不支持复合类型或嵌套类型。访问包含任何具有不受支持类型的列的表会导致错误。有关Impala和Hive数据类型之间的差异，请参阅:http://impala.apache.org/docs/build/html/topics/impala_langref_unsupported.html#langref_hiveql_deltaARRAY复杂类型（仅限Impala 2.3或更高版本）BIGINT数据类型BOOLEAN数据类型CHAR数据类型（仅限Impala 2.0或更高版本）DECIMAL数据类型（仅限Impala 3.0或更高版本）双数据类型FLOAT数据类型INT数据类型MAP复杂类型（仅限Impala 2.3或更高版本）REAL数据类型SMALLINT数据类型STRING数据类型STRUCT复杂类型（仅限Impala 2.3或更高版本）TIMESTAMP数据类型TINYINT数据类型VARCHAR数据类型（仅限Impala 2.0或更高版本）复杂类型（仅限Impala 2.3或更高版本）]]></content>
      <categories>
        <category>Impala</category>
      </categories>
      <tags>
        <tag>Impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch常用操作解析]]></title>
    <url>%2F2019%2F05%2F13%2FElasticsearch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[创建Maven管理的Java项目在pom.xml中添加依赖：1234567&lt;es.version&gt;6.1.1&lt;/es.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;$&#123;es.version&#125;&lt;/version&gt;&lt;/dependency&gt;然后创建一个单元测试类ESApp：1234567891011121314private TransportClient client; @Before public void setUp() throws Exception &#123; Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;mycluster&quot;) .put(&quot;client.transport.sniff&quot;, &quot;true&quot;)//增加自动嗅探配置 .build(); client = new PreBuiltTransportClient(settings); client.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;10.8.24.94&quot;), 9300)); System.out.println(client.toString()); &#125;运行后报错1java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonFactory123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt;&lt;/dependency&gt;运行后成功拿到ES的client：创建一个Index123456@Test public void createIndex() &#123; client.admin().indices().prepareCreate(INDEX).get(); System.out.println(&quot;创建Index成功&quot;); &#125;删除一个Index12345@Test public void deleteIndex() &#123; client.admin().indices().prepareDelete(INDEX).get(); System.out.println(&quot;删除Index成功&quot;); &#125;放入数据的三种方式1234567891011121314151617181920212223242526272829303132333435//不推荐使用，太繁琐拼json格式 @Test public void createDoc() &#123; String json = &quot;&#123;\&quot;name\&quot;:\&quot;若泽数据\&quot;&#125;&quot;; IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;100&quot;) .setSource(json, XContentType.JSON) .get(); &#125; //推荐使用 @Test public void test01() throws Exception &#123; Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;(); json.put(&quot;name&quot;, &quot;ruozedata&quot;); json.put(&quot;message&quot;, &quot;trying out Elasticsearch&quot;); IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;101&quot;).setSource(json).get(); System.out.println(response.getVersion()); &#125; //推荐使用 @Test public void test02() throws Exception &#123; XContentBuilder builder = jsonBuilder() .startObject() .field(&quot;user&quot;, &quot;ruoze&quot;) .field(&quot;postDate&quot;, new Date()) .field(&quot;message&quot;, &quot;trying out Elasticsearch&quot;) .endObject(); IndexResponse response = client.prepareIndex(INDEX, TYPE, &quot;102&quot;).setSource(builder).get(); System.out.println(response.getVersion()); &#125;拿到一条数据123456@Test public void getDoc() &#123; GetResponse response = client.prepareGet(INDEX, TYPE, &quot;100&quot;).get(); System.out.println(response.getSourceAsString()); &#125;拿到多条数据123456789101112131415161718@Test public void getDocsByIds() &#123; MultiGetResponse responses = client.prepareMultiGet() .add(INDEX, TYPE,&quot;100&quot;) .add(INDEX, TYPE, &quot;101&quot;, &quot;102&quot;, &quot;1000&quot;) .get(); for (MultiGetItemResponse response : responses) &#123; GetResponse res = response.getResponse(); if (res.isExists()) &#123; System.out.println(res); &#125; else &#123; System.out.println(&quot;没有这条数据&quot;); &#125; &#125; &#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)]]></title>
    <url>%2F2019%2F05%2F13%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)%2F</url>
    <content type="text"><![CDATA[若泽数据CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)视频:https://www.bilibili.com/video/av52167219PS:建议先看课程视频1-2篇，再根据视频或文档部署，如有问题，及时与@若泽数据J哥联系。一.准备工作1.离线部署主要分为三块:a.MySQL离线部署b.CM离线部署c.Parcel文件离线源部署2.规划:节点MySQL部署组件Parcel文件离线源CM服务进程大数据组件hadoop001MySQLParcelActivity MonitorNN RM DN NMhadoop002Alert PublisherEvent ServerDN NMhadoop003Host MonitorService MonitorDN NM3.下载源:CMcloudera-manager-centos7-cm5.16.1_x86_64.tar.gzParcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1manifest.jsonJDKhttps://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html下载jdk-8u202-linux-x64.tar.gzMySQLhttps://dev.mysql.com/downloads/mysql/5.7.html#downloads下载mysql-5.7.26-el7-x86_64.tar.gzMySQL jdbc jarmysql-connector-java-5.1.47.jar下载完成后要重命名去掉版本号，mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar###准备好百度云,下载安装包:链接:https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA 密码:viqp二.集群节点初始化1.阿里云上海区购买3台，按量付费虚拟机CentOS7.2操作系统，2核8G最低配置2.当前笔记本或台式机配置hosts文件MAC: /etc/hostsWindow: C:\windows\system32\drivers\etc\hosts1234公网地址: 106.15.234.222 hadoop001 106.15.235.200 hadoop002 106.15.234.239 hadoop0033.设置所有节点的hosts文件1234私有地铁、内网地址:echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts4.关闭所有节点的防火墙及清空规则123systemctl stop firewalld systemctl disable firewalldiptables -F5.关闭所有节点的selinux123vi /etc/selinux/config将SELINUX=enforcing改为SELINUX=disabled 设置后需要重启才能生效6.设置所有节点的时区一致及时钟同步1234567891011121314151617181920212223242526272829303132333435363738394041424344454647486.1.时区[root@hadoop001 ~]# dateSat May 11 10:07:53 CST 2019[root@hadoop001 ~]# timedatectl Local time: Sat 2019-05-11 10:10:31 CST Universal time: Sat 2019-05-11 02:10:31 UTC RTC time: Sat 2019-05-11 10:10:29 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: yes DST active: n/a#查看命令帮助，学习至关重要，无需百度，太👎[root@hadoop001 ~]# timedatectl --helptimedatectl [OPTIONS...] COMMAND ...Query or change system time and date settings. -h --help Show this help message --version Show package version --no-pager Do not pipe output into a pager --no-ask-password Do not prompt for password -H --host=[USER@]HOST Operate on remote host -M --machine=CONTAINER Operate on local container --adjust-system-clock Adjust system clock when changing local RTC modeCommands: status Show current time settings set-time TIME Set system time set-timezone ZONE Set system time zone list-timezones Show known time zones set-local-rtc BOOL Control whether RTC is in local time set-ntp BOOL Control whether NTP is enabled#查看哪些时区[root@hadoop001 ~]# timedatectl list-timezonesAfrica/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/AlgiersAfrica/AsmaraAfrica/Bamako#所有节点设置亚洲上海时区 [root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455566.2.时间#所有节点安装ntp[root@hadoop001 ~]# yum install -y ntp#选取hadoop001为ntp的主节点[root@hadoop001 ~]# vi /etc/ntp.conf #timeserver 0.asia.pool.ntp.orgserver 1.asia.pool.ntp.orgserver 2.asia.pool.ntp.orgserver 3.asia.pool.ntp.org#当外部时间不可用时，可使用本地硬件时间server 127.127.1.0 iburst local clock #允许哪些网段的机器来同步时间restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap#开启ntpd及查看状态[root@hadoop001 ~]# systemctl start ntpd[root@hadoop001 ~]# systemctl status ntpd ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago Main PID: 18518 (ntpd) CGroup: /system.slice/ntpd.service └─18518 /usr/sbin/ntpd -u ntp:ntp -gMay 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usecMay 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabledMay 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.#验证[root@hadoop001 ~]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== LOCAL(0) .LOCL. 10 l 726 64 0 0.000 0.000 0.000#其他从节点停止禁用ntpd服务 [root@hadoop002 ~]# systemctl stop ntpd[root@hadoop002 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#每天凌晨同步hadoop001节点时间[root@hadoop002 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop001 [root@hadoop003 ~]# systemctl stop ntpd[root@hadoop004 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#每天凌晨同步hadoop001节点时间[root@hadoop003 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop0017.部署集群的JDK123456789mkdir /usr/javatar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/#切记必须修正所属用户及用户组chown -R root:root /usr/java/jdk1.8.0_45echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profileecho &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profilesource /etc/profilewhich java8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)文档链接:https://github.com/Hackeruncle/MySQL视频链接:https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ 密码:whic9.创建CDH的元数据库和用户、amon服务的数据库及用户12345create database cmf DEFAULT CHARACTER SET utf8;create database amon DEFAULT CHARACTER SET utf8;grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;flush privileges;10.hadoop001节点部署mysql jdbc jar12mkdir -p /usr/share/java/cp mysql-connector-java.jar /usr/share/java/三.CDH部署1.离线部署cm server及agent1234567891011121314151617181920211.1.所有节点创建目录及解压mkdir /opt/cloudera-managertar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/1.2.所有节点修改agent的配置，指向server的节点hadoop001sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini1.3.主节点修改server的配置:vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=hadoop001com.cloudera.cmf.db.name=cmfcom.cloudera.cmf.db.user=cmfcom.cloudera.cmf.db.password=Ruozedata123456!com.cloudera.cmf.db.setupType=EXTERNAL1.4.所有节点创建用户useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm1.5.目录修改用户及用户组chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager2.hadoop001节点部署离线parcel源1234567891011121314151617182.1.部署离线parcel源$ mkdir -p /opt/cloudera/parcel-repo$ lltotal 3081664-rw-r--r-- 1 root root 2127506677 May 9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel-rw-r--r-- 1 root root 41 May 9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1-rw-r--r-- 1 root root 841524318 May 9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz-rw-r--r-- 1 root root 185515842 Aug 10 2017 jdk-8u144-linux-x64.tar.gz-rw-r--r-- 1 root root 66538 May 9 18:03 manifest.json-rw-r--r-- 1 root root 989495 May 25 2017 mysql-connector-java.jar$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/#切记cp时，重命名去掉1，不然在部署过程CM认为如上文件下载未完整，会持续下载$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha$ cp manifest.json /opt/cloudera/parcel-repo/2.2.目录修改用户及用户组$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/3.所有节点创建软件安装目录、用户及用户组权限mkdir -p /opt/cloudera/parcelschown -R cloudera-scm:cloudera-scm /opt/cloudera/4.hadoop001节点启动Server1234564.1.启动server/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start4.2.阿里云web界面，设置该hadoop001节点防火墙放开7180端口4.3.等待1min，打开 http://hadoop001:7180 账号密码:admin/admin4.4.假如打不开，去看server的log，根据错误仔细排查错误5.所有节点启动Agent1/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start6.接下来，全部Web界面操作http://hadoop001:7180/账号密码:admin/admin7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本9.感谢您选择Cloudera Manager和CDH10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选11.选择存储库12.集群安装–正在安装选定Parcel假如本地parcel离线源配置正确，则”下载”阶段瞬间完成，其余阶段视节点数与内部网络情况决定。13.检查主机正确性123456789101112131415161718192021222324252627282913.1.建议将/proc/sys/vm/swappiness设置为最大值10。swappiness值控制操作系统尝试交换内存的积极；swappiness=0：表示最大限度使用物理内存，之后才是swap空间；swappiness=100：表示积极使用swap分区，并且把内存上的数据及时搬迁到swap空间；如果是混合服务器，不建议完全禁用swap，可以尝试降低swappiness。临时调整：sysctl vm.swappiness=10永久调整：cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf# Adjust swappiness valuevm.swappiness=10EOF13.2.已启用透明大页面压缩，可能会导致重大性能问题，建议禁用此设置。临时调整：echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled永久调整：cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local# Disable transparent_hugepageecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledEOF# centos7.x系统，需要为&quot;/etc/rc.d/rc.local&quot;文件赋予执行权限chmod +x /etc/rc.d/rc.local14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务15.自定义角色分配16.数据库设置17.审改设置，默认即可18.首次运行19.恭喜您!20.主页CDH全套课程目录，如有buy，加微信(ruoze_star)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061620.青云环境介绍和使用 1.Preparation 谈谈怎样入门大数据 谈谈怎样做好一个大数据平台的运营工作 Linux机器,各软件版本介绍及安装(录播) 2.Introduction Cloudera、CM及CDH介绍 CDH版本选择 CDH安装几种方式解读 3.Install&amp;UnInstall 集群节点规划,环境准备(NTP,Jdk and etc) MySQL编译安装及常用命令 推荐:CDH离线安装(踩坑心得,全面剖析) 解读暴力卸载脚本 4.CDH Management CDH体系架构剖析 CDH配置文件深度解析 CM的常用命令 CDH集群正确启动和停止顺序 CDH Tsquery Language CDH常规管理(监控/预警/配置/资源/日志/安全) 5.Maintenance Experiment HDFS HA 配置 及hadoop/hdfs常规命令 Yarn HA 配置 及yarn常规命令 Other CDH Components HA 配置 CDH动态添加删除服务(hive/spark/hbase) CDH动态添加删除机器 CDH动态添加删除及迁移DataNode进程等 CDH升级(5.10.0--&gt;5.12.0) 6.Resource Management Linux Cgroups 静态资源池 动态资源池 多租户案例 7.Performance Tunning Memory/CPU/Network/Disk及集群规划 Linux参数 HDFS参数 MapReduce及Yarn参数 其他服务参数 8.Cases Share CDH4&amp;5之Alternatives命令 的研究 CDH5.8.2安装之Hash verification failed 记录一次CDH4.8.6 配置HDFS HA 坑 CDH5.0集群IP更改 CDH的active namenode exit(GC)和彩蛋分享 9. Kerberos Kerberos简介 Kerberos体系结构 Kerberos工作机制 Kerberos安装部署 CDH启用kerberos Kerberos开发使用(真实代码)10.Summary 总结Join us if you have a dream.若泽数据官网: http://ruozedata.com腾讯课堂，搜若泽数据: http://ruoze.ke.qq.comBilibili网站,搜若泽数据: https://space.bilibili.com/356836323若泽大数据–官方博客若泽大数据–博客一览若泽大数据–内部学员面试题扫一扫，学一学:]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产常用Spark累加器剖析之三(自定义累加器)]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%89(%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[思路 &amp; 需求参考IntAccumulatorParam的实现思路（上述文章中有讲）：1234567trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; // addInPlace有很多具体的实现类 // 如果想要实现自定义的话，就得实现这个方法 addInPlace(t1, t2) &#125;&#125;自定义也可以通过这个方法去实现，从而兼容我们自定义的累加器需求：这里实现一个简单的案例，用分布式的方法去实现随机数1234567891011121314151617181920212223242526272829303132333435363738** * 自定义的AccumulatorParam * * Created by lemon on 2018/7/28. */object UniqueKeyAccumulator extends AccumulatorParam[Map[Int, Int]] &#123; override def addInPlace(r1: Map[Int, Int], r2: Map[Int, Int]): Map[Int, Int] = &#123; // ++用于两个集合相加 r1++r2 &#125; override def zero(initialValue: Map[Int, Int]): Map[Int, Int] = &#123; var data: Map[Int, Int] = Map() data &#125;&#125;/** * 使用自定义的累加器，实现随机数 * * Created by lemon on 2018/7/28. */object CustomAccumulator &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(&quot;CustomAccumulator&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(sparkConf) val uniqueKeyAccumulator = sc.accumulable(Map[Int, Int]())(UniqueKeyAccumulator) val distData = sc.parallelize(1 to 10) val mapCount = distData.map(x =&gt; &#123; val randomNum = new Random().nextInt(20) // 构造一个k-v对 val map: Map[Int, Int] = Map[Int, Int](randomNum -&gt; randomNum) uniqueKeyAccumulator += map &#125;) println(mapCount.count()) // 获取到累加器的值 中的key值，并进行打印 uniqueKeyAccumulator.value.keys.foreach(println) sc.stop() &#125;&#125;运行结果如下图：]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>累加器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令以及安装mysql]]></title>
    <url>%2F2019%2F05%2F08%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.简介Docker是一个开源的应用容器引擎；是一个轻量级容器技术；Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；运行中的这个镜像称为容器，容器启动是非常快速的。2.核心概念docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；docker客户端(Client)：连接docker主机进行操作；docker仓库(Registry)：用来保存各种打包好的软件镜像；docker镜像(Images)：软件打包好的镜像；放在docker仓库中；docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用3.安装环境1234VM ware Workstation10CentOS-7-x86_64-DVD-1804.isouname -r3.10.0-862.el7.x86_64检查内核版本，必须是3.10及以上 查看命令：uname -r4.在linux虚拟机上安装docker步骤：1、检查内核版本，必须是3.10及以上1uname -r2、安装docker1yum install docker3、输入y确认安装12345Dependency Updated: audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1 Complete!(成功标志)4、启动docker123[root@hadoop000 ~]# systemctl start docker[root@hadoop000 ~]# docker -vDocker version 1.13.1, build 8633870/1.13.15、开机启动docker12[root@hadoop000 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6、停止docker1[root@hadoop000 ~]# systemctl stop docker5.常用命令镜像操作操作命令说明检索docker search 关键字 eg：docker search redis我们经常去docker hub上检索镜像的详细信息，如镜像的TAG拉取docker pull 镜像名:tag:tag是可选的，tag表示标签，多为软件的版本，默认是latest列表docker images查看所有本地镜像删除docker rmi image-id删除指定的本地镜像当然大家也可以在官网查找：https://hub.docker.com/容器操作软件镜像（QQ安装程序）—-运行镜像—-产生一个容器（正在运行的软件，运行的QQ）；步骤：1、搜索镜像[root@localhost ~]# docker search tomcat2、拉取镜像[root@localhost ~]# docker pull tomcat3、根据镜像启动容器docker run –name mytomcat -d tomcat:latest4、docker ps查看运行中的容器5、 停止运行中的容器docker stop 容器的id6、查看所有的容器docker ps -a7、启动容器docker start 容器id8、删除一个容器docker rm 容器id9、启动一个做了端口映射的tomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-d：后台运行-p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口10、为了演示简单关闭了linux的防火墙service firewalld status ；查看防火墙状态service firewalld stop：关闭防火墙systemctl disable firewalld.service #禁止firewall开机启动11、查看容器的日志docker logs container-name/container-id更多命令参看https://docs.docker.com/engine/reference/commandline/docker/可以参考镜像文档6.使用docker安装mysqldocker pull mysql123456789101112131415161718192021docker pull mysql Using default tag: latestTrying to pull repository docker.io/library/mysql ... latest: Pulling from docker.io/library/mysqla5a6f2f73cd8: Pull complete 936836019e67: Pull complete 283fa4c95fb4: Pull complete 1f212fb371f9: Pull complete e2ae0d063e89: Pull complete 5ed0ae805b65: Pull complete 0283dc49ef4e: Pull complete a7e1170b4fdb: Pull complete 88918a9e4742: Pull complete 241282fa67c2: Pull complete b0fecf619210: Pull complete bebf9f901dcc: Pull complete Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8Status: Downloaded newer image for docker.io/mysql:latest[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB启动1234567891011[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB[root@hadoop000 ~]# docker run --name mysql01 -d mysql756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a[root@hadoop000 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES756620c8e583 mysql &quot;docker-entrypoint...&quot; 9 seconds ago Exited (1) 7 seconds ago mysql01[root@hadoop000 ~]# docker logs 756620c8e583error: database is uninitialized and password option is not specified You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD可以看到上面启动的方式是错误的，提示我们要带上具体的密码12[root@hadoop000 ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqleae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc061234–name：给新创建的容器命名，此处命名为pwc-mysql-e：配置信息，此处配置mysql的root用户的登陆密码-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...查看是否启动成功123[root@hadoop000 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeae86796e132 mysql &quot;docker-entrypoint...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql02登陆MySQL12345678910111213141516docker exec -it mysql04 /bin/bashroot@e34aba02c0c3:/# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 80Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;其他的高级操作123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci指定mysql的一些配置参数]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽数据课程一览]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[若泽数据课程系列基础班LiunxVM虚拟机安装Liunx常用命令（重点）开发环境搭MySQL源码安装&amp;yum安装CRUD编写权限控制Hadoop架构介绍&amp;&amp;源码编译伪分布式安装&amp;&amp;企业应用HDFS（重点）架构设计副本放置策略读写流程YARN（重点）架构设计工作流程调度管理&amp;&amp;常见参数配置（调优）MapReduce架构设计wordcount原理&amp;&amp;join原理和案例Hive架构设计Hive DDL&amp;DMLjoin在大数据中的使用使用自带UDF和开发自定义UDFSqoop架构设计RDBMS导入导出整合项目将所有组件合作使用。人工智能基础python基础常用库——pandas、numpy、sklearn、keras高级班scala编程（重点）Spark（五星重点）Hadoop高级Hive高级FlumeKafkaHBaseFlinkCDH容器调度平台线下班]]></content>
      <categories>
        <category>生产课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kudu与Spark 生产最佳实践]]></title>
    <url>%2F2019%2F05%2F07%2FKudu%E4%B8%8ESpark%20%E7%94%9F%E4%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[环境12345 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;&lt;/properties&gt;测试代码123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import org.apache.kudu.client._import collection.JavaConverters._object KuduApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate() //Read a table from Kudu val df = spark.read .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;)) .format(&quot;kudu&quot;).load df.schema.printTreeString()// // Use KuduContext to create, delete, or write to Kudu tables// val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)////// // The schema is encoded in a string// val schemalString=&quot;id,age,name&quot;//// // Generate the schema based on the string of schema// val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))// val schema=StructType(fields)////// val KuduTable = kuduContext.createTable(// &quot;test_table&quot;, schema, Seq(&quot;id&quot;),// new CreateTableOptions()// .setNumReplicas(1)// .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema//// val id = KuduTable.getColumn(&quot;id&quot;)// print(id)//// kuduContext.tableExists(&quot;test_table&quot;) &#125;&#125;现象:通过spark sql 操作报如下错误:12345678910111213141516171819202122Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549) at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86) at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146) at cn.zhangyu.KuduApp$.main(KuduApp.scala:18) at cn.zhangyu.KuduApp.main(KuduApp.scala)Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533) at scala.util.Try$.apply(Try.scala:192) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533) at scala.util.Try.orElse(Try.scala:84) at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533) ... 7 more而通过KuduContext是可以操作的没有报错,代码为上面注解部分解决思路查询kudu官网:https://kudu.apache.org/docs/developing.html官网中说出了版本的问题:1234如果将Spark 2与Scala 2.11一起使用，请使用kudu-spark2_2.11工件。kudu-spark版本1.8.0及更低版本的语法略有不同。有关有效示例，请参阅您的版本的文档。可以在发布页面上找到版本化文档。spark-shell --packages org.apache.kudu:kudu-spark2_2.11:1.9.0看到了 官网使用的是1.9.0的版本.但是但是但是官网下面说到了下面几个集成问题:Spark 2.2+在运行时需要Java 8，即使Kudu Spark 2.x集成与Java 7兼容。Spark 2.2是Kudu 1.5.0的默认依赖版本。当注册为临时表时，必须为名称包含大写或非ascii字符的Kudu表分配备用名称。包含大写或非ascii字符的列名的Kudu表不能与SparkSQL一起使用。可以在Kudu中重命名列以解决此问题。&lt;&gt;并且OR谓词不会被推送到Kudu，而是由Spark任务进行评估。只有LIKE带有后缀通配符的谓词才会被推送到Kudu，这意味着它LIKE “FOO%”被推下但LIKE “FOO%BAR”不是。Kudu不支持Spark SQL支持的每种类型。例如， Date不支持复杂类型。Kudu表只能在SparkSQL中注册为临时表。使用HiveContext可能无法查询Kudu表。那就很奇怪了我用的1.5.0版本报错为:找不到类,数据源有问题但是把kudu改成1.9.0 问题解决运行结果:1234root |-- id: string (nullable = false) |-- age: string (nullable = true) |-- name: string (nullable = true)Spark集成最佳实践每个群集避免多个Kudu客户端。一个常见的Kudu-Spark编码错误是实例化额外的KuduClient对象。在kudu-spark中，a KuduClient属于KuduContext。Spark应用程序代码不应创建另一个KuduClient连接到同一群集。相反，应用程序代码应使用KuduContext访问KuduClient使用1234567KuduContext#syncClient。 // Use KuduContext to create, delete, or write to Kudu tables val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext) val list = kuduContext.syncClient.getTablesList.getTablesList if (list.iterator().hasNext)&#123; print(list.iterator().next()) &#125;要诊断KuduClientSpark作业中的多个实例，请查看主服务器的日志中的符号，这些符号会被来自不同客户端的许多GetTableLocations或 GetTabletLocations请求过载，通常大约在同一时间。这种症状特别适用于Spark Streaming代码，其中创建KuduClient每个任务将导致来自新客户端的主请求的周期性波。Spark操作kudu(Scala demo)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.zhangyuimport org.apache.kudu.spark.kudu._import org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.slf4j.LoggerFactoryimport org.apache.kudu.client._import collection.JavaConverters._object SparkTest &#123; //kuduMasters and tableName val kuduMasters = &quot;192.168.13.130:7051&quot; val tableName = &quot;kudu_spark_table&quot; //table column val idCol = &quot;id&quot; val ageCol = &quot;age&quot; val nameCol = &quot;name&quot; //replication val tableNumReplicas = Integer.getInteger(&quot;tableNumReplicas&quot;, 1) val logger = LoggerFactory.getLogger(SparkTest.getClass) def main(args: Array[String]): Unit = &#123; //create SparkSession val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate() //create kuduContext val kuduContext = new KuduContext(kuduMasters,spark.sparkContext) //schema val schema = StructType( List( StructField(idCol, IntegerType, false), StructField(nameCol, StringType, false), StructField(ageCol,StringType,false) ) ) var tableIsCreated = false try&#123; // Make sure the table does not exist if (kuduContext.tableExists(tableName)) &#123; throw new RuntimeException(tableName + &quot;: table already exists&quot;) &#125; //create kuduContext.createTable(tableName, schema, Seq(idCol), new CreateTableOptions() .addHashPartitions(List(idCol).asJava, 3) .setNumReplicas(tableNumReplicas)) tableIsCreated = true import spark.implicits._ //write logger.info(s&quot;writing to table &apos;$tableName&apos;&quot;) val data = Array(Person(1,&quot;12&quot;,&quot;zhangsan&quot;),Person(2,&quot;20&quot;,&quot;lisi&quot;),Person(3,&quot;30&quot;,&quot;wangwu&quot;)) val personRDD = spark.sparkContext.parallelize(data) val personDF = personRDD.toDF() kuduContext.insertRows(personDF,tableName) //useing SparkSQL read table val sqlDF = spark.sqlContext.read .options(Map(&quot;kudu.master&quot; -&gt; kuduMasters, &quot;kudu.table&quot; -&gt; tableName)) .format(&quot;kudu&quot;).kudu sqlDF.createOrReplaceTempView(tableName) spark.sqlContext.sql(s&quot;SELECT * FROM $tableName &quot;).show //upsert some rows val upsertPerson = Array(Person(1,&quot;10&quot;,&quot;jack&quot;)) val upsertPersonRDD = spark.sparkContext.parallelize(upsertPerson) val upsertPersonDF = upsertPersonRDD.toDF() kuduContext.updateRows(upsertPersonDF,tableName) //useing RDD read table val readCols = Seq(idCol,ageCol,nameCol) val readRDD = kuduContext.kuduRDD(spark.sparkContext, tableName, readCols) val userTuple = readRDD.map &#123; case Row( id: Int,age: String,name: String) =&gt; (id,age,name) &#125; println(&quot;count:&quot;+userTuple.count()) userTuple.collect().foreach(println(_)) //delete table kuduContext.deleteTable(tableName) &#125;catch &#123; // Catch, log and re-throw. Not the best practice, but this is a very // simplistic example. case unknown : Throwable =&gt; logger.error(s&quot;got an exception: &quot; + unknown) throw unknown &#125; finally &#123; // Clean up. if (tableIsCreated) &#123; logger.info(s&quot;deleting table &apos;$tableName&apos;&quot;) kuduContext.deleteTable(tableName) &#125; logger.info(s&quot;closing down the session&quot;) spark.close() &#125; &#125;&#125;case class Person(id: Int,age: String,name: String)]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>Kudu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019五一-线下项目第13期圆满结束]]></title>
    <url>%2F2019%2F05%2F05%2F2019%E4%BA%94%E4%B8%80-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC13%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019年五一，3天2夜北京线下班圆满结束一句话，北京温度适宜小伙伴们来自北京、上海、深圳大家为了一个真实目标学习真正企业级大数据生产项目2个生产项目+3个Topic分享一年我们只在节假日举办错过了就是错过了期待@端午节线下项目班第14期]]></content>
      <categories>
        <category>线下实战班</category>
      </categories>
      <tags>
        <tag>线下实战班</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产常用Spark累加器剖析之二]]></title>
    <url>%2F2019%2F04%2F26%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Driver端Driver端初始化构建Accumulator并初始化，同时完成了Accumulator注册，Accumulators.register(this)时Accumulator会在序列化后发送到Executor端Driver接收到ResultTask完成的状态更新后，会去更新Value的值 然后在Action操作执行后就可以获取到Accumulator的值了Executor端Executor端接收到Task之后会进行反序列化操作，反序列化得到RDD和function。同时在反序列化的同时也去反序列化Accumulator(在readObject方法中完成)，同时也会向TaskContext完成注册完成任务计算之后，随着Task结果一起返回给Driver结合源码分析Driver端初始化&ensp;&ensp;Driver端主要经过以下步骤，完成初始化操作：123val accum = sparkContext.accumulator(0, “AccumulatorTest”)val acc = new Accumulator(initialValue, param, Some(name))Accumulators.register(this)Executor端反序列化得到Accumulator&ensp;&ensp;反序列化是在调用ResultTask的runTask方式时候做的操作：123// 会反序列化出来RDD和自己定义的functionval (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)&ensp;&ensp;在反序列化的过程中，会调用Accumulable中的readObject方法：123456789101112131415161718private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123; in.defaultReadObject() // value的初始值为zero；该值是会被序列化的 value_ = zero deserialized = true // Automatically register the accumulator when it is deserialized with the task closure. // // Note internal accumulators sent with task are deserialized before the TaskContext is created // and are registered in the TaskContext constructor. Other internal accumulators, such SQL // metrics, still need to register here. val taskContext = TaskContext.get() if (taskContext != null) &#123; // 当前反序列化所得到的对象会被注册到TaskContext中 // 这样TaskContext就可以获取到累加器 // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor taskContext.registerAccumulator(this) &#125; &#125;注意Accumulable.scala中的value_，是不会被序列化的，@transient关键词修饰了1@volatile @transient private var value_ : R = initialValue // Current value on master累加器在各个节点的累加操作针对传入function中不同的操作，对应有不同的调用方法，以下列举几种（在Accumulator.scala中）：123def += (term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;def add(term: T) &#123; value_ = param.addAccumulator(value_, term) &#125;def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;根据不同的累加器参数，有不同实现的AccumulableParam（在Accumulator.scala中）：123456trait AccumulableParam[R, T] extends Serializable &#123; /** def addAccumulator(r: R, t: T): R def addInPlace(r1: R, r2: R): R def zero(initialValue: R): R&#125;不同的实现如下图所示：以IntAccumulatorParam为例：1234implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123; def addInPlace(t1: Int, t2: Int): Int = t1 + t2 def zero(initialValue: Int): Int = 0&#125;我们发现IntAccumulatorParam实现的是trait AccumulatorParam[T]：12345trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; addInPlace(t1, t2) &#125;&#125;在各个节点上的累加操作完成之后，就会紧跟着返回更新之后的Accumulators的value_值聚合操作在Task.scala中的run方法，会执行如下：123// 返回累加器，并运行task// 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map(runTask(context), context.collectAccumulators())在Executor端已经完成了一系列操作，需要将它们的值返回到Driver端进行聚合汇总，整个顺序如图累加器执行流程：根据执行流程，我们可以发现，在执行完collectAccumulators方法之后，最终会在DAGScheduler中调用updateAccumulators(event)，而在该方法中会调用Accumulators的add方法，从而完成聚合操作：12345678910111213141516171819def add(values: Map[Long, Any]): Unit = synchronized &#123; // 遍历传进来的值 for ((id, value) &lt;- values) &#123; if (originals.contains(id)) &#123; // Since we are now storing weak references, we must check whether the underlying data // is valid. // 根据id从注册的Map中取出对应的累加器 originals(id).get match &#123; // 将值给累加起来，最终将结果加到value里面 // ++=是被重载了 case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value case None =&gt; throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;) &#125; &#125; else &#123; logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;) &#125; &#125;&#125;获取累加器的值通过accum.value方法可以获取到累加器的值至此，累加器执行完毕。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>累加器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2详细介绍]]></title>
    <url>%2F2019%2F04%2F23%2Fspark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Spark发布了最新的版本spark-2.4.2根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的版本介绍Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。 我们强烈建议所有2.4用户升级到此稳定版本。显著的变化SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。详细更改BUGissues内容摘要[ SPARK-26961 ]在Spark Driver中发现Java死锁[ SPARK-26998 ]在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文[ SPARK-27216 ]将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题[ SPARK-27244 ]使用选项logConf = true时密码将以conf的明文形式记录[ SPARK-27267 ]用Snappy 1.1.7.1解压、压缩空序列化数据时失败[ SPARK-27275 ]EncryptedMessage.transferTo中的潜在损坏[ SPARK-27301 ]DStreamCheckpointData因文件系统已缓存而无法清理[ SPARK-27338 ]TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁[ SPARK-27351 ]在仅使用空值列的AggregateEstimation之后的错误outputRows估计[ SPARK-27390 ]修复包名称不匹配[ SPARK-27394 ]当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时[ SPARK-27403 ]修复updateTableStats以使用新统计信息或无更新表统计信息[ SPARK-27406 ]当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断[ SPARK-27419 ]将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败[ SPARK-27453 ]DSV1静默删除DataFrameWriter.partitionBy改进issues内容摘要[ SPARK-27346 ]松开在ExpressionInfo的’examples’字段中换行断言条件[ SPARK-27358 ]将jquery更新为1.12.x以获取安全修复程序[ SPARK-27479 ]隐藏“org.apache.spark.util.kvstore”的API文档工作issues内容摘要[ SPARK-27382 ]在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产常用Spark累加器剖析之一]]></title>
    <url>%2F2019%2F04%2F19%2F%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[由于最近在项目中需要用到Spark的累加器，同时需要自己去自定义实现Spark的累加器，从而满足生产上的需求。对此，对Spark的累加器实现机制进行了追踪学习。本系列文章，将从以下几个方面入手，对Spark累加器进行剖析：Spark累加器的基本概念累加器的重点类构成累加器的源码解析累加器的执行过程累加器使用中的坑自定义累加器的实现Spark累加器基本概念Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，只能累加，不能减少累加器只能在Driver端构建，并只能从Driver端读取结果，在Task端只能进行累加。至于这里为什么只能在Task累加呢？下面的内容将会进行详细的介绍，先简单介绍下：123在Task节点，准确的就是说在executor上；每个Task都会有一个累加器的变量，被序列化传输到executor端运行之后再返回过来都是独立运行的；如果在Task端去获取值的话，只能获取到当前Task的，Task与Task之间不会有影响累加器不会改变Spark lazy计算的特点，只会在Job触发的时候进行相关的累加操作现有累加器类型:累加器的重点类介绍class Accumulator extends Accumulable源码（源码中已经对这个类的作用做了十分详细的解释）：1234567891011121314151617181920212223242526272829/** * A simpler value of [[Accumulable]] where the result type being accumulated is the same * as the types of elements being merged, i.e. variables that are only &quot;added&quot; to through an * associative operation and can therefore be efficiently supported in parallel. They can be used * to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric * value types, and programmers can add support for new types. * * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]]. * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator. * However, they cannot read its value. Only the driver program can read the accumulator&apos;s value, * using its value method. * * @param initialValue initial value of accumulator * @param param helper object defining how to add elements of type `T` * @tparam T result type */class Accumulator[T] private[spark] ( @transient private[spark] val initialValue: T, param: AccumulatorParam[T], name: Option[String], internal: Boolean) extends Accumulable[T, T](initialValue, param, name, internal) &#123; def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123; this(initialValue, param, name, false) &#125; def this(initialValue: T, param: AccumulatorParam[T]) = &#123; this(initialValue, param, None, false) &#125;&#125;主要实现了累加器的初始化及封装了相关的累加器操作方法 同时在类对象构建的时候向Accumulators注册累加器 累加器的add操作的返回值类型和传入进去的值类型可以不一样 所以一定要定义好两步操作（即add方法）：累加操作/合并操作 object Accumulators该方法在Driver端管理着累加器，也包含了累加器的聚合操作 trait AccumulatorParam[T] extends AccumulableParam[T, T]源码：123456789101112/** * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be * available when you create Accumulators of a specific type. * * @tparam T type of value to accumulate */trait AccumulatorParam[T] extends AccumulableParam[T, T] &#123; def addAccumulator(t1: T, t2: T): T = &#123; addInPlace(t1, t2) &#125;&#125;AccumulatorParam的addAccumulator操作的泛型封装 具体的实现还是需要在具体实现类里面实现addInPlace方法 自定义实现累加器的关键 object AccumulatorParam源码：1234567891011121314151617181920212223object AccumulatorParam &#123; // The following implicit objects were in SparkContext before 1.2 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, as there are duplicate codes in SparkContext for backward // compatibility, please update them accordingly if you modify the following implicit objects. implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] &#123; def addInPlace(t1: Double, t2: Double): Double = t1 + t2 def zero(initialValue: Double): Double = 0.0 &#125; implicit object IntAccumulatorParam extends AccumulatorParam[Int] &#123; def addInPlace(t1: Int, t2: Int): Int = t1 + t2 def zero(initialValue: Int): Int = 0 &#125; implicit object LongAccumulatorParam extends AccumulatorParam[Long] &#123; def addInPlace(t1: Long, t2: Long): Long = t1 + t2 def zero(initialValue: Long): Long = 0L &#125; implicit object FloatAccumulatorParam extends AccumulatorParam[Float] &#123; def addInPlace(t1: Float, t2: Float): Float = t1 + t2 def zero(initialValue: Float): Float = 0f &#125; // TODO: Add AccumulatorParams for other types, e.g. lists and strings&#125;从源码中大量的implicit关键词，可以发现该类主要进行隐式类型转换的操作 TaskContextImpl在Executor端管理着我们的累加器，累加器是通过该类进行返回的 累加器的源码解析Driver端&ensp;&ensp;accumulator方法以下列这段代码中的accumulator方法为入口点，进入到相应的源码中去val acc = new Accumulator(initialValue, param, Some(name))源码：12345678910111213class Accumulator[T] private[spark] ( @transient private[spark] val initialValue: T, param: AccumulatorParam[T], name: Option[String], internal: Boolean) extends Accumulable[T, T](initialValue, param, name, internal) &#123; def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = &#123; this(initialValue, param, name, false) &#125; def this(initialValue: T, param: AccumulatorParam[T]) = &#123; this(initialValue, param, None, false) &#125;&#125;&ensp;&ensp;继承的Accumulable[T, T]源码：123456789101112131415class Accumulable[R, T] private[spark] ( initialValue: R, param: AccumulableParam[R, T], val name: Option[String], internal: Boolean) extends Serializable &#123;…// 这里的_value并不支持序列化// 注：有@transient的都不会被序列化@volatile @transient private var value_ : R = initialValue // Current value on master … // 注册了当前的累加器 Accumulators.register(this) …, &#125;&ensp;&ensp;Accumulators.register()源码：12345// 传入参数，注册累加器def register(a: Accumulable[_, _]): Unit = synchronized &#123;// 构造成WeakReferenceoriginals(a.id) = new WeakReference[Accumulable[_, _]](a)&#125;至此，Driver端的初始化已经完成 Executor端Executor端的反序列化是一个得到我们的对象的过程 初始化是在反序列化的时候就完成的，同时反序列化的时候还完成了Accumulator向TaskContextImpl的注册 &ensp;&ensp;TaskRunner中的run方法1234567891011121314151617181920212223242526272829303132// 在计算的过程中，会将RDD和function经过序列化之后传给Executor端private[spark] class Executor( executorId: String, executorHostname: String, env: SparkEnv, userClassPath: Seq[URL] = Nil, isLocal: Boolean = false) extends Logging &#123;... class TaskRunner( execBackend: ExecutorBackend, val taskId: Long, val attemptNumber: Int, taskName: String, serializedTask: ByteBuffer) extends Runnable &#123;…override def run(): Unit = &#123; …val (value, accumUpdates) = try &#123; // 调用TaskRunner中的task.run方法，触发task的运行 val res = task.run( taskAttemptId = taskId, attemptNumber = attemptNumber, metricsSystem = env.metricsSystem) threwException = false res &#125; finally &#123; … &#125;…&#125;&ensp;&ensp;Task中的collectAccumulators()方法1234567891011121314151617private[spark] abstract class Task[T](final def run( taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem) : (T, AccumulatorUpdates) = &#123; … try &#123; // 返回累加器，并运行task // 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map (runTask(context), context.collectAccumulators()) &#125; finally &#123; … &#125; … &#125;)&ensp;&ensp;ResultTask中的runTask方法123456789101112override def runTask(context: TaskContext): U = &#123; // Deserialize the RDD and the func using the broadcast variables. val deserializeStartTime = System.currentTimeMillis() val ser = SparkEnv.get.closureSerializer.newInstance() // 反序列化是在调用ResultTask的runTask方法的时候做的 // 会反序列化出来RDD和自己定义的function val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime metrics = Some(context.taskMetrics) func(context, rdd.iterator(partition, context))&#125;&ensp;&ensp;Accumulable中的readObject方法1234567891011121314151617181920// 在反序列化的过程中会调用Accumulable.readObject方法 // Called by Java when deserializing an object private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException &#123; in.defaultReadObject() // value的初始值为zero；该值是会被序列化的 value_ = zero deserialized = true // Automatically register the accumulator when it is deserialized with the task closure. // // Note internal accumulators sent with task are deserialized before the TaskContext is created // and are registered in the TaskContext constructor. Other internal accumulators, such SQL // metrics, still need to register here. val taskContext = TaskContext.get() if (taskContext != null) &#123; // 当前反序列化所得到的对象会被注册到TaskContext中 // 这样TaskContext就可以获取到累加器 // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor taskContext.registerAccumulator(this) &#125; &#125;&ensp;&ensp;Executor.scala12345678// 在executor端拿到accumuUpdates值之后，会去构造一个DirectTaskResultval directResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.orNull)val serializedDirectResult = ser.serialize(directResult)val resultSize = serializedDirectResult.limit…// 最终由ExecutorBackend的statusUpdate方法发送至Driver端// ExecutorBackend为一个Trait，有多种实现execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)&ensp;&ensp;CoarseGrainedExecutorBackend中的statusUpdate方法123456789// 通过ExecutorBackend的一个实现类：CoarseGrainedExecutorBackend 中的statusUpdate方法// 将数据发送至Driver端override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) &#123; val msg = StatusUpdate(executorId, taskId, state, data) driver match &#123; case Some(driverRef) =&gt; driverRef.send(msg) case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;) &#125; &#125;&ensp;&ensp;CoarseGrainedSchedulerBackend中的receive方法1234567// Driver端在接收到消息之后，会调用CoarseGrainedSchedulerBackend中的receive方法override def receive: PartialFunction[Any, Unit] = &#123; case StatusUpdate(executorId, taskId, state, data) =&gt; // 会在DAGScheduler的handleTaskCompletion方法中将结果返回 scheduler.statusUpdate(taskId, state, data.value) …&#125;&ensp;&ensp;TaskSchedulerImpl的statusUpdate方法123456789101112def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) &#123; … if (state == TaskState.FINISHED) &#123; taskSet.removeRunningTask(tid) // 将成功的Task入队 taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData) &#125; else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) &#123; taskSet.removeRunningTask(tid) taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData) &#125; …&#125;&ensp;&ensp;TaskResultGetter的enqueueSuccessfulTask方法12345def enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) &#123;… result.metrics.setResultSize(size) scheduler.handleSuccessfulTask(taskSetManager, tid, result)…&ensp;&ensp;TaskSchedulerImpl的handleSuccessfulTask方法123456def handleSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, taskResult: DirectTaskResult[_]): Unit = synchronized &#123; taskSetManager.handleSuccessfulTask(tid, taskResult) &#125;&ensp;&ensp;DAGScheduler的taskEnded方法123456789101112def taskEnded( task: Task[_], reason: TaskEndReason, result: Any, accumUpdates: Map[Long, Any], taskInfo: TaskInfo, taskMetrics: TaskMetrics): Unit = &#123; eventProcessLoop.post( // 给自身的消息循环体发了个CompletionEvent // 这个CompletionEvent会被handleTaskCompletion方法所接收到 CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics)) &#125;&ensp;&ensp;DAGScheduler的handleTaskCompletion方法12345678910111213141516171819202122232425// 与上述CoarseGrainedSchedulerBackend中的receive方法章节对应// 在handleTaskCompletion方法中，接收CompletionEvent// 不论是ResultTask还是ShuffleMapTask都会去调用updateAccumulators方法，更新累加器的值private[scheduler] def handleTaskCompletion(event: CompletionEvent) &#123; … event.reason match &#123; case Success =&gt; listenerBus.post(SparkListenerTaskEnd(stageId, stage.latestInfo.attemptId, taskType, event.reason, event.taskInfo, event.taskMetrics)) stage.pendingPartitions -= task.partitionId task match &#123; case rt: ResultTask[_, _] =&gt; // Cast to ResultStage here because it&apos;s part of the ResultTask // TODO Refactor this out to a function that accepts a ResultStage val resultStage = stage.asInstanceOf[ResultStage] resultStage.activeJob match &#123; case Some(job) =&gt; if (!job.finished(rt.outputId)) &#123; updateAccumulators(event) case smt: ShuffleMapTask =&gt; val shuffleStage = stage.asInstanceOf[ShuffleMapStage] updateAccumulators(event)&#125;…&#125;&ensp;&ensp;DAGScheduler的updateAccumulators方法1234567private def updateAccumulators(event: CompletionEvent): Unit = &#123; val task = event.task val stage = stageIdToStage(task.stageId) if (event.accumUpdates != null) &#123; try &#123; // 调用了累加器的add方法 Accumulators.add(event.accumUpdates)&ensp;&ensp;Accumulators的add方法12345678910111213141516171819def add(values: Map[Long, Any]): Unit = synchronized &#123; // 遍历传进来的值 for ((id, value) &lt;- values) &#123; if (originals.contains(id)) &#123; // Since we are now storing weak references, we must check whether the underlying data // is valid. // 根据id从注册的Map中取出对应的累加器 originals(id).get match &#123; // 将值给累加起来，最终将结果加到value里面 // ++=是被重载了 case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value case None =&gt; throw new IllegalAccessError(&quot;Attempted to access garbage collected Accumulator.&quot;) &#125; &#125; else &#123; logWarning(s&quot;Ignoring accumulator update for unknown accumulator id $id&quot;) &#125; &#125; &#125;&ensp;&ensp;Accumulators的++=方法1def ++= (term: R) &#123; value_ = param.addInPlace(value_, term)&#125;&ensp;&ensp;Accumulators的value方法1234567def value: R = &#123; if (!deserialized) &#123; value_ &#125; else &#123; throw new UnsupportedOperationException(&quot;Can&apos;t read accumulator value in task&quot;) &#125; &#125;此时我们的应用程序就可以通过 .value 的方式去获取计数器的值了]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>累加器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产Spark2.4.0如何Debug源代码]]></title>
    <url>%2F2019%2F04%2F17%2F%E7%94%9F%E4%BA%A7Spark2.4.0%E5%A6%82%E4%BD%95Debug%E6%BA%90%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[源码获取与编译直接从Spark官网获取源码或者从GitHub获取下载源码1wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0.tgz解压源码1tar -zxf spark-2.4.0.tgzSpark源码编译此处不再啰嗦，直接去腾讯课堂，搜索“若泽大数据”即可找到编译视频。源码导入IDEA运行hive-thriftserver2从spark-2.4.0-bin-2.6.0-cdh5.7.0/sbin/start-thriftserver.sh 脚本中找到 hive-thriftserver2 的入口类：1org.apache.spark.sql.hive.thriftserver.HiveThriftServer2配置运行环境1Menu -&gt; Run -&gt; Edit Configurations -&gt; 选择 + -&gt; Application-Dspark.master=local[2] 代表使用本地模式运行Spark代码运行之前需要做一件很重要的事情，将 hive-thriftserver 这个子项目的pom依赖全部由provided改为compile：12345678910&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;添加运行依赖的jars1Menu -&gt; File -&gt; Project Structure -&gt; Modules -&gt; spark-hive-thriftserver_2.11 -&gt; Dependencies 添加依赖 jars -&gt; &#123;Spark_home&#125;/assembly/target/scala-2.11/jars/中间遇到的问题问题一12345spark\sql\hive-thriftserver\src\main\java\org\apache\hive\service\cli\thrift\ThriftCLIService.javaError:(52, 75) not found: value TCLIServicepublic abstract class ThriftCLIService extends AbstractService implements TCLIService.Iface, Runnable &#123;………..解决办法： 在spark\sql\hive-thriftserver\src\gen\java右键中点Mark Directory as -&gt; Sources Root即可问题二12Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal at java.lang.ClassLoader.defineClass1(Native Method)解决办法：在 hive-thriftserve 子项目的pom文件中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;xml-apis&lt;/groupId&gt; &lt;artifactId&gt;xml-apis&lt;/artifactId&gt; &lt;version&gt;1.4.01&lt;/version&gt;&lt;/dependency&gt;问题三1java.net.BindException: Cannot assign requested address: Service &apos;sparkDriver&apos; failed after 16 retries (starting from 0)! Consider explicitly setting the appropriate port for the service &apos;sparkDriver&apos; (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.解决办法： 在 /etc/hosts 文件中配置相应的地址映射。成功运行在 HiveThriftServer2 中打断点进行调试源码即可。打一个断点如下所示：就能看到断点所打印出来的信息。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内存管理之三 UnifiedMemoryManager分析]]></title>
    <url>%2F2019%2F04%2F16%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%89%20UnifiedMemoryManager%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[acquireExecutionMemory方法UnifiedMemoryManager中的accquireExecutionMemory方法：当前的任务尝试从executor中获取numBytes这么大的内存该方法直接向ExecutionMemoryPool索要所需内存，索要内存有以下几个关注点：当ExecutionMemory 内存充足，则不会触发向Storage申请内存每个Task能够被使用的内存是被限制的索要内存的大小我们通过源码来进行分析UnifiedMemoryManager.scala中我们点进去后会发现，会调用ExecutionMemoryPool.acquireMemory()方法ExecutionMemoryPool.scala中我们可以发现每Task能够被使用的内存被限制在：poolSize / (2 * numActiveTasks) ~ maxPoolSize / numActiveTasks 之间val maxMemoryPerTask = maxPoolSize /numActiveTasksval minMemoryPerTask = poolSize / (2 * numActiveTasks)UnifiedMemoryManager.scala中其中maxPoolSize = maxMemory - math.min(storageMemoryUsed, storageRegionSize)maxMemory = storage + execution的最大内存poolSize = 当前这个pool的大小maxPoolSize = execution pool的最大内存UnifiedMemoryManager.scala中从上述代码中我们可以知道索要内存的大小：val memoryReclaimableFromStorage=math.max(storageMemoryPool.memoryFree, storageMemoryPool.poolSize -storageRegionSize)取决于StorageMemoryPool的剩余内存和 storageMemoryPool 从ExecutionMemory借来的内存哪个大，取最大的那个，作为可以重新归还的最大内存用公式表达出来就是这一个样子：ExecutionMemory 能借到的最大内存 = StorageMemory 借的内存 + StorageMemory 空闲内存注意：如果实际需要的小于能够借到的最大值，则以实际需要值为准能回收的内存大小为：val spaceToReclaim =storageMemoryPool.freeSpaceToShrinkPool ( math.min(extraMemoryNeeded,memoryReclaimableFromStorage))ExecutionMemoryPool.acquireMemory()解析1234567891011121314151617while (true) &#123; val numActiveTasks = memoryForTask.keys.size val curMem = memoryForTask(taskAttemptId) maybeGrowPool(numBytes - memoryFree) val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks) val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem)) val toGrant = math.min(maxToGrant, memoryFree) if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123; logInfo(s&quot;TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free&quot;) lock.wait() &#125; else &#123; memoryForTask(taskAttemptId) += toGrant return toGrant &#125;&#125;整体流程解析：程序一直处理该task的请求，直到系统判定无法满足该请求或者已经为该请求分配到足够的内存为止；如果当前execution内存池剩余内存不足以满足此次请求时，会向storage部分请求释放出被借走的内存以满足此次请求根据此刻execution内存池的总大小maxPoolSize，以及从memoryForTask中统计出的处于active状态的task的个数计算出：每个task能够得到的最大内存数 maxMemoryPerTask = maxPoolSize / numActiveTasks每个task能够得到的最少内存数 minMemoryPerTask = poolSize /(2 * numActiveTasks)根据申请内存的task当前使用的execution内存大小决定分配给该task多少内存，总的内存不能超过maxMemoryPerTask；但是如果execution内存池能够分配的最大内存小于numBytes，并且如果把能够分配的内存分配给当前task，但是该task最终得到的execution内存还是小于minMemoryPerTask时，该task进入等待状态，等其他task申请内存时再将其唤醒，唤醒之后如果此时满足，就会返回能够分配的内存数，并且更新memoryForTask，将该task使用的内存调整为分配后的值一个Task最少需要minMemoryPerTask才能开始执行acquireStorageMemory方法流程和acquireExecutionMemory类似，当storage的内存不足时，同样会向execution借内存，但区别是当且仅当ExecutionMemory有空闲内存时，StorageMemory 才能借走该内存UnifiedMemoryManager.scala中从上述代码中我们可以知道能借到的内存数为：val memoryBorrowedFromExecution = Math.min(onHeapExecutionMemoryPool.memoryFree,numBytes)所以StorageMemory从ExecutionMemory借走的内存，完全取决于当时ExecutionMemory是不是有空闲内存；借到内存后，storageMemoryPool增加借到的这部分内存，之后同上一样，会调用StorageMemoryPool的acquireMemory()方法StorageMemoryPool.acquireMemory整体流程解析：在申请内存时，如果numBytes大于此刻storage内存池的剩余内存，即if (numBytesToFree &gt; 0)，那么需要storage内存池释放一部分内存以满足申请需求注意：这里的numBytesToFree可以理解为numBytes大小减去Storage内存池剩余大小，大于0，即所需要申请的numBytes大于Storage内存池剩余的内存释放内存后如果memoryFree &gt;= numBytes，就会把这部分内存分配给申请内存的task，并且更新storage内存池的使用情况同时StorageMemoryPool与ExecutionMemoryPool不同的是，他不会像前者那样分不到资源就进行等待，acquireStorageMemory只会返回一个true或是false，告知内存分配是否成功]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内存管理之二 统一内存管理及设计理念]]></title>
    <url>%2F2019%2F04%2F10%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%BA%8C%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8F%8A%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[堆内内存Spark 1.6之后引入的统一内存管理机制，与静态内存管理的区别在于Storage和Execution共享同一块内存空间，可以动态占用对方的空闲区域其中最重要的优化在于动态占用机制，其规则如下：设定基本的Storage内存和Execution内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围双方的空间都不足时，则存储到硬盘，若己方空间不足而对方空余时，可借用对方的空间（存储空间不足是指不足以放下一个完整的 Block）Execution的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间Storage的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle过程中的很多因素，实现起来较为复杂动态内存占用机制动态占用机制如下图所示：凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧譬如：如果Storage的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理堆外内存如下图所示，相较于静态内存管理，引入了动态占用机制计算公式spark从1.6版本以后，默认的内存管理方式就调整为统一内存管理模式由UnifiedMemoryManager实现Unified MemoryManagement模型，重点是打破运行内存和存储内存之间的界限，使spark在运行时，不同用途的内存之间可以实现互相的拆借Reserved Memory这部分内存是预留给系统使用,在1.6.1默认为300MB，这一部分内存不计算在Execution和Storage中；可通过spark.testing.reservedMemory进行设置；然后把实际可用内存减去这个reservedMemor得到usableMemoryExecutionMemory 和 StorageMemory 会共享usableMemory * spark.memory.fraction(默认0.75)注意：在Spark 1.6.1 中spark.memory.fraction默认为0.75在Spark 2.2.0 中spark.memory.fraction默认为0.6User Memory分配Spark Memory剩余的内存，用户可以根据需要使用在Spark 1.6.1中，默认占(Java Heap - Reserved Memory) * 0.25在Spark 2.2.0中，默认占(Java Heap - Reserved Memory) * 0.4Spark Memory计算方式为：(Java Heap – ReservedMemory) * spark.memory.fraction在Spark 1.6.1中，默认为(Java Heap - 300M) * 0.75在Spark 2.2.0中，默认为(Java Heap - 300M) * 0.6Spark Memory又分为Storage Memory和Execution Memory两部分两个边界由spark.memory.storageFraction设定，默认为0.5对比相对于静态内存模型（即Storage和Execution相互隔离、彼此不可拆借），动态内存实现了存储和计算内存的动态拆借：当计算内存超了，它会从空闲的存储内存中借一部分内存使用存储内存不够用的时候，也会向空闲的计算内存中拆借值得注意的地方是：被借走用来执行运算的内存，在执行完任务之前是不会释放内存的通俗的讲，运行任务会借存储的内存，但是它直到执行完以后才能归还内存和动态内存相关的参数spark.memory.fraction12345Spark 1.6.1 默认0.75，Spark 2.2.0 默认0.6 这个参数用来配置存储和计算内存占整个可用内存的比例 这个参数设置的越低，也就是存储和计算内存占可用的比例越低，就越可能频繁的发生内存的释放（将内存中的数据写磁盘或者直接丢弃掉） 反之，如果这个参数越高，发生释放内存的可能性就越小 这个参数的目的是在jvm中留下一部分空间用来保存spark内部数据，用户数据结构，并且防止对数据的错误预估可能造成OOM的风险，这就是Other部分spark.memory.storageFraction1默认 0.5；在统一内存中存储内存所占的比例，默认是0.5，如果使用的存储内存超过了这个范围，缓存的数据会被驱赶spark.memory.useLegacyMode123456默认false；设置是否使用saprk1.5及以前遗留的内存管理模型，即静态内存模型，前面的文章介绍过这个，主要是设置以下几个参数： spark.storage.memoryFraction spark.storage.safetyFraction spark.storage.unrollFraction spark.shuffle.memoryFraction spark.shuffle.safetyFraction动态内存设计中的取舍因为内存可以被Execution和Storage拆借，我们必须明确在这种机制下，当内存压力上升的时候，该如何进行取舍？从三个角度进行分析：倾向于优先释放计算内存倾向于优先释放存储内存不偏不倚，平等竞争释放内存的代价释放存储内存的代价取决于Storage Level.：如果数据的存储level是MEMORY_ONLY的话代价最高，因为当你释放在内存中的数据的时候，你下次再复用的话只能重新计算了如果数据的存储level是MEMORY_AND_DIS_SER的时候，释放内存的代价最低，因为这种方式，当内存不够的时候，它会将数据序列化后放在磁盘上，避免复用的时候再计算，唯一的开销在I/O综述：释放计算内存的代价不是很显而易见：这里没有复用数据重计算的代价，因为计算内存中的任务数据会被移到硬盘，最后再归并起来（后面会有文章介绍到这点）最近的spark版本将计算的中间数据进行压缩使得序列化的代价降到了最低值得注意的是：移到硬盘的数据总会再重新读回来从存储内存移除的数据也许不会被用到，所以当没有重新计算的风险时，释放计算的内存要比释放存储内存的代价更高（假使计算内存部分刚好用于计算任务的时候）实现复杂度实现释放存储内存的策略很简单：我们只需要用目前的内存释放策略释放掉存储内存中的数据就好了实现释放计算内存却相对来说很复杂这里有2个释放计算内存的思路：当运行任务要拆借存储内存的时候，给所有这些任务注册一个回调函数以便日后调这个函数来回收内存协同投票来进行内存的释放值得我们注意的一个地方是，以上无论哪种方式，都需要考虑一种特殊情况：即如果我要释放正在运行的计算任务的内存，同时我们想要cache到存储内存的一部分数据恰巧是由这个计算任务产生的此时，如果我们现在释放掉正在运行的任务的计算内存，就需要考虑在这种环境下会造成的饥饿情况：即生成cache的数据的计算任务没有足够的内存空间来跑出cache的数据，而一直处于饥饿状态（因为计算内存已经不够了，再释放计算内存更加不可取）此外，我们还需要考虑：一旦我们释放掉计算内存，那么那些需要cache的数据应该怎么办？有2种方案：最简单的方式就是等待，直到计算内存有足够的空闲，但是这样就可能会造成死锁，尤其是当新的数据块依赖于之前的计算内存中的数据块的时候另一个可选的操作就是丢掉那些最新的正准备写入到磁盘中的块并且一旦当计算内存够了又马上加载回来。为了避免总是丢掉那些等待中的块，我们可以设置一个小的内存空间(比如堆内存的5%)去确保内存中至少有一定的比例的的数据块综述：所给的两种方法都会增加额外的复杂度，这两种方式在第一次的实现中都被排除了综上目前看来，释放掉存储内存中的计算任务在实现上比较繁琐，目前暂不考虑即计算内存借了存储内存用来计算任务，然后释放，这种不考虑；计算内存借来内存之后，是可以不还的结论：我们倾向于优先释放掉存储内存即如果存储内存拆借了计算内存，当计算内存需要进行计算并且内存空间不足的时候，优先把计算内存中这部分被用来存储的内存释放掉可选设计1.设计方案结合我们前面的描述，针对在内存压力下释放存储内存有以下几个可选设计：设计1：释放存储内存数据块，完全平滑计算和存储内存共享一片统一的区域，没有进行统一的划分内存压力上升，优先释放掉存储内存部分中的数据如果压力没有缓解，开始将计算内存中运行的任务数据进行溢写磁盘设计2：释放存储内存数据块，静态存储空间预留，存储空间的大小是定死的这种设计和1设计很像，不同的是会专门划分一个预留存储内存区域：在这个内存区域内，存储内存不会被释放，只有当存储内存超出这个预留区域，才会被释放（即超过50%了就被释放，当然50%为默认值）。这个参数由spark.memory.storageFraction（默认值为0.5，即计算和存储内存的分割线）配置设计3：释放存储内存数据块，动态存储空间预留这种设计于设计2很相似，但是存储空间的那一部分区域不再是静态设置的了，而是动态分配；这样设置带来的不同是计算内存可以尽可能借走存储内存中可用的部分，因为存储内存是动态分配的结论：最终采用的的是设计32.各个方案的优劣设计1被拒绝的原因设计1不适合那些对cache内存重度依赖的saprk任务，因为设计1中只要内存压力上升就释放存储内存设计2被拒绝的原因设计2在很多情况下需要用户去设置存储内存中那部分最小的区域另外无论我们设置一个具体值，只要它非0，那么计算内存最终也会达到一个上限，比如，如果我们将存储内存设置为0.6，那么有效的执行内存就是：Spark 1.6.1 可用内存0.40.75Spark 2.2.0 可用内存0.40.6那么如果用户没有cache数据，或是cache的数据达不到设置的0.6，那么这种情况就又回到了静态内存模型那种情况，并没有改善什么最终选择设计3的原因设计3就避免了2中的问题只要存储内存有空余的情况，那么计算内存就可以借用需要关注的问题是：当计算内存已经使用了存储内存中的所有可用内存但是又需要cache数据的时候应该怎么处理最早的版本中直接释放最新的block来避免引入执行驱赶策略（eviction策略，上述章节中有介绍）的复杂性设计3是唯一一个同时满足下列条件的：存储内存没有上限计算内存没有上限保障了存储空间有一个小的保留区域]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019清明-线下项目第12期圆满结束]]></title>
    <url>%2F2019%2F04%2F09%2F2019%E6%B8%85%E6%98%8E-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC12%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019年清明3天一句话，上海温度适宜小伙伴们来自五湖四海北京、成都、深圳、广州杭州、山东、齐齐哈尔大家为了一个真实目标学习真正企业级大数据生产项目2个生产项目+3个Topic分享一年我们只在节假日举办清明3天+2夜，错过了就是错过了期待@端午节线下项目班第13期]]></content>
      <categories>
        <category>线下实战班</category>
      </categories>
      <tags>
        <tag>线下实战班</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内存管理之一 静态内存管理]]></title>
    <url>%2F2019%2F04%2F03%2FSpark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B9%8B%E4%B8%80%20%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Spark内存管理简介Spark从1.6开始引入了动态内存管理模式，即执行内存和存储内存之间可以相互抢占Spark提供了2种内存分配模式：静态内存管理统一内存管理本系列文章将分别对这两种内存管理模式的优缺点以及设计原理进行分析（主要基于Spark 1.6.1的内存管理进行分析）在本篇文章中，将先对静态内存管理进行介绍堆内内存在Spark最初采用的静态内存管理机制下，存储内存、执行内存和其它内存的大小在Spark应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置，堆内内存的分配如下图所示：默认情况下，spark内存管理采用unified模式，如果要开启静态内存管理模式，需要将spark.memory.useLegacyMode参数调为true（默认为false），1.6.1版本的官网配置如下所示：将参数调整为true之后，就会进入到静态内存管理中来，可以通过SparkEnv.scala中发现：123如果spark.memory.useLegacyMode为true，就进入到StaticMemoryManager（静态内存管理）；如果为false，就进入到UnifiedMemoryManager（统一内存管理）；同时我们可以发现该参数的默认值为false，即默认情况下就会调用统一内存管理类。Execution内存####可用的Execution内存用于shuffle操作的内存，取决于join、sort、aggregation等过程频繁的IO需要的Buffer临时数据存储简单来说，spark在shuffle write的过程中，每个executor会将数据写到该executor的物理磁盘上，下一个stage的task会去上一个stage拉取其所需要处理的数据，并且是边拉取边进行处理的（和MapReduce的拉取合并数据基本一样），这个时候就会用到一个aggregate的数据结构，比如hashmap这种边拉取数据边进行聚合。这部分内存就被称为execution内存从StaticMemoryManager.scala中的getMaxExecutionMemory方法中，我们可以发现：每个executor分配给execution的内存为：123ExecutionMemory = systemMaxMemory * memoryFraction * safetyFraction 默认情况下为：systemMaxMemory * 0.2 * 0.8 = 0.16 * systemMaxMemory 即默认为executor最大可用内存 * 0.16Execution内存再运行的时候会被分配给运行在JVM上的task；这里不同的是，分配给每个task的内存并不是固定的，而是动态的；spark不是一上来就分配固定大小的内存块给task，而是允许一个task占据JVM所有execution内存每个JVM上的task可以最多申请至多1/N的execution内存，其中N为active task的个数，由spark.executor.cores指定；如果task的申请没有被批准，它会释放一部分内存，并且下次申请的时候，它会申请更小的一部分内存注：每个Executor单独运行在一个JVM进程中，每个Task则是运行在Executor中的线程spark.executor.cores设置的是每个executor的core数量task的数量就是partition的数量一般来说，一个core设置2~4个partition注意： 为了防止过多的spilling数据，只有当一个task分配到的内存达到execution内存1/2N的时候才会spill，如果目前空闲的内存达不到1/2N的时候，内存申请会被阻塞直到其它的task spill掉它们的内存；如果不这样限制，假设当前一个任务占据了绝大部分内存，那么新来的task会一直往硬盘spill数据，这样就会导致比较严重的I/O问题；而我们做了一定程度的限制，会进行一定程度的阻塞等待，对于频繁的小数据集的I/O会有一定的减缓例子：某executor先启动一个task A，并在task B启动前快速占用了所有可用的内存；在B启用之后N变成了2，task B会阻塞直到task A spill，自己可以获得1/2N=1/4的execution内存的时候；而一大task B获取到了1/4的内存，A和B就都有可能spill了预留内存Spark之所以会有一个SafetyFraction这样的参数，是为了避免潜在的OOM。例如，进行计算时，有一个提前未预料到的比较大的数据，会导致计算时间延长甚至OOM，safetyFraction为storage和execution都提供了额外的buffer以防止此类的数据倾斜；这部分内存叫作预留内存####Storage内存####可用的Storage内存该部分内存用作对RDD的缓存（如调用cache、persist等方法），节点间传输的广播变量StaticMemoryManager.scala中的getMaxStorageMemory方法发现：最后为每个executor分配到的storage的内存：123StorageMemory = systemMaxMemory * memoryFraction * safetyFraction 默认情况下为：systemMaxMemory * 0.6 * 0.9 = 0.54 * systemMaxMemory 即默认分配executor最大可用内存的0.54预留内存同Execution内存中的预留部分UnrollUnroll是storage中比较特殊的一部分，它默认占据storage总内存的20%BlockManager是spark自己实现的内部分布式文件系统，BlockManager接受数据（可能从本地或者其他节点）的时候是以iterator的形式，并且这些数据是有序列化和非序列化的，因此需要注意以下两点：Iterator在物理内存上是不连续的，如果后续spark要把数据装载进内存的话，就需要把这些数据放进一个array（物理上连续）另外，序列化数据需要进行展开，如果直接展开序列化的数据，会造成OOM，所以BlockManager会逐渐的展开这个iterator，并逐渐检查内存里是否还有足够的空间用来展开数据放进array里StaticMemoryManager.scala中的maxUnrollMemory方法：Unroll的优先级别还是比较高的，它使用的内存空间是可以从storage中借用的，如果在storage中没有现存的数据block，它甚至可以占据整个storage空间；如果storage中有数据block，它可以最大drop掉内存的数据是通过spark.storage.unrollFraction来控制的，通过源码可知这部分的默认值为0.2注意： 这个20%的空间并不是静态保留的，而是通过drop掉内存中的数据block来分配的（动态的分配过程）；如果unroll失败了，spark会把这部分数据evict到硬盘中去eviction策略在spark技术文档中，eviction一词经常出现，eviction并不是单纯字面上驱逐的意思。说句题外话，spark通常被我们叫做内存计算框架，但是从严格意义上说，spark并不是内存计算的新技术；无论是cache还是persist这类算子，spark在内存安排上，绝大多数用的都是LRU策略（LRU可以说是一种算法，也可以算是一种原则，用来判断如何从Cache中清除对象，而LRU就是“近期最少使用”原则，当Cache溢出时，最近最少使用的对象将被从Cache中清除）。即当内存不够的时候，会evict掉最远使用过的内存数据block；当evict的时候，spark会将该数据块evict到硬盘，而不是单纯的抛弃掉无论是storage还是execution的内存空间，当内存区域的空间不够用的时候，spark都会evict数据到硬盘Other部分这部分的内存用于程序本身运行所需要的内存，以及用户定义的数据结构和创建的对象，此内存由上面两部分：storage、execution决定的，默认为0.2堆外内存Spark1.6开始引入了Off-heap memory（详见SPARK-11389）堆外的空间分配较为简单，只有存储内存和执行内存，如图所示：可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定（默认为0.5），由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域局限性在Spark的设计文档中，指出了静态内存管理的局限性：没有适用于所有应用的默认配置，通常需要开发人员针对不同的应用进行不同的参数进行配置：比如根据任务的执行逻辑，调整shuffle和storage的内存占比来适应任务的需求这样需要开发人员具备较高的spark原理知识那些不cache数据的应用在运行的时候只会占用一小部分可用内存，而默认的内存配置中storage就用去了60%，造成了浪费]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产Spark Executor Dead快速剖析]]></title>
    <url>%2F2019%2F03%2F12%2F%E7%94%9F%E4%BA%A7Spark_Executor_Dead%E5%BF%AB%E9%80%9F%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[问题现象通过Spark UI查看Executors，发现存在Executor Dead的情况进一步查看dead Executor stderr日志，发现如下报错信息：解决过程 打开GC日志，配置如下12--conf &quot;spark.executor.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;--conf &quot;spark.driver.extraJavaOptions= -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;打开exeutor gc日志，发现一直在full gc，几乎每秒1次，基本处于拒绝服务状态至此找到问题原因，executor内存不够导致dead，调大executor内存即可 ，所以排错方法定位很重要！]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产Spark Streaming 黑名单过滤案例]]></title>
    <url>%2F2019%2F03%2F08%2F%E7%94%9F%E4%BA%A7Spark%20Streaming%20%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[测试数据(通过Socket传入)：12320180808,zs20180808,ls20180808,ww黑名单列表(生产存在表)：12zsls思路原始日志可以通过Streaming直接读取成一个DStream名单通过RDD来模拟一份逻辑实现将DStream转成以下格式(黑名单只有名字)(zs,(20180808,zs))(ls,(20180808,ls))(ww,( 20180808,ww))将黑名单转成(zs, true)(ls, true)DStram与RDD进行LeftJoin(DStream能与RDD进行Join就是借用的transform算子)具体代码实现及注释12345678910111213141516171819202122232425262728package com.soul.spark.Streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * @author soulChun * @create 2019-01-10-16:12 */object TransformApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(&quot;StatafulFunApp&quot;).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf,Seconds(10)) //构建黑名单 val blacks = List(&quot;zs&quot;, &quot;ls&quot;) //通过map操作将黑名单结构转换成(zs, true)(ls, true) val blackRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, true)) val lines = ssc.socketTextStream(&quot;localhost&quot;, 8769) //lines (20180808,zs) //lines 通过map.split(1)之后取得就是zs,然后加一个x就转成了(zs,(20180808,zs)).就可以和blackRDD进行Join了 val clicklog = lines.map(x =&gt; (x.split(&quot;,&quot;)(1), x)).transform(rdd =&gt; &#123; //Join之后数据结构就变成了(zs,[(20180808,zs),true]),过滤掉第二个元素中的第二个元素等于true的 rdd.leftOuterJoin(blackRDD).filter(x =&gt; x._2._2.getOrElse(false) != true) //我们最后要输出的格式是(20180808,zs)，所以取Join之后的第二个元素中的第一个元素 .map(x =&gt; x._2._1) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125;最后输出：]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>sparkstreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[刚出炉的3家大数据面试题(含高级),你会吗？]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%88%9A%E5%87%BA%E7%82%89%E7%9A%843%E5%AE%B6%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(%E5%90%AB%E9%AB%98%E7%BA%A7)%E4%BD%A0%E4%BC%9A%E5%90%97%2F</url>
    <content type="text"><![CDATA[第一家大数据开发的面试题:第二家大数据开发的面试题:第三家大数据开发的面试题:]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkShuffle详解剖析]]></title>
    <url>%2F2019%2F03%2F06%2FSparkShuffle%E8%AF%A6%E8%A7%A3%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashShuffle概述所谓Shuffle就是将不同节点上相同的Key拉取到一个节点的过程。这之中涉及到各种IO，所以执行时间势必会较长，Spark的Shuffle在1.2之前默认的计算引擎是HashShuffleManager，不过HashShuffleManager有一个十分严重的弊端，就是会产生大量的中间文件。在1.2之后默认Shuffle改为SortShuffleManager，相对于之前，在每个Task虽然也会产生大量中间文件，但是最后会将所有的临时文件合并（merge）成一个文件。因此Shuffle read只需要读取时，根据索引拿到每个磁盘的部分数据就可以了测试条件每个Executor只有一个CUP（core），同一时间每个Executor只能执行一个taskHashShuffleManager未优化版本首先从shuffle write阶段，主要是在一个stage结束后，为了下一个stage可以执行shuffle，将每一个task的数据按照key进行分类，对key进行hash算法，从而使相同的key写入同一个文件，每个磁盘文件都由下游stage的一个task读取。在写入磁盘时，先将数据写入内存缓冲，当内存缓冲填满后，才会溢写到磁盘文件（似乎所以写文件都需要写入先写入缓冲区，然后再溢写，防止频繁IO）我们可以先算一下当前stage的一个task会为下一个stage创建多少个磁盘文件。若下一个stage有100个task，则当前stage的每一个task都将创建100个文件，若当前stage要处理的task为50个，共有10个Executor，也就是说每个Executor共执行5个task，5x100x10=1000。也就是说这么一个小规模的操作会生产5000个文件。这是相当可观的。而shuffle read 通常是一个stage一开始要做的事情。此时stage的每一个task去将上一个stage的计算结果的所有相同的key从不同节点拉到自己所在节点。进行聚合或join操作。在shuffle write过程，每个task给下游的每个task都创建了一个磁盘文件。在read过程task只需要去上游stage的task中拉取属于自己的磁盘文件。shuffle read是边拉取边聚合。每一个read task都有一个buffer缓冲，然后通过内存中的Map进行聚合，每次只拉取buffer大小的数据，放到缓冲区中聚合，直到所有数据都拉取完。优化版本这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。开启这个机制之后，在shuffle write时，task并不是为下游的每一个task创建一个磁盘文件。引入了shuffleFileGroup的概念，每个shuffleFileGroup都对应一批磁盘文件。磁盘文件数量与下游task相同。只是仅仅第一批执行的task会创建一个shuffleFIleGroup，将数据写入到对应磁盘文件。在执行下一批的task时，会复用已经创建好的shuffleFIleGroup和磁盘文件，即数据会继续写入到已有的磁盘文件。该机制会允许不同task复用同一个磁盘文件，对于多个task进行了一定程度的合并，大幅度减少shuffle write时，文件的数量，提升性能。相对于优化前，每个Executor之前需要创建五百个磁盘文件，因为之前需要5个task线性执行，而使用参数优化之后，就每个Executor只需要100个就可以了，这样10个Executor就是1000个文件，这比优化前整整减少了4000个文件。SortShuffle在Spark1.2版本之后，出现了SortShuffle，这种方式以更少的中间磁盘文件产生而远远优于HashShuffle。而它的运行机制主要分为两种。一种为普通机制，另一种为bypass机制。而bypass机制的启动条件为，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。即当read task不是那么多的时候，采用bypass机制是更好的选择。普通运行机制在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一遍写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值，如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件。最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引，start offset和end offset。这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50个磁盘文件。bypass机制bypass机制运行条件：shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。不是聚合类的shuffle算子（比如reduceByKey）。在这种机制下，当前stage的task会为每个下游的task都创建临时磁盘文件。将数据按照key值进行hash，然后根据hash值，将key写入对应的磁盘文件中（个人觉得这也相当于一次另类的排序，将相同的key放在一起了）。最终，同样会将所有临时文件依次合并成一个磁盘文件，建立索引。优点该机制与未优化的hashshuffle相比，没有那么多磁盘文件，下游task的read操作相对性能会更好。该机制与sortshuffle的普通机制相比，在readtask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产SparkSQL如何读写本地外部数据源及排错]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%94%9F%E4%BA%A7SparkSQL%E5%A6%82%E4%BD%95%E8%AF%BB%E5%86%99%E6%9C%AC%E5%9C%B0%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E6%8E%92%E9%94%99%2F</url>
    <content type="text"><![CDATA[https://spark-packages.org/里有很多third-party数据源的package，spark把包加载进来就可以使用了csv格式在spark2.0版本之后是内置的，2.0之前属于第三方数据源读取本地外部数据源直接读取一个json文件12[hadoop@hadoop000 bin]$ ./spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.27.jar scala&gt; spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;).show运行报错：123456Caused by: java.lang.RuntimeException: file:/home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [49, 57, 125, 10] at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476) at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445) at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421) at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519) ... 32 more查看load方法的源码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by * a local or distributed file system). * * @since 1.4.0 */ def load(path: String): DataFrame = &#123; option(&quot;path&quot;, path).load(Seq.empty: _*) // force invocation of `load(...varargs...)` &#125;---------------------------------------------------------/** * Loads input in as a `DataFrame`, for data sources that support multiple paths. * Only works if the source is a HadoopFsRelationProvider. * * @since 1.6.0 */ @scala.annotation.varargs def load(paths: String*): DataFrame = &#123; if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) &#123; throw new AnalysisException(&quot;Hive data source can only be used with tables, you can not &quot; + &quot;read files of Hive data source directly.&quot;) &#125; val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf) if (classOf[DataSourceV2].isAssignableFrom(cls)) &#123; val ds = cls.newInstance() val options = new DataSourceOptions((extraOptions ++ DataSourceV2Utils.extractSessionConfigs( ds = ds.asInstanceOf[DataSourceV2], conf = sparkSession.sessionState.conf)).asJava) // Streaming also uses the data source V2 API. So it may be that the data source implements // v2, but has no v2 implementation for batch reads. In that case, we fall back to loading // the dataframe as a v1 source. val reader = (ds, userSpecifiedSchema) match &#123; case (ds: ReadSupportWithSchema, Some(schema)) =&gt; ds.createReader(schema, options) case (ds: ReadSupport, None) =&gt; ds.createReader(options) case (ds: ReadSupportWithSchema, None) =&gt; throw new AnalysisException(s&quot;A schema needs to be specified when using $ds.&quot;) case (ds: ReadSupport, Some(schema)) =&gt; val reader = ds.createReader(options) if (reader.readSchema() != schema) &#123; throw new AnalysisException(s&quot;$ds does not allow user-specified schemas.&quot;) &#125; reader case _ =&gt; null // fall back to v1 &#125; if (reader == null) &#123; loadV1Source(paths: _*) &#125; else &#123; Dataset.ofRows(sparkSession, DataSourceV2Relation(reader)) &#125; &#125; else &#123; loadV1Source(paths: _*) &#125; &#125; private def loadV1Source(paths: String*) = &#123; // Code path for data source v1. sparkSession.baseRelationToDataFrame( DataSource.apply( sparkSession, paths = paths, userSpecifiedSchema = userSpecifiedSchema, className = source, options = extraOptions.toMap).resolveRelation()) &#125;------------------------------------------------------private var source: String = sparkSession.sessionState.conf.defaultDataSourceName-------------------------------------------------------def defaultDataSourceName: String = getConf(DEFAULT_DATA_SOURCE_NAME)--------------------------------------------------------// This is used to set the default data source val DEFAULT_DATA_SOURCE_NAME = buildConf(&quot;spark.sql.sources.default&quot;) .doc(&quot;The default data source to use in input/output.&quot;) .stringConf .createWithDefault(&quot;parquet&quot;)从源码中可以看出，如果不指定format，load默认读取的是parquet文件12345678scala&gt; val users = spark.read.load(&quot;file:///home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)scala&gt; users.show()+------+--------------+----------------+ | name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+读取其他格式的文件，必须通过format指定文件格式，如下：12345678910//windows idea环境下val df1 = spark.read.format(&quot;json&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).load(&quot;hdfs://192.168.137.141:9000/data/people.json&quot;)df1.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)必须带上，不然报错1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX读取CSV格式文件123456789101112131415161718192021222324252627282930313233343536373839404142//源文件内容如下：[hadoop@hadoop001 ~]$ hadoop fs -text /data/people.csvname;age;jobJorge;30;DeveloperBob;32;Developer//windows idea环境下val df2 = spark.read.format(&quot;csv&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .option(&quot;sep&quot;,&quot;;&quot;) .option(&quot;header&quot;,&quot;true&quot;) //use first line of all files as header .option(&quot;inferSchema&quot;,&quot;true&quot;) .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;)df2.show()df2.printSchema()//输出结果：+-----+---+---------+| name|age| job|+-----+---+---------+|Jorge| 30|Developer|| Bob| 32|Developer|+-----+---+---------+root |-- name: string (nullable = true) |-- age: integer (nullable = true) |-- job: string (nullable = true)-----------------------------------------------------------//如果不指定option(&quot;sep&quot;,&quot;;&quot;)+------------------+| name;age;job|+------------------+|Jorge;30;Developer|| Bob;32;Developer|+------------------+//如果不指定option(&quot;header&quot;,&quot;true&quot;)+-----+---+---------+| _c0|_c1| _c2|+-----+---+---------+| name|age| job||Jorge| 30|Developer|| Bob| 32|Developer|+-----+---+---------+读取csv格式文件还可以自定义schema12345678910111213141516171819202122val peopleschema = StructType(Array(StructField(&quot;hlwname&quot;,StringType,true), StructField(&quot;hlwage&quot;,IntegerType,true), StructField(&quot;hlwjob&quot;,StringType,true)))val df2 = spark.read.format(&quot;csv&quot;).option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;).option(&quot;sep&quot;,&quot;;&quot;) .option(&quot;header&quot;,&quot;true&quot;) .schema(peopleschema) .load(&quot;hdfs://192.168.137.141:9000/data/people.csv&quot;) //打印测试 df2.show() df2.printSchema()输出结果：+-------+------+---------+|hlwname|hlwage| hlwjob|+-------+------+---------+| Jorge| 30|Developer|| Bob| 32|Developer|+-------+------+---------+root |-- hlwname: string (nullable = true) |-- hlwage: integer (nullable = true) |-- hlwjob: string (nullable = true)将读取的文件以其他格式写出1234567891011121314151617181920212223//将上文读取的users.parquet以json格式写出scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)[hadoop@hadoop000 ~]$ cd /home/hadoop/tmp/parquet2json[hadoop@hadoop000 parquet2json]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 56 Sep 24 10:15 part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json-rw-r--r--. 1 hadoop hadoop 0 Sep 24 10:15 _SUCCESS[hadoop@hadoop000 parquet2json]$ cat part-00000-dfbd9ba5-598f-4e0c-8e81-df85120333db-c000.json &#123;&quot;name&quot;:&quot;Alyssa&quot;&#125;&#123;&quot;name&quot;:&quot;Ben&quot;,&quot;favorite_color&quot;:&quot;red&quot;&#125;//将上文读取的people.json以csv格式写出df1.write.format(&quot;csv&quot;) .mode(&quot;overwrite&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .save(&quot;hdfs://192.168.137.141:9000/data/formatconverttest/&quot;)------------------------------------------[hadoop@hadoop001 ~]$ hadoop fs -text /data/formatconverttest/part-00000-6fd65eff-d0d3-43e5-9549-2b11bc3ca9de-c000.csv,Michael30,Andy19,Justin//发现若没有.option(&quot;header&quot;,&quot;true&quot;)，写出的csv丢失了首行的age,name信息//若不指定.option(&quot;sep&quot;,&quot;;&quot;)，默认逗号为分隔符此操作的目的在于学会类型转换，生产上最开始进来的数据大多都是text，json等行式存储的文件，一般都要转成ORC，parquet列式存储的文件，加上压缩，能把文件大小减小到10%左右，大幅度减小IO和数据处理量，提高性能此时如果再执行一次save，路径不变，则会报错：12345scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)org.apache.spark.sql.AnalysisException: path file:/home/hadoop/tmp/parquet2json already exists.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109) at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104).........................................................可以通过设置savemode来解决这个问题默认是errorifexists1scala&gt; users.select(&quot;name&quot;,&quot;favorite_color&quot;).write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(&quot;file:///home/hadoop/tmp/parquet2json/&quot;)]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最佳实践之Spark写入Hfile经典案例]]></title>
    <url>%2F2019%2F03%2F01%2F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8BSpark%E5%86%99%E5%85%A5Hfile%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[本文由小伙伴提供将HDFS上的数据解析出来，然后通过hfile方式批量写入Hbase(需要多列写入) 写⼊数据的关键api:123456saveAsNewAPIHadoopFile( stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)特殊地方：最初写hfile警告1Does it contain files in subdirectories that correspond to column family names这个原因大概三种 * 代码问题 * 数据源问题 * setMapOutputKeyClass 和 saveAsNewAPIHadoopFile中的Class不不⼀一致 这里是我的是数据源问题 正常写put操作的时候，服务端自动帮助排序，因此在使用put操作的时候没有涉及到这样的错误1Added a key not lexically larger than previous但是在写hfile的时候如果出现报错: 1Added a key not lexically larger than previous 这样的错误，一般会认为rowkey没有做好排序，然后傻fufu的去验证了一下，rowkey的确做了排序。 真正原因: `spark写hfile时候是按照rowkey+列族+列名进⾏行排序的，因此在写⼊数据的时候，要做到整体有序 (事情还没完)` 因为需要多列写入，最好的⽅式:要么反射来动态获取列名称和列值 、 要么通过datafame去获取(df.columns)反射方式：12345678910111213141516171819202122232425262728293031val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] = rdd.map&#123; line =&gt; val rowkey = line.vintime val clazz = Class.forName(XXXXXXXXXXXXXXXX) val fields = clazz.getDeclaredFields var list = new ListBuffer[String]() var kvlist = new ListBuffer[KeyValue]()// if (fields != null &amp;&amp; fields.size &gt; 0) &#123; for (field &lt;- fields) &#123; field.setAccessible(true) val column = field.getName list.append(column)&#125; &#125; val newList = list.sortWith(_ &lt; _) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) for(column &lt;- newList)&#123; val declaredField: Field =line.getClass.getDeclaredField(column)&#125; declaredField.setAccessible(true) val value = declaredField.get(line).toString val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)) kvlist.append(kv)&#125;(ik, kvlist)&#125;datafame的方式: 123456789101112131415161718192021222324val tmpData: RDD[(ImmutableBytesWritable, util.LinkedList[KeyValue])] =df.rdd.map( line =&gt;&#123; val rowkey = line.getAs[String](&quot;vintime&quot;) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) var linkedList = new util.LinkedList[KeyValue]() for (column &lt;- columns) &#123; val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(line.getAs[String](column))) linkedList.add(kv) &#125; (ik, linkedList) &#125;) val result: RDD[(ImmutableBytesWritable, KeyValue)] =tmpData.flatMapValues( s =&gt; &#123; val values: Iterator[KeyValue] =JavaConverters.asScalaIteratorConverter(s.iterator()).asScala values &#125; ).sortBy(x =&gt;x._1 , true) 仔细观察可以发现，其实两者都做了排序操作，但是即便经过(1)步骤后仍然报错: 1Added a key not lexically larger than previous 那么再回想⼀下之前写hfile的要求: rowkey+列族+列都要有序，那么如果出现数据的重复，也不算是有序的操作! 因为，做一下数据的去重: 12val key: RDD[(String, TransferTime)] = data.reduceByKey((x, y) =&gt; y)val unitData: RDD[TransferTime] = key.map(line =&gt; line._2) 果然，这样解决了:Added a key not lexically larger than previous这个异常 但是会报如下另⼀个异常: 1Kryo serialization failed: Buffer overflow 这个是因为在对⼀些类做kryo序列化时候，数据量的缓存⼤小超过了默认值，做⼀下调整即可 12sparkConf.set(&quot;spark.kryoserializer.buffer.max&quot; , &quot;256m&quot;)sparkConf.set(&quot;spark.kryoserializer.buffer&quot; , &quot;64m&quot;) 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100object WriteTransferTime extends WriteToHbase&#123; /*** @param data 要插⼊入的数据 * @param tableName 表名**/ override def bulkLoadData(data: RDD[Any], tableName: String ,columnFamily:String): Unit = &#123; val bean: RDD[TransferTime] = data.map(line =&gt;line.asInstanceOf[TransferTime]) val map: RDD[(String, TransferTime)] = bean.map(line =&gt; (line.vintime ,line)) val key: RDD[(String, TransferTime)] = map.reduceByKey((x, y) =&gt; y) val map1: RDD[TransferTime] = key.map(line =&gt; line._2) val by1: RDD[TransferTime] = map1.sortBy(f =&gt; f.vintime) val listData: RDD[(ImmutableBytesWritable, ListBuffer[KeyValue])] =by1.map &#123; line =&gt; val rowkey = line.vintime val clazz =Class.forName(&quot;com.dongfeng.code.Bean.message.TransferTime&quot;) val fields = clazz.getDeclaredFields var list = new ListBuffer[String]() var kvlist = new ListBuffer[KeyValue]()// if (fields != null &amp;&amp; fields.size &gt; 0) &#123; for (field &lt;- fields) &#123; field.setAccessible(true) val column = field.getName list.append(column)&#125; &#125; val newList = list.sortWith(_ &lt; _) val ik = new ImmutableBytesWritable(Bytes.toBytes(rowkey)) for(column &lt;- newList)&#123; val declaredField: Field = line.getClass.getDeclaredField(column) declaredField.setAccessible(true) val value = declaredField.get(line).toString val kv: KeyValue = new KeyValue( Bytes.toBytes(rowkey), Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)) kvlist.append(kv) &#125; (ik, kvlist) &#125; val result: RDD[(ImmutableBytesWritable, KeyValue)] =listData.flatMapValues( s =&gt; &#123; val values: Iterator[KeyValue] = s.iterator values&#125; ) val resultDD: RDD[(ImmutableBytesWritable, KeyValue)] = result.sortBy(x=&gt;x._1 , true) WriteToHbaseDB.hfile_load(result , TableName.valueOf(tableName) ,columnFamily)&#125; &#125; def hfile_load(rdd:RDD[(ImmutableBytesWritable , KeyValue)] , tableName:TableName , columnFamily:String): Unit =&#123;//声明表的信息var table: Table = null try&#123;val startTime = System.currentTimeMillis() println(s&quot;开始时间:--------&gt;$&#123;startTime&#125;&quot;) //⽣生成的HFile的临时保存路路径val stagingFolder = &quot;hdfs://cdh1:9000/hfile/&quot;+tableName+newDate().getTime//table = connection.getTable(tableName) //如果表不不存在，则创建表 if(!admin.tableExists(tableName))&#123; createTable(tableName , columnFamily) &#125;//开始导⼊val job = Job.getInstance(config) job.setJobName(&quot;DumpFile&quot;) job.setMapOutputKeyClass(classOf[ImmutableBytesWritable]) job.setMapOutputValueClass(classOf[KeyValue]) rdd.sortBy(x =&gt; x._1, true).saveAsNewAPIHadoopFile( stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration) val load = new LoadIncrementalHFiles(config) val regionLocator = connection.getRegionLocator(tableName) HFileOutputFormat2.configureIncrementalLoad(job, table,regionLocator) load.doBulkLoad(new Path(stagingFolder), table.asInstanceOf[HTable])// load.doBulkLoad(new Path(stagingFolder) , connection.getAdmin ,table , regionLocator)val endTime = System.currentTimeMillis() println(s&quot;结束时间:--------&gt;$&#123;endTime&#125;&quot;) println(s&quot;花费的时间:-----------------&gt;$&#123;(endTime - startTime)&#125;ms&quot;) &#125;catch&#123; case e:IOException =&gt; e.printStackTrace() &#125;finally &#123; if (table != null) &#123; try &#123;// 关闭HTable对象 table.close(); &#125; catch &#123; case e: IOException =&gt; e.printStackTrace();&#125; &#125;if (connection != null) &#123; try &#123; //关闭hbase连接. connection.close(); &#125; catch &#123; case e: IOException =&gt; e.printStackTrace(); &#125;&#125; &#125;&#125; &#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>高级</tag>
        <tag>Spark</tag>
        <tag>Hfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产Spark开发读取云主机HDFS异常剖析流程]]></title>
    <url>%2F2019%2F02%2F26%2F%E7%94%9F%E4%BA%A7Spark%E5%BC%80%E5%8F%91%E8%AF%BB%E5%8F%96%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS%E5%BC%82%E5%B8%B8%E5%89%96%E6%9E%90%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[问题背景：云主机是 Linux 环境，搭建 Hadoop 伪分布式公网 IP：139.198.xxx.xxx内网 IP：192.168.137.2主机名：hadoop001本地的core-site.xml配置如下：12345678910&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9001&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9001/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;本地的hdfs-site.xml配置如下：123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;云主机hosts文件配置：12345[hadoop@hadoop001 ~]$ cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hostname loopback address 192.168.137.2 hadoop001云主机将内网IP和主机名hadoop001做了映射本地hosts文件配置1139.198.18.XXX hadoop001本地已经将公网IP和域名hadoop001做了映射问题症状在云主机上开启 HDFS，JPS 查看进程都没有异常，通过 Shell 操作 HDFS 文件也没有问题通过浏览器访问 50070 端口管理界面也没有问题在本地机器上使用 Java API 操作远程 HDFS 文件，URI 使用公网 IP，代码如下：123456789val uri = new URI(&quot;hdfs://hadoop001:9001&quot;)val fs = FileSystem.get(uri,conf)val listfiles = fs.listFiles(new Path(&quot;/data&quot;),true) while (listfiles.hasNext) &#123; val nextfile = listfiles.next() println(&quot;get file path:&quot; + nextfile.getPath().toString()) &#125;------------------------------运行结果---------------------------------get file path:hdfs://hadoop001:9001/data/infos.txt在本地机器使用SparkSQL读取hdfs上的文件并转换为DF的过程中1234567891011object SparkSQLApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkSQLApp&quot;).master(&quot;local[2]&quot;).getOrCreate() val info = spark.sparkContext.textFile(&quot;/data/infos.txt&quot;) import spark.implicits._ val infoDF = info.map(_.split(&quot;,&quot;)).map(x=&gt;Info(x(0).toInt,x(1),x(2).toInt)).toDF() infoDF.show() spark.stop() &#125; case class Info(id:Int,name:String,age:Int)&#125;出现如下报错信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667.... .... .... 19/02/23 16:07:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0) 19/02/23 16:07:00 INFO HadoopRDD: Input split: hdfs://hadoop001:9001/data/infos.txt:0+17 19/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ..... .... 19/02/23 16:07:21 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry... 19/02/23 16:07:21 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 272.617680460432 msec. 19/02/23 16:07:42 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ... ... 19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3499) ... ... 19/02/23 16:08:12 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) ... ... 19/02/23 16:08:12 INFO DFSClient: Could not obtain BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 from any node: java.io.IOException: No live nodes contain block BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 after checking nodes = [DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]], ignoredNodes = null No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Will get new block locations from namenode and retry... 19/02/23 16:08:12 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11918.913311370841 msec. 19/02/23 16:08:45 WARN BlockReaderFactory: I/O error constructing remote block reader. java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ... ... 19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException 19/02/23 16:08:45 WARN DFSClient: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK] Dead nodes: DatanodeInfoWithStorage[192.168.137.2:50010,DS-fb2e7244-165e-41a5-80fc-4bb90ae2c8cd,DISK]. Throwing a BlockMissingException 19/02/23 16:08:45 WARN DFSClient: DFS Read org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... 19/02/23 16:08:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:648) ... ... 19/02/23 16:08:45 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job 19/02/23 16:08:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 19/02/23 16:08:45 INFO TaskSchedulerImpl: Cancelling stage 0 19/02/23 16:08:45 INFO DAGScheduler: ResultStage 0 (show at SparkSQLApp.scala:30) failed in 105.618 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1358284489-192.168.137.2-1550394746448:blk_1073741840_1016 file=/data/infos.txt at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1001) ... ... 问题分析本地 Shell 可以正常操作，排除集群搭建和进程没有启动的问题云主机没有设置防火墙，排除防火墙没关的问题云服务器防火墙开放了 DataNode 用于数据传输服务端口 默认是 50010我在本地搭建了另一台虚拟机，该虚拟机和本地在同一局域网，本地可以正常操作该虚拟机的hdfs，基本确定了是由于内外网的原因。查阅资料发现 HDFS 中的文件夹和文件名都是存放在 NameNode 上，操作不需要和 DataNode 通信，因此可以正常创建文件夹和创建文件说明本地和远程 NameNode 通信没有问题。那么很可能是本地和远程 DataNode 通信有问题问题猜想由于本地测试和云主机不在一个局域网，hadoop配置文件是以内网ip作为机器间通信的ip。在这种情况下,我们能够访问到namenode机器，namenode会给我们数据所在机器的ip地址供我们访问数据传输服务，但是当写数据的时候，NameNode 和DataNode 是通过内网通信的，返回的是datanode内网的ip,我们无法根据该IP访问datanode服务器。我们来看一下其中一部分报错信息：123419/02/23 16:07:21 WARN BlockReaderFactory: I/O error constructing remote block reader.java.net.ConnectException: Connection timed out: no further information...19/02/23 16:07:42 WARN DFSClient: Failed to connect to /192.168.137.2:50010 for block, add to deadNodes and continue....从报错信息中可以看出，连接不到192.168.137.2:50010，也就是datanode的地址，因为外网必须访问“139.198.18.XXX:50010”才能访问到datanode。为了能够让开发机器访问到hdfs，我们可以通过域名访问hdfs，让namenode返回给我们datanode的域名。问题解决尝试一：在开发机器的hosts文件中配置datanode对应的外网ip和域名（上文已经配置），并且在与hdfs交互的程序中添加如下代码:12val conf = new Configuration()conf.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)报错依旧尝试二：123456val spark = SparkSession .builder() .appName(&quot;SparkSQLApp&quot;) .master(&quot;local[2]&quot;) .config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;) .getOrCreate()报错依旧尝试三：在hdfs-site.xml中添加如下配置：1234&lt;property&gt; &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;运行成功通过查阅资料，建议在hdfs-site.xml中增加dfs.datanode.use.datanode.hostname属性，表示datanode之间的通信也通过域名方式1234&lt;property&gt; &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;这样能够使得更换内网IP变得十分简单、方便，而且可以让特定datanode间的数据交换变得更容易。但与此同时也存在一个副作用，当DNS解析失败时会导致整个Hadoop不能正常工作，所以要保证DNS的可靠总结：将默认的通过IP访问，改为通过域名方式访问。参考资料https://blog.csdn.net/vaf714/article/details/82996860https://www.cnblogs.com/krcys/p/9146329.htmlhttps://blog.csdn.net/dominic_tiger/article/details/71773656https://rainerpeter.wordpress.com/2014/02/12/connect-to-hdfs-running-in-ec2-using-public-ip-addresses/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark UI界面实现原理]]></title>
    <url>%2F2019%2F02%2F22%2FSpark%20UI%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[当Spark程序在运行时，会提供一个Web页面查看Application运行状态信息。是否开启UI界面由参数spark.ui.enabled(默认为true)来确定。下面列出Spark UI一些相关配置参数，默认值，以及其作用。本文接下来分成两个部分，第一部分基于Spark-1.6.0的源码，结合第二部分的图片内容来描述UI界面在Spark中的实现方式。第二部分以实例展示Spark UI界面显示的内容。Spark UI界面实现方式UI组件结构这部分先讲UI界面的实现方式，UI界面的实例在本文最后一部分。如果对这部分中的某些概念不清楚，那么最好先把第二部分了解一下。从下面UI界面的实例可以看出，不同的内容以Tab的形式展现在界面上，对应每一个Tab在下方显示具体内容。基本上Spark UI界面也是按这个层次关系实现的。以SparkUI类为容器，各个Tab，如JobsTab, StagesTab, ExecutorsTab等镶嵌在SparkUI上，对应各个Tab，有页面内容实现类JobPage, StagePage, ExecutorsPage等页面。这些类的继承和包含关系如下图所示：初始化过程从上面可以看出，SparkUI类型的对象是UI界面的根对象，它是在SparkContext类中构造出来的。12345678910private var _ui: Option[SparkUI] = None //定义_ui = //SparkUI对象的生成 if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123; Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener, _env.securityManager, appName, startTime = startTime)) &#125; else &#123; // For tests, do not enable the UI None &#125;_ui.foreach(_.bind()) //启动jetty。bind方法继承自WebUI，该类负责和真实的Jetty Server API打交道上面这段代码中可以看到SparkUI对象的生成过程，结合上面的类结构图，可以看到bind方法继承自WebUI类，进入WebUI类中1234567891011121314protected val handlers = ArrayBuffer[ServletContextHandler]() // 这个对象在下面bind方法中会使用到。 protected val pageToHandlers = new HashMap[WebUIPage, ArrayBuffer[ServletContextHandler]] // 将page绑定到handlers上 /** 将Http Server绑定到这个Web页面 */ def bind() &#123; assert(!serverInfo.isDefined, &quot;Attempted to bind %s more than once!&quot;.format(className)) try &#123; serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name)) logInfo(&quot;Started %s at http://%s:%d&quot;.format(className, publicHostName, boundPort)) &#125; catch &#123; case e: Exception =&gt; logError(&quot;Failed to bind %s&quot;.format(className), e) System.exit(1) &#125; &#125;上面代码中handlers对象维持了WebUIPage和Jetty之间的关系，org.eclipse.jetty.servlet.ServletContextHandler是标准jetty容器的handler。而对象pageToHandlers维持了WebUIPage到ServletContextHandler的对应关系。各Tab页以及该页内容的实现，基本上大同小异。接下来以AllJobsPage页面为例仔细梳理页面展示的过程。SparkUI中Tab的绑定从上面的类结构图中看到WebUIPage提供了两个重要的方法，render和renderJson用于相应页面请求，在WebUIPage的实现类中，具体实现了这两个方法。在SparkContext中构造出SparkUI的实例后，会执行SparkUI#initialize方法进行初始化。如下面代码中，调用SparkUI从WebUI继承的attacheTab方法，将各Tab页面绑定到UI上。1234567891011121314def initialize() &#123; attachTab(new JobsTab(this)) attachTab(stagesTab) attachTab(new StorageTab(this)) attachTab(new EnvironmentTab(this)) attachTab(new ExecutorsTab(this)) attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;)) attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath)) attachHandler(ApiRootResource.getServletHandler(this)) // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs attachHandler(createRedirectHandler( &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest, httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;))) &#125;页面内容绑定到Tab在上一节中，JobsTab标签绑定到SparkUI上之后，在JobsTab上绑定了AllJobsPage和JobPage类。AllJobsPage页面即访问SparkUI页面时列举出所有Job的那个页面，JobPage页面则是点击单个Job时跳转的页面。通过调用JobsTab从WebUITab继承的attachPage方法与JobsTab进行绑定。1234567891011private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123; val sc = parent.sc val killEnabled = parent.killEnabled val jobProgresslistener = parent.jobProgressListener val executorListener = parent.executorsListener val operationGraphListener = parent.operationGraphListener def isFairScheduler: Boolean = jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR) attachPage(new AllJobsPage(this)) attachPage(new JobPage(this))&#125;页面内容的展示知道了AllJobsPage页面如何绑定到SparkUI界面后，接下来分析这个页面的内容是如何显示的。进入AllJobsPage类，主要观察render方法。在页面展示上Spark直接利用了Scala对html/xml的语法支持，将页面的Html代码嵌入Scala程序中。具体的页面生成过程可以查看下面源码中的注释。这里可以结合第二部分的实例进行查看。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687def render(request: HttpServletRequest): Seq[Node] = &#123; val listener = parent.jobProgresslistener //获取jobProgresslistener对象，页面展示的数据都是从这里读取 listener.synchronized &#123; val startTime = listener.startTime // 获取application的开始时间，默认值为-1L val endTime = listener.endTime // 获取application的结束时间，默认值为-1L val activeJobs = listener.activeJobs.values.toSeq // 获取当前application中处于active状态的job val completedJobs = listener.completedJobs.reverse.toSeq // 获取当前application中完成状态的job val failedJobs = listener.failedJobs.reverse.toSeq // 获取当前application中失败状态的job val activeJobsTable = jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse) val completedJobsTable = jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val failedJobsTable = jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val shouldShowActiveJobs = activeJobs.nonEmpty val shouldShowCompletedJobs = completedJobs.nonEmpty val shouldShowFailedJobs = failedJobs.nonEmpty val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123; s&quot;$&#123;completedJobs.size&#125;&quot; &#125; else &#123; s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot; &#125; val summary: NodeSeq = &lt;div&gt; &lt;ul class=&quot;unstyled&quot;&gt; &lt;li&gt; &lt;strong&gt;Total Uptime:&lt;/strong&gt; // 显示当前Spark应用运行时间 &#123;// 如果还没有结束，就用系统当前时间减开始时间。如果已经结束，就用结束时间减开始时间 if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123; UIUtils.formatDuration(System.currentTimeMillis() - startTime) &#125; else if (endTime &gt; 0) &#123; UIUtils.formatDuration(endTime - startTime) &#125; &#125; &lt;/li&gt; &lt;li&gt; &lt;strong&gt;Scheduling Mode: &lt;/strong&gt; // 显示调度模式，FIFO或FAIR &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125; &lt;/li&gt; &#123; if (shouldShowActiveJobs) &#123; // 如果有active状态的job，则显示Active Jobs有多少个 &lt;li&gt; &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;activeJobs.size&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowCompletedJobs) &#123; // 如果有完成状态的job，则显示Completed Jobs的个数 &lt;li id=&quot;completed-summary&quot;&gt; &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;completedJobNumStr&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowFailedJobs) &#123; // 如果有失败状态的job，则显示Failed Jobs的个数 &lt;li&gt; &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;listener.numFailedJobs&#125; &lt;/li&gt; &#125; &#125; &lt;/ul&gt; &lt;/div&gt; var content = summary // 将上面的html代码写入content变量，在最后统一显示content中的内容 val executorListener = parent.executorListener // 这里获取EventTimeline中的信息 content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs, executorListener.executorIdToData, startTime)// 然后根据当前application中是否存在active， failed， completed状态的job，将这些信息显示在页面上。 if (shouldShowActiveJobs) &#123; content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++ activeJobsTable // 生成active状态job的展示表格，具体形式可参看第二部分。按提交时间倒序排列 &#125; if (shouldShowCompletedJobs) &#123; content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++ completedJobsTable &#125; if (shouldShowFailedJobs) &#123; content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++ failedJobsTable &#125; val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; + &quot; Click on a job to see information about the stages of tasks inside it.&quot; UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText)) // 最后将content中的所有内容全部展示在页面上 &#125; &#125;接下来以activeJobsTable代码为例分析Jobs信息展示表格的生成。这里主要的方法是makeRow，接收的是上面代码中的activeJobs, completedJobs, failedJobs。这三个对象都是包含在JobProgressListener对象中的，在JobProgressListener中的定义如下：1234// 这三个对象用于存储数据的主要是JobUIData类型， val activeJobs = new HashMap[JobId, JobUIData] val completedJobs = ListBuffer[JobUIData]() val failedJobs = ListBuffer[JobUIData]()将上面三个对象传入到下面这段代码中，继续执行。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123; val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined) val columns: Seq[Node] = &#123; // 显示的信息包括，Job Id(Job Group)以及Job描述，Job提交时间，Job运行时间，总的Stage/Task数，成功的Stage/Task数，以及一个进度条 &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;th&gt;Submitted&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt; &#125; def makeRow(job: JobUIData): Seq[Node] = &#123; val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job) val duration: Option[Long] = &#123; job.submissionTime.map &#123; start =&gt; // Job运行时长为系统时间，或者结束时间减去开始时间 val end = job.completionTime.getOrElse(System.currentTimeMillis()) end - start &#125; &#125; val formattedDuration = duration.map(d =&gt; // 格式化任务运行时间，显示为a h:b m:c s格式UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;) val formattedSubmissionTime = // 获取Job提交时间job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;) val jobDescription = UIUtils.makeDescription(lastStageDescription, parent.basePath) // 获取任务描述 val detailUrl = // 点击单个Job下面链接跳转到JobPage页面，传入参数为jobId &quot;%s/jobs/job?id=%s&quot;.format(UIUtils.prependBaseUri(parent.basePath), job.jobId) &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt; &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt; &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125; &lt;/td&gt; &lt;td&gt; &#123;jobDescription&#125; &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt; &lt;/td&gt; &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt; &#123;formattedSubmissionTime&#125; &lt;/td&gt; &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt; &lt;td class=&quot;stage-progress-cell&quot;&gt; &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125; &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125; &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125; &lt;/td&gt; &lt;td class=&quot;progress-cell&quot;&gt; // 进度条 &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks, failed = job.numFailedTasks, skipped = job.numSkippedTasks, total = job.numTasks - job.numSkippedTasks)&#125; &lt;/td&gt; &lt;/tr&gt; &#125; &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt; &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt; // 显示列名 &lt;tbody&gt; &#123;jobs.map(makeRow)&#125; // 调用上面的row生成方法，具体显示Job信息 &lt;/tbody&gt; &lt;/table&gt; &#125;从上面这些代码中可以看到，Job页面显示的所有数据，都是从JobProgressListener对象中获得的。SparkUI可以理解成一个JobProgressListener对象的消费者，页面上显示的内容都是JobProgressListener内在的展现。Spark UI界面实例默认情况下，当一个Spark Application运行起来后，可以通过访问hostname:4040端口来访问UI界面。hostname是提交任务的Spark客户端ip地址，端口号由参数spark.ui.port(默认值4040，如果被占用则顺序往后探查)来确定。由于启动一个Application就会生成一个对应的UI界面，所以如果启动时默认的4040端口号被占用，则尝试4041端口，如果还是被占用则尝试4042，一直找到一个可用端口号为止。下面启动一个Spark ThriftServer服务，并用beeline命令连接该服务，提交sql语句运行。则ThriftServer对应一个Application，每个sql语句对应一个Job，按照Job的逻辑划分Stage和Task。Jobs页面连接上该端口后，显示的就是上面的页面，也是Job的主页面。这里会显示所有Active，Completed, Cancled以及Failed状态的Job。默认情况下总共显示1000条Job信息，这个数值由参数spark.ui.retainedJobs(默认值1000)来确定。从上面还看到，除了Jobs选项卡之外，还可显示Stages, Storage, Enviroment, Executors, SQL以及JDBC/ODBC Server选项卡。分别如下图所示。Stages页面Storage页面Enviroment页面Executors页面单个Job包含的Stages页面Task页面]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark监控报错javax.servlet.http.HttpServletRequest.isAsyncStarted]]></title>
    <url>%2F2019%2F02%2F16%2FSpark%E7%9B%91%E6%8E%A7%E6%8A%A5%E9%94%99javax.servlet.http.HttpServletRequest.isAsyncStarted%2F</url>
    <content type="text"><![CDATA[环境Spark2.2.1Hadoop2.6IntelljScala2.11pom文件1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.common.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.hdfs.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.client.version&#125;&lt;/version&gt;&lt;/dependency&gt;报错信息如下所示：12345678910111213141516171819202122232425262728293031323334353637java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Zat org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)at org.spark_project.jetty.server.Server.handle(Server.java:499)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN HttpChannel: Could not send response error 500: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z16/11/08 21:37:43 WARN HttpChannel: /jobs/java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Zat org.spark_project.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:484)at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)at org.spark_project.jetty.server.Server.handle(Server.java:499)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN QueuedThreadPool: java.lang.NoSuchMethodError: javax.servlet.http.HttpServletResponse.getStatus()Iat org.spark_project.jetty.server.handler.ErrorHandler.handle(ErrorHandler.java:112)at org.spark_project.jetty.server.Response.sendError(Response.java:597)at org.spark_project.jetty.server.HttpChannel.handleException(HttpChannel.java:487)at org.spark_project.jetty.server.HttpConnection$HttpChannelOverHttp.handleException(HttpConnection.java:594)at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:387)at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)at java.lang.Thread.run(Thread.java:745)16/11/08 21:37:43 WARN QueuedThreadPool: Unexpected thread death: org.spark_project.jetty.util.thread.QueuedThreadPool$3@3ec5063f in SparkUI&#123;STARTED,8&lt;=8&lt;=200,i=4,q=0&#125;问题解决查看报错信息1java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z未找到HttpServletRequest类中的isAsyncStarted方法。问题定位使用搜索功能，查看该类存在于哪些包下。问题解决所有涉及到该类jar文件且版本低于3.0的均需要进行删除。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天起床第一句，看看Spark调度器]]></title>
    <url>%2F2019%2F01%2F18%2FSpark%E8%B0%83%E5%BA%A6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[之前呢，我们详细地分析了DAGScheduler的执行过程，我们知道，RDD形成的DAG经过DAGScheduler，依据shuffle将DAG划分为若干个stage，再由taskScheduler提交task到executor中执行，那么执行task的过程，就需要调度器来参与了。Spark调度器主要有两种模式，也是大家耳熟能详的FIFO和FAIR模式。默认情况下，Spark是FIFO（先入先出）模式，即谁先提交谁先执行。而FAIR（公平调度）模式会在调度池中为任务进行分组，可以有不同的权重，根据权重来决定执行顺序。那么源码中是怎么实现的呢？首先，当Stage划分好，会调用TaskSchedulerImpl.submitTasks()方法，以TaskSet的形式提交给TaskScheduler，并创建一个TaskSetManger对象添加进调度池。1234567891011override def submitTasks(taskSet: TaskSet) &#123; val tasks = taskSet.tasks //.... this.synchronized &#123; val manager = createTaskSetManager(taskSet, maxTaskFailures) val stage = taskSet.stageId val stageTaskSets = taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager]) stageTaskSets(taskSet.stageAttemptId) = manager //..... schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)SchedulerBulider通过TaskSchedulerImpl.initialize()进行了实例化，并调用了SchedulerBulider.buildPools()方法。具体怎么个build，就要看用户选择的schedulingMode了。123456789101112131415def initialize(backend: SchedulerBackend) &#123; this.backend = backend schedulableBuilder = &#123; schedulingMode match &#123; case SchedulingMode.FIFO =&gt; new FIFOSchedulableBuilder(rootPool) case SchedulingMode.FAIR =&gt; new FairSchedulableBuilder(rootPool, conf) case _ =&gt; throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; + s&quot;$schedulingMode&quot;) &#125; &#125; schedulableBuilder.buildPools() &#125;然后我们来看一下两个调度器的buildPools()方法。123override def buildPools() &#123; // nothing &#125;FIFO什么也没干~~1234567891011121314151617181920212223242526272829303132333435override def buildPools() &#123; var fileData: Option[(InputStream, String)] = None try &#123; fileData = schedulerAllocFile.map &#123; f =&gt; val fis = new FileInputStream(f) logInfo(s&quot;Creating Fair Scheduler pools from $f&quot;) Some((fis, f)) &#125;.getOrElse &#123; val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE) if (is != null) &#123; logInfo(s&quot;Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE&quot;) Some((is, DEFAULT_SCHEDULER_FILE)) &#125; else &#123; logWarning(&quot;Fair Scheduler configuration file not found so jobs will be scheduled in &quot; + s&quot;FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or &quot; + s&quot;set $SCHEDULER_ALLOCATION_FILE_PROPERTY to a file that contains the configuration.&quot;) None &#125; &#125; fileData.foreach &#123; case (is, fileName) =&gt; buildFairSchedulerPool(is, fileName) &#125; &#125; catch &#123; case NonFatal(t) =&gt; val defaultMessage = &quot;Error while building the fair scheduler pools&quot; val message = fileData.map &#123; case (is, fileName) =&gt; s&quot;$defaultMessage from $fileName&quot; &#125; .getOrElse(defaultMessage) logError(message, t) throw t &#125; finally &#123; fileData.foreach &#123; case (is, fileName) =&gt; is.close() &#125; &#125; // finally create &quot;default&quot; pool buildDefaultPool() &#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>高级</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再谈，某头条公司Spark结构化流的SQL实现]]></title>
    <url>%2F2019%2F01%2F10%2F%E5%86%8D%E8%B0%88%EF%BC%8C%E6%9F%90%E5%A4%B4%E6%9D%A1%E5%85%AC%E5%8F%B8Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%9A%84SQL%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前面介绍了大概的使用语句，接下来讲解基本的功能点的实现。SQL语句的解析(解析部分为开源项目flinkStreamSQL内容，直接拿过来用)123456789101112131415CREATE TABLE SocketTable( word String, valuecount int)WITH( type=&apos;socket&apos;, host=&apos;hadoop-sh1-core1&apos;, port=&apos;9998&apos;, delimiter=&apos; &apos;);create SINK console()WITH( type=&apos;console&apos;, outputmode=&apos;complete&apos;);insert into console select word,count(*) from SocketTable group by word;将create的内容根据正则解析出来，将field和配置相关的内容解析出来。insert into部分的内容则使用calsite解析出insert部分的target表和已经create的source表内容。因为spark没有定义好表之后直接可以insert的内容，所以要将需要sink的target解析出来另外处理。创建source输入123456789CREATE TABLE SocketTable( word String, valuecount int)WITH( type=&apos;socket&apos;, host=&apos;hadoop-sh1-core1&apos;, port=&apos;9998&apos;, delimiter=&apos; &apos;);解析出type中的内容，使用反射寻找到对应的处理类，解析各个参数是否合法，获得默认参数等。这里就会使用format(‘socket’)的方式，option中分别是host和port，分隔符是’ ‘空格。schema的定义schema的定义spark.readStream创建的是dataframe，比如socket，它创建的df只有一个列，schema是value，如果是kafka的话就更多了。接下来就是将定义的表中的field赋给df。本项目中采用的是json的方式传schema，具体原因也很简单，tuple不行，case class的话需要动态变化，难度大，rdd方式在里面行不通，就通过json来做了。窗口的定义flink中其实也有在sql中添加窗口相关的字段，比如group by proctime 之类的。在StructuredStreamingInSQL中添加，eventtime或者processtime的window sql，看源码中，其实定义一个窗口，就是为这个df添加了一个window的字段，window中有start、end等字段，知道了这个，我们在df中只要定义窗口的字段覆盖掉默认的window字段，就能使用processtime和eventtime的sql语句啦！sink的处理将create的source加上定义field，加上window字段之后，就是将insert into的sql解析，把target的表拿出来，select后的内容是逻辑的主体，sql执行的内容结束之后，就和前面一样，根据type中的内容，找到对应的sink内容，执行writeStream。动态添加在处理中可能有这样的情况，想要更新执行的sql，但又不希望spark程序停止，这个时候就可以通过在zk上创建监听器的方式来实现sql的动态添加。动态的替换的实现方式是，结构化流把所有的查询存在一个map中，key是jobid，value是query，通过获取旧的query的id，将其stop，新的query就会无缝对接，由于是新的query，bachid等内容都会从头开始计算。后续监控、自定义函数、压测、调优等功能(待分享)]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>结构化流</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019元旦-线下项目第11期圆满结束]]></title>
    <url>%2F2019%2F01%2F02%2F2019%E5%85%83%E6%97%A6-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC11%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[2019年元旦3天一句话，上海太冷小伙伴们来自五湖四海北京、成都、深圳、天津、广州、重庆等大家为了一个目标学习真正企业级大数据生产项目一年我们只在节假日举办元旦3天，错过了就是错过了期待线下项目班第12期]]></content>
      <categories>
        <category>线下实战班</category>
      </categories>
      <tags>
        <tag>线下实战班</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我司Kafka+Flink+MySQL生产完整案例代码]]></title>
    <url>%2F2018%2F12%2F20%2F%E6%88%91%E5%8F%B8Kafka%2BFlink%2BMySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1.版本信息：Flink Version:1.6.2Kafka Version:0.9.0.0MySQL Version:5.6.212.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]11.74.103.143 2018-12-20 18:12:00 &quot;GET /class/130.html HTTP/1.1&quot; 404 https://search.yahoo.com/search?p=Flink实战3.工程pom.xml12345678910111213141516171819202122232425262728&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;flink.version&gt;1.6.2&lt;/flink.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Flink-Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt;4.sConf类 定义与MySQL连接的JDBC的参数1234567891011package com.soul.conf;/** * @author 若泽数据soulChun * @create 2018-12-20-15:11 */public class sConf &#123; public static final String USERNAME = &quot;root&quot;; public static final String PASSWORD = &quot;www.ruozedata.com&quot;; public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;&#125;5.MySQLSlink类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.soul.kafka;import com.soul.conf.sConf;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * @author 若泽数据soulChun * @create 2018-12-20-15:09 */public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123; try &#123; if (connection == null) &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setString(3, value.f2); preparedStatement.setString(4, value.f3); preparedStatement.setString(5, value.f4); System.out.println(&quot;Start insert&quot;); preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void open(Configuration parms) throws Exception &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; public void close() throws Exception &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;6.数据清洗日期工具类1234567891011121314151617181920212223package com.soul.utils;import org.apache.commons.lang3.time.FastDateFormat;import java.util.Date;/** * @author soulChun * @create 2018-12-19-18:44 */public class DateUtils &#123; private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;); private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;); public static Long getTime(String time) throws Exception&#123; return SOURCE_FORMAT.parse(time).getTime(); &#125; public static String parseMinute(String time) throws Exception&#123; return TARGET_FORMAT.format(new Date(getTime(time))); &#125; //测试一下 public static void main(String[] args) throws Exception&#123; String time = &quot;2018-12-19 18:55:00&quot;; System.out.println(parseMinute(time)); &#125;&#125;7.MySQL建表123456789create table log_info(ID INT NOT NULL AUTO_INCREMENT,IP VARCHAR(50),TIME VARCHAR(50),CourseID VARCHAR(10),Status_Code VARCHAR(10),Referer VARCHAR(100),PRIMARY KEY ( ID ))ENGINE=InnoDB DEFAULT CHARSET=utf8;8.主程序：主要是将time的格式转成yyyyMMddHHmmss,还有取URL中的课程ID，将不是/class开头的过滤掉。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.soul.kafka;import com.soul.utils.DateUtils;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;import java.util.Properties;/** * @author soulChun * @create 2018-12-19-17:23 */public class FlinkCleanKafka &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔 properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔 properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties); DataStream&lt;String&gt; stream = env.addSource(myConsumer);// stream.print().setParallelism(2); DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123; String[] data = value.split(&quot;\\\t&quot;); String CourseID = null; String url = data[2].split(&quot;\\ &quot;)[2]; if (url.startsWith(&quot;/class&quot;)) &#123; String CourseHTML = url.split(&quot;\\/&quot;)[2]; CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));// System.out.println(CourseID); &#125; return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]); &#125; &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123; return value.f2 != null; &#125; &#125;); CleanData.addSink(new MySQLSink()); env.execute(&quot;Flink kafka&quot;); &#125;&#125;9.启动主程序，查看MySQL表数据在递增123456mysql&gt; select count(*) from log_info;+----------+| count(*) |+----------+| 15137 |+----------+Kafka过来的消息是我模拟的，一分钟产生100条。以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark在携程的实践（二）]]></title>
    <url>%2F2018%2F12%2F16%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[以下内容来自第三届携程大数据沙龙七、遇到的问题orc splitSpark读取Hive表用的各个文件格式的InuptFormat，计算读取表需要的task数量依赖于InputFormat#getSplits由于大部分表的存储格式主要使用的是orc，当一个orc文件超过256MB，split算法并行去读取orc元数据，有时候Driver内存飙升，OOM crash，Full GC导致network timeout，spark context stopHive读这些大表为何没有问题？因为Hive默认使用的是CombineHiveInputFormat，split是基于文件大小的。Spark也需要实现类似于Hive的CombineInputFormat，还能解决小文件过多导致提交task数量过多的问题。Executor Container killedExecutor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead原因：1.Shuffle Read时netty堆外内存的使用2.Window function spill threshold过小，导致每4096条或者64MB为一个文件写到磁盘外部排序同时打开每个文件，每个文件占用1MB的堆外内存，导致container使用的内存远超过申请的内存，遂被yarn kill。解决：Patch：[SPARK-19659] Fetch big blocks to disk when shuffle-read[SPARK-21369][CORE] Don’t use Scala Tuple2 in common/network-参数：spark.reducer.maxReqSizeShuffleToMem=209715200Patch：[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArray参数：spark.sql.windowExec.buffer.in.memory.threshold=4096spark.sql.windowExec.buffer.spill.threshold= 1024 1024 * 1024 / 2小文件问题Spark写数据时生成很多小文件，对NameNode产生巨大的压力，在一开始Spark灰度上线的时候，文件数和Block数飙升，文件变小导致压缩率降低，容量也跟着上去。移植Hive MergeFileTask的实现在Spark最后写目标表的阶段追加入了一个MergeFileTask，参考了Hive的实现org.apache.hadoop.hive.ql.io.merge.MergeFileTaskorg.apache.hadoop.hive.ql.exec.OrcFileMergeOperator无数据的情况下不创建空文件[SPARK-21435][SQL]Empty files should be skipped while write to file八、优化1.查询分区表时支持broadcast join，加速查询2.减少Broadcast join的内存压力 SPARK-221703.Fetch失败后能快速失败，以免作业卡几个小时 SPARK-197534.Spark Thrift Server稳定性经常挂掉，日志里异常，more than one active taskSet for stageApply SPARK-23433仍有少数挂掉的情况，提交SPARK-24677到社区，修复之5.作业hang住 SPARK-21834 SPARK-19326 SPARK-11334九、未来计划自动调优内存手机spark driver和executor内存使用情况根据作业历史的内存使用情况，在调度系统端自动设置合适的内存https://github.com/uber-common/jvm-profilerspark adaptive动态调整执行计划 SortMergeJoin转化为BroadcastHashJoin动态处理数据倾斜https://issues.apache.org/jira/browse/SPARK-23128https://github.com/Intel-bigdata/spark-adaptive]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark在携程的实践（一）]]></title>
    <url>%2F2018%2F12%2F09%2FSpark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、Spark在携程应用的现状集群规模：平均每天MR任务数：30W+开发平台：调度系统运行的任务数：10W+每天运行任务实例数：23W+ETL/计算任务：~58%查询平台:adhoc查询：2W+支持Spark/Hive/Presto二、Hive与Spark的区别Hive：优点：运行稳定，客户端内存消耗小。存在问题：生成多个MapReduce作业；中间结果落地，IO开销大；频繁申请和释放container，资源没有合理充分利用Spark：快：高效的DAG执行引擎，可以基于内存来高效的处理数据流，节省大量IO开销通用性：SparkSQL能直接使用HiveQL语法，Hive Metastore，Serdes，UDFs三、迁移SparkSQL的挑战兼容性：Hive原先的权限控制SQL语法，UDF和Hive的兼容性稳定性：迁移透明，低优先级用户无感知监控作业迁移后成功率及运行时长对比准确性：数据一致功能增强：用户体验，是否易用，报错信息是否可读潜在Bug周边系统配合改造血缘收集四、兼容性改造移植hive权限Spark没有权限认证模块，可对任意表进行查询，有安全隐患需要与Hive共享同一套权限方案：执行SQL时，对SQL解析得到LogicalPlan，对LogicalPlan进行遍历，提取读取的表及写入的表，调用Hvie的认证方法进行检查，如果有权限则继续执行，否则拒绝该用户的操作。SQL语法和hive兼容Spark创建的某些视图，在Hive查询时报错，Spark创建的视图不会对SQL进行展开，视图定义没有当前的DB信息，Hive不兼容读取这样的视图方案：、保持与Hive一致，在Spark创建和修改视图时，使用hive cli driver去执行create/alter view sqlUDF与hive兼容UDF计算结果不一样，即使是正常数据，Spark返回null，Hive结果正确；异常数据，Spark抛exception导致作业失败，Hive返回的null。方案：Spark函数修复，比如round函数将hive一些函数移植，并注册成永久函数整理Spark和Hive语法和UDF差异五、稳定性和准确性稳定性：迁移透明：调度系统对低优先级作业，按作业粒度切换成Spark执行，失败后再切换成hive灰度变更，多种变更规则：支持多版本Spark，自动切换引擎，Spark v2 -&gt; Spark v1 -&gt; Hive；灰度推送参数，调优参数，某些功能监控：每日统计spark和hive运行对比，每时收集作业粒度失败的Spark作业，分析失败原因准确性：数据质量系统：校验任务，检查数据准确性六、功能增强Spark Thrift Server：1.基于delegation token的impersontionDriver：为不同的用户拿delegation token，写到staging目录，记录User-&gt;SQL-&gt;Job映射关系，分发task带上对应的usernameExecutor：根据task信息带的username找到staging目录下的token，加到当前proxy user的ugi，实现impersonate2.基于zookeeper的服务发现，支持多台server这一块主要移植了Hive zookeeper的实现3.限制大查询作业，防止driver OOM限制每个job产生的task最大数量限制查询SQL的最大行数，客户端查询大批量数据，数据挤压在Thrift Server，堆内内存飙升，强制在只有查的SQL加上limit限制查询SQL的结果集数据大小4.监控对每个server定时查询，检测是否可用多运行时长较久的作业，主动kill用户体验用户看到的是类似Hive MR进度的日志，INFO级别日志收集到ES，可供日志的分析和排查问题收集生成的表或者分区的numRows numFile totalSize，输出到日志对简单的语句，如DDL语句，自动使用–master=local方式启动Combine input Format在HadoopTableReader#makeRDDForTable，拿到对应table的InputFormatClass，转换成对应格式的CombineInputFormat通过开关来决定是否启用这个特性set spark.sql.combine.input.splits.enable=true通过参数来调整每个split的total input sizemapreduce.input.fileinputformat.split.maxsize=256MB 10241024之前driver读大表高峰时段split需要30分钟不止，才把任务提交上，现在只要几分钟就算好split的数量并提交任务，也解决了一些表不大，小文件多，能合并到同一个task进行读取]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码 | Spark读取mongoDB数据写入Hive普通表和分区表]]></title>
    <url>%2F2018%2F11%2F20%2FSpark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[版本：spark 2.2.0hive 1.1.0scala 2.11.8hadoop-2.6.0-cdh5.7.0jdk 1.8MongoDB 3.6.4一 原始数据及Hive表MongoDB数据格式1234567&#123; &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;), &quot;id&quot; : &quot;1&quot;, &quot;name&quot; : &quot;lisi&quot;, &quot;age&quot; : &quot;18&quot;, &quot;deptno&quot; : &quot;01&quot;&#125;Hive普通表123456create table mg_hive_test(id string,name string,age string,deptno string)row format delimited fields terminated by &apos;\t&apos;;Hive分区表1234567create table mg_hive_external(id string,name string,age string)partitioned by (deptno string)row format delimited fields terminated by &apos;\t&apos;;二 IDEA+Maven+Java依赖1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt;代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/2 21:00 * */import com.mongodb.spark.MongoSpark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.RowFactory;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.hive.HiveContext;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructField;import org.apache.spark.sql.types.StructType;import org.bson.Document;import java.io.File;import java.util.ArrayList;import java.util.List;public class sparkreadmgtohive &#123; public static void main(String[] args) &#123; //spark 2.x String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath(); SparkSession spark = SparkSession.builder() .master(&quot;local[2]&quot;) .appName(&quot;SparkReadMgToHive&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;) .enableHiveSupport() .getOrCreate(); JavaSparkContext sc = new JavaSparkContext(spark.sparkContext()); //spark 1.x// JavaSparkContext sc = new JavaSparkContext(conf);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);// sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);// SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);// conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);// conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);// HiveContext sqlContext = new HiveContext(sc);// //create df from mongo// Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();// df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show(); String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;; String opType =&quot;P&quot;; SQLUtils sqlUtils = new SQLUtils(); List&lt;String&gt; column = sqlUtils.getColumns(querysql); //create rdd from mongo JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc); //将Document转成Object JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123; public Object call(Document document)&#123; List list = new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; list.add(String.valueOf(document.get(column.get(i)))); &#125; return list;// return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;); &#125; &#125;); System.out.println(Ordd.first()); //通过编程方式将RDD转成DF List ls= new ArrayList(); for (int i = 0; i &lt; column.size(); i++) &#123; ls.add(column.get(i)); &#125; String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;); System.out.println(schemaString); List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); for (String fieldName : schemaString.split(&quot;,&quot;)) &#123; StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true); fields.add(field); &#125; StructType schema = DataTypes.createStructType(fields); JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123; List fileds = (List) record;// String[] attributes = record.toString().split(&quot;,&quot;); return RowFactory.create(fileds.toArray()); &#125;); Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema); //将DF写入到Hive中 //选择Hive数据库 spark.sql(&quot;use datalake&quot;); //注册临时表 df.registerTempTable(&quot;mgtable&quot;); if (&quot;O&quot;.equals(opType.trim())) &#123; System.out.println(&quot;数据插入到Hive ordinary table&quot;); Long t1 = System.currentTimeMillis(); spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;); Long t2 = System.currentTimeMillis(); System.out.println(&quot;共耗时：&quot; + (t2 - t1) / 60000 + &quot;分钟&quot;); &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123; System.out.println(&quot;数据插入到Hive dynamic partition table&quot;); Long t3 = System.currentTimeMillis(); //必须设置以下参数 否则报错 spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;); //depton为分区字段 select语句最后一个字段必须是deptno spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;); Long t4 = System.currentTimeMillis(); System.out.println(&quot;共耗时：&quot;+(t4 -t3)/60000+ &quot;分钟&quot;); &#125; spark.stop(); &#125;&#125;工具类1234567891011121314151617181920212223242526272829303132333435package com.huawei.mongo;/* * @Author: Create by Achun *@Time: 2018/6/3 23:20 * */import java.util.ArrayList;import java.util.List;public class SQLUtils &#123; public List&lt;String&gt; getColumns(String querysql)&#123; List&lt;String&gt; column = new ArrayList&lt;String&gt;(); String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6, querysql.indexOf(&quot;from&quot;)).trim(); if (tmp.indexOf(&quot;*&quot;) == -1)&#123; String cols[] = tmp.split(&quot;,&quot;); for (String c:cols)&#123; column.add(c); &#125; &#125; return column; &#125; public String getTBname(String querysql)&#123; String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim(); int sx = tmp.indexOf(&quot; &quot;); if(sx == -1)&#123; return tmp; &#125;else &#123; return tmp.substring(0,sx); &#125; &#125;&#125;三 错误解决办法1 IDEA会获取不到Hive的数据库和表，将hive-site.xml放入resources文件中。并且将resources设置成配置文件(设置成功文件夹是蓝色否则是灰色)file–&gt;Project Structure–&gt;Modules–&gt;Source2 上面错误处理完后如果报JDO类型的错误，那么检查HIVE_HOME/lib下时候否mysql驱动，如果确定有，那么就是IDEA获取不到。解决方法如下：将mysql驱动拷贝到jdk1.8.0_171.jdk/Contents/Home/jre/lib/ext路径下(jdk/jre/lib/ext)在IDEA项目External Libraries下的&lt;1.8&gt;里面添加mysql驱动四 注意点由于将MongoDB数据表注册成了临时表和Hive表进行了关联，所以要将MongoDB中的id字段设置成索引字段，否则性能会很慢。MongoDB设置索引方法：1db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;查看索引：12db.getCollection(&apos;mgtest&apos;).getIndexes()MongoSpark网址：https://docs.mongodb.com/spark-connector/current/java-api/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)]]></title>
    <url>%2F2018%2F11%2F10%2F%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource%2BSinkToMySQL)%2F</url>
    <content type="text"><![CDATA[1.下载Flink安装包flink下载地址https://archive.apache.org/dist/flink/flink-1.5.0/因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可上传至机器的/opt目录下2.解压tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/3.配置master节点选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。jobmanager.rpc.address: node1(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)rest.port: 8088本次安装 master节点为node1，因为单机，slave节点也为node14.配置slaves将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。5.启动flink集群bin/start-cluster.sh打开 http://node1:8088 查看web页面Task Managers代表当前的flink只有一个节点，每个task还有两个slots6.测试依赖123456789101112131415161718192021222324252627&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;7.Socket测试代码12345678910111213141516171819202122232425262728293031public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; final String hostName; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(&quot;port&quot;); hostName = params.get(&quot;hostname&quot;); &#125; catch (Exception e) &#123; System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + &quot; : &quot; + count; &#125; &#125;&#125;打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)在命令行set MAVEN_OPTS= -Xms128m -Xmx512m继续执行mvn clean install生成FlinkTest.jar找到打成的jar，并upload，开始上传运行参数介绍提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个发送数据12345[root@hadoop000 flink-1.5.0]# nc -l 8099aaa bbbaaa cccaaa bbbbbb ccc点开running的job，你可以看见接收的字节数等信息到log目录下可以清楚的看见输出1234567891011[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.outaaa : 1ccc : 1ccc : 1bbb : 1ccc : 1bbb : 1bbb : 1ccc : 1bbb : 1ccc : 1除了可以在界面提交，还可以将jar上传的linux中进行提交任务运行flink上传的jar1bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1其他步骤一致。8.使用kafka作为source加上依赖1234&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;1234567891011121314151617public class KakfaSource010 &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); //可以通过正则表达式来匹配合适的topic FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties); //配置从最新的地方开始消费 kafkaSource.setStartFromLatest(); //使用addsource，将kafka的输入转变为datastream DataStream&lt;String&gt; consume = env.addSource(wordfre); ... //process and sink env.execute(&quot;KakfaSource010&quot;); &#125;&#125;9.使用mysql作为sinkflink本身并没有提供datastream输出到mysql，需要我们自己去实现首先，导入依赖12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt;自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下12345678910111213141516171819202122public class MysqlSink implements SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; String username = &quot;mysql.user&quot;; String password = &quot;mysql.password&quot;; String drivername = &quot;mysql.driver&quot;; String dburl = &quot;mysql.url&quot;; @Override public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下12345678910111213141516171819202122232425262728public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement preparedStatement = null; private String userName = null; private String password = null; private String driverName = null; private String DBUrl = null; public MysqlSink() &#123; userName = &quot;mysql.username&quot;; password = &quot;mysql.password&quot;; driverName = &quot;mysql.driverName&quot;; DBUrl = &quot;mysql.DBUrl&quot;; &#125; public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; if(connection==null)&#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; String sql =&quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,value.f0); preparedStatement.setString(2,value.f1); preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0 &#125; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; @Override public void close() throws Exception &#123; if(preparedStatement!=null)&#123; preparedStatement.close(); &#125; if(connection!=null)&#123; connection.close(); &#125; &#125;&#125;Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。使用这个mysqlsink也非常简单1//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());10.总结本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19生产预警平台项目之sparkdemo.jar运行在yarn上过程]]></title>
    <url>%2F2018%2F09%2F28%2F19%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bsparkdemo.jar%E8%BF%90%E8%A1%8C%E5%9C%A8yarn%E4%B8%8A%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.将之前打包的jar包上传[root@sht-sgmhadoopnn-01 spark]# pwd/root/learnproject/app/spark[root@sht-sgmhadoopnn-01 spark]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring sparkdemo.jar…100% 164113 KB 421 KB/sec 00:06:29 0 Errors2.以下是错误2.112ERROR1: Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributesIDEA打包的jar包,需要使用zip删除指定文件1zip -d sparkdemo.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF2.21ERROR2: Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/learn/java/main/OnLineLogAnalysis2 : Unsupported major.minor version 52.0yarn环境的jdk版本低于编译jar包的jdk版本(需要一致或者高于;每个节点需要安装jdk,同时修改每个节点的hadoop-env.sh文件的JAVA_HOME参数指向)2.31234567891011ERROR3: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; 17/02/15 17:30:35 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch; at org.influxdb.impl.InfluxDBImpl.ping(InfluxDBImpl.java:178) at org.influxdb.impl.InfluxDBImpl.version(InfluxDBImpl.java:201) at com.learn.java.main.OnLineLogAnalysis2.main(OnLineLogAnalysis2.java:69) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)抛错信息为NoSuchMethodError，表示 guava可能有多版本，则低版本12345678910111213141516171819202122232425[root@sht-sgmhadoopnn-01 app]# pwd /root/learnproject/app [root@sht-sgmhadoopnn-01 app]# ll total 470876 -rw-r--r-- 1 root root 7509833 Jan 16 22:11 AdminLTE.zip drwxr-xr-x 12 root root 4096 Feb 14 11:21 hadoop -rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz drwxr-xr-x 7 root root 4096 Feb 7 11:16 kafka-manager-1.3.2.1 -rw-r--r-- 1 root root 59682993 Dec 26 14:44 kafka-manager-1.3.2.1.zip drwxr-xr-x 2 root root 4096 Jan 7 16:21 kafkaoffsetmonitor drwxr-xr-x 2 777 root 4096 Feb 14 14:48 pid drwxrwxr-x 4 1000 1000 4096 Oct 29 01:46 sbt -rw-r--r-- 1 root root 1049906 Dec 25 21:29 sbt-0.13.13.tgz drwxrwxr-x 6 root root 4096 Mar 4 2016 scala -rw-r--r-- 1 root root 28678231 Mar 4 2016 scala-2.11.8.tgz drwxr-xr-x 13 root root 4096 Feb 15 17:01 spark -rw-r--r-- 1 root root 187426587 Nov 12 06:54 spark-2.0.2-bin-hadoop2.7.tgz [root@sht-sgmhadoopnn-01 app]# [root@sht-sgmhadoopnn-01 app]# find ./ -name *guava* [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar ./hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/yarn/lib/[root@sht-sgmhadoopnn-01 app]# mv ./spark/jars/guava-14.0.1.jar ./spark/jars/guava-14.0.1.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./spark/jars/ [root@sht-sgmhadoopnn-01 app]# mv ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar ./hadoop/share/hadoop/common/lib/guava-11.0.2.jar.bak [root@sht-sgmhadoopnn-01 app]# cp ./spark/libs/guava-20.0.jar ./hadoop/share/hadoop/common/lib/3.后台提交jar包运行123456789101112131415161718[root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# nohup /root/learnproject/app/spark/bin/spark-submit \&gt; --name onlineLogsAnalysis \&gt; --master yarn \&gt; --deploy-mode cluster \&gt; --conf &quot;spark.scheduler.mode=FAIR&quot; \&gt; --conf &quot;spark.sql.codegen=true&quot; \&gt; --driver-memory 2G \&gt; --executor-memory 2G \&gt; --executor-cores 1 \&gt; --num-executors 3 \&gt; --class com.learn.java.main.OnLineLogAnalysis2 \&gt; /root/learnproject/app/spark/sparkdemo.jar &amp;[1] 22926[root@sht-sgmhadoopnn-01 spark]# nohup: ignoring input and appending output to `nohup.out&apos;[root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# [root@sht-sgmhadoopnn-01 spark]# tail -f nohup.out4.yarn web界面查看运行logApplicationMaster：打开为spark history server web界面logs： 查看stderr 和 stdout日志 (system.out.println方法输出到stdout日志中)5.查看spark history web6.查看DashBoard ,实时可视化]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18Spark on Yarn配置日志Web UI(HistoryServer服务)]]></title>
    <url>%2F2018%2F09%2F26%2F18Spark%20on%20Yarn%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97Web%20UI(HistoryServer%E6%9C%8D%E5%8A%A1)%2F</url>
    <content type="text"><![CDATA[1.进入spark目录和配置文件12[root@sht-sgmhadoopnn-01 ~]# cd /root/learnproject/app/spark/conf[root@sht-sgmhadoopnn-01 conf]# cp spark-defaults.conf.template spark-defaults.conf2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)1234567891011 [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls / Found 3 items drwxr-xr-x - root root 0 2017-02-14 12:43 /spark drwxrwx--- - root root 0 2017-02-14 12:58 /tmp drwxr-xr-x - root root 0 2017-02-14 12:58 /user You have new mail in /var/spool/mail/root [root@sht-sgmhadoopnn-01 conf]# hdfs dfs -ls /spark Found 1 items drwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@sht-sgmhadoopnn-01 conf]# hdfs dfs -mkdir /spark/historylog#在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息3.配置12345678[root@sht-sgmhadoopnn-01 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080#spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建#spark.yarn.historyServer.address : Spark history server的地址(不加http://).这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.4.添加SPARK_HISTORY_OPTS参数123456789101112 [root@sht-sgmhadoopnn-01 conf]# vi spark-env.sh #!/usr/bin/env bash export SCALA_HOME=/root/learnproject/app/scala export JAVA_HOME=/usr/java/jdk1.8.0_111 export SPARK_MASTER_IP=172.16.101.55 export SPARK_WORKER_MEMORY=1g export SPARK_PID_DIR=/root/learnproject/app/pid export HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.启动服务和查看1234567891011121314151617 [root@sht-sgmhadoopnn-01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out [root@sht-sgmhadoopnn-01 ~]# jps 28905 HistoryServer 30407 ProdServerStart 30373 ResourceManager 30957 NameNode 16949 Jps 30280 DFSZKFailoverController31445 JobHistoryServer[root@sht-sgmhadoopnn-01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/java [root@sht-sgmhadoopnn-01 ~]#]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[17生产预警平台项目之使用IDEA将工程Build成jar包]]></title>
    <url>%2F2018%2F09%2F25%2F17%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E4%BD%BF%E7%94%A8IDEA%E5%B0%86%E5%B7%A5%E7%A8%8BBuild%E6%88%90jar%E5%8C%85%2F</url>
    <content type="text"><![CDATA[1.File–&gt;Project Structure2.Artifacts–&gt;+–&gt;JAR–&gt;From modules with dependencies3. 单击… –&gt;选择OnLineLogAnalysis24.选择项目的根目录5.修改Name–&gt;选择输出目录–&gt;选择Output directory–&gt;Apply–&gt;OK6.Build–&gt;Build Artifacts–&gt;Build===================================说明:1.打包方式很多，大家自行google.2.由于我是引用influxdb的源码包,需要引入许多依赖jar包,所以我需要将相关依赖jar包全部打包到本程序的jar包,故该jar包大概160M。(当然也可以只需要打本程序的jar包，只不过需要事先将相关的所有或者部分依赖jar包，前提上传到集群，然后spark-submit使用–jars引用即可)]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16生产预警平台项目之grafana-4.1.1 Install和新建日志分析的DashBoard]]></title>
    <url>%2F2018%2F09%2F19%2F16%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bgrafana-4.1.1%20Install%E5%92%8C%E6%96%B0%E5%BB%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%9A%84DashBoard%2F</url>
    <content type="text"><![CDATA[1.下载wget https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.1-1484211277.linux-x64.tar.gz2.解压tar -zxvf grafana-4.1.1-1484211277.linux-x64.tar.gz3.配置文件cd grafana-4.1.1-1484211277cp conf/sample.ini conf/custom.ini#make changes to conf/custom.ini then start grafana-server4.后台启动./bin/grafana-server &amp;5.打开webhttp://172.16.101.66:3000/ admin/admin6.配置数据源influxdb还要填写Database 为 online_log_analysis7.IDEA本机运行OnLineLogAanlysis2.class，实时计算存储到influxdb8.新建dashboard和 cdh_hdfs_warn曲线图参考:http://grafana.org/download/http://docs.grafana.org/]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15生产预警平台项目之基于Spark Streaming+Saprk SQL开发OnLineLogAanlysis2]]></title>
    <url>%2F2018%2F09%2F18%2F15%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%2BSaprk%20SQL%E5%BC%80%E5%8F%91OnLineLogAanlysis2%2F</url>
    <content type="text"><![CDATA[1.influxdb创建database[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0create database online_log_analysis2.导入源代码项目中原本想将 influxdb-java https://github.com/influxdata/influxdb-java的InfluxDBTest.java 文件的加到项目中，所以必须要引入 influxdb-java 的包；但是由于GitHub的上的class文件的某些方法，是版本是2.6，而maven中的最高也就2.5版本，所以将Github的源代码下载导入到idea中，编译导出2.6.jar包；可是 引入2.6jar包，其在InfluxDBTest.class文件的 无法import org.influxdb（百度谷歌很长时间，尝试很多方法不行）。最后索性将 influx-java的源代码全部添加到项目中即可，如下图所示。3.运行OnLineLogAanlysis2.javahttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis2.java比如 logtype_count,host_service_logtype=hadoopnn-01_namenode_WARN count=12logtype_count 是表host_service_logtype=hadoopnn-01_namenode_WARN 是 tag–标签，在InfluxDB中，tag是一个非常重要的部分，表名+tag一起作为数据库的索引，是“key-value”的形式。count=12 是 field–数据，field主要是用来存放数据的部分，也是“key-value”的形式。tag、field 中间是要有空格的4.influxdb查询数据]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13生产预警平台项目之舍弃Redis+echarts3,选择InfluxDB+Grafana]]></title>
    <url>%2F2018%2F09%2F17%2F13%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E8%88%8D%E5%BC%83Redis%2Becharts3%2C%E9%80%89%E6%8B%A9InfluxDB%2BGrafana%2F</url>
    <content type="text"><![CDATA[1.最初选择Redis作为存储，是主要有4个原因:a.redis是一个key-value的存储系统，数据是存储在内存中，读写性能很高；b.支持多种数据类型，如set,zset,list,hash,string；c.key过期策略；d.最主要是网上的博客全是sparkstreaming+redis，都互相模仿；至于缺点，当时还没考虑到。2.然后开始添加CDHRolelog.class类和将redis模块加入代码中，使计算结果（本次使用spark streaming+spark sql，之前仅仅是spark streaming，具体看代码）存储到redis中，当然存储到redis中，有两种存储格式。2.1 key为机器名称,服务名称,日志级别拼接的字符串，如hadoopnn-01_namenode_WARN，value为数据类型list，其存储为json格式的 [{“timeStamp”: “2017-02-09 17:16:14.249”,”hostName”: “hadoopnn-01”,”serviceName”: “namenode”,”logType”:”WARN”,”count”:”12” }]代码url,下载导入idea,运行即可:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis3.java2.2 key为timestamp如 2017-02-09 18:09:02.462,value 为 [ {“host_service_logtype”: “hadoopnn-01_namenode_INFO”,”count”:”110” }, {“host_service_logtype”: “hadoopnn-01_namenode_DEBUG”,”count”:”678” }, {“host_service_logtype”: “hadoopnn-01_namenode_WARN”,”count”:”12” }]代码url,下载导入idea,运行即可:https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis5.java3.做可视化这块，我们选择adminLTE+flask+echarts3, 计划和编程开发尝试去从redis实时读取数据，动态绘制图表；后来开发调研大概1周，最终2.1 和2.2方法的存储格式都不能有效适合我们，进行开发可视化Dashboard，所以我们最终调研采取InfluxDB+Grafana来做存储和可视化展示及预警。4.InfluxDB是时序数据库https://docs.influxdata.com/influxdb/v1.2/5.Grafana是可视化组件http://grafana.org/https://github.com/grafana/grafana]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14生产预警平台项目之influxdb-1.2.0 Install和概念，语法等学习]]></title>
    <url>%2F2018%2F09%2F17%2F14%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Binfluxdb-1.2.0%20Install%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E8%AF%AD%E6%B3%95%E7%AD%89%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.下载rpmhttps://dl.influxdata.com/influxdb/releases/influxdb-1.2.0.x86_64.rpm我选择用window7 浏览器下载，然后rz上传到linux机器上2.安装yum install influxdb-1.2.0.x86_64.rpm3.启动service influxdb start参考:https://docs.influxdata.com/influxdb/v1.2/introduction/installation/编译安装:https://anomaly.io/compile-influxdb/4.进入123[root@sht-sgmhadoopdn-04 app]# influx -precision rfc3339Connected to http://localhost:8086 version 1.2.0InfluxDB shell version: 1.2.0语法参考:https://docs.influxdata.com/influxdb/v1.2/introduction/getting_started/学习url:http://www.linuxdaxue.com/influxdb-study-series-manual.html]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12生产预警平台项目之RedisLive监控工具的详细安装]]></title>
    <url>%2F2018%2F09%2F14%2F12%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BRedisLive%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1GitHub: https://github.com/nkrode/RedisLive1.安装python2.7.5 和pip1http://blog.itpub.net/30089851/viewspace-2132450/2.下载RedisLive123456789101112[root@sht-sgmhadoopdn-04 app]# wget https://github.com/nkrode/RedisLive/archive/master.zip[root@sht-sgmhadoopdn-04 app]# unzip master [root@sht-sgmhadoopdn-04 app]# mv RedisLive-master RedisLive[root@sht-sgmhadoopdn-04 app]# cd RedisLive/[root@sht-sgmhadoopdn-04 RedisLive]# lltotal 20drwxr-xr-x 2 root root 4096 Aug 20 2015 design-rw-r--r-- 1 root root 1067 Aug 20 2015 MIT-LICENSE.txt-rw-r--r-- 1 root root 902 Aug 20 2015 README.md-rw-r--r-- 1 root root 58 Aug 20 2015 requirements.txtdrwxr-xr-x 7 root root 4096 Aug 20 2015 src[root@sht-sgmhadoopdn-04 RedisLive]#3.查看版本要求(刚开始安装没注意版本，直接pip导致后面各种问题，所以请仔细看下面过程)123456[root@sht-sgmhadoopdn-04 RedisLive]# cat requirements.txtargparse==1.2.1python-dateutil==1.5redistornado==2.1.1[root@sht-sgmhadoopdn-04 RedisLive]# cd ../4.pip安装环境要求1234[root@sht-sgmhadoopdn-04 app]# pip install tornado[root@sht-sgmhadoopdn-04 app]# pip install redis[root@sht-sgmhadoopdn-04 app]# pip install python-dateutil[root@sht-sgmhadoopdn-04 app]# pip install argparse5.进入 /root/learnproject/app/RedisLive/src目录,配置redis-live.conf文件12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopdn-04 app]# cd -/root/learnproject/app/RedisLive[root@sht-sgmhadoopdn-04 RedisLive]# cd src[root@sht-sgmhadoopdn-04 src]# lltotal 40drwxr-xr-x 4 root root 4096 Aug 20 2015 apidrwxr-xr-x 2 root root 4096 Aug 20 2015 dataproviderdrwxr-xr-x 2 root root 4096 Aug 20 2015 db-rw-r--r-- 1 root root 0 Aug 20 2015 __init__.py-rw-r--r-- 1 root root 381 Aug 20 2015 redis-live.conf.example-rwxr-xr-x 1 root root 1343 Aug 20 2015 redis-live.py-rwxr-xr-x 1 root root 9800 Aug 20 2015 redis-monitor.pydrwxr-xr-x 2 root root 4096 Aug 20 2015 utildrwxr-xr-x 4 root root 4096 Aug 20 2015 wwwYou have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp redis-live.conf.example redis-live.conf[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# vi redis-live.conf&#123; &quot;RedisServers&quot;: [ &#123; &quot;server&quot;: &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125; ], &quot;DataStoreType&quot; : &quot;redis&quot;, &quot;RedisStatsServer&quot;: &#123; &quot;server&quot; : &quot;172.16.101.66&quot;, &quot;port&quot; : 6379 &#125;&#125;6.第一次尝试启动redis-monitor.py抛错 _sqlite312345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named _sqlite3[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite-devel[root@sht-sgmhadoopdn-04 src]# yum install -y sqlite[root@sht-sgmhadoopdn-04 ~]# find / -name _sqlite3.so/usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so/usr/local/Python-2.7.5/build/lib.linux-x86_64-2.7/_sqlite3.so/usr/lib64/python2.6/lib-dynload/_sqlite3.so[root@sht-sgmhadoopdn-04 ~]# cp /usr/local/python27/lib/python2.7/lib-dynload/_sqlite3.so /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 ~]# pythonPython 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sqlite3&gt;&gt;&gt;参考: http://ju.outofmemory.cn/entry/976587.第二次尝试启动redis-monitor.py抛错 redis12345678910111213141516[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ImportError: No module named redis[root@sht-sgmhadoopdn-04 src]# find / -name redis/etc/rc.d/init.d/redis/root/learnproject/app/redis/root/learnproject/app/redis-monitor/src/main/java/sun/redis/root/learnproject/app/redis-monitor/src/test/java/sun/redis/usr/local/redis/usr/local/python27/lib/python2.7/site-packages/redis[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/redis /usr/local/lib/python2.7/lib-dynload/[root@sht-sgmhadoopdn-04 src]# python Python 2.7.5 (default, Sep 17 2016, 15:34:31) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import redis8.第三次尝试启动redis-monitor.py，成功；按ctrl+c中断掉1234[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 ^Cshutting down...You have mail in /var/spool/mail/root[root@sht-sgmhadoopdn-04 src]#9.尝试第一次启动redis-live.py ，tornado.ioloop12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 3, in &lt;module&gt; import tornado.ioloopImportError: No module named tornado.ioloop[root@sht-sgmhadoopdn-04 src]# find / -name tornado/usr/local/python27/lib/python2.7/site-packages/tornado[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/tornado /usr/local/lib/python2.7/lib-dynload/10.尝试第二次启动redis-live.py ，singledispatch123456789[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 6, in &lt;module&gt; import tornado.web File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/web.py&quot;, line 84, in &lt;module&gt; from tornado import gen File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/gen.py&quot;, line 98, in &lt;module&gt; from singledispatch import singledispatch # backportImportError: No module named singledispatch这个 singledispatch 错误，其实就是在tornado里的，谷歌和思考过后，怀疑是版本问题，于是果断卸载tornado12345[root@sht-sgmhadoopdn-04 src]# pip uninstall tornado[root@sht-sgmhadoopdn-04 src]# rm -rf /usr/local/lib/python2.7/lib-dynload/tornado[root@sht-sgmhadoopdn-04 src]# find / -name tornado[root@sht-sgmhadoopdn-04 src]# 假如find有的话 ，就要手工删除掉11.于是想想其他也是要卸载掉12345[root@sht-sgmhadoopdn-04 src]# pip uninstall argparse[root@sht-sgmhadoopdn-04 src]# pip uninstall python-dateutil[root@sht-sgmhadoopdn-04 src]# find / -name argparse[root@sht-sgmhadoopdn-04 src]# find / -name python-dateutil假如find有的话 ，就要手工删除掉12.关键一步: 根据step3的指定版本来安装123[root@sht-sgmhadoopdn-04 src]# pip install -v tornado==2.1.1[root@sht-sgmhadoopdn-04 src]# pip install -v argparse==1.2.1[root@sht-sgmhadoopdn-04 src]# pip install -v python-dateutil==1.513.再次尝试启动redis-live.py ，抛错dateutil.parser1234567891011121314151617[root@sht-sgmhadoopdn-04 src]# ./redis-live.py Traceback (most recent call last): File &quot;./redis-live.py&quot;, line 10, in &lt;module&gt; from api.controller.ServerListController import ServerListController File &quot;/root/learnproject/app/RedisLive/src/api/controller/ServerListController.py&quot;, line 1, in &lt;module&gt; from BaseController import BaseController File &quot;/root/learnproject/app/RedisLive/src/api/controller/BaseController.py&quot;, line 4, in &lt;module&gt; import dateutil.parserImportError: No module named dateutil.parser[root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# [root@sht-sgmhadoopdn-04 src]# find / -name dateutil/usr/local/python27/lib/python2.7/site-packages/dateutil[root@sht-sgmhadoopdn-04 src]# cp -r /usr/local/python27/lib/python2.7/site-packages/dateutil /usr/local/lib/python2.7/lib-dynload/You have mail in /var/spool/mail/root14.再在尝试启动redis-live.py ，成功了，然后按ctrl+c中断掉12345678[root@sht-sgmhadoopdn-04 src]# ./redis-live.py ^CTraceback (most recent call last): File &quot;./redis-live.py&quot;, line 36, in &lt;module&gt; tornado.ioloop.IOLoop.instance().start() File &quot;/usr/local/lib/python2.7/lib-dynload/tornado/ioloop.py&quot;, line 283, in start event_pairs = self._impl.poll(poll_timeout)KeyboardInterrupt[root@sht-sgmhadoopdn-04 src]#15.启动12[root@sht-sgmhadoopdn-04 src]# ./redis-monitor.py --duration 120 &amp;[root@sht-sgmhadoopdn-04 src]# ./redis-live.py &amp;打开web界面http://172.16.101.66:8888/index.html16.总结a.安装 python2.7+pipb.pip指定版本去安装那几个组件17.说明:redis live 实时redis监控面板可以同时监控多个redis实例 , 包括 内存使用 、分db显示的key数、客户端连接数、 命令处理数、 系统运行时间 , 以及各种直观的折线图柱状图.缺点是使用了monitor 命令监控 , 对性能有影响 ,最好不要长时间启动 .redis-monitor.py:用来调用redis的monitor命令来收集redis的命令来进行统计redis-live.py:启动web服务]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11生产预警平台项目之redis-3.2.5 install(单节点)]]></title>
    <url>%2F2018%2F09%2F12%2F11%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bredis-3.2.5%20install(%E5%8D%95%E8%8A%82%E7%82%B9)%2F</url>
    <content type="text"><![CDATA[1.安装jdk1.8123456789101112[root@sht-sgmhadoopdn-04 ~]# cd /usr/java/[root@sht-sgmhadoopdn-04 java]# wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# tar -zxvf jdk-8u111-linux-x64.tar.gz[root@sht-sgmhadoopdn-04 java]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.8.0_111export path=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-04 java]# source /etc/profile[root@sht-sgmhadoopdn-04 java]# java -versionjava version &quot;1.8.0_111&quot;Java(TM) SE Runtime Environment (build 1.8.0_111-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)[root@sht-sgmhadoopdn-04 java]#2.安装 redis 3.2.52.1 安装编绎所需包gcc,tcl12[root@sht-sgmhadoopdn-04 local]# yum install gcc[root@sht-sgmhadoopdn-04 local]# yum install tcl2.2 下载redis-3.2.5123456789[root@sht-sgmhadoopdn-04 local]# wget http://download.redis.io/releases/redis-3.2.5.tar.gz--2016-11-12 20:16:40-- http://download.redis.io/releases/redis-3.2.5.tar.gzResolving download.redis.io (download.redis.io)... 109.74.203.151Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1544040 (1.5M) [application/x-gzip]Saving to: ‘redis-3.2.5.tar.gz’100%[==========================================================================================================================&gt;] 1,544,040 221KB/s in 6.8s 2016-11-12 20:16:47 (221 KB/s) - ‘redis-3.2.5.tar.gz’ saved [1544040/1544040]2.3 安装redis12345678910111213[root@sht-sgmhadoopdn-04 local]# mkdir /usr/local/redis[root@sht-sgmhadoopdn-04 local]# tar xzvf redis-3.2.5.tar.gz[root@sht-sgmhadoopdn-04 local]# cd redis-3.2.5[root@sht-sgmhadoopdn-04 redis-3.2.5]# make PREFIX=/usr/local/redis install[root@sht-sgmhadoopdn-04 redis-3.2.5]# cd ../[root@sht-sgmhadoopdn-04 redis-3.2.5]# ll /usr/local/redis/bin/total 15056-rwxr-xr-x 1 root root 2431728 Nov 12 20:45 redis-benchmark-rwxr-xr-x 1 root root 25165 Nov 12 20:45 redis-check-aof-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-check-rdb-rwxr-xr-x 1 root root 2584443 Nov 12 20:45 redis-clilrwxrwxrwx 1 root root 12 Nov 12 20:45 redis-sentinel -&gt; redis-server-rwxr-xr-x 1 root root 5182191 Nov 12 20:45 redis-server2.4 配置redis为服务1234567891011121314[root@server redis-3.2.5]# cp utils/redis_init_script /etc/rc.d/init.d/redis[root@server redis-3.2.5]# vi /etc/rc.d/init.d/redis 在第二行添加：#chkconfig: 2345 80 90EXEC=/usr/local/bin/redis-server 修改成 EXEC=/usr/local/redis/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cli 修改成 CLIEXEC=/usr/local/redis/bin/redis-cliCONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot; 修改成 CONF=&quot;/usr/local/redis/conf/$&#123;REDISPORT&#125;.conf&quot;$EXEC $CONF 修改成 $EXEC $CONF &amp;[root@server redis-3.2.5]# mkdir /usr/local/redis/conf/[root@server redis-3.2.5]# chkconfig --add redis[root@server redis-3.2.5]# cp redis.conf /usr/local/redis/conf/6379.conf [root@server redis-3.2.5]# vi /usr/local/redis/conf/6379.conf daemonize yespidfile /var/run/redis_6379.pidbind 172.16.101.662.5 启动redis123456[root@server redis-3.2.5]# cd ../redis[root@sht-sgmhadoopdn-04 redis]# service redis startStarting Redis server...[root@sht-sgmhadoopdn-04 redis]# netstat -tnlp|grep redistcp 0 0 172.16.100.79:6379 0.0.0.0:* LISTEN 30032/redis-server [root@sht-sgmhadoopdn-04 redis]#2.6 添加环境变量123456[root@sht-sgmhadoopdn-04 redis]# vi /etc/profileexport REDIS_HOME=/usr/local/redisexport PATH=$REDIS_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@sht-sgmhadoopdn-04 redis]# source /etc/profile[root@sht-sgmhadoopdn-04 redis]# which redis-cli/usr/local/redis/bin/redis-cli2.7 测试 和 设置密码(本次实验未设置密码)1234567891011121314151617181920212223242526[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; sht-sgmhadoopdn-04:6379&gt; set testkey testvalue OKsht-sgmhadoopdn-04:6379&gt; get test(nil)sht-sgmhadoopdn-04:6379&gt; get testkey&quot;testvalue&quot;sht-sgmhadoopdn-04:6379&gt;[root@sht-sgmhadoopdn-04 redis]# vi /usr/local/redis/conf/6379.conf /*添加一个验证密码*/requirepass 123456[root@sht-sgmhadoopdn-04 redis]# service redis stop[root@sht-sgmhadoopdn-04 redis]# service redis start[root@sht-sgmhadoopdn-04 redis]# redis-cli -h sht-sgmhadoopdn-04sht-sgmhadoopdn-04:6379&gt; set key ss(error) NOAUTH Authentication required. [root@server redis-3.2.5]# redis-cli -h sht-sgmhadoopdn-04 -a 123456sht-sgmhadoopdn-04:6379&gt; set a bOKsht-sgmhadoopdn-04:6379&gt; get a&quot;b&quot;sht-sgmhadoopdn-04:6379&gt; exit;[root@sht-sgmhadoopdn-04 redis]#]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10生产预警平台项目之基于Spark Streaming开发OnLineLogAanlysis1]]></title>
    <url>%2F2018%2F09%2F11%2F10%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%E5%BC%80%E5%8F%91OnLineLogAanlysis1%2F</url>
    <content type="text"><![CDATA[1.GitHubhttps://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/OnLineLogAnalysis1.java2.使用IDEA 本地运行测试（未打jar包）]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09生产预警平台项目之基于Spark Streaming Direct方式的WordCount最详细案例(java版)]]></title>
    <url>%2F2018%2F09%2F10%2F09%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9F%BA%E4%BA%8ESpark%20Streaming%20Direct%E6%96%B9%E5%BC%8F%E7%9A%84WordCount%E6%9C%80%E8%AF%A6%E7%BB%86%E6%A1%88%E4%BE%8B(java%E7%89%88)%2F</url>
    <content type="text"><![CDATA[1.前提a. flume 收集–》flume 聚合–》kafka ，启动进程和启动kafka manager监控，参考08【在线日志分析】之Flume Agent(聚合节点) sink to kafka clusterb.window7 安装jdk1.7 或者1.8(本次环境是1.8)c.window7 安装IDEA开发工具(以下仅供参考)使用IntelliJ IDEA 配置Maven（入门）:http://blog.csdn.net/qq_32588349/article/details/51461182IDEA Java/Scala混合项目Maven打包:http://blog.csdn.net/rongyongfeikai2/article/details/51404611Intellij idea使用java编写并执行spark程序:http://blog.csdn.net/yhao2014/article/details/442390212.源代码（可下载单个java文件，加入projet 或者 整个工程下载，IDEA选择open 即可）GitHub: https://github.com/Hackeruncle/OnlineLogAnalysis/blob/master/online_log_analysis/src/main/java/com/learn/java/main/SparkStreamingFromKafka_WordCount.java3.使用IDEA 本地运行测试（未打jar包）海康威视校招电话面试：1.数据倾斜的解决，怎么知道哪里倾斜2.自定义类的广播3.cache机制，rdd和df的cache什么区别4.spark动态内存，堆内和堆外5.rdd算子，map,mappartitions,foreach，union6.宽依赖，窄依赖7.spark DAG过程，doOnrecive，eventloop执行过程8.stage和task怎么分类9.spark调优10.概念，executor，worker，job,task和partition的关系11.用没用过spark什么log，没记住12.讲讲sparkSQL数据清洗过程13.捎带一点项目]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08生产预警平台项目之Flume Agent(聚合节点) sink to kafka cluster]]></title>
    <url>%2F2018%2F09%2F07%2F08%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent(%E8%81%9A%E5%90%88%E8%8A%82%E7%82%B9)%20sink%20to%20kafka%20cluster%2F</url>
    <content type="text"><![CDATA[1.创建logtopic1[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic logtopic2.创建avro_memory_kafka.properties (kafka sink)12345678910111213141516171819202122232425262728293031323334[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# cp avro_memory_hdfs.properties avro_memory_kafka.properties[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_kafka.properties #Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = logtopica1.sinks.k1.kafka.bootstrap.servers = 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092a1.sinks.k1.kafka.flumeBatchSize = 6000a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1a1.sinks.ki.kafka.producer.compression.type = snappy#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90a1.channels.c1.capacity = 2000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c13.后台启动 flume-ng agent(聚合节点)和查看nohup.out123456789[root@sht-sgmhadoopcm-01 ~]# source /etc/profile[root@sht-sgmhadoopcm-01 ~]# cd /tmp/flume-ng/[root@sht-sgmhadoopcm-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_kafka.properties -n a1 -Dflume.root.logger=INFO,console &amp;[1] 4971[root@sht-sgmhadoopcm-01 flume-ng]# nohup: ignoring input and appending output to `nohup.out&apos;[root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# [root@sht-sgmhadoopcm-01 flume-ng]# cat nohup.out4.检查log收集的三台(收集节点)开启没12345678[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;5.打开kafka manager监控http://172.16.101.55:9999]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07生产预警平台项目之kafka-manager监控工具的搭建(sbt安装与编译)]]></title>
    <url>%2F2018%2F09%2F06%2F07%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bkafka-manager%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA(sbt%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BC%96%E8%AF%91)%2F</url>
    <content type="text"><![CDATA[1.下载sbt123456http://www.scala-sbt.org/download.html[root@sht-sgmhadoopnn-01 app]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring sbt-0.13.13.tgz... 100% 1025 KB 1025 KB/sec 00:00:01 0 Errors2.解压1234567891011[root@sht-sgmhadoopnn-01 app]# tar -zxvf sbt-0.13.13.tgzsbt-launcher-packaging-0.13.13/sbt-launcher-packaging-0.13.13/conf/sbt-launcher-packaging-0.13.13/conf/sbtconfig.txtsbt-launcher-packaging-0.13.13/conf/sbtoptssbt-launcher-packaging-0.13.13/bin/sbt-launcher-packaging-0.13.13/bin/sbt.batsbt-launcher-packaging-0.13.13/bin/sbtsbt-launcher-packaging-0.13.13/bin/sbt-launch.jarsbt-launcher-packaging-0.13.13/bin/sbt-launch-lib.bash[root@sht-sgmhadoopnn-01 app]# mv sbt-launcher-packaging-0.13.13 sbt3.添加脚本文件12345[root@sht-sgmhadoopnn-01 bin]# vi sbt#!/usr/bin/env bashBT_OPTS=&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;java $SBT_OPTS -jar /root/learnproject/app/sbt/bin/sbt-launch.jar &quot;$@&quot;4.修改权限和环境变量123456[root@sht-sgmhadoopnn-01 bin]# chmod u+x sbt[root@sht-sgmhadoopnn-01 bin]# vi /etc/profileexport SBT_HOME=/root/learnproject/app/sbtexport PATH=$SBT_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&quot;/etc/profile&quot; 94L, 2265C written[root@sht-sgmhadoopnn-01 bin]# source /etc/profile5.测试12345678/*第一次执行时，会下载一些文件包，然后才能正常使用，要确保联网了，安装成功后显示如下*/[root@sht-sgmhadoopnn-01 bin]# sbt sbt-version[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[info] Set current project to bin (in build file:/root/learnproject/app/sbt/bin/)[info] 0.13.13[root@sht-sgmhadoopnn-01 bin]#]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06生产预警平台项目之 KafkaOffsetMonitor监控工具的搭建]]></title>
    <url>%2F2018%2F09%2F05%2F06%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%20KafkaOffsetMonitor%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.下载在window7 手工下载好下面的链接1https://github.com/quantifind/KafkaOffsetMonitor/releases/tag/v0.2.112345678910[root@sht-sgmhadoopnn-01 app]# mkdir kafkaoffsetmonitor[root@sht-sgmhadoopnn-01 app]# cd kafkaoffsetmonitor#使用rz命令上传[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# rzrz waiting to receive.Starting zmodem transfer. Press Ctrl+C to cancel.Transferring KafkaOffsetMonitor-assembly-0.2.1.jar... 100% 51696 KB 12924 KB/sec 00:00:04 0 Errors You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#2.新建一个kafkaMonitor.sh文件，文件内容如下：12345678910[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# vi kafkaoffsetmonitor.sh! /bin/bashjava -cp KafkaOffsetMonitor-assembly-0.2.1.jar \com.quantifind.kafka.offsetapp.OffsetGetterWeb \--zk 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka \--port 8089 \--refresh 5.seconds \--retain 7.days[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# chmod +x *.sh[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]#参数说明：–zk 这里写的地址和端口，是zookeeper集群的各个地址和端口。应和kafka/bin文件夹中的zookeeper.properties中的host.name和clientPort一致。–port 这个是本软件KafkaOffsetMonitor的端口。注意不要使用那些著名的端口号，例如80,8080等。我采用了8089.–refresh 这个是软件刷新间隔时间，不要太短也不要太长。–retain 这个是数据在数据库中保存的时间。3.后台启动1234567891011121314 1[root@sht-sgmhadoopnn-01 kafkaoffsetmonitor]# nohup ./kafkaoffsetmonitor.sh &amp; 2serving resources from: jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp 3SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;. 4SLF4J: Defaulting to no-operation (NOP) logger implementation 5SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. 6log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection). 7log4j:WARN Please initialize the log4j system properly. 8log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 9log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).10log4j:WARN Please initialize the log4j system properly.11log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.122016-12-25 22:00:24.252:INFO:oejs.Server:jetty-7.x.y-SNAPSHOT132016-12-25 22:00:24.319:INFO:oejsh.ContextHandler:started o.e.j.s.ServletContextHandler&#123;/,jar:file:/root/learnproject/app/kafkaoffsetmonitor/KafkaOffsetMonitor-assembly-0.2.1.jar!/offsetapp&#125;142016-12-25 22:00:24.328:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:80894.IE浏览器打开1http://172.16.101.55:8089]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05生产预警平台项目之Kafka 0.10.1.0 Cluster的搭建和Topic简单操作实验]]></title>
    <url>%2F2018%2F09%2F04%2F05%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BKafka%200.10.1.0%20Cluster%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8CTopic%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[【kafka cluster机器】:机器名称 用户名称sht-sgmhadoopdn-01/02/03 root【安装目录】: /root/learnproject/app1.将scala文件夹同步到集群其他机器(scala 2.11版本，可单独下载解压)123456789101112131415161718192021[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-01:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-02:/root/learnproject/app/[root@sht-sgmhadoopnn-01 app]# scp -r scala root@sht-sgmhadoopdn-03:/root/learnproject/app/#环境变量[root@sht-sgmhadoopdn-01 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-02 app]# vi /etc/profileexport SCALA_HOME=/root/learnproject/app/scalaexport PATH=$SCALA_HOME/bin:$HADOOP_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 app]# source /etc/profile[root@sht-sgmhadoopdn-02 app]# source /etc/profile[root@sht-sgmhadoopdn-03 app]# source /etc/profile2.下载基于Scala 2.11的kafka版本为0.10.1.012345[root@sht-sgmhadoopdn-01 app]# pwd/root/learnproject/app[root@sht-sgmhadoopdn-01 app]# wget http://www-eu.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz[root@sht-sgmhadoopdn-01 app]# tar xzvf kafka_2.11-0.10.1.0.tgz [root@sht-sgmhadoopdn-01 app]# mv kafka_2.11-0.10.1.0 kafka3.创建logs目录和修改server.properties(前提zookeeper cluster部署好，见“03【在线日志分析】之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)” )123456789[root@sht-sgmhadoopdn-01 app]# cd kafka[root@sht-sgmhadoopdn-01 kafka]# mkdir logs[root@sht-sgmhadoopdn-01 kafka]# cd config/[root@sht-sgmhadoopdn-01 config]# vi server.propertiesbroker.id=1port=9092host.name=172.16.101.58log.dirs=/root/learnproject/app/kafka/logszookeeper.connect=172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka4.同步到02/03服务器，更改broker.id 及host.name123456789101112[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-01 app]# scp -r kafka sht-sgmhadoopdn-03:/root/learnproject/app/[root@sht-sgmhadoopdn-02 config]# vi server.properties broker.id=2port=9092host.name=172.16.101.59[root@sht-sgmhadoopdn-03 config]# vi server.properties broker.id=3port=9092host.name=172.16.101.605.环境变量1234567891011[root@sht-sgmhadoopdn-01 kafka]# vi /etc/profileexport KAFKA_HOME=/root/learnproject/app/kafkaexport PATH=$KAFKA_HOME/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-02:/etc/profile[root@sht-sgmhadoopdn-01 kafka]# scp /etc/profile sht-sgmhadoopdn-03:/etc/profile[root@sht-sgmhadoopdn-01 kafka]#[root@sht-sgmhadoopdn-01 kafka]# source /etc/profile[root@sht-sgmhadoopdn-02 kafka]# source /etc/profile[root@sht-sgmhadoopdn-03 kafka]# source /etc/profile6.启动/停止123456[root@sht-sgmhadoopdn-01 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-02 kafka]# nohup kafka-server-start.sh config/server.properties &amp;[root@sht-sgmhadoopdn-03 kafka]# nohup kafka-server-start.sh config/server.properties &amp;###停止bin/kafka-server-stop.sh7.topic相关的操作12345678910111213141516171819202122232425262728293031323334353637a.创建topic，如能成功创建topic则表示集群安装完成，也可以用jps命令查看kafka进程是否存在。[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --create --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --replication-factor 3 --partitions 1 --topic testb.通过list命令查看创建的topic:[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --list --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafkac.查看创建的Topic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:1 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2[root@sht-sgmhadoopdn-01 kafka]# 第一行列出了这个topic的总体情况，如topic名称，分区数量，副本数量等。第二行开始，每一行列出了一个分区的信息，如它是第几个分区，这个分区的leader是哪个broker，副本位于哪些broker，有哪些副本处理同步状态。Partition： 分区Leader ： 负责读写指定分区的节点Replicas ： 复制该分区log的节点列表Isr ： “in-sync” replicas，当前活跃的副本列表（是一个子集），并且可能成为Leader我们可以通过Kafka自带的bin/kafka-console-producer.sh和bin/kafka-console-consumer.sh脚本，来验证演示如果发布消息、消费消息。d.删除topic[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-topics.sh --delete --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic teste.修改topic使用—-alert原则上可以修改任何配置，以下列出了一些常用的修改选项：（1）改变分区数量[root@sht-sgmhadoopdn-02 kafka]#bin/kafka-topics.sh --alter --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic test --partitions 3[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --topic testTopic:test PartitionCount:3 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: test Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: test Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1[root@sht-sgmhadoopdn-02 kafka]#（2）增加、修改或者删除一个配置参数 bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --config key=value bin/kafka-topics.sh —alter --zookeeper 192.168.172.98:2181/kafka --topic my_topic_name --deleteConfig key8.模拟实验11234567在一个终端，启动Producer，并向我们上面创建的名称为my-replicated-topic5的Topic中生产消息，执行如下脚本：[root@sht-sgmhadoopdn-01 kafka]# bin/kafka-console-producer.sh --broker-list 172.16.101.58:9092,172.16.101.59:9092,172.16.101.60:9092 --topic test在另一个终端，启动Consumer，并订阅我们上面创建的名称为my-replicated-topic5的Topic中生产的消息，执行如下脚本：[root@sht-sgmhadoopdn-02 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.16.101.58:2181,172.16.101.59:2181,172.16.101.60:2181/kafka --from-beginning --topic test可以在Producer终端上输入字符串消息行，就可以在Consumer终端上看到消费者消费的消息内容。]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04生产预警平台项目之Flume Agent的3台收集+1台聚合到hdfs的搭建]]></title>
    <url>%2F2018%2F09%2F03%2F04%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume%20Agent%E7%9A%843%E5%8F%B0%E6%94%B6%E9%9B%86%2B1%E5%8F%B0%E8%81%9A%E5%90%88%E5%88%B0hdfs%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[【log收集】:机器名称 服务名称 用户flume-agent-01: namenode hdfsflume-agent-02: datanode hdfsflume-agent-03: datanode hdfs【log聚合】:机器名称 用户sht-sgmhadoopcm-01(172.16.101.54) root【sink到hdfs】:hdfs://172.16.101.56:8020/testwjp/1.下载apache-flume-1.7.0-bin.tar.gz1234567891011[hdfs@flume-agent-01 tmp]$ wget http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz--2017-01-04 20:40:10-- http://www-eu.apache.org/dist/flume/1.7.0/apache-flume-1.7.0-bin.tar.gzResolving www-eu.apache.org... 88.198.26.2, 2a01:4f8:130:2192::2Connecting to www-eu.apache.org|88.198.26.2|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 55711670 (53M) [application/x-gzip]Saving to: “apache-flume-1.7.0-bin.tar.gz”100%[===============================================================================================================================================================================================&gt;] 55,711,670 473K/s in 74s 2017-01-04 20:41:25 (733 KB/s) - “apache-flume-1.7.0-bin.tar.gz” saved [55711670/55711670]2.解压重命名1234[hdfs@flume-agent-01 tmp]$ [hdfs@flume-agent-01 tmp]$ tar -xzvf apache-flume-1.7.0-bin.tar.gz [hdfs@flume-agent-01 tmp]$ mv apache-flume-1.7.0-bin flume-ng[hdfs@flume-agent-01 tmp]$ cd flume-ng/conf3.拷贝flume环境配置和agent配置文件12[hdfs@flume-agent-01 tmp]$ cp flume-env.sh.template flume-env.sh[hdfs@flume-agent-01 tmp]$ cp flume-conf.properties.template exec_memory_avro.properties4.添加hdfs用户的环境变量文件123456789101112131415161718192021[hdfs@flume-agent-01 tmp]$ cd[hdfs@flume-agent-01 ~]$ ls -latotal 24drwxr-xr-x 3 hdfs hadoop 4096 Jul 8 14:05 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_historydrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo[hdfs@flume-agent-01 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-01 ~]$ ls -latotal 36drwxr-xr-x 3 hdfs hadoop 4096 Jan 4 20:49 .drwxr-xr-x. 35 root root 4096 Dec 10 2015 ..-rw------- 1 hdfs hdfs 4471 Jul 8 17:22 .bash_history-rw-r--r-- 1 hdfs hdfs 18 Jan 4 20:49 .bash_logout-rw-r--r-- 1 hdfs hdfs 176 Jan 4 20:49 .bash_profile-rw-r--r-- 1 hdfs hdfs 124 Jan 4 20:49 .bashrcdrwxrwxrwt 2 hdfs hadoop 4096 Nov 19 2014 cache-rw------- 1 hdfs hdfs 3131 Jul 8 14:05 .viminfo5.添加flume的环境变量123456[hdfs@flume-agent-01 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-01 ~]$ . .bash_profile6.修改flume环境配置文件12[hdfs@flume-agent-01 conf]$ vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_257.将基于Flume-ng Exec Source开发自定义插件AdvancedExecSource的AdvancedExecSource.jar包上传到$FLUME_HOME/lib/1http://blog.itpub.net/30089851/viewspace-2131995/12345[hdfs@LogshedNameNodeLogcollector lib]$ pwd/tmp/flume-ng/lib[hdfs@LogshedNameNodeLogcollector lib]$ ll AdvancedExecSource.jar -rw-r--r-- 1 hdfs hdfs 10618 Jan 5 23:50 AdvancedExecSource.jar[hdfs@LogshedNameNodeLogcollector lib]$8.修改flume的agent配置文件1234567891011121314151617181920212223242526[hdfs@flume-agent-01 conf]$ vi exec_memory_avro.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the custom exec sourcea1.sources.r1.type = com.onlinelog.analysis.AdvancedExecSourcea1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-NAMENODE-flume-agent-01.log.outa1.sources.r1.hostname = flume-agent-01a1.sources.r1.servicename = namenode#Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 172.16.101.54a1.sinks.k1.port = 4545#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 60 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 2000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c19.将flume-agent-01的flume-ng打包,scp到flume-agent-02/03 和 sht-sgmhadoopcm-01(172.16.101.54)12345[hdfs@flume-agent-01 tmp]$ zip -r flume-ng.zip flume-ng/*[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-02:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip flume-agent-03:/tmp/[jpwu@flume-agent-01 ~]$ scp /tmp/flume-ng.zip sht-sgmhadoopcm-01:/tmp/10.在flume-agent-02配置hdfs用户环境变量和解压，修改agent配置文件12345678910111213141516171819[hdfs@flume-agent-02 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-02 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-02 ~]$ . .bash_profile[hdfs@flume-agent-02 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-02 tmp]$ cd flume-ng/conf##修改以下参数即可[hdfs@flume-agent-02 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-02.log.outa1.sources.r1.hostname = flume-agent-02a1.sources.r1.servicename = datanode###要检查flume-env.sh的JAVA_HOME目录是否存在11.在flume-agent-03配置hdfs用户环境变量和解压，修改agent配置文件12345678910111213141516171819[hdfs@flume-agent-03 ~]$ cp /etc/skel/.* ./cp: omitting directory `/etc/skel/.&apos;cp: omitting directory `/etc/skel/..&apos;[hdfs@flume-agent-03 ~]$ vi .bash_profileexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin[hdfs@flume-agent-03 ~]$ . .bash_profile[hdfs@flume-agent-03 tmp]$ unzip flume-ng.zip[hdfs@flume-agent-03 tmp]$ cd flume-ng/conf##修改以下参数即可[hdfs@flume-agent-03 conf]$ vi exec_memory_avro.propertiesa1.sources.r1.command = tail -f /var/log/hadoop-hdfs/hadoop-cmf-hdfs1-DATANODE-flume-agent-03.log.outa1.sources.r1.hostname = flume-agent-03a1.sources.r1.servicename = datanode###要检查flume-env.sh的JAVA_HOME目录是否存在12.聚合端 sht-sgmhadoopcm-01，配置root用户环境变量和解压，修改agent配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@sht-sgmhadoopcm-01 tmp]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport FLUME_HOME=/tmp/flume-ngexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$FLUME_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopcm-01 tmp]# source /etc/profile[root@sht-sgmhadoopcm-01 tmp]#[root@sht-sgmhadoopcm-01 tmp]# unzip flume-ng.zip[root@sht-sgmhadoopcm-01 tmp]# cd flume-ng/conf[root@sht-sgmhadoopcm-01 conf]# vi flume-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera ###测试: 先聚合, sink到hdfs端[root@sht-sgmhadoopcm-01 conf]# vi avro_memory_hdfs.properties#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 172.16.101.54a1.sources.r1.port = 4545#Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://172.16.101.56:8020/testwjp/a1.sinks.k1.hdfs.filePrefix = logsa1.sinks.k1.hdfs.inUsePrefix = .a1.sinks.k1.hdfs.rollInterval = 0###roll 16 m = 16777216 bytesa1.sinks.k1.hdfs.rollSize = 1048576a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.batchSize = 6000a1.sinks.k1.hdfs.writeFormat = texta1.sinks.k1.hdfs.fileType = DataStream#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.keep-alive = 90 a1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 6000#Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c113.后台启动123456789101112[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile[root@sht-sgmhadoopnn-01 flume-ng]# nohup flume-ng agent -c conf -f /tmp/flume-ng/conf/avro_memory_hdfs.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;[hdfs@flume-agent-01 flume-ng]$ nohup flume-ng agent -c /tmp/flume-ng/conf -f /tmp/flume-ng/conf/exec_memory_avro.properties -n a1 -Dflume.root.logger=INFO,console &amp;14.校验：将集群的日志下载到本地，打开查看即可(略)12345678910111213141516171819202122232425262728293031------------------------------------------------------------------------------------------------------------------------------------------------【备注】: 1.错误1 flume-ng安装的机器上没有hadoop环境，所以假如sink到hdfs话，需要用到hdfs的jar包[ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:146)] Failed to start agent because dependencies were not found in classpath. Error follows.java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType只需在其他安装hadoop机器上搜索以下5个jar包，拷贝到$FLUME_HOME/lib目录即可。搜索方法: find $HADOOP_HOME/ -name commons-configuration*.jarcommons-configuration-1.6.jarhadoop-auth-2.7.3.jarhadoop-common-2.7.3.jarhadoop-hdfs-2.7.3.jarhadoop-mapreduce-client-core-2.7.3.jarprotobuf-java-2.5.0.jarhtrace-core-3.1.0-incubating.jarcommons-io-2.4.jar2.错误2 无法加载自定义插件的类 Unable to load source type: com.onlinelog.analysis.AdvancedExecSource2017-01-06 21:10:48,278 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:142)] Failed to load configuration data. Exception follows.org.apache.flume.FlumeException: Unable to load source type: com.onlinelog.analysis.AdvancedExecSource, class: com.onlinelog.analysis.AdvancedExecSource执行hdfs或者root用户的环境变量即可[root@sht-sgmhadoopcm-01 flume-ng]# source /etc/profile[hdfs@flume-agent-01 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-02 flume-ng]$ . ~/.bash_profile [hdfs@flume-agent-03 flume-ng]$ . ~/.bash_profile]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03生产预警平台项目之hadoop-2.7.3编译和搭建集群环境(HDFS HA,Yarn HA)]]></title>
    <url>%2F2018%2F09%2F03%2F03%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8Bhadoop-2.7.3%E7%BC%96%E8%AF%91%E5%92%8C%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83(HDFS%20HA%2CYarn%20HA)%2F</url>
    <content type="text"><![CDATA[1.下载hadoop2.7.3最新源码123456789101112131415161718192021222324252627282930313233343536373839404142[root@sht-sgmhadoopnn-01 ~]# mkdir -p learnproject/compilesoft[root@sht-sgmhadoopnn-01 ~]# cd learnproject/compilesoft[root@sht-sgmhadoopnn-01 compilesoft]# wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar -xzvf hadoop-2.7.3-src.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-src[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cat BUILDING.txt Build instructions for Hadoop----------------------------------------------------------------------------------Requirements:* Unix System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac* Zlib devel (if compiling native code)* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )* Internet connection for first build (to fetch all Maven and Hadoop dependencies)----------------------------------------------------------------------------------Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:* Oracle JDK 1.7 (preferred) $ sudo apt-get purge openjdk* $ sudo apt-get install software-properties-common $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-java7-installer* Maven $ sudo apt-get -y install maven* Native libraries $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev* ProtocolBuffer 2.5.0 (required) $ sudo apt-get -y install libprotobuf-dev protobuf-compilerOptional packages:* Snappy compression $ sudo apt-get install snappy libsnappy-dev* Bzip2 $ sudo apt-get install bzip2 libbz2-dev* Jansson (C Library for JSON) $ sudo apt-get install libjansson-dev* Linux FUSE $ sudo apt-get install fuse libfuse-dev2.安装依赖包1[root@sht-sgmhadoopnn-01 compilesoft]# yum install svn autoconf automake libtool cmake ncurses-devel openssl-devel gcc*3.安装jdk12345678910[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport PATH=$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# java -versionjava version &quot;1.7.0_67&quot;Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 compilesoft]#4.安装maven123456789101112131415161718[root@sht-sgmhadoopnn-01compilesoft]# wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz -O apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# tar xvf apache-maven-3.3.9-bin.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-clouderaexport MAVEN_HOME=/root/learnproject/compilesoft/apache-maven-3.3.9#在编译过程中为了防止Java内存溢出，需要加入以下环境变量export MAVEN_OPTS=&quot;-Xmx2048m -XX:MaxPermSize=512m&quot;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH[root@sht-sgmhadoopnn-01 compilesoft]# source /etc/profile[root@sht-sgmhadoopnn-01 compilesoft]# mvn -versionApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /root/learnproject/compilesoft/apache-maven-3.3.9Java version: 1.7.0_67, vendor: Oracle CorporationJava home: /usr/java/jdk1.7.0_67-cloudera/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;You have new mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 apache-maven-3.3.9]#5.编译安装protobuf12345678910111213[root@sht-sgmhadoopnn-01compilesoft]# wget ftp://ftp.netbsd.org/pub/pkgsrc/distfiles/protobuf-2.5.0.tar.gz -O protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# tar -zxvf protobuf-2.5.0.tar.gz[root@hadoop-01 compilesoft]# cd protobuf-2.5.0/[root@hadoop-01 protobuf-2.5.0]# ./configure [root@hadoop-01 protobuf-2.5.0]# make[root@hadoop-01 protobuf-2.5.0]# make install#查看protobuf版本以测试是否安装成功[root@hadoop-01 protobuf-2.5.0]# protoc --versionprotoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory[root@hadoop-01 protobuf-2.5.0]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib[root@hadoop-01 protobuf-2.5.0]# protoc --versionlibprotoc 2.5.0[root@hadoop-01 protobuf-2.5.0]#6.安装snappy123456789101112131415[root@sht-sgmhadoopnn-01 compilesoft]# wget http://pkgs.fedoraproject.org/repo/pkgs/snappy/snappy-1.1.1.tar.gz/8887e3b7253b22a31f5486bca3cbc1c2/snappy-1.1.1.tar.gz#用root用户执行以下命令[root@sht-sgmhadoopnn-01 compilesoft]#tar -zxvf snappy-1.1.1.tar.gz[root@sht-sgmhadoopnn-01 compilesoft]# cd snappy-1.1.1/[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ./configure[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make[root@sht-sgmhadoopnn-01 snappy-1.1.1]# make install#查看snappy库文件[root@sht-sgmhadoopnn-01 snappy-1.1.1]# ls -lh /usr/local/lib |grep snappy-rw-r--r-- 1 root root 229K Jun 21 15:46 libsnappy.a-rwxr-xr-x 1 root root 953 Jun 21 15:46 libsnappy.lalrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so -&gt; libsnappy.so.1.2.0lrwxrwxrwx 1 root root 18 Jun 21 15:46 libsnappy.so.1 -&gt; libsnappy.so.1.2.0-rwxr-xr-x 1 root root 145K Jun 21 15:46 libsnappy.so.1.2.0[root@sht-sgmhadoopnn-01 snappy-1.1.1]#7.编译123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101[root@sht-sgmhadoopnn-01 compilesoft]# cd hadoop-2.7.3-srcmvn clean package -Pdist,native -DskipTests -Dtar或mvn package -Pdist,native -DskipTests -Dtar[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# mvn clean package –Pdist,native –DskipTests –Dtar[INFO] Executing tasksmain: [exec] $ tar cf hadoop-2.7.3.tar hadoop-2.7.3 [exec] $ gzip -f hadoop-2.7.3.tar [exec] [exec] Hadoop dist tar available at: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz [exec] [INFO] Executed tasks[INFO] [INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---[INFO] Building jar: /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-dist-2.7.3-javadoc.jar[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [ 14.707 s][INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 6.832 s][INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.989 s][INFO] Apache Hadoop Annotations .......................... SUCCESS [ 14.258 s][INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.411 s][INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 4.814 s][INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 23.566 s][INFO] Apache Hadoop MiniKDC .............................. SUCCESS [02:31 min][INFO] Apache Hadoop Auth ................................. SUCCESS [ 29.587 s][INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 13.954 s][INFO] Apache Hadoop Common ............................... SUCCESS [03:03 min][INFO] Apache Hadoop NFS .................................. SUCCESS [ 9.285 s][INFO] Apache Hadoop KMS .................................. SUCCESS [ 45.068 s][INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.049 s][INFO] Apache Hadoop HDFS ................................. SUCCESS [03:49 min][INFO] Apache Hadoop HttpFS ............................... SUCCESS [01:08 min][INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.935 s][INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 4.599 s][INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.044 s][INFO] hadoop-yarn ........................................ SUCCESS [ 0.043 s][INFO] hadoop-yarn-api .................................... SUCCESS [02:49 min][INFO] hadoop-yarn-common ................................. SUCCESS [ 40.792 s][INFO] hadoop-yarn-server ................................. SUCCESS [ 0.041 s][INFO] hadoop-yarn-server-common .......................... SUCCESS [ 15.750 s][INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 25.311 s][INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 6.415 s][INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 12.274 s][INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 27.555 s][INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 7.751 s][INFO] hadoop-yarn-client ................................. SUCCESS [ 11.347 s][INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [ 5.612 s][INFO] hadoop-yarn-applications ........................... SUCCESS [ 0.038 s][INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [ 4.029 s][INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 2.611 s][INFO] hadoop-yarn-site ................................... SUCCESS [ 0.077 s][INFO] hadoop-yarn-registry ............................... SUCCESS [ 8.045 s][INFO] hadoop-yarn-project ................................ SUCCESS [ 5.456 s][INFO] hadoop-mapreduce-client ............................ SUCCESS [ 0.226 s][INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 28.462 s][INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 25.872 s][INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 6.697 s][INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.121 s][INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 9.328 s][INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 23.801 s][INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [ 2.412 s][INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 8.876 s][INFO] hadoop-mapreduce ................................... SUCCESS [ 4.237 s][INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 14.285 s][INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.759 s][INFO] Apache Hadoop Archives ............................. SUCCESS [ 3.069 s][INFO] Apache Hadoop Rumen ................................ SUCCESS [ 7.446 s][INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 5.765 s][INFO] Apache Hadoop Data Join ............................ SUCCESS [ 3.752 s][INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 2.771 s][INFO] Apache Hadoop Extras ............................... SUCCESS [ 5.612 s][INFO] Apache Hadoop Pipes ................................ SUCCESS [ 10.332 s][INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 7.131 s][INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [01:32 min][INFO] Apache Hadoop Azure support ........................ SUCCESS [ 10.622 s][INFO] Apache Hadoop Client ............................... SUCCESS [ 12.540 s][INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 1.142 s][INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 7.354 s][INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.269 s][INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.035 s][INFO] Apache Hadoop Distribution ......................... SUCCESS [ 58.051 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 26:29 min[INFO] Finished at: 2016-12-24T21:07:09+08:00[INFO] Final Memory: 214M/740M[INFO] ------------------------------------------------------------------------You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# [root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cp /root/learnproject/compilesoft/hadoop-2.7.3-src/hadoop-dist/target/hadoop-2.7.3.tar.gz ../../You have mail in /var/spool/mail/root[root@sht-sgmhadoopnn-01 hadoop-2.7.3-src]# cd ../../[root@sht-sgmhadoopnn-01 learnproject]# lltotal 193152drwxr-xr-x 5 root root 4096 Dec 24 20:24 compilesoft-rw-r--r-- 1 root root 197782815 Dec 24 21:16 hadoop-2.7.3.tar.gz[root@sht-sgmhadoopnn-01 learnproject]#8.搭建HDFS HA,YARN HA集群（5个节点）参考:http://blog.itpub.net/30089851/viewspace-1994585/https://github.com/Hackeruncle/Hadoop9.搭建集群,验证版本和支持的压缩信息1234567891011121314151617181920[root@sht-sgmhadoopnn-01 app]# hadoop versionHadoop 2.7.3Subversion Unknown -r UnknownCompiled by root on 2016-12-24T12:45ZCompiled with protoc 2.5.0From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4This command was run using /root/learnproject/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar[root@sht-sgmhadoopnn-01 app]# hadoop checknative16/12/25 15:55:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native16/12/25 15:55:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0zlib: true /lib64/libz.so.1snappy: true /usr/local/lib/libsnappy.so.1lz4: true revision:99bzip2: true /lib64/libbz2.so.1openssl: true /usr/lib64/libcrypto.so[root@sht-sgmhadoopnn-01 app]# file /root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0/root/learnproject/app/hadoop/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped[root@sht-sgmhadoopnn-01 app]#[参考]http://happyshome.cn/blog/deploy/centos/hadoop2.7.2.htmlhttp://blog.csdn.net/haohaixingyun/article/details/52800048]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02生产预警平台项目之Flume-1.7.0源码编译导入eclipse]]></title>
    <url>%2F2018%2F08%2F28%2F02%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8BFlume-1.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AF%BC%E5%85%A5eclipse%2F</url>
    <content type="text"><![CDATA[【前提】:1.windows 7 安装maven-3.3.9其中在conf/setting.xml文件添加D:\software\apache-maven-3.3.9\repositoryhttp://blog.csdn.net/defonds/article/details/419572872.windows 7 安装eclipse 64位(百度下载，解压即可)3.eclipse安装maven插件，选择第二种方式linkhttp://blog.csdn.net/lfsfxy9/article/details/9397937其中 eclipse-maven3-plugin.7z 这个包可以加群258669058找我，分享给你【flume-ng 1.7.0源码的编译导入eclipse】:1.下载官网的源码(不要下载GitHub上源码，因为这时pom文件中版本为1.8.0，编译会有问题)http://archive.apache.org/dist/flume/1.7.0/a.下载apache-flume-1.7.0-src.tar.gzb.解压重命名为flume-1.7.02.修改pom.xml (大概在621行，将自带的repository注释掉，添加以下的)1234&lt;repository&gt; &lt;id&gt;maven.tempo-db.com&lt;/id&gt; &lt;url&gt;http://maven.oschina.net/service/local/repositories/sonatype-public-grid/content/&lt;/url&gt; &lt;/repository&gt;3.打开cmd,编译cd /d D:[WORK]\Training\05Hadoop\Compile\flume-1.7.0mvn compile4.打开eclipse,单击Window–&gt;Perferences–&gt;左侧的Maven–&gt;User Settings然后设置自己的mvn的setting.xml路径和Local Repository(最好使用Maven3.3.x版本以上，我是3.3.9)5.关闭eclipse的 Project–&gt;Buid Automatically6.关闭eclipse的Download repository index updates on startup7.导入flume1.7.0源码a.File–&gt;Import–&gt;Maven–&gt;Existing Maven Projects–&gt;Nextb.选择目录–&gt; Finish8.检查源码，没有抛任何错误]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这是一篇热腾腾的面经]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[伟梦：1.主要还是项目？基本上没问什么技术，我就说了一遍项目流程，然后说几个优化点，比如上次讲的血案，我也顺带提了一下。2.在大数据中，有没有什么是不足的，遇到过什么问题？微盟：1.SparkStreaming处理完一批次的数据，写偏移量之前挂了，数据怎么保证不重？2.Maxwell的底层原理？3.手写Spring？4.遍历二叉树？5.用过什么算法？6.多线程方面，怎么实现一个主线程，等待其他子线程完成后再运行？7.Maxwell和Cannal的比较？8.direct比较receiver的优势？9.原来是把数据传入到Hive，之后改了架构，怎么把Hive的数据导入到Hbase？10.为什么用Kafka自己存储offset来替代checkpoint，怎么防止了数据双份落地，数据双份是指什么？11.单例用过吗？平安：1.问项目，流程，业务？2.数据量，增量？3.几个人开发的，代码量多少？4.你主要做什么的？5.什么场景，用SparkSql分析什么东西？总结：基本上都是围绕项目来面，第一家问的比较少，而且都是关于项目；微盟的面试官做的项目，跟简历上的项目，架构上基本一样，所以问的比较深，问我Maxwell的底层原理，对比Cannal有什么优势，为什么选择它，这个我没回答上来，后来让手写Spring，算法，后来就让我走了；平安也是基本围绕项目，业务，数据量，没问什么技术，而且我说了关于优化的点(面试官说不要说网上都有的东西)。]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01生产预警平台项目之项目概述]]></title>
    <url>%2F2018%2F08%2F27%2F01%E7%94%9F%E4%BA%A7%E9%A2%84%E8%AD%A6%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1.前期基本架构图2.最终基本架构图3.版本组件版本Flume:1.7Hadoop:2.7.3Scala:2.11Kafka:0.10.1.0Spark:2.0.2InfluxDB:1.2.0Grafana:4.1.1maven:3.3.94.主要目的主要是想基于Exec Source开发自定义插件AdvancedExecSource，将机器名称 和 服务名称 添加到cdh 服务的角色log数据的每一行前面，则格式为：机器名称 服务名称 年月日 时分秒.毫秒 日志级别 日志信息 ；然后在后面的spark streaming 实时计算我们所需求：比如统计每台机器的服务的每秒出现的error次数 、统计每5秒的warn，error次数等等；来实时可视化展示和邮件短信、微信企业号通知。其实主要我们现在的很多监控服务基本达不到秒级的通知，都为5分钟等等，为了方便我们自己的维护；其实对一些即将出现的问题可以提前预知；其实最主要可以有效扩展到实时计算数据库级别日志，比如MySQL慢查询日志，nginx，tomcat，linux的系统级别日志等等。5.大概流程1.搭建hadoop cluster2.eclipse 导入flume源代码（window7 安装maven，eclipse，eclipse与maven集成）3.开发flume-ng 自定义插件4.flume 收集，汇聚到hdfs(主要测试是否汇聚成功，后期也可以做离线处理)5.flume 收集，汇聚到kafka6.搭建kafka monitor7.搭建 spark client8.window7装ieda开发工具9.idea开发 spark streaming 的wc10.读取kafka日志，开发spark streaming的这块日志分析11.写入influxdb12.grafana可视化展示13.集成邮件###说明：针对自身情况，自行选择，步骤如上，但不是固定的，有些顺序是可以打乱的，例如开发工具的安装，可以一起操作的，再如这几个组件的下载编译，如果不想编译可以直接下tar包的，自行选择就好，但是建议还是自己编译，遇到坑才能更好的记住这个东西，本身这个项目就是学习提升的过程，要是什么都是现成的，那就没什么意义了]]></content>
      <categories>
        <category>生产预警平台项目</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>生产预警平台项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark中配置启用LZO压缩]]></title>
    <url>%2F2018%2F08%2F20%2Fspark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Spark中配置启用LZO压缩，步骤如下：一、spark-env.sh配置123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*二、spark-defaults.conf配置12spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar注：指向编译生成lzo的jar包三、测试1、读取Lzo文件123spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)2、写出lzo文件1234spark-shell --master local[2]scala&gt; import com.hadoop.compression.lzo.LzopCodecscala&gt; val lzoTest = sc.parallelize(1 to 10)scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])结果：12345[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzoFound 3 items-rw-r--r-- 1 hadoop supergroup 0 2018-03-16 23:24 /input/test_lzo/_SUCCESS-rw-r--r-- 1 hadoop supergroup 60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo-rw-r--r-- 1 hadoop supergroup 61 2018-03-16 23:24 /input/test_lzo/part-00001.lzo至此配置与测试完成。四、配置与测试中存问题1、引用native，缺少LD_LIBRARY_PATH1.1、错误提示：1234567891011121314151617181920Caused by: java.lang.RuntimeException: native-lzo library not available at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120) at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178) at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246) at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容：123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/nativeexport SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*2、无法找到LzopCodec类2.1、错误提示：1234567Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found. at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135) at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175) at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980) at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容：123spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jarspark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS之垃圾回收箱配置及使用]]></title>
    <url>%2F2018%2F07%2F18%2FHDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HDFS为每个用户创建一个回收站:目录:/user/用户/.Trash/Current, 系统回收站都有一个周期,周期过后hdfs会彻底删除清空,周期内可以恢复。一、HDFS删除文件,无法恢复12[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.logDeleted /123.log二、 启用回收站功能12345678910111213[hadoop@hadoop001 hadoop]$ vim core-site.xml&lt;property&gt;&lt;!--多长时间创建CheckPoint NameNode节点上运行的CheckPointer 从Current文件夹创建CheckPoint; 默认: 0 由fs.trash.interval项指定 --&gt;&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认:不启用 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; -- 清除周期分钟(24小时)&lt;/property&gt;1、重启hdfs服务12[hadoop@hadoop001 sbin]$ ./stop-dfs.sh[hadoop@hadoop001 sbin]$ ./start-dfs.sh2、测试回收站功能123[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /[hadoop@hadoop001 opt]$ hdfs dfs -ls /-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /123.log文件删除成功存放回收站路径下12345[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log[hadoop@hadoop001 opt]$ hdfs dfs -ls /Found 1 itemsdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /user恢复文件12345[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log[hadoop@hadoop001 ~]$ hdfs dfs -ls /Found 2 items-rw-r--r-- 1 hadoop supergroup 162 2018-05-23 11:30 /456.logdrwx------ - hadoop supergroup 0 2018-05-23 11:32 /user删除文件跳过回收站123[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.logDeleted /456.log源码参考：https://blog.csdn.net/tracymkgld/article/details/17557655]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark序列化，你了解吗]]></title>
    <url>%2F2018%2F07%2F16%2FSpark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。Java serialization在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。测试代码：12345678910111213141516171819202122232425262728package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;测试结果：Kryo serializationSpark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。测试代码：12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125; &#125;测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。修改代码后在测试一次12345678910111213141516171819202122232425262728293031package com.hihi.learn.sparkCoreimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ArrayBuffercase class Student(id: String, name: String, age: Int, gender: String)object SerializationDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;SerializationDemo&quot;) .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo val sc = new SparkContext(conf) val stduentArr = new ArrayBuffer[Student]() for (i &lt;- 1 to 1000000) &#123; stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;)) &#125; val JavaSerialization = sc.parallelize(stduentArr) JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count() while(true) &#123; Thread.sleep(10000) &#125; sc.stop() &#125;测试结果：总结：Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。12345678910111213141516171819202122232425262728 /** * Component which configures serialization, compression and encryption for various Spark * components, including automatic selection of which [[Serializer]] to use for shuffles. */private[spark] class SerializerManager( defaultSerializer: Serializer, conf: SparkConf, encryptionKey: Option[Array[Byte]]) &#123; def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None) private[this] val kryoSerializer = new KryoSerializer(conf) private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]] private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123; val primitiveClassTags = Set[ClassTag[_]]( ClassTag.Boolean, ClassTag.Byte, ClassTag.Char, ClassTag.Double, ClassTag.Float, ClassTag.Int, ClassTag.Long, ClassTag.Null, ClassTag.Short ) val arrayClassTags = primitiveClassTags.map(_.wrap) primitiveClassTags ++ arrayClassTags]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 状态管理函数，你了解吗]]></title>
    <url>%2F2018%2F06%2F25%2FSpark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[Spark Streaming 状态管理函数包括updateStateByKey和mapWithState一、updateStateByKey官网原话：In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.统计全局的key的状态，但是就算没有数据输入，他也会在每一个批次的时候返回之前的key的状态。这样的缺点：如果数据量太大的话，我们需要checkpoint数据会占用较大的存储。而且效率也不高123456789101112131415161718192021222324252627282930//[root@bda3 ~]# nc -lk 9999 object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //注意：updateStateByKey必须设置checkpoint目录 ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() // 一定要写 ssc.awaitTermination() &#125; /*状态更新函数 * @param currentValues key相同value形成的列表 * @param preValues key对应的value，前一状态 * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seq列表中所有value求和 val pre = preValues.getOrElse(0) //获取上一状态值 Some(curr + pre) &#125; &#125;二、mapWithState (效率更高，生产中建议使用)mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储。官方代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every * second starting with initial value of word count. * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive * data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example * org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999` */ object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;) // Create the context with a 1 second batch size val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) // Initial state RDD for mapWithState operation val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1))) // Create a ReceiverInputDStream on target ip:port and count the // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) // Update the cumulative count using mapWithState // This will give a DStream made of state (which is the cumulative count of the words) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark和DL/AI结合，谁与争锋? 期待Spark3.0的到来！]]></title>
    <url>%2F2018%2F06%2F22%2FAI%E7%BB%93%E5%90%88%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20%2F</url>
    <content type="text"><![CDATA[不知各位，是否关注社区的发展？关注Spark呢？官网的Spark图标和解释语已经发生变化了。然而在6-18号，社区提出Spark and DL/AI相结合，这无比再一次说明，Spark在大数据的地位是无法撼动的！期待Spark3.0的到来！接下来对SPARK-24579的翻译:在大数据和人工智能的十字路口，我们看到了Apache Spark作为一个统一的分析引擎以及AI框架如TensorFlow和Apache MXNet (正在孵化中)的兴起及这两大块的巨大成功 。大数据和人工智能都是推动企业创新的不可或缺的组成部分， 两个社区的多次尝试，使他们结合在一起。我们看到AI社区的努力，为AI框架实现数据解决方案，如TF.DATA和TF.Tror。然而，50+个数据源和内置SQL、数据流和流特征，Spark仍然是对于大数据社区选择。这就是为什么我们看到许多努力,将DL/AI框架与Spark结合起来，以利用它的力量，例如，Spark数据源TFRecords、TensorFlowOnSpark, TensorFrames等。作为项目Hydrogen的一部分，这个SPIP将Spark+AI从不同的角度统一起来。没有在Spark和外部DL/AI框架之间交换数据，这些集成都是不可能的,也有性能问题。然而，目前还没有一种标准的方式来交换数据，因此实现和性能优化就陷入了困境。例如，在Python中，TensorFlowOnSpark使用Hadoop InputFormat/OutputFormat作为TensorFlow的TFRecords，来加载和保存数据，并将RDD数据传递给TensorFlow。TensorFrames使用TensorFlow的Java API，转换为 Spark DataFrames Rows to/from TensorFlow Tensors 。我们怎样才能降低复杂性呢?这里的建议是标准化Spark和DL/AI框架之间的数据交换接口(或格式)，并优化从/到这个接口的数据转换。因此，DL/AI框架可以利用Spark从任何地方加载数据，而无需花费额外的精力构建复杂的数据解决方案，比如从生产数据仓库读取特性或流模型推断。Spark用户可以使用DL/AI框架，而无需学习那里实现的特定数据api。而且双方的开发人员都可以独立地进行性能优化，因为接口本身不会带来很大的开销。ISSUE: https://issues.apache.org/jira/browse/SPARK-24579若泽数据，星星本人水平有限，翻译多多包涵。对了忘记说了，本ISSUE有个PDF文档，赶快去下载吧。https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产开发必用-Spark RDD转DataFrame的两种方法]]></title>
    <url>%2F2018%2F06%2F14%2F%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:https://blog.csdn.net/lemonzhaotao/article/details/80211231在本篇文章中，将介绍RDD转换为DataFrame的2种方式官网之RDD转DF:http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rddsDataFrame 与 RDD 的交互Spark SQL它支持两种不同的方式转换已经存在的RDD到DataFrame方法一第一种方式是使用反射的方式，用反射去推倒出来RDD里面的schema。这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常非常多的，是不可能只有20来个字段的。示例：12345678910111213141516171819202122/** * convert rdd to dataframe 1 * @param spark */private def runInferSchemaExample(spark:SparkSession): Unit =&#123; import spark.implicits._ val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;) val df = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; People(x(0), x(1).trim.toInt)) //将rdd的每一行都转换成了一个people .toDF //必须先导入import spark.implicits._ 不然这个方法会报错 df.show() df.createOrReplaceTempView(&quot;people&quot;) // 这个DF包含了两个字段name和age val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;) // teenager(0)代表第一个字段 // 取值的第一种方式：index from zero teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show() // 取值的第二种方式：byName teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()&#125;// 注意：case class必须定义在main方法之外；否则会报错case class People(name:String, age:Int)方法二创建一个DataFrame，使用编程的方式 这个方式用的非常多。通过编程方式指定schema ，对于第一种方式的schema其实定义在了case class里面了。官网解读：当我们的case class不能提前定义(因为业务处理的过程当中，你的字段可能是在变化的),因此使用case class很难去提前定义。使用该方式创建DF的三大步骤：Create an RDD of Rows from the original RDD;Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.示例：1234567891011121314151617181920212223242526/** * convert rdd to dataframe 2 * @param spark */private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123; // 1.转成RDD val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;) // 2.定义schema，带有StructType的 // 定义schema信息 val schemaString = &quot;name age&quot; // 对schema信息按空格进行分割 // 最终fileds里包含了2个StructField val fields = schemaString.split(&quot; &quot;) // 字段类型，字段名称判断是不是为空 .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // 3.把我们的schema信息作用到RDD上 // 这个RDD里面包含了一些行 // 形成Row类型的RDD val rowRDD = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; Row(x(0), x(1).trim)) // 通过SparkSession创建一个DataFrame // 传进来一个rowRDD和schema，将schema作用到rowRDD上 val peopleDF = spark.createDataFrame(rowRDD, schema) peopleDF.show()&#125;[扩展]生产上创建DataFrame的代码举例在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，这里将展示部分代码：Schema的定义1234567891011121314151617181920212223242526272829303132333435363738394041object AccessConvertUtil &#123; val struct = StructType( Array( StructField(&quot;url&quot;,StringType), StructField(&quot;cmsType&quot;,StringType), StructField(&quot;cmsId&quot;,LongType), StructField(&quot;traffic&quot;,LongType), StructField(&quot;ip&quot;,StringType), StructField(&quot;city&quot;,StringType), StructField(&quot;time&quot;,StringType), StructField(&quot;day&quot;,StringType) ) ) /** * 根据输入的每一行信息转换成输出的样式 */ def parseLog(log:String) = &#123; try &#123; val splits = log.split(&quot;\t&quot;) val url = splits(1) val traffic = splits(2).toLong val ip = splits(3) val domain = &quot;http://www.imooc.com/&quot; val cms = url.substring(url.indexOf(domain) + domain.length) val cmsTypeId = cms.split(&quot;/&quot;) var cmsType = &quot;&quot; var cmsId = 0l if (cmsTypeId.length &gt; 1) &#123; cmsType = cmsTypeId(0) cmsId = cmsTypeId(1).toLong &#125; val city = IpUtils.getCity(ip) val time = splits(0) val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;) //这个Row里面的字段要和struct中的字段对应上 Row(url, cmsType, cmsId, traffic, ip, city, time, day) &#125; catch &#123; case e: Exception =&gt; Row(0) &#125; &#125;&#125;创建DataFrame1234567891011121314object SparkStatCleanJob &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;) .master(&quot;local[2]&quot;).getOrCreate() val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;) //accessRDD.take(10).foreach(println) //RDD ==&gt; DF，创建生成DataFrame val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct) accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite) .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;) spark.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最前沿！带你读Structured Streaming重量级论文！]]></title>
    <url>%2F2018%2F06%2F14%2F%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81%2F</url>
    <content type="text"><![CDATA[1.论文下载地址https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf2.前言建议首先阅读Structured Streaming官网：http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html以及这两篇Databricks在2016年关于Structured Streaming的文章：https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.htmlhttps://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html言归正传该论文收录自2018年ACM SIGMOD会议，是由美国计算机协会（ACM）发起的、在数据库领域具有最高学术地位的国际性学术会议。论文的作者为Databricks的工程师及Spark的开发者，其权威性、重要程度不言而喻。文章开头为该论文的下载地址，供读者阅读交流。本文对该论文进行简要的总结，希望大家能够下载原文细细品读，了解最前沿的大数据技术。3.论文简要总结题目：Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark3.1 摘要摘要是一篇论文的精髓，这里给出摘要完整的翻译。随着实时数据的普遍存在，我们需要可扩展的、易用的、易于集成的流式处理系统。结构化流是基于我们对Spark Streaming的经验开发出来的高级别的Spark流式API。结构化流与其他现有的流式API，如谷歌的Dataflow，主要有两点不同。第一，它是一个基于自动增量化的关系型查询API，无需用户自己构建DAG；第二，结构化流旨在于支持端到端的实时应用并整合流与批处理的交互分析。在实践中，我们发现这种整合是一个关键的挑战。结构化流通过Spark SQL的代码生成引擎实现了很高的性能，是Apache Flink的两倍以及Apache Kafka的90倍。它还提供了丰富的运行特性，如回滚、代码更新以及流/批混合执行。最后我们描述了系统的设计以及部署在Databricks几百个生产节点的一个用例。3.2 流式处理面临的挑战(1) 复杂、低级别的API(2) 端到端应用的集成(3) 运行时挑战：容灾，代码更新，监控等(4) 成本和性能挑战3.3 结构化流基本概念图1 结构化流的组成部分(1) Input and OutputInput sources 必须是 replayable 的，支持节点宕机后从当前输入继续读取。例如：Apache Kinesis和Apache Kafka。Output sinks 必须支持 idempotent （幂等），确保在节点宕机时可靠的恢复。(2) APIs编写结构化流程序时，可以使用Spark SQL的APIs：DataFrame和SQL来查询streams和tables，该查询定义了一个output table（输出表），用来接收来自steam的数据。engine决定如何计算并将输出表 incrementally（增量地）写入sink。不同的sinks支持不同的output modes（输出模式，后面会提到）。为了处理流式数据，结构化流还增加了一些APIs与已有的Spark SQL API相配合：a. Triggers 控制engine多久执行一次计算b. event time 是数据源的时间戳；watermark 策略，与event time 相差一段时间后不再接收数据。c.Stateful operator（状态算子），类似于Spark Streaming 的updateStateByKey。(3) 执行一旦接收到了查询，结构化流就会进行优化递增，并开始执行。结构化流使用两种持久化存储的方式实现容错：a.write-ahead log （WAL：预写日志）持续追踪哪些数据已被执行，确保数据的可靠写入。b.系统采用大规模的 state store（状态存储）来保存长时间运行的聚合算子的算子状态快照。3.4 编程模型结构化流将谷歌的Dataflow、增量查询和Spark Streaming 结合起来，以便在Spark SQL下实现流式处理。a. A Short Example首先从一个批处理作业开始，统计一个web应用在不同国家的点击数。假设输入数据是一个JSON文件，输出一个Parquet文件，该作业可以通过DataFrame来完成： 1234561// Define a DataFrame to read from static data2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a static data sink6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)把该作业变成使用结构化流仅仅需要改变输入和输出源，例如，如果新的JSON文件continually（持续地）上传，我们只需要改变第一行和最后一行。12345671// Define a DataFrame to read streaming data2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)3// Transform it to compute a result4counts = data . groupBy ($&quot; country &quot;). count ()5// Write to a streaming data sink6counts . writeStream . format (&quot; parquet &quot;)7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)结构化流也支持 windowing（窗口）和通过Spark SQL已存在的聚合算子处理event time。例如：我们可以通过修改中间的代码，计算1小时的滑动窗口，每五分钟前进一次：121// Count events by windows on the &quot; time &quot; field2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()b. 编程模型语义图 2 两种输出模式i. 每一个输入源提供了一个基于时间的部分有序的记录集（set of records），例如，Kafka将流式数据分为各自有序的partitions。ii. 用户提供跨输入数据执行的查询，该输入数据可以在任意给定的处理时间点输出一个 result table（结果表）。结构化流总会产生与所有输入源的数据的前缀上（prefix of the data in all input sources）查询相一致的结果。iii. Triggers 告诉系统何时去运行一个新的增量计算，何时更新result table。例如，在微批处理模式，用户希望会每分钟触发一次增量计算。iiii. engine支持三种output mode： Complete：engine一次写所有result table。 Append：engine仅仅向sink增加记录。 Update：engine基于key更新每一个record，更新值改变的keys。 该模型有两个特性：第一，结果表的内容独立于输出模式。第二，该模型具有很强的语义一致性，被称为prefix consistency。 c.流式算子加入了两种类型的算子：watermarking算子告诉系统何时关闭event time window和输出结果；结构化流允许用户通过withWatermark算子来设置一个watermark，该算子给系统设置一个给定时间戳C的延迟阈值tC，在任意时间点，C的watermark是max（C）-tC。 stateful operators允许用户编写自定义逻辑来实现复杂的功能。 1234567891011121314 1// Define an update function that simply tracks the 2// number of events for each key as its state , returns 3// that as its result , and times out keys after 30 min. 4def updateFunc (key: UserId , newValues : Iterator [ Event ], 5state : GroupState [Int ]): Int = &#123; 6val totalEvents = state .get () + newValues . size () 7state . update ( totalEvents ) 8state . setTimeoutDuration (&quot;30 min&quot;) 9return totalEvents10&#125;11// Use this update function on a stream , returning a12// new table lens that contains the session lengths .13lens = events . groupByKey ( event =&gt; event . userId )14. mapGroupsWithState ( updateFunc )用mapGroupWithState算子来追踪每个会话的事件数量，30分钟后关闭会话。3.5 运行特性(1) 代码更新（code update）开发者能够在编程过程中更新UDF，并且可以简单的重启以使用新版本的代码。(2) 手动回滚（manual rollback）有时在用户发现之前，程序会输出错误的结果，因此回滚至关重要。结构化流很容易定位问题所在。同时手动回滚与前面提到的prefix consistency有很好的交互。(3) 流式和批次混合处理这是结构化流最显而易见的好处，用户能够共用流式处理和批处理作业的代码。(4) 监控结构化流使用Spark已有的API和结构化日志来报告信息，例如处理过的记录数量，跨网络的字节数等。这些接口被Spark开发者所熟知，并易于连接到不同的UI工具。3.6 生产用例与总结给出简要架构图，篇幅原因不再赘述，希望详细了解的下载论文自行阅读。本文只挑选了部分关键点进行了浅层次的叙述，希望读者能够将论文下载下来认真品读，搞懂开发者的开发思路，跟上大数据的前沿步伐。]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java可扩展线程池之ThreadPoolExecutor]]></title>
    <url>%2F2018%2F06%2F13%2FJava%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[1、ThreadPoolExecutor我们知道ThreadPoolExecutor是可扩展的,它提供了几个可以在子类中改写的空方法如下：123protected void beforeExecute(Thread t, Runnable r) &#123; &#125;protected void beforeExecute(Thread t, Runnable r) &#123; &#125; protected void terminated() &#123; &#125;2、为什么要进行扩展？因为在实际应用中，可以对线程池运行状态进行跟踪，输出一些有用的调试信息，以帮助故障诊断。3、ThreadPoolExecutor.Worker的run方法实现通过看源码我们发现 ThreadPoolExecutor的工作线程其实就是Worker实例，Worker.runTask()会被线程池以多线程模式异步调用，则以上三个方法也将被多线程同时访问。1234567891011121314151617181920212223242526272829303132333435363738391// 基于jdk1.8.0_161final void runWorker(Worker w) &#123; 2 Thread wt = Thread.currentThread(); 3 Runnable task = w.firstTask; 4 w.firstTask = null; 5 w.unlock(); // allow interrupts 6 boolean completedAbruptly = true; 7 try &#123; 8 while (task != null || (task = getTask()) != null) &#123; 9 w.lock(); 10 if ((runStateAtLeast(ctl.get(), STOP) ||11 (Thread.interrupted() &amp;&amp;12 runStateAtLeast(ctl.get(), STOP))) &amp;&amp;13 !wt.isInterrupted())14 wt.interrupt(); 15 try &#123;16 beforeExecute(wt, task);17 Throwable thrown = null; 18 try &#123;19 task.run();20 &#125; catch (RuntimeException x) &#123;21 thrown = x; throw x;22 &#125; catch (Error x) &#123;23 thrown = x; throw x;24 &#125; catch (Throwable x) &#123;25 thrown = x; throw new Error(x);26 &#125; finally &#123;27 afterExecute(task, thrown);28 &#125;29 &#125; finally &#123;30 task = null;31 w.completedTasks++;32 w.unlock();33 &#125;34 &#125;35 completedAbruptly = false;36 &#125; finally &#123;37 processWorkerExit(w, completedAbruptly);38 &#125;39 &#125;4、扩展线程池实现1234567891011121314151617181920212223242526272829303132333435 1public class ExtThreadPool &#123; 2 public static class MyTask implements Runnable &#123; 3 public String name; 4 public MyTask(String name) &#123; 5 this.name = name; 6 &#125; 7 public void run() &#123; 8 System.out.println(&quot;正在执行:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name); try &#123; 9 Thread.sleep(100);10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;14 &#125; 15public static void main(String args[]) throws InterruptedException &#123;16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123; 18protected void beforeExecute(Thread t, Runnable r) &#123;19 System.out.println(&quot;准备执行：&quot; + ((MyTask) r).name);20&#125; 21protected void afterExecute(Thread t, Runnable r) &#123;22 System.out.println(&quot;执行完成&quot; + ((MyTask) r).name);23&#125; 24protected void terminated() &#123;25 System.out.println(&quot;线程池退出！&quot;);26&#125;27&#125;; 28for (int i = 0; i &lt; 5; i++) &#123;29 MyTask task = new MyTask(&quot;TASK--&quot; + i);30 executorService.execute(task);31 Thread.sleep(10);32 &#125;33 executorService.shutdown();34 &#125;35&#125;输出结果如下：1234567891011准备执行：TASK–0 正在执行:Thread ID:10,Task Name:TASK–0 准备执行：TASK–1 正在执行:Thread ID:11,Task Name:TASK–1 准备执行：TASK–2 正在执行:Thread ID:12,Task Name:TASK–2 准备执行：TASK–3 正在执行:Thread ID:13,Task Name:TASK–3 准备执行：TASK–4 正在执行:Thread ID:14,Task Name:TASK–4 线程池退出！这样就实现了在执行前后进行的一些控制，除此之外我们还可以输出每个线程的执行时间，或者一些其他增强操作。5、思考？请读者思考shutdownNow和shutdown方法的区别？如何优雅的关闭线程池？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你大爷永远是你大爷，RDD血缘关系源码详解！]]></title>
    <url>%2F2018%2F06%2F13%2F%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81%2F</url>
    <content type="text"><![CDATA[一、RDD的依赖关系RDD的依赖关系分为两类：宽依赖和窄依赖。我们可以这样认为：（1）窄依赖：每个parent RDD 的 partition 最多被 child RDD 的一个partition 使用。（2）宽依赖：每个parent RDD partition 被多个 child RDD 的partition 使用。窄依赖每个 child RDD 的 partition 的生成操作都是可以并行的，而宽依赖则需要所有的 parent RDD partition shuffle 结果得到后再进行。二、org.apache.spark.Dependency.scala 源码解析Dependency是一个抽象类：1234// Denpendency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125;它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。（1）NarrowDependency也是一个抽象类定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。1234567891011// Denpendency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125;窄依赖又有两个具体的实现：OneToOneDependency和RangeDependency。（a）OneToOneDependency指child RDD的partition只依赖于parent RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。1234567891011121314151617// Denpendency.scalaclass OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; （b）RangeDependency指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。// Denpendency.scalaclass RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123;//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引 override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart)//表示于当前索引的相对位置 &#125; else &#123; Nil &#125; &#125;&#125;（2）ShuffleDependency指宽依赖表示一个parent RDD的partition会被child RDD的partition使用多次。需要经过shuffle才能形成。123456789101112131415161718192021// Denpendency.scalaclass ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123; //shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的 override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) //获取shuffleId val shuffleId: Int = _rdd.context.newShuffleId() //向shuffleManager注册shuffle信息 val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。三、两种依赖的区分首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark 技术团队开源机器学习平台 MLflow]]></title>
    <url>%2F2018%2F06%2F12%2FApache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow%2F</url>
    <content type="text"><![CDATA[近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。MLflow 正是受现有的 ML 平台启发，主打开放性：开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。MLflow 目前的 alpha 版本包含三个组件：其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。1234567891011121314import mlflow# Log parameters (key-value pairs)mlflow.log_param(&quot;num_dimensions&quot;, 8)mlflow.log_param(&quot;regularization&quot;, 0.1)# Log a metric; metrics can be updated throughout the runmlflow.log_metric(&quot;accuracy&quot;, 0.1)...mlflow.log_metric(&quot;accuracy&quot;, 0.45)# Log artifacts (output files)mlflow.log_artifact(&quot;roc.png&quot;)mlflow.log_artifact(&quot;model.pkl&quot;)MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。123456789101112name: My Projectconda_env: conda.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot; validate: parameters: data_file: path command: &quot;python validate.py &#123;data_file&#125;&quot;MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。12345678time_created: 2018-02-21T13:21:34.12flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn pickled_model: model.pkl]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming 状态管理函数的选择比较]]></title>
    <url>%2F2018%2F06%2F06%2FSparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[一、updateStateByKey官网原话：1In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.也即是说它会统计全局的key的状态，就算没有数据输入，它也会在每一个批次的时候返回之前的key的状态。缺点：若数据量太大的话，需要checkpoint的数据会占用较大的存储，效率低下。程序示例如下：12345678910111213141516171819202122232425262728object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //注意：要使用updateStateByKey必须设置checkpoint目录 ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; /*状态更新函数 * @param currentValues key相同value形成的列表 * @param preValues key对应的value，前一状态 * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seq列表中所有value求和 val pre = preValues.getOrElse(0) //获取上一状态值 Some(curr + pre) &#125; &#125;二、mapWithStatemapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。效率更高，生产中建议使用官方代码如下：1234567891011121314151617181920212223242526272829303132object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulNetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) val initialRDD = ssc.sparkContext .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1))) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;三、源码分析upateStateByKey：map返回的是MappedDStream，而MappedDStream并没有updateStateByKey方法，并且它的父类DStream中也没有该方法。但是DStream的伴生对象中有一个隐式转换函数：123456object DStream &#123; implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairDStreamFunctions[K, V] = &#123; new PairDStreamFunctions[K, V](stream) &#125;跟进去 PairDStreamFunctions ，发现最终调用的是自己的updateStateByKey。其中updateFunc就要传入的参数，他是一个函数，Seq[V]表示当前key对应的所有值，123456Option[S] 是当前key的历史状态，返回的是新的状态。def updateStateByKey[S: ClassTag]( updateFunc: (Seq[V], Option[S]) =&gt; Option[S] ): DStream[(K, S)] = ssc.withScope &#123; updateStateByKey(updateFunc, defaultPartitioner())&#125;最终调用：12345678910def updateStateByKey[S: ClassTag]( updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)], partitioner: Partitioner, rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123; val cleanedFunc = ssc.sc.clean(updateFunc) val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123; cleanedFunc(it) &#125; new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)&#125;再跟进去 new StateDStream:在这里面new出了一个StateDStream对象。在其compute方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中StateDStream的父类计算出的RDD（本次batch的单词计数）分别是prevStateRDD和parentRDD，然后在调用 computeUsingPreviousRDD 方法：1234567891011121314151617181920private [this] def computeUsingPreviousRDD( batchTime: Time, parentRDD: RDD[(K, V)], prevStateRDD: RDD[(K, S)]) = &#123; // Define the function for the mapPartition operation on cogrouped RDD; // first map the cogrouped tuple to tuples of required type, // and then apply the update function val updateFuncLocal = updateFunc val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123; val i = iterator.map &#123; t =&gt; val itr = t._2._2.iterator val headOption = if (itr.hasNext) Some(itr.next()) else None (t._1, t._2._1.toSeq, headOption) &#125; updateFuncLocal(batchTime, i) &#125; val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner) val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning) Some(stateRDD)&#125;在这里两个RDD进行cogroup然后应用updateStateByKey传入的函数。我们知道cogroup的性能是比较低下，参考http://lxw1234.com/archives/2015/07/384.htm。mapWithState:123456789@Experimentaldef mapWithState[StateType: ClassTag, MappedType: ClassTag]( spec: StateSpec[K, V, StateType, MappedType] ): MapWithStateDStream[K, V, StateType, MappedType] = &#123; new MapWithStateDStreamImpl[K, V, StateType, MappedType]( self, spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]] )&#125;说明：StateSpec 封装了状态管理函数，并在该方法中创建了MapWithStateDStreamImpl对象。MapWithStateDStreamImpl 中创建了一个InternalMapWithStateDStream类型对象internalStream，在MapWithStateDStreamImpl的compute方法中调用了internalStream的getOrCompute方法。12345678910111213141516private[streaming] class MapWithStateDStreamImpl[ KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag]( dataStream: DStream[(KeyType, ValueType)], spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType]) extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123; private val internalStream = new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec) override def slideDuration: Duration = internalStream.slideDuration override def dependencies: List[DStream[_]] = List(internalStream) override def compute(validTime: Time): Option[RDD[MappedType]] = &#123; internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125; &#125;InternalMapWithStateDStream中没有getOrCompute方法，这里调用的是其父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream的Compute方法：12345678910111213141516171819202122232425262728293031323334/** Method that generates an RDD for the given time */override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123; // Get the previous state or create a new empty state RDD val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123; case Some(rdd) =&gt; if (rdd.partitioner != Some(partitioner)) &#123; // If the RDD is not partitioned the right way, let us repartition it using the // partition index as the key. This is to ensure that state RDD is always partitioned // before creating another state RDD using it MapWithStateRDD.createFromRDD[K, V, S, E]( rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime) &#125; else &#123; rdd &#125; case None =&gt; MapWithStateRDD.createFromPairRDD[K, V, S, E]( spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)), partitioner, validTime ) &#125; // Compute the new state RDD with previous state RDD and partitioned data RDD // Even if there is no data RDD, use an empty one to create a new state RDD val dataRDD = parent.getOrCompute(validTime).getOrElse &#123; context.sparkContext.emptyRDD[(K, V)] &#125; val partitionedDataRDD = dataRDD.partitionBy(partitioner) val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt; (validTime - interval).milliseconds &#125; Some(new MapWithStateRDD( prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))&#125;根据给定的时间生成一个MapWithStateRDD，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD:dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取partitionedDataRDD。最后将preStateRDD，partitionedDataRDD和用户定义的函数mappingFunction传给新生成的MapWithStateRDD对象返回。后续若有兴趣可以继续跟进MapWithStateRDD的compute方法，限于篇幅不再展示。]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL 之外部数据源如何成为在企业开发中的一把利器]]></title>
    <url>%2F2018%2F06%2F06%2FSpark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 概述1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。2.读取json文件启动shell进行测试1234567891011121314151617181920//标准写法val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)//另外一种写法spark.read.json(&quot;path&quot;)看看源码这两者之间到底有啥不同呢？/** * Loads a JSON file and returns the results as a `DataFrame`. * * See the documentation on the overloaded `json()` method with varargs for more details. * * @since 1.4.0 */ def json(path: String): DataFrame = &#123; // This method ensures that calls that explicit need single argument works, see SPARK-16009 json(Seq(path): _*) &#125;我们调用josn() 方法其实进行了 overloaded ，我们继续查看 def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*) 这句话是不是很熟悉，其实就是我们的标准写法1234567891011121314151617 scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+3 读取parquet数据12345678910val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+4 读取hive中的数据1234567891011121314151617181920212223242526272829303132spark.sql(&quot;show tables&quot;).show+--------+----------+-----------+|database| tableName|isTemporary|+--------+----------+-----------+| default|states_raw| false|| default|states_seq| false|| default| t1| false|+--------+----------+-----------+spark.table(&quot;states_raw&quot;).show+-----+------+| code| name|+-----+------+|hello| java||hello|hadoop||hello| hive||hello| sqoop||hello| hdfs||hello| spark|+-----+------+scala&gt; spark.sql(&quot;select name from states_raw &quot;).show+------+| name|+------+| java||hadoop|| hive|| sqoop|| hdfs|| spark|+------+5 保存数据注意：保存的文件夹不能存在，否则报错(默认情况下，可以选择不同的模式)：org.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;保存成文本格式，只能保存一列，否则报错：org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;123456789101112131415161718192021222324252627282930val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)//保存df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)结果：[hadoop@hadoop out]$ pwd/home/hadoop/data/out[hadoop@hadoop out]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:34 _SUCCESS[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt MichaelAndyJustin//保存为json格式df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)结果[hadoop@hadoop data]$ cd out1[hadoop@hadoop out1]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:35 _SUCCESS[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;上面说了在保存数据时如果目录已经存在，在默认模式下会报错，那我们下面讲解保存的几种模式：6 读取mysql中的数据1234567891011121314151617181920212223val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;).option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123456&quot;).load()scala&gt; jdbcDF.printSchemaroot |-- TBL_ID: long (nullable = false) |-- CREATE_TIME: integer (nullable = false) |-- DB_ID: long (nullable = true) |-- LAST_ACCESS_TIME: integer (nullable = false) |-- OWNER: string (nullable = true) |-- RETENTION: integer (nullable = false) |-- SD_ID: long (nullable = true) |-- TBL_NAME: string (nullable = true) |-- TBL_TYPE: string (nullable = true) |-- VIEW_EXPANDED_TEXT: string (nullable = true) |-- VIEW_ORIGINAL_TEXT: string (nullable = true)jdbcDF.show7 spark SQL操作mysql表数据123456789101112131415161718192021222324252627282930313233CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url &quot;jdbc:mysql://localhost:3306&quot;, dbtable &quot;basic01.tbls&quot;, user &apos;root&apos;, password &apos;123456&apos;, driver &quot;com.mysql.jdbc.Driver&quot;);查看：show tables;default states_raw falsedefault states_seq falsedefault t1 falsejdbctable trueselect * from jdbctable;1 1519944170 6 0 hadoop 0 1 page_views MANAGED_TABLE NULL NULL2 1519944313 6 0 hadoop 0 2 page_views_bzip2 MANAGED_TABLE NULL NULL3 1519944819 6 0 hadoop 0 3 page_views_snappy MANAGED_TABLE NULL NULL21 1520067771 6 0 hadoop 0 21 tt MANAGED_TABLE NULL NULL22 1520069148 6 0 hadoop 0 22 page_views_seq MANAGED_TABLE NULL NULL23 1520071381 6 0 hadoop 0 23 page_views_rcfile MANAGED_TABLE NULL NULL24 1520074675 6 0 hadoop 0 24 page_views_orc_zlib MANAGED_TABLE NULL NULL27 1520078184 6 0 hadoop 0 27 page_views_lzo_index MANAGED_TABLE NULL NULL30 1520083461 6 0 hadoop 0 30 page_views_lzo_index1 MANAGED_TABLE NULL NULL31 1524370014 1 0 hadoop 0 31 t1 EXTERNAL_TABLE NULL NULL37 1524468636 1 0 hadoop 0 37 states_raw MANAGED_TABLE NULL NULL38 1524468678 1 0 hadoop 0 38 states_seq MANAGED_TABLE NULL NULLmysql中的tbls的数据已经存在jdbctable表中了。jdbcDF.show8 分区推测（Partition Discovery）表分区是在像Hive这样的系统中使用的常见优化方法。 在分区表中，数据通常存储在不同的目录中，分区列值在每个分区目录的路径中编码。 所有内置的文件源（包括Text / CSV / JSON / ORC / Parquet）都能够自动发现和推断分区信息。 例如，我们创建如下的目录结构;123456789hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN添加json文件：people.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN我们使用spark sql读取外部数据源：1234567891011121314151617val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+我们改变读取的目录12345678910111213141516val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) |-- country: string (nullable = true)scala&gt; df.show+----+-------+-------+| age| name|country|+----+-------+-------+|null|Michael| CN|| 30| Andy| CN|| 19| Justin| CN|+----+-------+-------+大家有没有发现什么呢？Spark SQL将自动从路径中提取分区信息。注意，分区列的数据类型是自动推断的。目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以通过spark.sql.sources.partitionColumnTypeInference.enabled进行配置，默认为true。当禁用类型推断时，字符串类型将用于分区列。从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下找到分区。对于上面的示例，如果用户将路径/table/gender=male传递给SparkSession.read.parquet或SparkSession.read.load，则不会将性别视为分区列。如果用户需要指定启动分区发现的基本路径，则可以basePath在数据源选项中进行设置。例如，当path/to/table/gender=male是数据路径并且用户将basePath设置为path/to/table/时，性别将是分区列。]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统重要参数调优，你知道吗]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%2F</url>
    <content type="text"><![CDATA[当前会话生效ulimit -u -&gt; 查看当前最大进程数ulimit -n -&gt;查看当前最大文件数ulimit -u xxx -&gt; 修改当前最大进程数为xxxulimit -n xxx -&gt; 修改当前最大文件数为xxx永久生效1.vi /etc/security/limits.conf，添加如下的行soft noproc 11000hard noproc 11000soft nofile 4100hard nofile 4100 说明：代表针对所有用户noproc 是代表最大进程数nofile 是代表最大文件打开数2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制：1)、vi /etc/ssh/sshd_config把 UserLogin 的值改为 yes，并把 # 注释去掉2)、重启 sshd 服务/etc/init.d/sshd restart3)、修改所有 linux 用户的环境变量文件：vi /etc/profileulimit -u 10000ulimit -n 4096ulimit -d unlimitedulimit -m unlimitedulimit -s unlimitedulimit -t unlimitedulimit -v unlimited4)、生效source /etc/profile]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark动态内存管理源码解析！]]></title>
    <url>%2F2018%2F06%2F03%2FSpark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81%2F</url>
    <content type="text"><![CDATA[一、Spark内存管理模式Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。12345678// SparkEnv.scala val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false) val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) &#125;二、Spark动态内存管理空间分配相比于Static MemoryManager模式，Unified MemoryManager模型打破了存储内存和运行内存的界限，使每一个内存区能够动态伸缩，降低OOM的概率。由上图可知，executor JVM内存主要由以下几个区域组成：（1）Reserved Memory（预留内存）：这部分内存预留给系统使用，默认为300MB，可通过spark.testing.reservedMemory进行设置。12// UnifiedMemoryManager.scalaprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024另外，JVM内存的最小值也与reserved Memory有关，即minSystemMemory = reserved Memory1.5，即默认情况下JVM内存最小值为300MB1.5=450MB。12// UnifiedMemoryManager.scala val minSystemMemory = (reservedMemory * 1.5).ceil.toLong（2）Spark Memeoy:分为execution Memory和storage Memory。去除掉reserved Memory，剩下usableMemory的一部分用于execution和storage这两类堆内存，默认是0.6，可通过spark.memory.fraction进行设置。例如：JVM内存是1G，那么用于execution和storage的默认内存为（1024-300）*0.6=434MB。1234// UnifiedMemoryManager.scala val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6) (usableMemory * memoryFraction).toLong他们的边界由spark.memory.storageFraction设定，默认为0.5。即默认状态下storage Memory和execution Memory为1：1.1234// UnifiedMemoryManager.scala onHeapStorageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)（3）user Memory:剩余内存，用户根据需要使用，默认占usableMemory的（1-0.6）=0.4.三、内存控制详解首先我们先来了解一下Spark内存管理实现类之前的关系。1.MemoryManager主要功能是：（1）记录用了多少StorageMemory和ExecutionMemory；（2）申请Storage、Execution和Unroll Memory；（3）释放Stroage和Execution Memory。Execution内存用来执行shuffle、joins、sorts和aggegations操作，Storage内存用于缓存和广播数据，每一个JVM中都存在着一个MemoryManager。构造MemoryManager需要指定onHeapStorageMemory和onHeapExecutionMemory参数。123456 // MemoryManager.scalaprivate[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging &#123;创建StorageMemoryPool和ExecutionMemoryPool对象，用来创建堆内或堆外的Storage和Execution内存池，管理Storage和Execution的内存分配。123456789// MemoryManager.scala @GuardedBy(&quot;this&quot;) protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(&quot;this&quot;) protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)默认情况下，不使用堆外内存，可通过saprk.memory.offHeap.enabled设置，默认堆外内存为0，可使用spark.memory.offHeap.size参数设置。123456789101112// All the code you will ever need final val tungstenMemoryMode: MemoryMode = &#123; if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123; require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0, &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;) require(Platform.unaligned(), &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;) MemoryMode.OFF_HEAP &#125; else &#123; MemoryMode.ON_HEAP &#125; &#125;12// MemoryManager.scala protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)释放numBytes字节的Execution内存方法12345678910// MemoryManager.scaladef releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) &#125; &#125;释放指定task的所有Execution内存并将该task标记为inactive。12345// MemoryManager.scala private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123; onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) &#125;释放numBytes字节的Stoarge内存方法1234567// MemoryManager.scaladef releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes) &#125; &#125;释放所有Storage内存方法12345// MemoryManager.scalafinal def releaseAllStorageMemory(): Unit = synchronized &#123; onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() &#125;2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？UnifiedMemoryManage继承了MemoryManager1234567891011// UnifiedMemoryManage.scalaprivate[spark] class UnifiedMemoryManager private[memory] ( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) &#123;重写了maxOnHeapStorageMemory方法，最大Storage内存=最大内存-最大Execution内存。1234// UnifiedMemoryManage.scala override def maxOnHeapStorageMemory: Long = synchronized &#123; maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed &#125;核心方法acquireStorageMemory：申请Storage内存。12345678910111213141516171819202122232425262728293031// UnifiedMemoryManage.scalaoverride def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized &#123; assertInvariants() assert(numBytes &gt;= 0) val (executionPool, storagePool, maxMemory) = memoryMode match &#123; //根据不同的内存模式去创建StorageMemoryPool和ExecutionMemoryPool case MemoryMode.ON_HEAP =&gt; ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP =&gt; ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapMemory) &#125; if (numBytes &gt; maxMemory) &#123; // 若申请内存大于最大内存，则申请失败 logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; + s&quot;memory limit ($maxMemory bytes)&quot;) return false &#125; if (numBytes &gt; storagePool.memoryFree) &#123; // 如果Storage内存池没有足够的内存，则向Execution内存池借用 val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//当Execution内存有空闲时，Storage才能借到内存 executionPool.decrementPoolSize(memoryBorrowedFromExecution)//缩小Execution内存 storagePool.incrementPoolSize(memoryBorrowedFromExecution)//增加Storage内存 &#125; storagePool.acquireMemory(blockId, numBytes)核心方法acquireExecutionMemory：申请Execution内存。123456789// UnifiedMemoryManage.scalaoverride private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized &#123;//使用了synchronized关键字，调用acquireExecutionMemory方法可能会阻塞，直到Execution内存池有足够的内存。 ... executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize) &#125;方法最后调用了ExecutionMemoryPool的acquireMemory方法，该方法的参数需要两个函数：maybeGrowExecutionPool()和computeMaxExecutionPoolSize()。每个Task能够使用的内存被限制在pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks。其中maxPoolSize代表了execution pool的最大内存，poolSize表示当前这个pool的大小。1234// ExecutionMemoryPool.scala val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks)maybeGrowExecutionPool()方法实现了如何动态增加Execution内存区的大小。在每次申请execution内存的同时，execution内存池会进行多次尝试，每次尝试都可能会回收一些存储内存。123456789101112131415// UnifiedMemoryManage.scala def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123; if (extraMemoryNeeded &gt; 0) &#123;//如果申请的内存大于0 //计算execution可借到的storage内存，是storage剩余内存和可借出内存的最大值 val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage &gt; 0) &#123;//如果可以申请到内存 val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//实际需要的内存，取实际需要的内存和storage内存区域全部可用内存大小的最小值 storagePool.decrementPoolSize(spaceToReclaim)//storage内存区域减少 executionPool.incrementPoolSize(spaceToReclaim)//execution内存区域增加 &#125; &#125; &#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽大数据-零基础学员深圳某司高薪面试题]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[啥也不说！直接上题面试时间：20180531简单说下hdfs读文件和写文件的流程每天数据量有多大？生产集群规模有多大？说几个spark开发中遇到的问题，和解决的方案阐述一下最近开发的项目，以及担任的角色位置kafka有做过哪些调优我们项目中数据倾斜的场景和解决方案零基础➕四个月紧跟若泽大数据学习之后是这样]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从Hive中的stored as file_foramt看hive调优]]></title>
    <url>%2F2018%2F05%2F30%2F%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[一、行式数据库和列式数据库的对比1、存储比较行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。2、压缩比较对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。3、查询比较假设执行的查询操作是：select id,name from table_emp;对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。假设执行的查询操作是：select * from table_emp;对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。二、stored as file_format 详解1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。123456789101112131415161718192021CREATE TABLE tt (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;CREATE TABLE tt2 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;CREATE TABLE tt3 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;#以上三种方式存储的格式都是TEXTFILE。2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较12345678file_format: | SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameTEXTFILE: 只是hive中表数据默认的存储格式，它将所有类型的数据都存储为String类型，不便于数据的解析，但它却比较通用。不具备随机读写的能力。支持压缩。SEQUENCEFILE: 这种储存格式比TEXTFILE格式多了头部、标识、信息长度等信息，这些信息使得其具备随机读写的能力。支持压缩，但压缩的是value。（存储相同的数据，SEQUENCEFILE比TEXTFILE略大）RCFILE（Record Columnar File）: 现在水平上划分为很多个Row Group,每个Row Group默认大小4MB，Row Group内部再按列存储信息。由facebook开源，比标准行式存储节约10%的空间。ORC: 优化过后的RCFile,现在水平上划分为多个Stripes,再在Stripe中按列存储。每个Stripe由一个Index Data、一个Row Data、一个Stripe Footer组成。每个Stripes的大小为250MB，每个Index Data记录的是整型数据最大值最小值、字符串数据前后缀信息，每个列的位置等等诸如此类的信息。这就使得查询十分得高效，默认每一万行数据建立一个Index Data。ORC存储大小为TEXTFILE的40%左右，使用压缩则可以进一步将这个数字降到10%~20%。ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=OrcThe parameters are all placed in the TBLPROPERTIES (see Create Table). They are:Key|Default|Notes|-|-|-|orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)|orc.compress.size|262,144|number of bytes in each compression chunk|orc.stripe.size|67,108,864|number of bytes in each stripe|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)|orc.create.index|true|whether to create row indexes|orc.bloom.filter.columns |””| comma separated list of column names for which bloom filter should be created|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)示例：创建带压缩的ORC存储表1234567create table Addresses ( name string, street string, city string, state string, zip int) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);PARQUET: 存储大小为TEXTFILE的60%~70%，压缩后在20%~30%之间。注意：不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。如果要使用TEXTFILE作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“insert into table table_stored_file_ORC select from table_t0;”创建。或者使用”create table as select from table_t0;”创建。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark之序列化在生产中的应用]]></title>
    <url>%2F2018%2F05%2F29%2FSpark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。Java serialization在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。测试代码：测试结果：Kryo serializationSpark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。测试代码：测试结果：测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。修改代码后在测试一次。测试结果：总结：Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若泽数据带你随时了解业界面试题，随时跳高薪]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA%2F</url>
    <content type="text"><![CDATA[链家(一面，二面)0.自我介绍1.封装继承多态概念2.mvc设计思想3.线程池,看过源码吗4.ssh框架中分别对应mvc中那一层5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）6.spring ioc aop 原理7.单利模式8.SQL题，想不起来了。。9.jvm 运行时数据区域10.spring mvc知道吗。。11.工厂模式12.mr 计算流程13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费 查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）14.git的使用15.hadoop的理解16.hive内部表和外部表的区别17.hive存储格式和压缩格式18.对spark了解吗？ 当时高级班还没学。。19.hive于关系型数据库的区别20.各种排序 手写堆排序,说说原理21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。22中间也穿插了项目。无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次跳槽经历（阿里/美团/头条/网易/有赞...)]]></title>
    <url>%2F2018%2F05%2F24%2F%E6%9C%89%E8%B5%9E...)%2F</url>
    <content type="text"><![CDATA[为啥跳槽每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。面试过程（先打个广告，有兴趣加入阿里的欢迎发简历至 zhangzb2007@gmail.com，或简书上给我发信息）面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。上半场曹操专车这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。美亚柏科估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。有赞绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。字节跳动(今日头条)HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。下半场一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。美团这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。两点半进去。一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。出来的时候已经是六点半。网易面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。阿里这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。总结从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中自定义UDAF函数生产小案例]]></title>
    <url>%2F2018%2F05%2F23%2FHive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[一、UDAF 回顾1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。2.Hive有两种UDAF：简单和通用简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。3.一个计算函数必须实现的5个方法的具体含义如下：init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。二、需求使用UDAF简单方式实现统计区域产品用户访问排名三、自定义UDAF函数代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hive.org.ruozedata;import java.util.*;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import org.apache.log4j.Logger;public class UserClickUDAF extends UDAF &#123; // 日志对象初始化 public static Logger logger = Logger.getLogger(UserClickUDAF.class); // 静态类实现UDAFEvaluator public static class Evaluator implements UDAFEvaluator &#123; // 设置成员变量，存储每个统计范围内的总记录数 private static Map&lt;String, String&gt; courseScoreMap; private static Map&lt;String, String&gt; city_info; private static Map&lt;String, String&gt; product_info; private static Map&lt;String, String&gt; user_click; //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用 public Evaluator() &#123; init(); &#125; // 初始化函数间传递的中间变量 public void init() &#123; courseScoreMap = new HashMap&lt;String, String&gt;(); city_info = new HashMap&lt;String, String&gt;(); product_info = new HashMap&lt;String, String&gt;(); &#125; //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出 public boolean iterate(String pcid, String pcname, String pccount) &#123; if (pcid == null || pcname == null || pccount == null) &#123; return true; &#125; if (pccount.equals(&quot;-1&quot;)) &#123; // 城市表 city_info.put(pcid, pcname); &#125; else if (pccount.equals(&quot;-2&quot;)) &#123; // 产品表 product_info.put(pcid, pcname); &#125; else &#123; // 处理用户点击关联 unionCity_Prod_UserClic1(pcid, pcname, pccount); &#125; return true; &#125; // 处理用户点击关联 private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123; if (product_info.containsKey(pcid)) &#123; if (city_info.containsKey(pcname)) &#123; String city_name = city_info.get(pcname); String prod_name = product_info.get(pcid); String cp_name = city_name + prod_name; // 如果之前已经Put过Key值为区域信息，则把记录相加处理 if (courseScoreMap.containsKey(cp_name)) &#123; int pcrn = 0; String strTemp = courseScoreMap.get(cp_name); String courseScoreMap_pn = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim(); pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn); courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn)); &#125; else &#123; courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount); &#125; &#125; &#125; &#125; /** * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可 */ public Map&lt;String, String&gt; terminatePartial() &#123; return courseScoreMap; &#125; // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果 public boolean merge(Map&lt;String, String&gt; mapOutput) &#123; this.courseScoreMap.putAll(mapOutput); return true; &#125; // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理 public String terminate() &#123; return courseScoreMap.toString(); &#125; &#125;&#125;四、创建hive中的临时函数123DROP TEMPORARY FUNCTION user_click;add jar /data/hive_udf-1.0.jar;CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;五、调用自定义UDAF函数处理数据1234567891011121314151617insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from ( select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from ( select * from ( select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info union all select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info union all select count(1) as type, product_id as pcid, city_id as pcname from user_click where action_time=&apos;2016-05-05&apos; group by product_id,city_id ) a order by type) b) c ;六、创建Hive临时外部表1234567create external table tmp1(city_name string,product_name string,rn string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;location &apos;/works/tmp1&apos;;七、统计最终区域前3产品排名123456789select * from (select city_name, product_name, floor(sum(rn)) visit_num, row_number()over(partition by city_name order by sum(rn) desc) rn, &apos;2016-05-05&apos; action_time from tmp1 group by city_name,product_name) a where rn &lt;=3 ;八、最终结果]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 基本概念]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）spark名词解释Application jar：应用程序jar包包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；Driver Program：这个进程运行应用程序的 main 方法并且新建 SparkContext ；Cluster Manager：集群管理者在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）Deploy mode：部署模式告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；Worker Node：工作节点集群中任何可以运行应用代码的节点；（yarn上就是node manager）Executor：在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；Task：任务被送到某个 executor 上执行的工作单元；Job：包含很多并行计算的task。一个 action 就会产生一个job；Stage：一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。spark工作流程1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。2、spark并不关心底层的集群管理。3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark History Server Web UI配置]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.进入spark目录和配置文件12[root@hadoop000 ~]# cd /opt/app/spark/conf[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)12345678[root@hadoop000 conf]# hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - root root 0 2017-02-14 12:43 /sparkdrwxrwx--- - root root 0 2017-02-14 12:58 /tmpdrwxr-xr-x - root root 0 2017-02-14 12:58 /userYou have new mail in /var/spool/mail/root[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 itemsdrwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息3.配置12345[root@hadoop000 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建spark.yarn.historyServer.address : Spark history server的地址(不加http://).这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.4.添加SPARK_HISTORY_OPTS参数12345[root@hadoop01 conf]# vi spark-env.sh#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.启动服务和查看123456[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01 ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer[root@hadoop01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@hadoop01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/java以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark不得不理解的重要概念——从源码角度看RDD]]></title>
    <url>%2F2018%2F05%2F20%2FSpark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD%2F</url>
    <content type="text"><![CDATA[1.RDD是什么Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合2.RDD五大特性A list of partitions每个rdd有多个分区protected def getPartitions: Array[Partition]A function for computing each split计算作用到每个分区def compute(split: Partition, context: TaskContext): Iterator[T]A list of dependencies on other RDDsrdd之间存在依赖（RDD的血缘关系）如：RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDDprotected def getDependencies: Seq[Dependency[_]] = depsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)可选，默认哈希的分区@transient val partitioner: Option[Partitioner] = NoneOptionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）protected def getPreferredLocations(split: Partition): Seq[String] = Nil源码来自github。3.如何创建RDD创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式1234567891011121314151617parallelize() def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) &#125;textfile（） def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;源码总结：1）.取_2是因为数据为（key（偏移量），value（数据））4.常见的transformation和action由于比较简单，大概说一下常用的用处，不做代码测试transformationMap：对数据集的每一个元素进行操作FlatMap：先对数据集进行扁平化处理，然后再MapFilter：对数据进行过滤，为true则通过destinct：去重操作actionreduce：对数据进行聚集reduceBykey：对key值相同的进行操作collect：没有效果的action，但是很有用saveAstextFile：数据存入文件系统foreach：对每个元素进行func的操作]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美味不用等大数据面试题(201804月)]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)%2F</url>
    <content type="text"><![CDATA[1.若泽大数据线下班，某某某的小伙伴现场面试题截图:2.分享另外1家的忘记名字公司的大数据面试题：]]></content>
      <categories>
        <category>面试真题</category>
      </categories>
      <tags>
        <tag>大数据面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD、DataFrame和DataSet的区别]]></title>
    <url>%2F2018%2F05%2F19%2FSpark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！一 、共性1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出4、三者都有partition的概念。二、RDD优缺点优点：1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。2、面向对象的编程风格3、编译时类型安全，编译时就能检查出类型错误缺点：1、序列化和反序列化的性能开销2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC三、DataFrame1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如12345df.foreach&#123; x =&gt; val v1=x.getAs[String](&quot;v1&quot;) val v2=x.getAs[String](&quot;v2&quot;)&#125;2、DataFrame引入了schema和off-heapschema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表4、兼容Hive，支持Hql、UDF有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.四、DataSet1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。3、Dataset等同于DataFrame（Spark 2.X）]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决]]></title>
    <url>%2F2018%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3%2F</url>
    <content type="text"><![CDATA[一.数据源同步中间件：Canalhttps://github.com/alibaba/canalhttps://github.com/Hackeruncle/syncClientMaxwellhttps://github.com/zendesk/maxwell二.架构使用MySQL —- 中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra 增量的a.全量 bootstrapb.增量1.对比Canal(服务端)Maxwell(服务端+客户端)语言JavaJava活跃度活跃活跃HA支持定制 但是支持断点还原功能数据落地定制落地到kafka分区支持支持bootstrap(引导)不支持支持数据格式格式自由json(格式固定) spark json–&gt;DF文档较详细较详细随机读支持支持个人选择Maxwella.服务端+客户端一体，轻量级的b.支持断点还原功能+bootstrap+jsonCan do SELECT * from table (bootstrapping) initial loads of a table.supports automatic position recover on master promotionflexible partitioning schemes for Kakfa - by database, table, primary key, or columnMaxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).2.官网解读B站视频3.部署3.1 MySQL Installhttps://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txthttps://ke.qq.com/course/262452?tuin=11cffd503.2 修改12345678910111213141516171819202122$ vi /etc/my.cnf[mysqld]binlog_format=row$ service mysql start3.3 创建Maxwell的db和用户mysql&gt; create database maxwell;Query OK, 1 row affected (0.03 sec)mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt;3.4解压1[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz3.5测试STDOUT:123bin/maxwell --user=&apos;maxwell&apos; \--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \--producer=stdout测试1：insert sql：12mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);Query OK, 1 row affected (0.03 sec)maxwell输出：123456789101112131415161718&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;insert&quot;, &quot;ts&quot;: 1525959044, &quot;xid&quot;: 201, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 18, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;updateuser&quot;: null &#125;&#125;测试1：update sql:1mysql&gt; update ruozedata set age=29 where id=999;问题: ROW，你觉得binlog更新几个字段？maxwell输出：12345678910111213141516171819202122&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;update&quot;, &quot;ts&quot;: 1525959208, &quot;xid&quot;: 255, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 29, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;, &quot;updateuser&quot;: null &#125;, &quot;old&quot;: &#123; &quot;age&quot;: 18, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot; &#125;&#125;4.其他注意点和新特性4.1 kafka_version 版本Using kafka version: 0.11.0.1 0.10jar:12345678[root@hadoop000 kafka-clients]# lltotal 4000-rw-r--r--. 1 ruoze games 746207 May 8 06:34 kafka-clients-0.10.0.1.jar-rw-r--r--. 1 ruoze games 951041 May 8 06:35 kafka-clients-0.10.2.1.jar-rw-r--r--. 1 ruoze games 1419544 May 8 06:35 kafka-clients-0.11.0.1.jar-rw-r--r--. 1 ruoze games 324016 May 8 06:34 kafka-clients-0.8.2.2.jar-rw-r--r--. 1 ruoze games 641408 May 8 06:34 kafka-clients-0.9.0.1.jar[root@hadoop000 kafka-clients]#]]></content>
      <categories>
        <category>其他组件</category>
      </categories>
      <tags>
        <tag>高级</tag>
        <tag>maxwell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on YARN-Cluster和YARN-Client的区别]]></title>
    <url>%2F2018%2F05%2F12%2FSpark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[一. YARN-Cluster和YARN-Client的区别（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。二. yarn client 模式yarn-client 模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。三.yarn cluster 模式yarn-cluster 模式的话， client 关闭是可以提交任务的 ，总结:1.spark-shell/spark-sql 只支持 yarn-client模式；2.spark-submit对于两种模式都支持。]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产改造Spark1.6源代码，create table语法支持Oracle列表分区]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1.需求通过Spark SQL JDBC 方法，抽取Oracle表数据。2.问题大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。参考 http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases3.Oracle的分区3.1列表分区:该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。例一:1234567891011CREATE TABLE PROBLEM_TICKETS(PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,STATUS VARCHAR2(20))PARTITION BY LIST (STATUS)(PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02)3.2散列分区:这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。例一:1234567891011CREATE TABLE HASH_TABLE(COL NUMBER(8),INF VARCHAR2(100) )PARTITION BY HASH (COL)(PARTITION PART01 TABLESPACE HASH_TS01, PARTITION PART02 TABLESPACE HASH_TS02, PARTITION PART03 TABLESPACE HASH_TS03)4.改造蓝色代码是改造Spark源代码,加课程顾问领取PDF。1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。12345678910111213CREATE TEMPORARY TABLE TBLS_INUSING org.apache.spark.sql.jdbc OPTIONS (driver &quot;com.mysql.jdbc.Driver&quot;,url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,fetchSize &quot;1000&quot;,partitionColumn &quot;TBL_ID&quot;,numPartitions &quot;null&quot;,lowerBound &quot;null&quot;,upperBound &quot;null&quot;,user &quot;hive2user&quot;,password &quot;hive2user&quot;,partitionInRule &quot;1|15,16,18,19|20,21&quot;);2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748override def createRelation(sqlContext: SQLContext,parameters: Map[String, String]): BaseRelation = &#123;val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)// add partition in ruleval partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)// validind all the partition in rule if (partitionColumn != null&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)&amp;&amp; partitionInRule == null )&#123; sys.error(&quot;Partitioning incompletely specified&quot;) &#125;val partitionInfo = if (partitionColumn == null) &#123; null&#125; else &#123; val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123; val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot; upperBound = &quot;0&quot; inGroups &#125; else&#123; Array[String]() &#125; JDBCPartitioningInfo( partitionColumn, lowerBound.toLong, upperBound.toLong, numPartitions.toInt, inPartitions)&#125;val parts = JDBCRelation.columnPartition(partitionInfo)val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))// parameters is immutableif(numPartitions != null)&#123;properties.put(&quot;numPartitions&quot; , numPartitions) &#125;JDBCRelation(url, table, parts, properties)(sqlContext) &#125; &#125;3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))val column = partitioning.columnvar i: Int = 0var ans = new ArrayBuffer[Partition]()// partition by long if(partitioning.inPartitions.length == 0)&#123;val numPartitions = partitioning.numPartitionsif (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.// Here we get a little roundoff, but that&apos;s (hopefully) OK.val stride: Long = (partitioning.upperBound / numPartitions- partitioning.lowerBound / numPartitions)var currentValue: Long = partitioning.lowerBoundwhile (i &lt; numPartitions) &#123;val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else nullcurrentValue += strideval upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =if (upperBound == null) &#123; lowerBound&#125; else if (lowerBound == null) &#123; upperBound&#125; else &#123; s&quot;$lowerBound AND $upperBound&quot; &#125; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125;&#125;// partition by in else&#123; while(i &lt; partitioning.inPartitions.length)&#123; val inContent = partitioning.inPartitions(i) val whereClause = s&quot;$column in ($inContent)&quot; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125; &#125; ans.toArray &#125;4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc123456789101112def jdbc(url: String,table: String,columnName: String,lowerBound: Long,upperBound: Long,numPartitions: Int,inPartitions: Array[String] = Array[String]()): DataFrame = &#123;read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)&#125;]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产中Hive静态和动态分区表，该怎样抉择呢？]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[一.需求按照不同部门作为分区，导数据到目标表二.使用静态分区表来完成71.创建静态分区表：12345678910create table emp_static_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;2.插入数据：12hive&gt;insert into table emp_static_partition partition(deptno=10) select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;3.查询数据：1hive&gt;select * from emp_static_partition;三.使用动态分区表来完成1.创建动态分区表：123456789create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的2.插入数据：12hive&gt;insert into table emp_dynamic_partition partition(deptno) select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where需要设置属性的值：1hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；假如不设置，报错如下:3.查询数据：1hive&gt;select * from emp_dynamic_partition;分区列为deptno，实现了动态分区四.总结在生产上我们更倾向是选择动态分区，无需手工指定数据导入的具体分区，而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5min掌握，Hive的HiveServer2 和JDBC客户端&代码的生产使用]]></title>
    <url>%2F2018%2F05%2F04%2F5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF%26%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 介绍：两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，客户端可以在不启动CLI的情况下对Hive中的数据进行操作，两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，不能通过修改HiveServer的代码修正。因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。2.配置参数Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：参数 | 含义 |-|-|hive.server2.thrift.min.worker.threads| 最小工作线程数，默认为5。hive.server2.thrift.max.worker.threads| 最小工作线程数，默认为500。hive.server2.thrift.port| TCP 的监听端口，默认为10000。hive.server2.thrift.bind.host| TCP绑定的主机，默认为localhost配置监听端口和路径123456789vi hive-site.xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.48.130&lt;/value&gt;&lt;/property&gt;3. 启动hiveserver2使用hadoop用户启动123[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/[hadoop@hadoop001 bin]$ hiveserver2 which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)4. 重新开个窗口，使用beeline方式连接-n 指定机器登陆的名字，当前机器的登陆用户名-u 指定一个连接串每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK如果命令错误，hiveserver2那个窗口就会抛出异常使用hadoop用户启动123456789[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoopwhich: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)scan complete in 4msConnecting to jdbc:hive2://localhost:10000/defaultConnected to: Apache Hive (version 1.1.0-cdh5.7.0)Driver: Hive JDBC (version 1.1.0-cdh5.7.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.7.0 by Apache Hive0: jdbc:hive2://localhost:10000/default&gt;使用SQL123456789101112131415160: jdbc:hive2://localhost:10000/default&gt; show databases;INFO : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)INFO : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 secondsINFO : Concurrency mode is disabled, not creating a lock managerINFO : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Starting task [Stage-0:DDL] in serial modeINFO : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 secondsINFO : OK+----------------+--+| database_name |+----------------+--+| default |+----------------+--+1 row selected5.使用编写java代码方式连接5.1使用maven构建项目，pom.xml文件如下：123456789101112131415161718192021222324252627282930313233343536373839&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;5.2JdbcApp.java文件代码:123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class JdbcApp &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws Exception &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); //select table:ename String tableName = &quot;emp&quot;; String sql = &quot;select ename from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); while(res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2min快速了解，Hive内部表和外部表]]></title>
    <url>%2F2018%2F05%2F01%2F2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[在了解内部表和外部表区别前，我们需要先了解一下Hive架构 ：大家可以简单看一下这个架构图，我介绍其中要点：Hive的数据分为两种，一种为普通数据，一种为元数据。元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。下面我们来介绍表的两种类型：内部表和外部表内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表而内部表和外部表的主要区别就是内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；1.准备数据: 按tab键制表符作为字段分割符cat /tmp/ruozedata.txt 1 jepson 32 110 2 ruoze 22 112 3 www.ruozedata.com 18 120 2.内部表测试：在Hive里面创建一个表：123456789hive&gt; create table ruozedata(id int, &gt; name string, &gt; age int, &gt; tele string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\t&apos; &gt; STORED AS TEXTFILE;OKTime taken: 0.759 seconds这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;内部表删除1hive&gt; drop table ruozedata;3.外部表测试:创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’12345678hive&gt; create external table exter_ruozedata( &gt; id int, &gt; name string, &gt; age int, &gt; tel string) &gt; location &apos;/hive/external&apos;;OKTime taken: 0.098 seconds创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）外部表导入数据和内部表一样：1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;删除外部表1hive&gt; drop table exter_ruozedata;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈我和大数据的情缘及入门]]></title>
    <url>%2F2018%2F05%2F01%2F%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[&#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。&#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。&#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。&#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。&#8195;后来这样的进度太慢了，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。&#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。&#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。1. 心态要端正。既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。2. 心目中要有计划。先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。3. 各种方式学习。QQ群，博客，上下班看技术文章，选择好的老师和课程培训，(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。4. 项目经验。很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。而面试，就看看其他人面试分享，学习他人。最后，总结一句话，坚持才是最重要的。最后，总结一句话，坚持才是最重要的。最后，总结一句话，坚持才是最重要的。]]></content>
      <categories>
        <category>有缘大数据</category>
      </categories>
      <tags>
        <tag>人生感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive自定义函数(UDF)的部署使用，你会吗？]]></title>
    <url>%2F2018%2F04%2F27%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Hive自定义函数(UDF)的部署使用，你会吗，三种方式！一.临时函数idea编写udf打包Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Buildrz上传至服务器添加jar包hive&gt;add xxx.jar jar_filepath;查看jar包hive&gt;list jars;创建临时函数hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;二.持久函数idea编写udf打包Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Buildrz上传至服务器上传到HDFS$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jar创建持久函数hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;注意点：此方法在show functions时是看不到的，但是可以使用需要上传至hdfs三.持久函数，并注册环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9下载源码hive-1.1.0-cdh5.7.0-src.tar.gzhttp://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz解压源码tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/cd /home/hadoop/hive-1.1.0-cdh5.7.0将HelloUDF.java文件增加到HIVE源码中cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/修改FunctionRegistry.java 文件1234cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/vi FunctionRegistry.java在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);重新编译cd /home/hadoop/hive-1.1.0-cdh5.7.0mvn clean package -DskipTests -Phadoop-2 -Pdist编译结果全部为：BUILD SUCCESS文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target配置hive环境配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：7.1. 全部配置：参照之前文档 Hive全网最详细的编译及部署7.2. 将编译后带UDF函数的包复制到旧hive环境到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉命令：1234cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/libmv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bakcd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libcp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib最终启动hive测试：123hivehive (default)&gt; show functions ; -- 能查看到有 helloudfhive(default)&gt;select deptno,dname,helloudf(dname) from dept; -- helloudf函数生效]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive自定义函数(UDF)的编程开发，你会吗？]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本地开发环境：IntelliJ IDEA+Maven3.3.91. 创建工程打开IntelliJ IDEAFile–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart2. 配置在工程中找到pom.xml文件，添加hadoop、hive依赖3. 创建类、并编写一个HelloUDF.java，代码如下：首先一个UDF必须满足下面两个条件:一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）一个UDF必须至少实现了evaluate()方法4. 测试，右击运行run ‘HelloUDF.main()’5. 打包在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包执行成功后在日志中找：[INFO] Building jar: (路径)/hive-1.0.jar]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDL，你真的了解吗？]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你全面剖析Hive DDL！概念DatabaseHive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）TableHive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/tablePartition分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：/user/hadoop/hive/warehouse/[databasename.db]/tableDDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。COMMENT：数据库的描述LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下WITH DBPROPERTIES：数据库的属性Drop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICT：默认是restrict，如果该数据库还有表存在则报错；CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。Alter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)Use Database12USE database_name;USE DEFAULT;Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;“ | ”：可以选择其中一种“[ ]”：可选项LIKE ‘identifier_with_wildcards’：模糊查询数据库Describe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；EXTENDED：加上数据库键值对的属性信息。hive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s)Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type1: ARRAY &lt; data_type &gt;map_type1: MAP &lt; primitive_type, data_type &gt;struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameconstraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARY（临时表）Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。语法：CREATE TEMPORARY TABLE …注意：如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表临时表限制：不支持分区字段和创建索引EXTERNAL（外部表）Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; );PARTITIONED BY（分区表）产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。单分区：123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OK多分区：123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMAT官网解释：1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]DELIMITED：分隔符（可以自定义分隔符）；FIELDS TERMINATED BY char:每个字段之间使用的分割；例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OK创建demo2表，并指定其他字段：123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED AS（存储格式）Create Table As Select创建表（拷贝表结构及数据，并且会运行MapReduce作业）12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;加载数据1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;复制整张表12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt;复制表中的一些字段1create table emp3 as select empno,ename from emp;LIKE使用like创建表时，只会复制表的结构，不会复制表的数据1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt;并没有查询到数据desc formatted table_name查询表的详细信息12hive&gt; desc formatted emp;OKcol_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno intDetailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt;通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;查询数据库下的所有表1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt;查询创建表的语法123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later)指定PURGE后，数据不会放到回收箱，会直接删除DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失删除EXTERNAL表时，表中的数据不会从文件系统中删除Alter Table重命名1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。hive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 seconds查询结果123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);查看分区语句12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s)按分区查询1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[又又又是源码！RDD 作业的DAG是如何切分的？]]></title>
    <url>%2F2018%2F04%2F23%2F%E5%8F%88%E5%8F%88%E5%8F%88%E6%98%AF%E6%BA%90%E7%A0%81%EF%BC%81RDD%20%E4%BD%9C%E4%B8%9A%E7%9A%84DAG%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[我们都知道，RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。那么这整个过程在源码中是如何体现的呢？1.作业的提交123// SparkContext.scala dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll())123// DAGScheduler.scala def runJob[T, U]( val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)可以看到，SparkContext的runjob方法调用了DAGScheduler的runjob方法正式向集群提交任务，最终调用了submitJob方法。12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2 def submitJob[T, U]( 3 rdd: RDD[T], 4 func: (TaskContext, Iterator[T]) =&gt; U, 5 partitions: Seq[Int], 6 callSite: CallSite, 7 resultHandler: (Int, U) =&gt; Unit, 8 properties: Properties): JobWaiter[U] = &#123; 9 // Check to make sure we are not launching a task on a partition that does not exist.10 val maxPartitions = rdd.partitions.length11 partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;12 throw new IllegalArgumentException(13 &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +14 &quot;Total number of partitions: &quot; + maxPartitions)15 &#125;1617 val jobId = nextJobId.getAndIncrement()18 if (partitions.size == 0) &#123;19 // Return immediately if the job is running 0 tasks20 return new JobWaiter[U](this, jobId, 0, resultHandler)21 &#125;2223 assert(partitions.size &gt; 0)24 val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]25 val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)26 //给eventProcessLoop发送JobSubmitted消息27 eventProcessLoop.post(JobSubmitted(28 jobId, rdd, func2, partitions.toArray, callSite, waiter,29 SerializationUtils.clone(properties)))30 waiter31 &#125;这里向eventProcessLoop对象发送了JobSubmitted消息。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481// DAGScheduler.scala2 private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) eventProcessLoop是DAGSchedulerEventProcessLoop类的一个对象。 1// DAGScheduler.scala 2 private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; 3 case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; 4 dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) 5 6 case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; 7 dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) 8 9 case StageCancelled(stageId) =&gt;10 dagScheduler.handleStageCancellation(stageId)1112 case JobCancelled(jobId) =&gt;13 dagScheduler.handleJobCancellation(jobId)1415 case JobGroupCancelled(groupId) =&gt;16 dagScheduler.handleJobGroupCancelled(groupId)1718 case AllJobsCancelled =&gt;19 dagScheduler.doCancelAllJobs()2021 case ExecutorAdded(execId, host) =&gt;22 dagScheduler.handleExecutorAdded(execId, host)2324 case ExecutorLost(execId, reason) =&gt;25 val filesLost = reason match &#123;26 case SlaveLost(_, true) =&gt; true27 case _ =&gt; false28 &#125;29 dagScheduler.handleExecutorLost(execId, filesLost)3031 case BeginEvent(task, taskInfo) =&gt;32 dagScheduler.handleBeginEvent(task, taskInfo)3334 case GettingResultEvent(taskInfo) =&gt;35 dagScheduler.handleGetTaskResult(taskInfo)3637 case completion: CompletionEvent =&gt;38 dagScheduler.handleTaskCompletion(completion)3940 case TaskSetFailed(taskSet, reason, exception) =&gt;41 dagScheduler.handleTaskSetFailed(taskSet, reason, exception)4243 case ResubmitFailedStages =&gt;44 dagScheduler.resubmitFailedStages()45 &#125;DAGSchedulerEventProcessLoop对接收到的消息进行处理，在doOnReceive方法中形成一个event loop。接下来将调用submitStage()方法进行stage的划分。2.stage的划分12345678910111213141516171819202122 1// DAGScheduler.scala 2 private def submitStage(stage: Stage) &#123; 3 val jobId = activeJobForStage(stage)//查找该Stage的所有激活的job 4 if (jobId.isDefined) &#123; 5 logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;) 6 if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; 7 val missing = getMissingParentStages(stage).sortBy(_.id)//得到Stage的父Stage，并排序 8 logDebug(&quot;missing: &quot; + missing) 9 if (missing.isEmpty) &#123;10 logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)11 submitMissingTasks(stage, jobId.get)//如果Stage没有父Stage，则提交任务集12 &#125; else &#123;13 for (parent &lt;- missing) &#123;//如果有父Stage，递归调用submiStage14 submitStage(parent)15 &#125;16 waitingStages += stage//将其标记为等待状态，等待下次提交17 &#125;18 &#125;19 &#125; else &#123;20 abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)//如果该Stage没有激活的job，则丢弃该Stage21 &#125;22 &#125;在submitStage方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。12345678910111213141516171819202122232425262728293031 1// DAGScheduler.scala 2private def getMissingParentStages(stage: Stage): List[Stage] = &#123; 3 val missing = new HashSet[Stage] //用于存放父Stage 4 val visited = new HashSet[RDD[_]] //用于存放已访问过的RDD 5 6 val waitingForVisit = new Stack[RDD[_]] 7 def visit(rdd: RDD[_]) &#123; 8 if (!visited(rdd)) &#123; //如果RDD没有被访问过，则进行访问 9 visited += rdd //添加到已访问RDD的HashSet中10 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)11 if (rddHasUncachedPartitions) &#123;12 for (dep &lt;- rdd.dependencies) &#123; //获取该RDD的依赖13 dep match &#123;14 case shufDep: ShuffleDependency[_, _, _] =&gt;//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage15 val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//生成父Stage16 if (!mapStage.isAvailable) &#123;//若父Stage不存在，则添加到父Stage的HashSET中17 missing += mapStage18 &#125;19 case narrowDep: NarrowDependency[_] =&gt;//若为窄依赖，则继续访问父RDD20 waitingForVisit.push(narrowDep.rdd)21 &#125;22 &#125;23 &#125;24 &#125;25 &#125;26 waitingForVisit.push(stage.rdd)27 while (waitingForVisit.nonEmpty) &#123;//循环遍历所有RDD28 visit(waitingForVisit.pop())29 &#125;30 missing.toList31 &#125;getmissingParentStages()方法为核心方法。这里我们要懂得这样一个逻辑：我们都知道，Stage是通过shuffle划分的，所以，每一Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive生产上，压缩和存储结合使用案例]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[你们Hive生产上，压缩和存储，结合使用了吗？案例：原文件大小：19M1. ORC+Zlip结合12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views;用ORC+Zlip之后的文件为2.8M用ORC+Zlip之后的文件为2.8M###### 2. Parquet+gzip结合12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;用Parquet+gzip之后的文件为3.9M3. Parquet+Lzo结合3.1 安装Lzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile3.2 安装Lzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile3.3 软连接1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop3.4 测试lzoplzop xxx.log若生成xxx.log.lzo文件，则说明成功3.5 安装Hadoop-LZO12345 git或svn 下载https://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/3.6 配置在core-site.xml配置1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;在mapred-site.xml中配置 &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;在hadoop-env.sh中配置export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib3.7 测试12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views;用Parquet+Lzo(未建立索引)之后的文件为5.9M]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive存储格式的生产应用]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。原始大小: 19M1. TextFile(默认) 文件大小为18.1M2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views;用SequenceFile存储后的文件为19.6M3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views;用RcFile存储后的文件为17.9M4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views;用ORCFile存储后的文件为7.7M5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; 用ORCFile存储后的文件为13.1M总结：磁盘空间占用大小比较ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据压缩，你们真的了解吗？]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你们剖析大数据之压缩！1. 压缩的好处和坏处好处减少存储磁盘空间降低IO(网络的IO和磁盘的IO)加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度坏处由于使用数据时，需要先将数据解压，加重CPU负荷2. 压缩格式压缩比压缩时间可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2压缩格式优点缺点gzip压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便不支持splitlzo压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）snappy压缩速度快；支持hadoop native库不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2bzip2支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便压缩/解压速度慢；不支持native总结：不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。应用场景：一般在HDFS 、Hive、HBase中会使用；当然一般较多的是结合Spark 来一起使用。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 全网最详细的源码编译]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[若泽大数据，Spark2.2.0 全网最详细的源码编译环境准备JDK： Spark 2.2.0及以上版本只支持JDK1.8Maven：3.3.9设置maven环境变量时，需设置maven内存：export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”Scala：2.11.8Git编译下载spark的tar包，并解压12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz编辑dev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh注释以下内容：#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n)添加以下内容：1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1编辑pom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml添加在repositorys内&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;安装1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn稍微等待几小时，网络较好的话，非常快。也可以参考J哥博客：基于CentOS6.4环境编译Spark-2.1.0源码 http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop常用命令大全]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop常用命令大全1. 单独启动和关闭hadoop服务功能命令启动名称节点hadoop-daemon.sh start namenode启动数据节点hadoop-daemons.sh start datanode slave启动secondarynamenodehadoop-daemon.sh start secondarynamenode启动resourcemanageryarn-daemon.sh start resourcemanager启动nodemanagerbin/yarn-daemons.sh start nodemanager停止数据节点hadoop-daemons.sh stop datanode2. 常用的命令功能命令创建目录hdfs dfs -mkdir /input查看hdfs dfs -ls递归查看hdfs dfs ls -R上传hdfs dfs -put下载hdfs dfs -get删除hdfs dfs -rm从本地剪切粘贴到hdfshdfs fs -moveFromLocal /input/xx.txt /input/xx.txt从hdfs剪切粘贴到本地hdfs fs -moveToLocal /input/xx.txt /input/xx.txt追加一个文件到另一个文件到末尾hdfs fs -appedToFile ./hello.txt /input/hello.txt查看文件内容hdfs fs -cat /input/hello.txt显示一个文件到末尾hdfs fs -tail /input/hello.txt以字符串的形式打印文件的内容hdfs fs -text /input/hello.txt修改文件权限hdfs fs -chmod 666 /input/hello.txt修改文件所属hdfs fs -chown ruoze.ruoze /input/hello.txt从本地文件系统拷贝到hdfs里hdfs fs -copyFromLocal /input/hello.txt /input/从hdfs拷贝到本地hdfs fs -copyToLocal /input/hello.txt /input/从hdfs到一个路径拷贝到另一个路径hdfs fs -cp /input/xx.txt /output/xx.txt从hdfs到一个路径移动到另一个路径hdfs fs -mv /input/xx.txt /output/xx.txt统计文件系统的可用空间信息hdfs fs -df -h /统计文件夹的大小信息hdfs fs -du -s -h /统计一个指定目录下的文件节点数量hadoop fs -count /aaa设置hdfs的文件副本数量hadoop fs -setrep 3 /input/xx.txt总结：一定要学会查看命令帮助1.hadoop命令直接回车查看命令帮助2.hdfs命令、hdfs dfs命令直接回车查看命令帮助3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我们生产上要选择Spark On Yarn模式？]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，为什么我们生产上要选择Spark On Yarn？开发上我们选择local[2]模式生产上跑任务Job，我们选择Spark On Yarn模式 ，将Spark Application部署到yarn中，有如下优点：1.部署Application和服务更加方便只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。2.资源隔离机制yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。3.资源弹性管理Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。运行client模式：“./spark-shell –master yarn”“./spark-shell –master yarn-client”“./spark-shell –master yarn –deploy-mode client”运行的是cluster模式“./spark-shell –master yarn-cluster”“./spark-shell –master yarn –deploy-mode cluster”client和cluster模式的主要区别：a. client的driver是运行在客户端进程中b. cluster的driver是运行在Application Master之中]]></content>
      <categories>
        <category>Spark Other</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>高级</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive全网最详细的编译及部署]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hive全网最详细的编译及部署一、需要安装的软件相关环境：jdk-7u80hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7apache-maven-3.3.9mysql5.1hadoop伪分布集群已启动二、安装jdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profile三、安装maven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profile四、安装mysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges;五、下载hive源码包：输入：http://archive.cloudera.com/cdh5/cdh/5/根据cdh版本选择对应hive软件包：hive-1.1.0-cdh5.7.1-src.tar.gz解压后使用maven命令编译成安装包六、编译:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# 编译生成的包在以下位置：# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz七、安装编译生成的Hive包，然后测试12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile八、更改环境变量12345su - hadoopcd /usr/local/hivecd conf1、hive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop2、hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;九、拷贝mysql驱动包到$HIVE_HOME/lib上方的hive-site.xml使用了java的mysql驱动包需要将这个包上传到hive的lib目录之下解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/未拷贝有相关报错：The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH.Please check your CLASSPATH specification,and the name of the driver.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)修改mapred-site.xml1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改yarn-site.xml12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;启动123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh关闭1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS)1.添加hadoop用户123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# 找到root ALL=(ALL) ALL，添加hadoop ALL=(ALL) NOPASSWD:ALL2.上传并解压123[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz3.软连接1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop4.设置环境变量1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile5.设置用户、用户组123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt6.切换hadoop用户1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: 可执行文件# etc: 配置文件# sbin: shell脚本，启动关闭hdfs,yarn等7.配置文件12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # 配置自己机器的IP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;8.配置hadoop用户的ssh信任关系8.1公钥/密钥 配置无密码登录12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys8.2 查看日期，看是否配置成功1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 20179.格式化和启动123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found.9.1解决方法:添加环境变量12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为export JAVA_HOME=/usr/java/jdk1.8.0_4512345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied9.2解决方法:添加权限123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop9.3 继续启动1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh9.4检查是否成功123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode9.5访问： http://192.168.137.130:500709.6修改dfs启动的进程，以hadoop-01启动启动的三个进程：namenode: hadoop-01 bin/hdfs getconf -namenodesdatanode: localhost datanodes (using default slaves file) etc/hadoop/slavessecondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt;9.7重启123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（二）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（二）实时查看文件内容 tail filenametail -f filename 当文件(名)被修改后，不能监视文件内容tail -F filename 当文件(名)被修改后，依然可以监视文件内容复制、移动文件cp oldfilename newfilename 复制mv oldfilename newfilename 移动/重命名echoecho “xxx” 输出echo “xxx” &gt; filename 覆盖echo “xxx” &gt;&gt; filename 追加删除 rmrm -f 强制删除rm -rf 强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件别名 aliasalias x=”xxxxxx” 临时引用别名alias x=”xxxxxx” 配置到环境变量中即为永久生效查看历史命令 historyhistory 显示出所有历史记录history n 显示出n条记录!n 执行第n条记录管道命令 （ | ）管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入查看进程、查看id、端口ps -ef ｜grep 进程名 查看进程基本信息netstat -npl｜grep 进程名或进程id 查看服务id和端口杀死进程 killkill -9 进程名/pid 强制删除kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程rpm 搜索、卸载rpm -qa | grep xxx 搜索xxxrpm –nodeps -e xxx 删除xxx–nodeps 不验证包的依赖性查询find 路径 -name xxx (推荐)which xxxlocal xxx查看磁盘、内存、系统的情况df -h 查看磁盘大小及其使用情况free -m 查看内存大小及其使用情况top 查看系统情况软连接ln -s 原始目录 目标目录压缩、解压tar -czf 压缩 tar -xzvf 解压zip 压缩 unzip 解压]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令（三）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（三）用户、用户组用户useradd 用户名 添加用户userdel 用户名 删除用户id 用户名 查看用户信息passwd 用户名 修改用户密码su - 用户名 切换用户ll /home/ 查看已有的用户用户组groupadd 用户组 添加用户组cat /etc/group 用户组的文件usermod -a -G 用户组 用户 将用户添加到用户组中给一个普通用户添加sudo权限123vi /etc/sudoers #在root ALL=(ALL) ALL 下面添加一行 用户 ALL=(ALL) NOPASSWD:ALL修改文件权限chown 修改文件或文件夹的所属用户和用户组chown -R 用户:用户组 文件夹名 -R 为递归参数，指针对文件夹chown 用户:用户组 文件名chmod: 修改文件夹或者文件的权限chmod -R 700 文件夹名chmod 700 文件夹名r =&gt; 4 w =&gt; 2 x =&gt; 1 后台执行命令&amp;nohupscreen多人合作 screenscreen -list 查看会话screen -S 建立一个后台的会话screen -r 进入会话ctrl+a+d 退出会话]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（一）]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（一）查看当前目录 pwd查看IPifconfig 查看虚拟机iphostname 主机名字i 查看主机名映射的IP切换目录 cdcd ~ 切换家目录（root为/root，普通用户为/home/用户名）cd /filename 以绝对路径切换目录cd - 返回上一次操作路径，并输出路径cd ../ 返回上一层目录清理桌面 clear显示当前目录文件和文件夹 lsls -l(ll) 显示详细信息ls -la 显示详细信息+隐藏文件（以 . 开头，例：.ssh）ls -lh 显示详细信息+文件大小ls -lrt 显示详细信息+按时间排序查看文件夹大小 du -sh命令帮助man 命令命令 –help创建文件夹 mkdirmkdir -p filename1/filename2 递归创建文件夹创建文件 touch/vi/echo xx&gt;filename查看文件内容cat filename 直接打印所有内容more filename 根据窗口大小进行分页显示文件编辑 vivi分为命令行模式，插入模式，尾行模式命令行模式—&gt;插入模式：按i或a键插入模式—&gt;命令行模式：按Esc键命令行模式—&gt;尾行模式：按Shift和:键插入模式dd 删除光标所在行n+dd 删除光标以下的n行dG 删除光标以下行gg 第一行第一个字母G 最后一行第一个字母shift+$ 该行最后一个字母尾行模式q! 强制退出qw 写入并退出qw! 强制写入退出x 退出，如果存在改动，则保存再退出]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构设计及副本放置策略]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFS架构设计及副本放置策略HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。NameNode和DataNode架构图NameNode(名称节点)存储：元信息的种类，包含:文件名称文件目录结构文件的属性[权限,创建时间,副本数]文件对应哪些数据块–&gt;数据块对应哪些datanode节点作用：管理着文件系统命名空间维护这文件系统树及树中的所有文件和目录维护所有这些文件或目录的打开、关闭、移动、重命名等操作DataNode(数据节点)存储：数据块、数据块校验、与NameNode通信作用：读写文件的数据块NameNode的指示来进行创建、删除、和复制等操作通过心跳定期向NameNode发送所存储文件块列表信息Scondary NameNode(第二名称节点)存储: 命名空间镜像文件fsimage+编辑日志editlog作用: 定期合并fsimage+editlog文件为新的fsimage推送给NamenNode副本放置策略第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上第二副本：放置在与第一个副本不同的机架的节点上第三副本：与第二个副本相同机架的不同节点上如果还有更多的副本：随机放在节点中]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置多台虚拟机之间的SSH信任]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[本机环境3台机器执行命令ssh-keygen选取第一台,生成authorized_keys文件hadoop002 hadoop003传输id_rsa.pub文件到hadoop001hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys设置每台机器的权限12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keys将authorized_keys分发到hadoop002、hadoop003机器验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
