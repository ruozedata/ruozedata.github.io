<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®-CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®žè·µ)]]></title>
    <url>%2F2019%2F05%2F13%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½æ•°æ®CDH5.16.1é›†ç¾¤ä¼ä¸šçœŸæ­£ç¦»çº¿éƒ¨ç½²(å…¨ç½‘æœ€ç»†ï¼Œé…å¥—è§†é¢‘ï¼Œç”Ÿäº§å¯å®žè·µ)è§†é¢‘:https://www.bilibili.com/video/av52167219PS:å»ºè®®å…ˆçœ‹è¯¾ç¨‹è§†é¢‘1-2ç¯‡ï¼Œå†æ ¹æ®è§†é¢‘æˆ–æ–‡æ¡£éƒ¨ç½²ï¼Œå¦‚æœ‰é—®é¢˜ï¼ŒåŠæ—¶ä¸Ž@è‹¥æ³½æ•°æ®Jå“¥è”ç³»ã€‚ä¸€.å‡†å¤‡å·¥ä½œ1.ç¦»çº¿éƒ¨ç½²ä¸»è¦åˆ†ä¸ºä¸‰å—:a.MySQLç¦»çº¿éƒ¨ç½²b.CMç¦»çº¿éƒ¨ç½²c.Parcelæ–‡ä»¶ç¦»çº¿æºéƒ¨ç½²2.è§„åˆ’:èŠ‚ç‚¹MySQLéƒ¨ç½²ç»„ä»¶Parcelæ–‡ä»¶ç¦»çº¿æºCMæœåŠ¡è¿›ç¨‹å¤§æ•°æ®ç»„ä»¶hadoop001MySQLParcelActivity MonitorNN RM DN NMhadoop002Alert PublisherEvent ServerDN NMhadoop003Host MonitorService MonitorDN NM3.ä¸‹è½½æº:CMcloudera-manager-centos7-cm5.16.1_x86_64.tar.gzParcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1manifest.jsonJDKhttps://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.htmlä¸‹è½½jdk-8u202-linux-x64.tar.gzMySQLhttps://dev.mysql.com/downloads/mysql/5.7.html#downloadsä¸‹è½½mysql-5.7.26-el7-x86_64.tar.gzMySQL jdbc jarmysql-connector-java-5.1.47.jarä¸‹è½½å®ŒæˆåŽè¦é‡å‘½ååŽ»æŽ‰ç‰ˆæœ¬å·ï¼Œmv mysql-connector-java-5.1.47.jar mysql-connector-java.jar###å‡†å¤‡å¥½ç™¾åº¦äº‘,ä¸‹è½½å®‰è£…åŒ…:é“¾æŽ¥:https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA å¯†ç :viqpäºŒ.é›†ç¾¤èŠ‚ç‚¹åˆå§‹åŒ–1.é˜¿é‡Œäº‘ä¸Šæµ·åŒºè´­ä¹°3å°ï¼ŒæŒ‰é‡ä»˜è´¹è™šæ‹ŸæœºCentOS7.2æ“ä½œç³»ç»Ÿï¼Œ2æ ¸8Gæœ€ä½Žé…ç½®2.å½“å‰ç¬”è®°æœ¬æˆ–å°å¼æœºé…ç½®hostsæ–‡ä»¶MAC: /etc/hostsWindow: C:\windows\system32\drivers\etc\hosts1234å…¬ç½‘åœ°å€: 106.15.234.222 hadoop001 106.15.235.200 hadoop002 106.15.234.239 hadoop0033.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„hostsæ–‡ä»¶1234ç§æœ‰åœ°é“ã€å†…ç½‘åœ°å€:echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hostsecho &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts4.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„é˜²ç«å¢™åŠæ¸…ç©ºè§„åˆ™123systemctl stop firewalld systemctl disable firewalldiptables -F5.å…³é—­æ‰€æœ‰èŠ‚ç‚¹çš„selinux123vi /etc/selinux/configå°†SELINUX=enforcingæ”¹ä¸ºSELINUX=disabled è®¾ç½®åŽéœ€è¦é‡å¯æ‰èƒ½ç”Ÿæ•ˆ6.è®¾ç½®æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶åŒºä¸€è‡´åŠæ—¶é’ŸåŒæ­¥1234567891011121314151617181920212223242526272829303132333435363738394041424344454647486.1.æ—¶åŒº[root@hadoop001 ~]# dateSat May 11 10:07:53 CST 2019[root@hadoop001 ~]# timedatectl Local time: Sat 2019-05-11 10:10:31 CST Universal time: Sat 2019-05-11 02:10:31 UTC RTC time: Sat 2019-05-11 10:10:29 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yesNTP synchronized: yes RTC in local TZ: yes DST active: n/a#æŸ¥çœ‹å‘½ä»¤å¸®åŠ©ï¼Œå­¦ä¹ è‡³å…³é‡è¦ï¼Œæ— éœ€ç™¾åº¦ï¼Œå¤ªðŸ‘Ž[root@hadoop001 ~]# timedatectl --helptimedatectl [OPTIONS...] COMMAND ...Query or change system time and date settings. -h --help Show this help message --version Show package version --no-pager Do not pipe output into a pager --no-ask-password Do not prompt for password -H --host=[USER@]HOST Operate on remote host -M --machine=CONTAINER Operate on local container --adjust-system-clock Adjust system clock when changing local RTC modeCommands: status Show current time settings set-time TIME Set system time set-timezone ZONE Set system time zone list-timezones Show known time zones set-local-rtc BOOL Control whether RTC is in local time set-ntp BOOL Control whether NTP is enabled#æŸ¥çœ‹å“ªäº›æ—¶åŒº[root@hadoop001 ~]# timedatectl list-timezonesAfrica/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/AlgiersAfrica/AsmaraAfrica/Bamako#æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®äºšæ´²ä¸Šæµ·æ—¶åŒº [root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455566.2.æ—¶é—´#æ‰€æœ‰èŠ‚ç‚¹å®‰è£…ntp[root@hadoop001 ~]# yum install -y ntp#é€‰å–hadoop001ä¸ºntpçš„ä¸»èŠ‚ç‚¹[root@hadoop001 ~]# vi /etc/ntp.conf #timeserver 0.asia.pool.ntp.orgserver 1.asia.pool.ntp.orgserver 2.asia.pool.ntp.orgserver 3.asia.pool.ntp.org#å½“å¤–éƒ¨æ—¶é—´ä¸å¯ç”¨æ—¶ï¼Œå¯ä½¿ç”¨æœ¬åœ°ç¡¬ä»¶æ—¶é—´server 127.127.1.0 iburst local clock #å…è®¸å“ªäº›ç½‘æ®µçš„æœºå™¨æ¥åŒæ­¥æ—¶é—´restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap#å¼€å¯ntpdåŠæŸ¥çœ‹çŠ¶æ€[root@hadoop001 ~]# systemctl start ntpd[root@hadoop001 ~]# systemctl status ntpd ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago Main PID: 18518 (ntpd) CGroup: /system.slice/ntpd.service â””â”€18518 /usr/sbin/ntpd -u ntp:ntp -gMay 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usecMay 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabledMay 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.#éªŒè¯[root@hadoop001 ~]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== LOCAL(0) .LOCL. 10 l 726 64 0 0.000 0.000 0.000#å…¶ä»–ä»ŽèŠ‚ç‚¹åœæ­¢ç¦ç”¨ntpdæœåŠ¡ [root@hadoop002 ~]# systemctl stop ntpd[root@hadoop002 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´[root@hadoop002 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop001 [root@hadoop003 ~]# systemctl stop ntpd[root@hadoop004 ~]# systemctl disable ntpdRemoved symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop00111 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec#æ¯å¤©å‡Œæ™¨åŒæ­¥hadoop001èŠ‚ç‚¹æ—¶é—´[root@hadoop003 ~]# crontab -e00 00 * * * /usr/sbin/ntpdate hadoop0017.éƒ¨ç½²é›†ç¾¤çš„JDK123456789mkdir /usr/javatar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/#åˆ‡è®°å¿…é¡»ä¿®æ­£æ‰€å±žç”¨æˆ·åŠç”¨æˆ·ç»„chown -R root:root /usr/java/jdk1.8.0_45echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profileecho &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profilesource /etc/profilewhich java8.hadoop001èŠ‚ç‚¹ç¦»çº¿éƒ¨ç½²MySQL5.7(å‡å¦‚è§‰å¾—å›°éš¾å“Ÿï¼Œå°±è‡ªè¡Œç™¾åº¦RPMéƒ¨ç½²ï¼Œå› ä¸ºè¯¥éƒ¨ç½²æ–‡æ¡£æ˜¯æˆ‘å¸ç”Ÿäº§æ–‡æ¡£)æ–‡æ¡£é“¾æŽ¥:https://github.com/Hackeruncle/MySQLè§†é¢‘é“¾æŽ¥:https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ å¯†ç :whic9.åˆ›å»ºCDHçš„å…ƒæ•°æ®åº“å’Œç”¨æˆ·ã€amonæœåŠ¡çš„æ•°æ®åº“åŠç”¨æˆ·12345create database cmf DEFAULT CHARACTER SET utf8;create database amon DEFAULT CHARACTER SET utf8;grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;flush privileges;10.hadoop001èŠ‚ç‚¹éƒ¨ç½²mysql jdbc jar12mkdir -p /usr/share/java/cp mysql-connector-java.jar /usr/share/java/ä¸‰.CDHéƒ¨ç½²1.ç¦»çº¿éƒ¨ç½²cm serveråŠagent1234567891011121314151617181920211.1.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç›®å½•åŠè§£åŽ‹mkdir /opt/cloudera-managertar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/1.2.æ‰€æœ‰èŠ‚ç‚¹ä¿®æ”¹agentçš„é…ç½®ï¼ŒæŒ‡å‘serverçš„èŠ‚ç‚¹hadoop001sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini1.3.ä¸»èŠ‚ç‚¹ä¿®æ”¹serverçš„é…ç½®:vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=hadoop001com.cloudera.cmf.db.name=cmfcom.cloudera.cmf.db.user=cmfcom.cloudera.cmf.db.password=Ruozedata123456!com.cloudera.cmf.db.setupType=EXTERNAL1.4.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç”¨æˆ·useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm1.5.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager2.hadoop001èŠ‚ç‚¹éƒ¨ç½²ç¦»çº¿parcelæº1234567891011121314151617182.1.éƒ¨ç½²ç¦»çº¿parcelæº$ mkdir -p /opt/cloudera/parcel-repo$ lltotal 3081664-rw-r--r-- 1 root root 2127506677 May 9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel-rw-r--r-- 1 root root 41 May 9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1-rw-r--r-- 1 root root 841524318 May 9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz-rw-r--r-- 1 root root 185515842 Aug 10 2017 jdk-8u144-linux-x64.tar.gz-rw-r--r-- 1 root root 66538 May 9 18:03 manifest.json-rw-r--r-- 1 root root 989495 May 25 2017 mysql-connector-java.jar$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/#åˆ‡è®°cpæ—¶ï¼Œé‡å‘½ååŽ»æŽ‰1ï¼Œä¸ç„¶åœ¨éƒ¨ç½²è¿‡ç¨‹CMè®¤ä¸ºå¦‚ä¸Šæ–‡ä»¶ä¸‹è½½æœªå®Œæ•´ï¼Œä¼šæŒç»­ä¸‹è½½$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha$ cp manifest.json /opt/cloudera/parcel-repo/2.2.ç›®å½•ä¿®æ”¹ç”¨æˆ·åŠç”¨æˆ·ç»„$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/3.æ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶å®‰è£…ç›®å½•ã€ç”¨æˆ·åŠç”¨æˆ·ç»„æƒé™mkdir -p /opt/cloudera/parcelschown -R cloudera-scm:cloudera-scm /opt/cloudera/4.hadoop001èŠ‚ç‚¹å¯åŠ¨Server1234564.1.å¯åŠ¨server/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start4.2.é˜¿é‡Œäº‘webç•Œé¢ï¼Œè®¾ç½®è¯¥hadoop001èŠ‚ç‚¹é˜²ç«å¢™æ”¾å¼€7180ç«¯å£4.3.ç­‰å¾…1minï¼Œæ‰“å¼€ http://hadoop001:7180 è´¦å·å¯†ç :admin/admin4.4.å‡å¦‚æ‰“ä¸å¼€ï¼ŒåŽ»çœ‹serverçš„logï¼Œæ ¹æ®é”™è¯¯ä»”ç»†æŽ’æŸ¥é”™è¯¯5.æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨Agent1/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start6.æŽ¥ä¸‹æ¥ï¼Œå…¨éƒ¨Webç•Œé¢æ“ä½œhttp://hadoop001:7180/è´¦å·å¯†ç :admin/admin7.æ¬¢è¿Žä½¿ç”¨Cloudera Managerâ€“æœ€ç»ˆç”¨æˆ·è®¸å¯æ¡æ¬¾ä¸Žæ¡ä»¶ã€‚å‹¾é€‰8.æ¬¢è¿Žä½¿ç”¨Cloudera Managerâ€“æ‚¨æƒ³è¦éƒ¨ç½²å“ªä¸ªç‰ˆæœ¬ï¼Ÿé€‰æ‹©Cloudera Expresså…è´¹ç‰ˆæœ¬9.æ„Ÿè°¢æ‚¨é€‰æ‹©Cloudera Managerå’ŒCDH10.ä¸ºCDHé›†ç¾¤å®‰è£…æŒ‡å¯¼ä¸»æœºã€‚é€‰æ‹©[å½“å‰ç®¡ç†çš„ä¸»æœº]ï¼Œå…¨éƒ¨å‹¾é€‰11.é€‰æ‹©å­˜å‚¨åº“12.é›†ç¾¤å®‰è£…â€“æ­£åœ¨å®‰è£…é€‰å®šParcelå‡å¦‚æœ¬åœ°parcelç¦»çº¿æºé…ç½®æ­£ç¡®ï¼Œåˆ™â€ä¸‹è½½â€é˜¶æ®µçž¬é—´å®Œæˆï¼Œå…¶ä½™é˜¶æ®µè§†èŠ‚ç‚¹æ•°ä¸Žå†…éƒ¨ç½‘ç»œæƒ…å†µå†³å®šã€‚13.æ£€æŸ¥ä¸»æœºæ­£ç¡®æ€§123456789101112131415161718192021222324252627282913.1.å»ºè®®å°†/proc/sys/vm/swappinessè®¾ç½®ä¸ºæœ€å¤§å€¼10ã€‚swappinesså€¼æŽ§åˆ¶æ“ä½œç³»ç»Ÿå°è¯•äº¤æ¢å†…å­˜çš„ç§¯æžï¼›swappiness=0ï¼šè¡¨ç¤ºæœ€å¤§é™åº¦ä½¿ç”¨ç‰©ç†å†…å­˜ï¼Œä¹‹åŽæ‰æ˜¯swapç©ºé—´ï¼›swappiness=100ï¼šè¡¨ç¤ºç§¯æžä½¿ç”¨swapåˆ†åŒºï¼Œå¹¶ä¸”æŠŠå†…å­˜ä¸Šçš„æ•°æ®åŠæ—¶æ¬è¿åˆ°swapç©ºé—´ï¼›å¦‚æžœæ˜¯æ··åˆæœåŠ¡å™¨ï¼Œä¸å»ºè®®å®Œå…¨ç¦ç”¨swapï¼Œå¯ä»¥å°è¯•é™ä½Žswappinessã€‚ä¸´æ—¶è°ƒæ•´ï¼šsysctl vm.swappiness=10æ°¸ä¹…è°ƒæ•´ï¼šcat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf# Adjust swappiness valuevm.swappiness=10EOF13.2.å·²å¯ç”¨é€æ˜Žå¤§é¡µé¢åŽ‹ç¼©ï¼Œå¯èƒ½ä¼šå¯¼è‡´é‡å¤§æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ç¦ç”¨æ­¤è®¾ç½®ã€‚ä¸´æ—¶è°ƒæ•´ï¼šecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledæ°¸ä¹…è°ƒæ•´ï¼šcat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local# Disable transparent_hugepageecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledEOF# centos7.xç³»ç»Ÿï¼Œéœ€è¦ä¸º&quot;/etc/rc.d/rc.local&quot;æ–‡ä»¶èµ‹äºˆæ‰§è¡Œæƒé™chmod +x /etc/rc.d/rc.local14.è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©éƒ¨ç½²Zookeeperã€HDFSã€YarnæœåŠ¡15.è‡ªå®šä¹‰è§’è‰²åˆ†é…16.æ•°æ®åº“è®¾ç½®17.å®¡æ”¹è®¾ç½®ï¼Œé»˜è®¤å³å¯18.é¦–æ¬¡è¿è¡Œ19.æ­å–œæ‚¨!20.ä¸»é¡µCDHå…¨å¥—è¯¾ç¨‹ç›®å½•ï¼Œå¦‚æœ‰buyï¼ŒåŠ å¾®ä¿¡(ruoze_star)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061620.é’äº‘çŽ¯å¢ƒä»‹ç»å’Œä½¿ç”¨ 1.Preparation è°ˆè°ˆæ€Žæ ·å…¥é—¨å¤§æ•°æ® è°ˆè°ˆæ€Žæ ·åšå¥½ä¸€ä¸ªå¤§æ•°æ®å¹³å°çš„è¿è¥å·¥ä½œ Linuxæœºå™¨,å„è½¯ä»¶ç‰ˆæœ¬ä»‹ç»åŠå®‰è£…(å½•æ’­) 2.Introduction Clouderaã€CMåŠCDHä»‹ç» CDHç‰ˆæœ¬é€‰æ‹© CDHå®‰è£…å‡ ç§æ–¹å¼è§£è¯» 3.Install&amp;UnInstall é›†ç¾¤èŠ‚ç‚¹è§„åˆ’,çŽ¯å¢ƒå‡†å¤‡(NTP,Jdk and etc) MySQLç¼–è¯‘å®‰è£…åŠå¸¸ç”¨å‘½ä»¤ æŽ¨è:CDHç¦»çº¿å®‰è£…(è¸©å‘å¿ƒå¾—,å…¨é¢å‰–æž) è§£è¯»æš´åŠ›å¸è½½è„šæœ¬ 4.CDH Management CDHä½“ç³»æž¶æž„å‰–æž CDHé…ç½®æ–‡ä»¶æ·±åº¦è§£æž CMçš„å¸¸ç”¨å‘½ä»¤ CDHé›†ç¾¤æ­£ç¡®å¯åŠ¨å’Œåœæ­¢é¡ºåº CDH Tsquery Language CDHå¸¸è§„ç®¡ç†(ç›‘æŽ§/é¢„è­¦/é…ç½®/èµ„æº/æ—¥å¿—/å®‰å…¨) 5.Maintenance Experiment HDFS HA é…ç½® åŠhadoop/hdfså¸¸è§„å‘½ä»¤ Yarn HA é…ç½® åŠyarnå¸¸è§„å‘½ä»¤ Other CDH Components HA é…ç½® CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœåŠ¡(hive/spark/hbase) CDHåŠ¨æ€æ·»åŠ åˆ é™¤æœºå™¨ CDHåŠ¨æ€æ·»åŠ åˆ é™¤åŠè¿ç§»DataNodeè¿›ç¨‹ç­‰ CDHå‡çº§(5.10.0--&gt;5.12.0) 6.Resource Management Linux Cgroups é™æ€èµ„æºæ±  åŠ¨æ€èµ„æºæ±  å¤šç§Ÿæˆ·æ¡ˆä¾‹ 7.Performance Tunning Memory/CPU/Network/DiskåŠé›†ç¾¤è§„åˆ’ Linuxå‚æ•° HDFSå‚æ•° MapReduceåŠYarnå‚æ•° å…¶ä»–æœåŠ¡å‚æ•° 8.Cases Share CDH4&amp;5ä¹‹Alternativeså‘½ä»¤ çš„ç ”ç©¶ CDH5.8.2å®‰è£…ä¹‹Hash verification failed è®°å½•ä¸€æ¬¡CDH4.8.6 é…ç½®HDFS HA å‘ CDH5.0é›†ç¾¤IPæ›´æ”¹ CDHçš„active namenode exit(GC)å’Œå½©è›‹åˆ†äº« 9. Kerberos Kerberosç®€ä»‹ Kerberosä½“ç³»ç»“æž„ Kerberoså·¥ä½œæœºåˆ¶ Kerberoså®‰è£…éƒ¨ç½² CDHå¯ç”¨kerberos Kerberoså¼€å‘ä½¿ç”¨(çœŸå®žä»£ç )10.Summary æ€»ç»“Join us if you have a dream.è‹¥æ³½æ•°æ®å®˜ç½‘: http://ruozedata.comè…¾è®¯è¯¾å ‚ï¼Œæœè‹¥æ³½æ•°æ®: http://ruoze.ke.qq.comBilibiliç½‘ç«™,æœè‹¥æ³½æ•°æ®: https://space.bilibili.com/356836323è‹¥æ³½å¤§æ•°æ®â€“å®˜æ–¹åšå®¢è‹¥æ³½å¤§æ•°æ®â€“åšå®¢ä¸€è§ˆè‹¥æ³½å¤§æ•°æ®â€“å†…éƒ¨å­¦å‘˜é¢è¯•é¢˜æ‰«ä¸€æ‰«ï¼Œå­¦ä¸€å­¦:]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®è¯¾ç¨‹ä¸€è§ˆ]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½æ•°æ®è¯¾ç¨‹ç³»åˆ—åŸºç¡€ç­LiunxVMè™šæ‹Ÿæœºå®‰è£…Liunxå¸¸ç”¨å‘½ä»¤ï¼ˆé‡ç‚¹ï¼‰å¼€å‘çŽ¯å¢ƒæ­MySQLæºç å®‰è£…&amp;yumå®‰è£…CRUDç¼–å†™æƒé™æŽ§åˆ¶Hadoopæž¶æž„ä»‹ç»&amp;&amp;æºç ç¼–è¯‘ä¼ªåˆ†å¸ƒå¼å®‰è£…&amp;&amp;ä¼ä¸šåº”ç”¨HDFSï¼ˆé‡ç‚¹ï¼‰æž¶æž„è®¾è®¡å‰¯æœ¬æ”¾ç½®ç­–ç•¥è¯»å†™æµç¨‹YARNï¼ˆé‡ç‚¹ï¼‰æž¶æž„è®¾è®¡å·¥ä½œæµç¨‹è°ƒåº¦ç®¡ç†&amp;&amp;å¸¸è§å‚æ•°é…ç½®ï¼ˆè°ƒä¼˜ï¼‰MapReduceæž¶æž„è®¾è®¡wordcountåŽŸç†&amp;&amp;joinåŽŸç†å’Œæ¡ˆä¾‹Hiveæž¶æž„è®¾è®¡Hive DDL&amp;DMLjoinåœ¨å¤§æ•°æ®ä¸­çš„ä½¿ç”¨ä½¿ç”¨è‡ªå¸¦UDFå’Œå¼€å‘è‡ªå®šä¹‰UDFSqoopæž¶æž„è®¾è®¡RDBMSå¯¼å…¥å¯¼å‡ºæ•´åˆé¡¹ç›®å°†æ‰€æœ‰ç»„ä»¶åˆä½œä½¿ç”¨ã€‚äººå·¥æ™ºèƒ½åŸºç¡€pythonåŸºç¡€å¸¸ç”¨åº“â€”â€”pandasã€numpyã€sklearnã€kerasé«˜çº§ç­scalaç¼–ç¨‹ï¼ˆé‡ç‚¹ï¼‰Sparkï¼ˆäº”æ˜Ÿé‡ç‚¹ï¼‰Hadoopé«˜çº§Hiveé«˜çº§FlumeKafkaHBaseFlinkCDHå®¹å™¨è°ƒåº¦å¹³å°çº¿ä¸‹ç­]]></content>
      <categories>
        <category>è¯¾ç¨‹</category>
      </categories>
      <tags>
        <tag>è¯¾ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerå¸¸ç”¨å‘½ä»¤ä»¥åŠå®‰è£…mysql]]></title>
    <url>%2F2019%2F05%2F08%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.ç®€ä»‹Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„åº”ç”¨å®¹å™¨å¼•æ“Žï¼›æ˜¯ä¸€ä¸ªè½»é‡çº§å®¹å™¨æŠ€æœ¯ï¼›Dockeræ”¯æŒå°†è½¯ä»¶ç¼–è¯‘æˆä¸€ä¸ªé•œåƒï¼›ç„¶åŽåœ¨é•œåƒä¸­å„ç§è½¯ä»¶åšå¥½é…ç½®ï¼Œå°†é•œåƒå‘å¸ƒå‡ºåŽ»ï¼Œå…¶ä»–ä½¿ç”¨è€…å¯ä»¥ç›´æŽ¥ä½¿ç”¨è¿™ä¸ªé•œåƒï¼›è¿è¡Œä¸­çš„è¿™ä¸ªé•œåƒç§°ä¸ºå®¹å™¨ï¼Œå®¹å™¨å¯åŠ¨æ˜¯éžå¸¸å¿«é€Ÿçš„ã€‚2.æ ¸å¿ƒæ¦‚å¿µdockerä¸»æœº(Host)ï¼šå®‰è£…äº†Dockerç¨‹åºçš„æœºå™¨ï¼ˆDockerç›´æŽ¥å®‰è£…åœ¨æ“ä½œç³»ç»Ÿä¹‹ä¸Šï¼‰ï¼›dockerå®¢æˆ·ç«¯(Client)ï¼šè¿žæŽ¥dockerä¸»æœºè¿›è¡Œæ“ä½œï¼›dockerä»“åº“(Registry)ï¼šç”¨æ¥ä¿å­˜å„ç§æ‰“åŒ…å¥½çš„è½¯ä»¶é•œåƒï¼›dockeré•œåƒ(Images)ï¼šè½¯ä»¶æ‰“åŒ…å¥½çš„é•œåƒï¼›æ”¾åœ¨dockerä»“åº“ä¸­ï¼›dockerå®¹å™¨(Container)ï¼šé•œåƒå¯åŠ¨åŽçš„å®žä¾‹ç§°ä¸ºä¸€ä¸ªå®¹å™¨ï¼›å®¹å™¨æ˜¯ç‹¬ç«‹è¿è¡Œçš„ä¸€ä¸ªæˆ–ä¸€ç»„åº”ç”¨3.å®‰è£…çŽ¯å¢ƒ1234VM ware Workstation10CentOS-7-x86_64-DVD-1804.isouname -r3.10.0-862.el7.x86_64æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Š æŸ¥çœ‹å‘½ä»¤ï¼šuname -r4.åœ¨linuxè™šæ‹Ÿæœºä¸Šå®‰è£…dockeræ­¥éª¤ï¼š1ã€æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ï¼Œå¿…é¡»æ˜¯3.10åŠä»¥ä¸Šuname -r2ã€å®‰è£…dockeryum install docker3ã€è¾“å…¥yç¡®è®¤å®‰è£…Dependency Updated:audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1Complete!(æˆåŠŸæ ‡å¿—)4ã€å¯åŠ¨docker123[root@hadoop000 ~]# systemctl start docker[root@hadoop000 ~]# docker -vDocker version 1.13.1, build 8633870/1.13.15ã€å¼€æœºå¯åŠ¨docker12[root@hadoop000 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6ã€åœæ­¢docker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@hadoop000 ~]# systemctl stop docker``` ### 5.å¸¸ç”¨å‘½ä»¤é•œåƒæ“ä½œ|æ“ä½œ|å‘½ä»¤|è¯´æ˜Ž||---|---|---|æ£€ç´¢ |docker search å…³é”®å­— egï¼šdocker search redis| æˆ‘ä»¬ç»å¸¸åŽ»docker hubä¸Šæ£€ç´¢é•œåƒçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚é•œåƒçš„TAGã€‚|æ‹‰å– |docker pull é•œåƒå:tag| :tagæ˜¯å¯é€‰çš„ï¼Œtagè¡¨ç¤ºæ ‡ç­¾ï¼Œå¤šä¸ºè½¯ä»¶çš„ç‰ˆæœ¬ï¼Œé»˜è®¤æ˜¯lateståˆ—è¡¨| docker images |æŸ¥çœ‹æ‰€æœ‰æœ¬åœ°é•œåƒåˆ é™¤|docker rmi image-id |åˆ é™¤æŒ‡å®šçš„æœ¬åœ°é•œåƒå½“ç„¶å¤§å®¶ä¹Ÿå¯ä»¥åœ¨å®˜ç½‘æŸ¥æ‰¾ï¼šhttps://hub.docker.com/å®¹å™¨æ“ä½œè½¯ä»¶é•œåƒï¼ˆQQå®‰è£…ç¨‹åºï¼‰----è¿è¡Œé•œåƒ----äº§ç”Ÿä¸€ä¸ªå®¹å™¨ï¼ˆæ­£åœ¨è¿è¡Œçš„è½¯ä»¶ï¼Œè¿è¡Œçš„QQï¼‰ï¼›æ­¥éª¤ï¼š- 1ã€æœç´¢é•œåƒ[root@localhost ~]# docker search tomcat- 2ã€æ‹‰å–é•œåƒ[root@localhost ~]# docker pull tomcat- 3ã€æ ¹æ®é•œåƒå¯åŠ¨å®¹å™¨docker run --name mytomcat -d tomcat:latest- 4ã€docker ps æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨- 5ã€ åœæ­¢è¿è¡Œä¸­çš„å®¹å™¨docker stop å®¹å™¨çš„id- 6ã€æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨docker ps -a- 7ã€å¯åŠ¨å®¹å™¨docker start å®¹å™¨id- 8ã€åˆ é™¤ä¸€ä¸ªå®¹å™¨ docker rm å®¹å™¨id- 9ã€å¯åŠ¨ä¸€ä¸ªåšäº†ç«¯å£æ˜ å°„çš„tomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-dï¼šåŽå°è¿è¡Œ-p: å°†ä¸»æœºçš„ç«¯å£æ˜ å°„åˆ°å®¹å™¨çš„ä¸€ä¸ªç«¯å£ ä¸»æœºç«¯å£:å®¹å™¨å†…éƒ¨çš„ç«¯å£- 10ã€ä¸ºäº†æ¼”ç¤ºç®€å•å…³é—­äº†linuxçš„é˜²ç«å¢™service firewalld status ï¼›æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€service firewalld stopï¼šå…³é—­é˜²ç«å¢™systemctl disable firewalld.service #ç¦æ­¢firewallå¼€æœºå¯åŠ¨- 11ã€æŸ¥çœ‹å®¹å™¨çš„æ—¥å¿—docker logs container-name/container-idæ›´å¤šå‘½ä»¤å‚çœ‹https://docs.docker.com/engine/reference/commandline/docker/å¯ä»¥å‚è€ƒé•œåƒæ–‡æ¡£### 6.ä½¿ç”¨dockerå®‰è£…mysql- docker pull mysqldocker pull mysqlUsing default tag: latestTrying to pull repository docker.io/library/mysql â€¦latest: Pulling from docker.io/library/mysqla5a6f2f73cd8: Pull complete936836019e67: Pull complete283fa4c95fb4: Pull complete1f212fb371f9: Pull completee2ae0d063e89: Pull complete5ed0ae805b65: Pull complete0283dc49ef4e: Pull completea7e1170b4fdb: Pull complete88918a9e4742: Pull complete241282fa67c2: Pull completeb0fecf619210: Pull completebebf9f901dcc: Pull completeDigest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8Status: Downloaded newer image for docker.io/mysql:latest[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB1- å¯åŠ¨[root@hadoop000 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest f991c20cb508 10 days ago 486 MB[root@hadoop000 ~]# docker run â€“name mysql01 -d mysql756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a[root@hadoop000 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES756620c8e583 mysql â€œdocker-entrypointâ€¦â€ 9 seconds ago Exited (1) 7 seconds ago mysql01[root@hadoop000 ~]# docker logs 756620c8e583error: database is uninitialized and password option is not specifiedYou need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD1å¯ä»¥çœ‹åˆ°ä¸Šé¢å¯åŠ¨çš„æ–¹å¼æ˜¯é”™è¯¯çš„ï¼Œæç¤ºæˆ‘ä»¬è¦å¸¦ä¸Šå…·ä½“çš„å¯†ç [root@hadoop000 ~]# docker run -p 3306:3306 â€“name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqleae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc0612345```â€“nameï¼šç»™æ–°åˆ›å»ºçš„å®¹å™¨å‘½åï¼Œæ­¤å¤„å‘½åä¸ºpwc-mysql-eï¼šé…ç½®ä¿¡æ¯ï¼Œæ­¤å¤„é…ç½®mysqlçš„rootç”¨æˆ·çš„ç™»é™†å¯†ç -pï¼šç«¯å£æ˜ å°„ï¼Œæ­¤å¤„æ˜ å°„ä¸»æœº3306ç«¯å£åˆ°å®¹å™¨pwc-mysqlçš„3306ç«¯å£-dï¼šæˆåŠŸå¯åŠ¨å®¹å™¨åŽè¾“å‡ºå®¹å™¨çš„å®Œæ•´IDï¼Œä¾‹å¦‚ä¸Šå›¾ 73f8811f669ee...æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ123[root@hadoop000 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeae86796e132 mysql &quot;docker-entrypoint...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql02ç™»é™†MySQL12345678910111213141516docker exec -it mysql04 /bin/bashroot@e34aba02c0c3:/# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 80Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;å…¶ä»–çš„é«˜çº§æ“ä½œ123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tagæŠŠä¸»æœºçš„/conf/mysqlæ–‡ä»¶å¤¹æŒ‚è½½åˆ° mysqldockerå®¹å™¨çš„/etc/mysql/conf.dæ–‡ä»¶å¤¹é‡Œé¢æ”¹mysqlçš„é…ç½®æ–‡ä»¶å°±åªéœ€è¦æŠŠmysqlé…ç½®æ–‡ä»¶æ”¾åœ¨è‡ªå®šä¹‰çš„æ–‡ä»¶å¤¹ä¸‹ï¼ˆ/conf/mysqlï¼‰docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ciæŒ‡å®šmysqlçš„ä¸€äº›é…ç½®å‚æ•°]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2è¯¦ç»†ä»‹ç»]]></title>
    <url>%2F2019%2F04%2F23%2Fspark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Sparkå‘å¸ƒäº†æœ€æ–°çš„ç‰ˆæœ¬spark-2.4.2æ ¹æ®å®˜ç½‘ä»‹ç»ï¼Œæ­¤ç‰ˆæœ¬å¯¹äºŽä½¿ç”¨spark2.4çš„ç”¨æˆ·æ¥è¯´å¸®åŠ©æ˜¯å·¨å¤§çš„ç‰ˆæœ¬ä»‹ç»Spark2.4.2æ˜¯ä¸€ä¸ªåŒ…å«ç¨³å®šæ€§ä¿®å¤çš„ç»´æŠ¤ç‰ˆæœ¬ã€‚ æ­¤ç‰ˆæœ¬åŸºäºŽSpark2.4ç»´æŠ¤åˆ†æ”¯ã€‚ æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‰€æœ‰2.4ç”¨æˆ·å‡çº§åˆ°æ­¤ç¨³å®šç‰ˆæœ¬ã€‚æ˜¾è‘—çš„å˜åŒ–SPARK-27419ï¼šåœ¨spark2.4ä¸­å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äºŽ1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥ã€‚ å› ä¸ºè¯¥å€¼å°†è½¬æ¢ä¸º0ï¼Œå¿ƒè·³å°†å§‹ç»ˆè¶…æ—¶ï¼Œå¹¶æœ€ç»ˆç»ˆæ­¢æ‰§è¡Œç¨‹åºã€‚è¿˜åŽŸSPARK-25250ï¼šå¯èƒ½å¯¼è‡´ä½œä¸šæ°¸ä¹…æŒ‚èµ·ï¼Œåœ¨2.4.2ä¸­è¿˜åŽŸã€‚è¯¦ç»†æ›´æ”¹BUGissueså†…å®¹æ‘˜è¦[ SPARK-26961 ]åœ¨Spark Driverä¸­å‘çŽ°Javaæ­»é”[ SPARK-26998 ]åœ¨Standaloneæ¨¡å¼ä¸‹æ‰§è¡Œâ€™ps -efâ€™ç¨‹åºè¿›ç¨‹,è¾“å‡ºspark.ssl.keyStorePasswordçš„æ˜Žæ–‡[ SPARK-27216 ]å°†RoaringBitmapå‡çº§åˆ°0.7.45ä»¥ä¿®å¤Kryoä¸å®‰å…¨çš„ser / dseré—®é¢˜[ SPARK-27244 ]ä½¿ç”¨é€‰é¡¹logConf = trueæ—¶å¯†ç å°†ä»¥confçš„æ˜Žæ–‡å½¢å¼è®°å½•[ SPARK-27267 ]ç”¨Snappy 1.1.7.1è§£åŽ‹ã€åŽ‹ç¼©ç©ºåºåˆ—åŒ–æ•°æ®æ—¶å¤±è´¥[ SPARK-27275 ]EncryptedMessage.transferToä¸­çš„æ½œåœ¨æŸå[ SPARK-27301 ]DStreamCheckpointDataå› æ–‡ä»¶ç³»ç»Ÿå·²ç¼“å­˜è€Œæ— æ³•æ¸…ç†[ SPARK-27338 ]TaskMemoryManagerå’ŒUnsafeExternalSorter $ SpillableIteratorä¹‹é—´çš„æ­»é”[ SPARK-27351 ]åœ¨ä»…ä½¿ç”¨ç©ºå€¼åˆ—çš„AggregateEstimationä¹‹åŽçš„é”™è¯¯outputRowsä¼°è®¡[ SPARK-27390 ]ä¿®å¤åŒ…åç§°ä¸åŒ¹é…[ SPARK-27394 ]å½“æ²¡æœ‰ä»»åŠ¡å¼€å§‹æˆ–ç»“æŸæ—¶ï¼ŒUI çš„é™ˆæ—§æ€§å¯èƒ½æŒç»­æ•°åˆ†é’Ÿæˆ–æ•°å°æ—¶[ SPARK-27403 ]ä¿®å¤updateTableStatsä»¥ä½¿ç”¨æ–°ç»Ÿè®¡ä¿¡æ¯æˆ–æ— æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯[ SPARK-27406 ]å½“ä¸¤å°æœºå™¨å…·æœ‰ä¸åŒçš„Oopså¤§å°æ—¶ï¼ŒUnsafeArrayDataåºåˆ—åŒ–ä¼šä¸­æ–­[ SPARK-27419 ]å°†spark.executor.heartbeatIntervalè®¾ç½®ä¸ºå°äºŽ1ç§’çš„å€¼æ—¶ï¼Œå®ƒå°†å§‹ç»ˆå¤±è´¥[ SPARK-27453 ]DSV1é™é»˜åˆ é™¤DataFrameWriter.partitionByæ”¹è¿›issueså†…å®¹æ‘˜è¦[ SPARK-27346 ]æ¾å¼€åœ¨ExpressionInfoçš„â€™examplesâ€™å­—æ®µä¸­æ¢è¡Œæ–­è¨€æ¡ä»¶[ SPARK-27358 ]å°†jqueryæ›´æ–°ä¸º1.12.xä»¥èŽ·å–å®‰å…¨ä¿®å¤ç¨‹åº[ SPARK-27479 ]éšè—â€œorg.apache.spark.util.kvstoreâ€çš„APIæ–‡æ¡£å·¥ä½œissueså†…å®¹æ‘˜è¦[ SPARK-27382 ]åœ¨HiveExternalCatalogVersionsSuiteä¸­æ›´æ–°Spark 2.4.xæµ‹è¯•]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘å¸Kafka+Flink+MySQLç”Ÿäº§å®Œæ•´æ¡ˆä¾‹ä»£ç ]]></title>
    <url>%2F2018%2F12%2F20%2F%E6%88%91%E5%8F%B8Kafka%2BFlink%2BMySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1.ç‰ˆæœ¬ä¿¡æ¯ï¼šFlink Version:1.6.2Kafka Version:0.9.0.0MySQL Version:5.6.212.Kafka æ¶ˆæ¯æ ·ä¾‹åŠæ ¼å¼ï¼š[IP TIME URL STATU_CODE REFERER]11.74.103.143 2018-12-20 18:12:00 &quot;GET /class/130.html HTTP/1.1&quot; 404 https://search.yahoo.com/search?p=Flinkå®žæˆ˜3.å·¥ç¨‹pom.xml12345678910111213141516171819202122232425262728&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;flink.version&gt;1.6.2&lt;/flink.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Flink-Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt;4.sConfç±» å®šä¹‰ä¸ŽMySQLè¿žæŽ¥çš„JDBCçš„å‚æ•°1234567891011package com.soul.conf;/** * @author è‹¥æ³½æ•°æ®soulChun * @create 2018-12-20-15:11 */public class sConf &#123; public static final String USERNAME = &quot;root&quot;; public static final String PASSWORD = &quot;www.ruozedata.com&quot;; public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;&#125;5.MySQLSlinkç±»123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.soul.kafka;import com.soul.conf.sConf;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * @author è‹¥æ³½æ•°æ®soulChun * @create 2018-12-20-15:09 */public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123; try &#123; if (connection == null) &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setString(3, value.f2); preparedStatement.setString(4, value.f3); preparedStatement.setString(5, value.f4); System.out.println(&quot;Start insert&quot;); preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void open(Configuration parms) throws Exception &#123; Class.forName(sConf.DRIVERNAME); connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD); &#125; public void close() throws Exception &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;6.æ•°æ®æ¸…æ´—æ—¥æœŸå·¥å…·ç±»1234567891011121314151617181920212223package com.soul.utils;import org.apache.commons.lang3.time.FastDateFormat;import java.util.Date;/** * @author soulChun * @create 2018-12-19-18:44 */public class DateUtils &#123; private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;); private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;); public static Long getTime(String time) throws Exception&#123; return SOURCE_FORMAT.parse(time).getTime(); &#125; public static String parseMinute(String time) throws Exception&#123; return TARGET_FORMAT.format(new Date(getTime(time))); &#125; //æµ‹è¯•ä¸€ä¸‹ public static void main(String[] args) throws Exception&#123; String time = &quot;2018-12-19 18:55:00&quot;; System.out.println(parseMinute(time)); &#125;&#125;7.MySQLå»ºè¡¨123456789create table log_info(ID INT NOT NULL AUTO_INCREMENT,IP VARCHAR(50),TIME VARCHAR(50),CourseID VARCHAR(10),Status_Code VARCHAR(10),Referer VARCHAR(100),PRIMARY KEY ( ID ))ENGINE=InnoDB DEFAULT CHARSET=utf8;8.ä¸»ç¨‹åºï¼šä¸»è¦æ˜¯å°†timeçš„æ ¼å¼è½¬æˆyyyyMMddHHmmss,è¿˜æœ‰å–URLä¸­çš„è¯¾ç¨‹IDï¼Œå°†ä¸æ˜¯/classå¼€å¤´çš„è¿‡æ»¤æŽ‰ã€‚12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.soul.kafka;import com.soul.utils.DateUtils;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;import java.util.Properties;/** * @author soulChun * @create 2018-12-19-17:23 */public class FlinkCleanKafka &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafkaçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·åˆ†éš” properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeperçš„èŠ‚ç‚¹çš„IPæˆ–è€…hostNameï¼Œå¤šä¸ªä½¿ç”¨é€—å·è¿›è¡Œåˆ†éš” properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flinkçš„æ¶ˆè´¹è€…çš„group.id FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties); DataStream&lt;String&gt; stream = env.addSource(myConsumer);// stream.print().setParallelism(2); DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123; String[] data = value.split(&quot;\\\t&quot;); String CourseID = null; String url = data[2].split(&quot;\\ &quot;)[2]; if (url.startsWith(&quot;/class&quot;)) &#123; String CourseHTML = url.split(&quot;\\/&quot;)[2]; CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));// System.out.println(CourseID); &#125; return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]); &#125; &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123; return value.f2 != null; &#125; &#125;); CleanData.addSink(new MySQLSink()); env.execute(&quot;Flink kafka&quot;); &#125;&#125;9.å¯åŠ¨ä¸»ç¨‹åºï¼ŒæŸ¥çœ‹MySQLè¡¨æ•°æ®åœ¨é€’å¢ž123456mysql&gt; select count(*) from log_info;+----------+| count(*) |+----------+| 15137 |+----------+Kafkaè¿‡æ¥çš„æ¶ˆæ¯æ˜¯æˆ‘æ¨¡æ‹Ÿçš„ï¼Œä¸€åˆ†é’Ÿäº§ç”Ÿ100æ¡ã€‚ä»¥ä¸Šæ˜¯æˆ‘å¸ç”Ÿäº§é¡¹ç›®ä»£ç çš„æŠ½å–å‡ºæ¥çš„æ¡ˆä¾‹ä»£ç V1ã€‚ç¨åŽè¿˜æœ‰WaterMarkä¹‹ç±»ä¼šåšåˆ†äº«ã€‚]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœ€å…¨çš„Flinkéƒ¨ç½²åŠå¼€å‘æ¡ˆä¾‹(KafkaSource+SinkToMySQL)]]></title>
    <url>%2F2018%2F11%2F10%2F%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource%2BSinkToMySQL)%2F</url>
    <content type="text"><![CDATA[1.ä¸‹è½½Flinkå®‰è£…åŒ…flinkä¸‹è½½åœ°å€https://archive.apache.org/dist/flink/flink-1.5.0/å› ä¸ºä¾‹å­ä¸éœ€è¦hadoopï¼Œä¸‹è½½flink-1.5.0-bin-scala_2.11.tgzå³å¯ä¸Šä¼ è‡³æœºå™¨çš„/optç›®å½•ä¸‹2.è§£åŽ‹tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/3.é…ç½®masterèŠ‚ç‚¹é€‰æ‹©ä¸€ä¸ª masterèŠ‚ç‚¹(JobManager)ç„¶åŽåœ¨conf/flink-conf.yamlä¸­è®¾ç½®jobmanager.rpc.address é…ç½®é¡¹ä¸ºè¯¥èŠ‚ç‚¹çš„IP æˆ–è€…ä¸»æœºåã€‚ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹æœ‰æœ‰ä¸€æ ·çš„jobmanager.rpc.address é…ç½®ã€‚jobmanager.rpc.address: node1(é…ç½®ç«¯å£å¦‚æžœè¢«å ç”¨ä¹Ÿè¦æ”¹ å¦‚é»˜è®¤8080å·²ç»è¢«sparkå ç”¨ï¼Œæ”¹æˆäº†8088)rest.port: 8088æœ¬æ¬¡å®‰è£… masterèŠ‚ç‚¹ä¸ºnode1ï¼Œå› ä¸ºå•æœºï¼ŒslaveèŠ‚ç‚¹ä¹Ÿä¸ºnode14.é…ç½®slaveså°†æ‰€æœ‰çš„ worker èŠ‚ç‚¹ ï¼ˆTaskManagerï¼‰çš„IP æˆ–è€…ä¸»æœºåï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰å¡«å…¥conf/slaves æ–‡ä»¶ä¸­ã€‚5.å¯åŠ¨flinké›†ç¾¤bin/start-cluster.shæ‰“å¼€ http://node1:8088 æŸ¥çœ‹webé¡µé¢Task Managersä»£è¡¨å½“å‰çš„flinkåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªtaskè¿˜æœ‰ä¸¤ä¸ªslots6.æµ‹è¯•ä¾èµ–123456789101112131415161718192021222324252627&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;7.Socketæµ‹è¯•ä»£ç 12345678910111213141516171819202122232425262728293031public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; final String hostName; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(&quot;port&quot;); hostName = params.get(&quot;hostname&quot;); &#125; catch (Exception e) &#123; System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(&quot;\\s&quot;)) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + &quot; : &quot; + count; &#125; &#125;&#125;æ‰“åŒ…mvn clean install (å¦‚æžœæ‰“åŒ…è¿‡ç¨‹ä¸­æŠ¥é”™java.lang.OutOfMemoryError)åœ¨å‘½ä»¤è¡Œset MAVEN_OPTS= -Xms128m -Xmx512mç»§ç»­æ‰§è¡Œmvn clean installç”ŸæˆFlinkTest.jaræ‰¾åˆ°æ‰“æˆçš„jarï¼Œå¹¶uploadï¼Œå¼€å§‹ä¸Šä¼ è¿è¡Œå‚æ•°ä»‹ç»æäº¤ç»“æŸä¹‹åŽåŽ»overviewç•Œé¢çœ‹ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå¯ç”¨çš„slotså˜æˆäº†ä¸€ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬çš„socketç¨‹åºå ç”¨äº†ä¸€ä¸ªï¼Œæ­£åœ¨runningçš„jobå˜æˆäº†ä¸€ä¸ªå‘é€æ•°æ®12345[root@hadoop000 flink-1.5.0]# nc -l 8099aaa bbbaaa cccaaa bbbbbb cccç‚¹å¼€runningçš„jobï¼Œä½ å¯ä»¥çœ‹è§æŽ¥æ”¶çš„å­—èŠ‚æ•°ç­‰ä¿¡æ¯åˆ°logç›®å½•ä¸‹å¯ä»¥æ¸…æ¥šçš„çœ‹è§è¾“å‡º1234567891011[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.outaaa : 1ccc : 1ccc : 1bbb : 1ccc : 1bbb : 1bbb : 1ccc : 1bbb : 1ccc : 1é™¤äº†å¯ä»¥åœ¨ç•Œé¢æäº¤ï¼Œè¿˜å¯ä»¥å°†jarä¸Šä¼ çš„linuxä¸­è¿›è¡Œæäº¤ä»»åŠ¡è¿è¡Œflinkä¸Šä¼ çš„jar1bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1å…¶ä»–æ­¥éª¤ä¸€è‡´ã€‚8.ä½¿ç”¨kafkaä½œä¸ºsourceåŠ ä¸Šä¾èµ–1234&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;1234567891011121314151617public class KakfaSource010 &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); //å¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…åˆé€‚çš„topic FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties); //é…ç½®ä»Žæœ€æ–°çš„åœ°æ–¹å¼€å§‹æ¶ˆè´¹ kafkaSource.setStartFromLatest(); //ä½¿ç”¨addsourceï¼Œå°†kafkaçš„è¾“å…¥è½¬å˜ä¸ºdatastream DataStream&lt;String&gt; consume = env.addSource(wordfre); ... //process and sink env.execute(&quot;KakfaSource010&quot;); &#125;&#125;9.ä½¿ç”¨mysqlä½œä¸ºsinkflinkæœ¬èº«å¹¶æ²¡æœ‰æä¾›datastreamè¾“å‡ºåˆ°mysqlï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±åŽ»å®žçŽ°é¦–å…ˆï¼Œå¯¼å…¥ä¾èµ–12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt;è‡ªå®šä¹‰sinkï¼Œé¦–å…ˆæƒ³åˆ°çš„æ˜¯extends SinkFunctionï¼Œé›†æˆflinkè‡ªå¸¦çš„sinkfunctionï¼Œå†å½“ä¸­å®žçŽ°æ–¹æ³•ï¼Œå®žçŽ°å¦‚ä¸‹12345678910111213141516171819202122public class MysqlSink implements SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Connection connection; private PreparedStatement preparedStatement; String username = &quot;mysql.user&quot;; String password = &quot;mysql.password&quot;; String drivername = &quot;mysql.driver&quot;; String dburl = &quot;mysql.url&quot;; @Override public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125;è¿™æ ·å®žçŽ°æœ‰ä¸ªé—®é¢˜ï¼Œæ¯ä¸€æ¡æ•°æ®ï¼Œéƒ½è¦æ‰“å¼€mysqlè¿žæŽ¥ï¼Œå†å…³é—­ï¼Œæ¯”è¾ƒè€—æ—¶ï¼Œè¿™ä¸ªå¯ä»¥ä½¿ç”¨flinkä¸­æ¯”è¾ƒå¥½çš„Richæ–¹å¼æ¥å®žçŽ°ï¼Œä»£ç å¦‚ä¸‹12345678910111213141516171819202122232425262728public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement preparedStatement = null; private String userName = null; private String password = null; private String driverName = null; private String DBUrl = null; public MysqlSink() &#123; userName = &quot;mysql.username&quot;; password = &quot;mysql.password&quot;; driverName = &quot;mysql.driverName&quot;; DBUrl = &quot;mysql.DBUrl&quot;; &#125; public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123; if(connection==null)&#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; String sql =&quot;insert into table(name,nickname) values(?,?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,value.f0); preparedStatement.setString(2,value.f1); preparedStatement.executeUpdate();//è¿”å›žæˆåŠŸçš„è¯å°±æ˜¯ä¸€ä¸ªï¼Œå¦åˆ™å°±æ˜¯0 &#125; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName(driverName); connection = DriverManager.getConnection(DBUrl, userName, password); &#125; @Override public void close() throws Exception &#123; if(preparedStatement!=null)&#123; preparedStatement.close(); &#125; if(connection!=null)&#123; connection.close(); &#125; &#125;&#125;Richæ–¹å¼çš„ä¼˜ç‚¹åœ¨äºŽï¼Œæœ‰ä¸ªopenå’Œcloseæ–¹æ³•ï¼Œåœ¨åˆå§‹åŒ–çš„æ—¶å€™å»ºç«‹ä¸€æ¬¡è¿žæŽ¥ï¼Œä¹‹åŽä¸€ç›´ä½¿ç”¨è¿™ä¸ªè¿žæŽ¥å³å¯ï¼Œç¼©çŸ­å»ºç«‹å’Œå…³é—­è¿žæŽ¥çš„æ—¶é—´ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿žæŽ¥æ± å®žçŽ°ï¼Œè¿™é‡Œåªæ˜¯æä¾›è¿™æ ·ä¸€ç§æ€è·¯ã€‚ä½¿ç”¨è¿™ä¸ªmysqlsinkä¹Ÿéžå¸¸ç®€å•1//ç›´æŽ¥addsinkï¼Œå³å¯è¾“å‡ºåˆ°è‡ªå®šä¹‰çš„mysqlä¸­ï¼Œä¹Ÿå¯ä»¥å°†mysqlçš„å­—æ®µç­‰å†™æˆå¯é…ç½®çš„ï¼Œæ›´åŠ æ–¹ä¾¿å’Œé€šç”¨proceDataStream.addSink(new MysqlSink());10.æ€»ç»“æœ¬æ¬¡çš„ç¬”è®°åšäº†ç®€å•çš„éƒ¨ç½²ã€æµ‹è¯•ã€kafkademoï¼Œä»¥åŠè‡ªå®šä¹‰å®žçŽ°mysqlsinkçš„ä¸€äº›å†…å®¹ï¼Œå…¶ä¸­æ¯”è¾ƒé‡è¦çš„æ˜¯Richçš„ä½¿ç”¨ï¼Œå¸Œæœ›å¤§å®¶èƒ½æœ‰æ‰€æ”¶èŽ·ã€‚]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§å¼€å‘å¿…ç”¨-Spark RDDè½¬DataFrameçš„ä¸¤ç§æ–¹æ³•]]></title>
    <url>%2F2018%2F06%2F14%2F%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[æœ¬ç¯‡æ–‡ç« å°†ä»‹ç»Spark SQLä¸­çš„DataFrameï¼Œå…³äºŽDataFrameçš„ä»‹ç»å¯ä»¥å‚è€ƒ:https://blog.csdn.net/lemonzhaotao/article/details/80211231åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œå°†ä»‹ç»RDDè½¬æ¢ä¸ºDataFrameçš„2ç§æ–¹å¼å®˜ç½‘ä¹‹RDDè½¬DF:http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rddsDataFrame ä¸Ž RDD çš„äº¤äº’Spark SQLå®ƒæ”¯æŒä¸¤ç§ä¸åŒçš„æ–¹å¼è½¬æ¢å·²ç»å­˜åœ¨çš„RDDåˆ°DataFrameæ–¹æ³•ä¸€ç¬¬ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨åå°„çš„æ–¹å¼ï¼Œç”¨åå°„åŽ»æŽ¨å€’å‡ºæ¥RDDé‡Œé¢çš„schemaã€‚è¿™ä¸ªæ–¹å¼ç®€å•ï¼Œä½†æ˜¯ä¸å»ºè®®ä½¿ç”¨ï¼Œå› ä¸ºåœ¨å·¥ä½œå½“ä¸­ï¼Œä½¿ç”¨è¿™ç§æ–¹å¼æ˜¯æœ‰é™åˆ¶çš„ã€‚å¯¹äºŽä»¥å‰çš„ç‰ˆæœ¬æ¥è¯´ï¼Œcase classæœ€å¤šæ”¯æŒ22ä¸ªå­—æ®µå¦‚æžœè¶…è¿‡äº†22ä¸ªå­—æ®µï¼Œæˆ‘ä»¬å°±å¿…é¡»è¦è‡ªå·±å¼€å‘ä¸€ä¸ªç±»ï¼Œå®žçŽ°productæŽ¥å£æ‰è¡Œã€‚å› æ­¤è¿™ç§æ–¹å¼è™½ç„¶ç®€å•ï¼Œä½†æ˜¯ä¸é€šç”¨ï¼›å› ä¸ºç”Ÿäº§ä¸­çš„å­—æ®µæ˜¯éžå¸¸éžå¸¸å¤šçš„ï¼Œæ˜¯ä¸å¯èƒ½åªæœ‰20æ¥ä¸ªå­—æ®µçš„ã€‚ç¤ºä¾‹ï¼š12345678910111213141516171819202122/** * convert rdd to dataframe 1 * @param spark */private def runInferSchemaExample(spark:SparkSession): Unit =&#123; import spark.implicits._ val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;) val df = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; People(x(0), x(1).trim.toInt)) //å°†rddçš„æ¯ä¸€è¡Œéƒ½è½¬æ¢æˆäº†ä¸€ä¸ªpeople .toDF //å¿…é¡»å…ˆå¯¼å…¥import spark.implicits._ ä¸ç„¶è¿™ä¸ªæ–¹æ³•ä¼šæŠ¥é”™ df.show() df.createOrReplaceTempView(&quot;people&quot;) // è¿™ä¸ªDFåŒ…å«äº†ä¸¤ä¸ªå­—æ®µnameå’Œage val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;) // teenager(0)ä»£è¡¨ç¬¬ä¸€ä¸ªå­—æ®µ // å–å€¼çš„ç¬¬ä¸€ç§æ–¹å¼ï¼šindex from zero teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show() // å–å€¼çš„ç¬¬äºŒç§æ–¹å¼ï¼šbyName teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()&#125;// æ³¨æ„ï¼šcase classå¿…é¡»å®šä¹‰åœ¨mainæ–¹æ³•ä¹‹å¤–ï¼›å¦åˆ™ä¼šæŠ¥é”™case class People(name:String, age:Int)æ–¹æ³•äºŒåˆ›å»ºä¸€ä¸ªDataFrameï¼Œä½¿ç”¨ç¼–ç¨‹çš„æ–¹å¼ è¿™ä¸ªæ–¹å¼ç”¨çš„éžå¸¸å¤šã€‚é€šè¿‡ç¼–ç¨‹æ–¹å¼æŒ‡å®šschema ï¼Œå¯¹äºŽç¬¬ä¸€ç§æ–¹å¼çš„schemaå…¶å®žå®šä¹‰åœ¨äº†case classé‡Œé¢äº†ã€‚å®˜ç½‘è§£è¯»ï¼šå½“æˆ‘ä»¬çš„case classä¸èƒ½æå‰å®šä¹‰(å› ä¸ºä¸šåŠ¡å¤„ç†çš„è¿‡ç¨‹å½“ä¸­ï¼Œä½ çš„å­—æ®µå¯èƒ½æ˜¯åœ¨å˜åŒ–çš„),å› æ­¤ä½¿ç”¨case classå¾ˆéš¾åŽ»æå‰å®šä¹‰ã€‚ä½¿ç”¨è¯¥æ–¹å¼åˆ›å»ºDFçš„ä¸‰å¤§æ­¥éª¤ï¼šCreate an RDD of Rows from the original RDD;Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.ç¤ºä¾‹ï¼š1234567891011121314151617181920212223242526/** * convert rdd to dataframe 2 * @param spark */private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123; // 1.è½¬æˆRDD val rdd = spark.sparkContext.textFile(&quot;E:/å¤§æ•°æ®/data/people.txt&quot;) // 2.å®šä¹‰schemaï¼Œå¸¦æœ‰StructTypeçš„ // å®šä¹‰schemaä¿¡æ¯ val schemaString = &quot;name age&quot; // å¯¹schemaä¿¡æ¯æŒ‰ç©ºæ ¼è¿›è¡Œåˆ†å‰² // æœ€ç»ˆfiledsé‡ŒåŒ…å«äº†2ä¸ªStructField val fields = schemaString.split(&quot; &quot;) // å­—æ®µç±»åž‹ï¼Œå­—æ®µåç§°åˆ¤æ–­æ˜¯ä¸æ˜¯ä¸ºç©º .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // 3.æŠŠæˆ‘ä»¬çš„schemaä¿¡æ¯ä½œç”¨åˆ°RDDä¸Š // è¿™ä¸ªRDDé‡Œé¢åŒ…å«äº†ä¸€äº›è¡Œ // å½¢æˆRowç±»åž‹çš„RDD val rowRDD = rdd.map(_.split(&quot;,&quot;)) .map(x =&gt; Row(x(0), x(1).trim)) // é€šè¿‡SparkSessionåˆ›å»ºä¸€ä¸ªDataFrame // ä¼ è¿›æ¥ä¸€ä¸ªrowRDDå’Œschemaï¼Œå°†schemaä½œç”¨åˆ°rowRDDä¸Š val peopleDF = spark.createDataFrame(rowRDD, schema) peopleDF.show()&#125;[æ‰©å±•]ç”Ÿäº§ä¸Šåˆ›å»ºDataFrameçš„ä»£ç ä¸¾ä¾‹åœ¨å®žé™…ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å…¶å®žé€‰æ‹©çš„æ˜¯æ–¹å¼äºŒè¿™ç§è¿›è¡Œåˆ›å»ºDataFrameçš„ï¼Œè¿™é‡Œå°†å±•ç¤ºéƒ¨åˆ†ä»£ç ï¼šSchemaçš„å®šä¹‰1234567891011121314151617181920212223242526272829303132333435363738394041object AccessConvertUtil &#123; val struct = StructType( Array( StructField(&quot;url&quot;,StringType), StructField(&quot;cmsType&quot;,StringType), StructField(&quot;cmsId&quot;,LongType), StructField(&quot;traffic&quot;,LongType), StructField(&quot;ip&quot;,StringType), StructField(&quot;city&quot;,StringType), StructField(&quot;time&quot;,StringType), StructField(&quot;day&quot;,StringType) ) ) /** * æ ¹æ®è¾“å…¥çš„æ¯ä¸€è¡Œä¿¡æ¯è½¬æ¢æˆè¾“å‡ºçš„æ ·å¼ */ def parseLog(log:String) = &#123; try &#123; val splits = log.split(&quot;\t&quot;) val url = splits(1) val traffic = splits(2).toLong val ip = splits(3) val domain = &quot;http://www.imooc.com/&quot; val cms = url.substring(url.indexOf(domain) + domain.length) val cmsTypeId = cms.split(&quot;/&quot;) var cmsType = &quot;&quot; var cmsId = 0l if (cmsTypeId.length &gt; 1) &#123; cmsType = cmsTypeId(0) cmsId = cmsTypeId(1).toLong &#125; val city = IpUtils.getCity(ip) val time = splits(0) val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;) //è¿™ä¸ªRowé‡Œé¢çš„å­—æ®µè¦å’Œstructä¸­çš„å­—æ®µå¯¹åº”ä¸Š Row(url, cmsType, cmsId, traffic, ip, city, time, day) &#125; catch &#123; case e: Exception =&gt; Row(0) &#125; &#125;&#125;åˆ›å»ºDataFrame1234567891011121314object SparkStatCleanJob &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;) .master(&quot;local[2]&quot;).getOrCreate() val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;) //accessRDD.take(10).foreach(println) //RDD ==&gt; DFï¼Œåˆ›å»ºç”ŸæˆDataFrame val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct) accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite) .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;) spark.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½ å¤§çˆ·æ°¸è¿œæ˜¯ä½ å¤§çˆ·ï¼ŒRDDè¡€ç¼˜å…³ç³»æºç è¯¦è§£ï¼]]></title>
    <url>%2F2018%2F06%2F13%2F%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81%2F</url>
    <content type="text"><![CDATA[ä¸€ã€RDDçš„ä¾èµ–å…³ç³»RDDçš„ä¾èµ–å…³ç³»åˆ†ä¸ºä¸¤ç±»ï¼šå®½ä¾èµ–å’Œçª„ä¾èµ–ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¤ä¸ºï¼šï¼ˆ1ï¼‰çª„ä¾èµ–ï¼šæ¯ä¸ªparent RDD çš„ partition æœ€å¤šè¢« child RDD çš„ä¸€ä¸ªpartition ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰å®½ä¾èµ–ï¼šæ¯ä¸ªparent RDD partition è¢«å¤šä¸ª child RDD çš„partition ä½¿ç”¨ã€‚çª„ä¾èµ–æ¯ä¸ª child RDD çš„ partition çš„ç”Ÿæˆæ“ä½œéƒ½æ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œè€Œå®½ä¾èµ–åˆ™éœ€è¦æ‰€æœ‰çš„ parent RDD partition shuffle ç»“æžœå¾—åˆ°åŽå†è¿›è¡Œã€‚äºŒã€org.apache.spark.Dependency.scala æºç è§£æžDependencyæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼š1234// Denpendency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125;å®ƒæœ‰ä¸¤ä¸ªå­ç±»ï¼šNarrowDependency å’Œ ShuffleDenpendencyï¼Œåˆ†åˆ«å¯¹åº”çª„ä¾èµ–å’Œå®½ä¾èµ–ã€‚ï¼ˆ1ï¼‰NarrowDependencyä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»å®šä¹‰äº†æŠ½è±¡æ–¹æ³•getParentsï¼Œè¾“å…¥partitionIdï¼Œç”¨äºŽèŽ·å¾—child RDD çš„æŸä¸ªpartitionä¾èµ–çš„parent RDDçš„æ‰€æœ‰ partitionsã€‚1234567891011// Denpendency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125;çª„ä¾èµ–åˆæœ‰ä¸¤ä¸ªå…·ä½“çš„å®žçŽ°ï¼šOneToOneDependencyå’ŒRangeDependencyã€‚ï¼ˆaï¼‰OneToOneDependencyæŒ‡child RDDçš„partitionåªä¾èµ–äºŽparent RDD çš„ä¸€ä¸ªpartitionï¼Œäº§ç”ŸOneToOneDependencyçš„ç®—å­æœ‰mapï¼Œfilterï¼ŒflatMapç­‰ã€‚å¯ä»¥çœ‹åˆ°getParentså®žçŽ°å¾ˆç®€å•ï¼Œå°±æ˜¯ä¼ è¿›åŽ»ä¸€ä¸ªpartitionIdï¼Œå†æŠŠpartitionIdæ”¾åœ¨Listé‡Œé¢ä¼ å‡ºåŽ»ã€‚1234567891011121314151617// Denpendency.scalaclass OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; ï¼ˆbï¼‰RangeDependencyæŒ‡child RDD partitionåœ¨ä¸€å®šçš„èŒƒå›´å†…ä¸€å¯¹ä¸€çš„ä¾èµ–äºŽparent RDD partitionï¼Œä¸»è¦ç”¨äºŽunionã€‚// Denpendency.scalaclass RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123;//inStartè¡¨ç¤ºparent RDDçš„å¼€å§‹ç´¢å¼•ï¼ŒoutStartè¡¨ç¤ºchild RDD çš„å¼€å§‹ç´¢å¼• override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart)//è¡¨ç¤ºäºŽå½“å‰ç´¢å¼•çš„ç›¸å¯¹ä½ç½® &#125; else &#123; Nil &#125; &#125;&#125;ï¼ˆ2ï¼‰ShuffleDependencyæŒ‡å®½ä¾èµ–è¡¨ç¤ºä¸€ä¸ªparent RDDçš„partitionä¼šè¢«child RDDçš„partitionä½¿ç”¨å¤šæ¬¡ã€‚éœ€è¦ç»è¿‡shuffleæ‰èƒ½å½¢æˆã€‚123456789101112131415161718192021// Denpendency.scalaclass ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123; //shuffleéƒ½æ˜¯åŸºäºŽPairRDDè¿›è¡Œçš„ï¼Œæ‰€ä»¥ä¼ å…¥çš„RDDè¦æ˜¯key-valueç±»åž‹çš„ override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) //èŽ·å–shuffleId val shuffleId: Int = _rdd.context.newShuffleId() //å‘shuffleManageræ³¨å†Œshuffleä¿¡æ¯ val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;ç”±äºŽshuffleæ¶‰åŠåˆ°ç½‘ç»œä¼ è¾“ï¼Œæ‰€ä»¥è¦æœ‰åºåˆ—åŒ–serializerï¼Œä¸ºäº†å‡å°‘ç½‘ç»œä¼ è¾“ï¼Œå¯ä»¥mapç«¯èšåˆï¼Œé€šè¿‡mapSideCombineå’ŒaggregatoræŽ§åˆ¶ï¼Œè¿˜æœ‰keyæŽ’åºç›¸å…³çš„keyOrderingï¼Œä»¥åŠé‡è¾“å‡ºçš„æ•°æ®å¦‚ä½•åˆ†åŒºçš„partitionerï¼Œè¿˜æœ‰ä¸€äº›classä¿¡æ¯ã€‚Partitionä¹‹é—´çš„å…³ç³»åœ¨shuffleå¤„æˆ›ç„¶è€Œæ­¢ï¼Œå› æ­¤shuffleæ˜¯åˆ’åˆ†stageçš„ä¾æ®ã€‚ä¸‰ã€ä¸¤ç§ä¾èµ–çš„åŒºåˆ†é¦–å…ˆï¼Œçª„ä¾èµ–å…è®¸åœ¨ä¸€ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Šä»¥æµæ°´çº¿çš„æ–¹å¼ï¼ˆpipelineï¼‰è®¡ç®—æ‰€æœ‰çˆ¶åˆ†åŒºã€‚ä¾‹å¦‚ï¼Œé€ä¸ªå…ƒç´ åœ°æ‰§è¡Œmapã€ç„¶åŽfilteræ“ä½œï¼›è€Œå®½ä¾èµ–åˆ™éœ€è¦é¦–å…ˆè®¡ç®—å¥½æ‰€æœ‰çˆ¶åˆ†åŒºæ•°æ®ï¼Œç„¶åŽåœ¨èŠ‚ç‚¹ä¹‹é—´è¿›è¡ŒShuffleï¼Œè¿™ä¸ŽMapReduceç±»ä¼¼ã€‚ç¬¬äºŒï¼Œçª„ä¾èµ–èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤±æ•ˆèŠ‚ç‚¹çš„æ¢å¤ï¼Œå³åªéœ€é‡æ–°è®¡ç®—ä¸¢å¤±RDDåˆ†åŒºçš„çˆ¶åˆ†åŒºï¼Œè€Œä¸”ä¸åŒèŠ‚ç‚¹ä¹‹é—´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼›è€Œå¯¹äºŽä¸€ä¸ªå®½ä¾èµ–å…³ç³»çš„Lineageå›¾ï¼Œå•ä¸ªèŠ‚ç‚¹å¤±æ•ˆå¯èƒ½å¯¼è‡´è¿™ä¸ªRDDçš„æ‰€æœ‰ç¥–å…ˆä¸¢å¤±éƒ¨åˆ†åˆ†åŒºï¼Œå› è€Œéœ€è¦æ•´ä½“é‡æ–°è®¡ç®—ã€‚]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark æŠ€æœ¯å›¢é˜Ÿå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow]]></title>
    <url>%2F2018%2F06%2F12%2FApache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow%2F</url>
    <content type="text"><![CDATA[è¿‘æ—¥ï¼Œæ¥è‡ª Databricks çš„ Matei Zaharia å®£å¸ƒæŽ¨å‡ºå¼€æºæœºå™¨å­¦ä¹ å¹³å° MLflow ã€‚Matei Zaharia æ˜¯ Apache Spark å’Œ Apache Mesos çš„æ ¸å¿ƒä½œè€…ï¼Œä¹Ÿæ˜¯ Databrick çš„é¦–å¸­æŠ€æœ¯ä¸“å®¶ã€‚Databrick æ˜¯ç”± Apache Spark æŠ€æœ¯å›¢é˜Ÿæ‰€åˆ›ç«‹çš„å•†ä¸šåŒ–å…¬å¸ã€‚MLflow ç›®å‰å·²å¤„äºŽæ—©æœŸæµ‹è¯•é˜¶æ®µï¼Œå¼€å‘è€…å¯ä¸‹è½½æºç ä½“éªŒã€‚Matei Zaharia è¡¨ç¤ºå½“å‰åœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ çš„å…¬å¸æ™®éå­˜åœ¨å·¥å…·è¿‡å¤šã€éš¾ä»¥è·Ÿè¸ªå®žéªŒã€éš¾ä»¥é‡çŽ°ç»“æžœã€éš¾ä»¥éƒ¨ç½²ç­‰é—®é¢˜ã€‚ä¸ºè®©æœºå™¨å­¦ä¹ å¼€å‘å˜å¾—ä¸Žä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸€æ ·å¼ºå¤§ã€å¯é¢„æµ‹å’Œæ™®åŠï¼Œè®¸å¤šä¼ä¸šå·²å¼€å§‹æž„å»ºå†…éƒ¨æœºå™¨å­¦ä¹ å¹³å°æ¥ç®¡ç† MLç”Ÿå‘½å‘¨æœŸã€‚åƒæ˜¯ Facebookã€Google å’Œ Uber å°±å·²åˆ†åˆ«æž„å»ºäº† FBLearner Flowã€TFX å’Œ Michelangelo æ¥ç®¡ç†æ•°æ®ã€æ¨¡åž‹åŸ¹è®­å’Œéƒ¨ç½²ã€‚ä¸è¿‡ç”±äºŽè¿™äº›å†…éƒ¨å¹³å°å­˜åœ¨å±€é™æ€§å’Œç»‘å®šæ€§ï¼Œæ— æ³•å¾ˆå¥½åœ°ä¸Žç¤¾åŒºå…±äº«æˆæžœï¼Œå…¶ä»–ç”¨æˆ·ä¹Ÿæ— æ³•è½»æ˜“ä½¿ç”¨ã€‚MLflow æ­£æ˜¯å—çŽ°æœ‰çš„ ML å¹³å°å¯å‘ï¼Œä¸»æ‰“å¼€æ”¾æ€§ï¼šå¼€æ”¾æŽ¥å£ï¼šå¯ä¸Žä»»æ„ ML åº“ã€ç®—æ³•ã€éƒ¨ç½²å·¥å…·æˆ–ç¼–ç¨‹è¯­è¨€ä¸€èµ·ä½¿ç”¨ã€‚å¼€æºï¼šå¼€å‘è€…å¯è½»æ¾åœ°å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œå¹¶è·¨ç»„ç»‡å…±äº«å·¥ä½œæµæ­¥éª¤å’Œæ¨¡åž‹ã€‚MLflow ç›®å‰çš„ alpha ç‰ˆæœ¬åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šå…¶ä¸­ï¼ŒMLflow Trackingï¼ˆè·Ÿè¸ªç»„ä»¶ï¼‰æä¾›äº†ä¸€ç»„ API å’Œç”¨æˆ·ç•Œé¢ï¼Œç”¨äºŽåœ¨è¿è¡Œæœºå™¨å­¦ä¹ ä»£ç æ—¶è®°å½•å’ŒæŸ¥è¯¢å‚æ•°ã€ä»£ç ç‰ˆæœ¬ã€æŒ‡æ ‡å’Œè¾“å‡ºæ–‡ä»¶ï¼Œä»¥ä¾¿ä»¥åŽå¯è§†åŒ–å®ƒä»¬ã€‚1234567891011121314import mlflow# Log parameters (key-value pairs)mlflow.log_param(&quot;num_dimensions&quot;, 8)mlflow.log_param(&quot;regularization&quot;, 0.1)# Log a metric; metrics can be updated throughout the runmlflow.log_metric(&quot;accuracy&quot;, 0.1)...mlflow.log_metric(&quot;accuracy&quot;, 0.45)# Log artifacts (output files)mlflow.log_artifact(&quot;roc.png&quot;)mlflow.log_artifact(&quot;model.pkl&quot;)MLflow Projectsï¼ˆé¡¹ç›®ç»„ä»¶ï¼‰æä¾›äº†æ‰“åŒ…å¯é‡ç”¨æ•°æ®ç§‘å­¦ä»£ç çš„æ ‡å‡†æ ¼å¼ã€‚æ¯ä¸ªé¡¹ç›®éƒ½åªæ˜¯ä¸€ä¸ªåŒ…å«ä»£ç æˆ– Git å­˜å‚¨åº“çš„ç›®å½•ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªæè¿°ç¬¦æ–‡ä»¶æ¥æŒ‡å®šå®ƒçš„ä¾èµ–å…³ç³»ä»¥åŠå¦‚ä½•è¿è¡Œä»£ç ã€‚æ¯ä¸ª MLflow é¡¹ç›®éƒ½æ˜¯ç”±ä¸€ä¸ªç®€å•çš„åä¸º MLproject çš„ YAML æ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰ã€‚123456789101112name: My Projectconda_env: conda.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot; validate: parameters: data_file: path command: &quot;python validate.py &#123;data_file&#125;&quot;MLflow Modelsï¼ˆæ¨¡åž‹ç»„ä»¶ï¼‰æä¾›äº†ä¸€ç§ç”¨å¤šç§æ ¼å¼æ‰“åŒ…æœºå™¨å­¦ä¹ æ¨¡åž‹çš„è§„èŒƒï¼Œè¿™äº›æ ¼å¼è¢«ç§°ä¸º â€œflavorâ€ ã€‚MLflow æä¾›äº†å¤šç§å·¥å…·æ¥éƒ¨ç½²ä¸åŒ flavor çš„æ¨¡åž‹ã€‚æ¯ä¸ª MLflow æ¨¡åž‹è¢«ä¿å­˜æˆä¸€ä¸ªç›®å½•ï¼Œç›®å½•ä¸­åŒ…å«äº†ä»»æ„æ¨¡åž‹æ–‡ä»¶å’Œä¸€ä¸ª MLmodel æè¿°ç¬¦æ–‡ä»¶ï¼Œæ–‡ä»¶ä¸­åˆ—å‡ºäº†ç›¸åº”çš„ flavor ã€‚12345678time_created: 2018-02-21T13:21:34.12flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn pickled_model: model.pkl]]></content>
      <categories>
        <category>Spark MLlib</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL ä¹‹å¤–éƒ¨æ•°æ®æºå¦‚ä½•æˆä¸ºåœ¨ä¼ä¸šå¼€å‘ä¸­çš„ä¸€æŠŠåˆ©å™¨]]></title>
    <url>%2F2018%2F06%2F06%2FSpark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 æ¦‚è¿°1.Spark1.2ä¸­ï¼ŒSpark SQLå¼€å§‹æ­£å¼æ”¯æŒå¤–éƒ¨æ•°æ®æºã€‚Spark SQLå¼€æ”¾äº†ä¸€ç³»åˆ—æŽ¥å…¥å¤–éƒ¨æ•°æ®æºçš„æŽ¥å£ï¼Œæ¥è®©å¼€å‘è€…å¯ä»¥å®žçŽ°ã€‚ä½¿å¾—Spark SQLå¯ä»¥åŠ è½½ä»»ä½•åœ°æ–¹çš„æ•°æ®ï¼Œä¾‹å¦‚mysqlï¼Œhiveï¼Œhdfsï¼Œhbaseç­‰ï¼Œè€Œä¸”æ”¯æŒå¾ˆå¤šç§æ ¼å¼å¦‚json, parquet, avro, csvæ ¼å¼ã€‚æˆ‘ä»¬å¯ä»¥å¼€å‘å‡ºä»»æ„çš„å¤–éƒ¨æ•°æ®æºæ¥è¿žæŽ¥åˆ°Spark SQLï¼Œç„¶åŽæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIæ¥è¿›è¡Œæ“ä½œã€‚2.æˆ‘ä»¬é€šè¿‡å¤–éƒ¨æ•°æ®æºAPIè¯»å–å„ç§æ ¼å¼çš„æ•°æ®ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªDataFrameï¼Œè¿™æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„æ–¹å¼å•Šï¼Œå°±å¯ä»¥ä½¿ç”¨DataFrameçš„APIæˆ–è€…SQLçš„APIè¿›è¡Œæ“ä½œå“ˆã€‚3.å¤–éƒ¨æ•°æ®æºçš„APIå¯ä»¥è‡ªåŠ¨åšä¸€äº›åˆ—çš„è£å‰ªï¼Œä»€ä¹ˆå«åˆ—çš„è£å‰ªï¼Œå‡å¦‚ä¸€ä¸ªuserè¡¨æœ‰id,name,age,gender4ä¸ªåˆ—ï¼Œåœ¨åšselectçš„æ—¶å€™ä½ åªéœ€è¦id,nameè¿™ä¸¤åˆ—ï¼Œé‚£ä¹ˆå…¶ä»–åˆ—ä¼šé€šè¿‡åº•å±‚çš„ä¼˜åŒ–åŽ»ç»™æˆ‘ä»¬è£å‰ªæŽ‰ã€‚4.ä¿å­˜æ“ä½œå¯ä»¥é€‰æ‹©ä½¿ç”¨SaveModeï¼ŒæŒ‡å®šå¦‚ä½•ä¿å­˜çŽ°æœ‰æ•°æ®ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰ã€‚2.è¯»å–jsonæ–‡ä»¶å¯åŠ¨shellè¿›è¡Œæµ‹è¯•1234567891011121314151617181920//æ ‡å‡†å†™æ³•val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)//å¦å¤–ä¸€ç§å†™æ³•spark.read.json(&quot;path&quot;)çœ‹çœ‹æºç è¿™ä¸¤è€…ä¹‹é—´åˆ°åº•æœ‰å•¥ä¸åŒå‘¢ï¼Ÿ/** * Loads a JSON file and returns the results as a `DataFrame`. * * See the documentation on the overloaded `json()` method with varargs for more details. * * @since 1.4.0 */ def json(path: String): DataFrame = &#123; // This method ensures that calls that explicit need single argument works, see SPARK-16009 json(Seq(path): _*) &#125;æˆ‘ä»¬è°ƒç”¨josn() æ–¹æ³•å…¶å®žè¿›è¡Œäº† overloaded ï¼Œæˆ‘ä»¬ç»§ç»­æŸ¥çœ‹ def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*) è¿™å¥è¯æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œå…¶å®žå°±æ˜¯æˆ‘ä»¬çš„æ ‡å‡†å†™æ³•1234567891011121314151617 scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+3 è¯»å–parquetæ•°æ®12345678910val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+4 è¯»å–hiveä¸­çš„æ•°æ®1234567891011121314151617181920212223242526272829303132spark.sql(&quot;show tables&quot;).show+--------+----------+-----------+|database| tableName|isTemporary|+--------+----------+-----------+| default|states_raw| false|| default|states_seq| false|| default| t1| false|+--------+----------+-----------+spark.table(&quot;states_raw&quot;).show+-----+------+| code| name|+-----+------+|hello| java||hello|hadoop||hello| hive||hello| sqoop||hello| hdfs||hello| spark|+-----+------+scala&gt; spark.sql(&quot;select name from states_raw &quot;).show+------+| name|+------+| java||hadoop|| hive|| sqoop|| hdfs|| spark|+------+5 ä¿å­˜æ•°æ®æ³¨æ„ï¼šä¿å­˜çš„æ–‡ä»¶å¤¹ä¸èƒ½å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™(é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å¼)ï¼šorg.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;ä¿å­˜æˆæ–‡æœ¬æ ¼å¼ï¼Œåªèƒ½ä¿å­˜ä¸€åˆ—ï¼Œå¦åˆ™æŠ¥é”™ï¼šorg.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;123456789101112131415161718192021222324252627282930val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)//ä¿å­˜df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)ç»“æžœï¼š[hadoop@hadoop out]$ pwd/home/hadoop/data/out[hadoop@hadoop out]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:34 _SUCCESS[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt MichaelAndyJustin//ä¿å­˜ä¸ºjsonæ ¼å¼df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)ç»“æžœ[hadoop@hadoop data]$ cd out1[hadoop@hadoop out1]$ lltotal 4-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json-rw-r--r--. 1 hadoop hadoop 0 Apr 24 00:35 _SUCCESS[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;ä¸Šé¢è¯´äº†åœ¨ä¿å­˜æ•°æ®æ—¶å¦‚æžœç›®å½•å·²ç»å­˜åœ¨ï¼Œåœ¨é»˜è®¤æ¨¡å¼ä¸‹ä¼šæŠ¥é”™ï¼Œé‚£æˆ‘ä»¬ä¸‹é¢è®²è§£ä¿å­˜çš„å‡ ç§æ¨¡å¼ï¼š6 è¯»å–mysqlä¸­çš„æ•°æ®1234567891011121314151617181920212223val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;).option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123456&quot;).load()scala&gt; jdbcDF.printSchemaroot |-- TBL_ID: long (nullable = false) |-- CREATE_TIME: integer (nullable = false) |-- DB_ID: long (nullable = true) |-- LAST_ACCESS_TIME: integer (nullable = false) |-- OWNER: string (nullable = true) |-- RETENTION: integer (nullable = false) |-- SD_ID: long (nullable = true) |-- TBL_NAME: string (nullable = true) |-- TBL_TYPE: string (nullable = true) |-- VIEW_EXPANDED_TEXT: string (nullable = true) |-- VIEW_ORIGINAL_TEXT: string (nullable = true)jdbcDF.show7 spark SQLæ“ä½œmysqlè¡¨æ•°æ®123456789101112131415161718192021222324252627282930313233CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url &quot;jdbc:mysql://localhost:3306&quot;, dbtable &quot;basic01.tbls&quot;, user &apos;root&apos;, password &apos;123456&apos;, driver &quot;com.mysql.jdbc.Driver&quot;);æŸ¥çœ‹ï¼šshow tables;default states_raw falsedefault states_seq falsedefault t1 falsejdbctable trueselect * from jdbctable;1 1519944170 6 0 hadoop 0 1 page_views MANAGED_TABLE NULL NULL2 1519944313 6 0 hadoop 0 2 page_views_bzip2 MANAGED_TABLE NULL NULL3 1519944819 6 0 hadoop 0 3 page_views_snappy MANAGED_TABLE NULL NULL21 1520067771 6 0 hadoop 0 21 tt MANAGED_TABLE NULL NULL22 1520069148 6 0 hadoop 0 22 page_views_seq MANAGED_TABLE NULL NULL23 1520071381 6 0 hadoop 0 23 page_views_rcfile MANAGED_TABLE NULL NULL24 1520074675 6 0 hadoop 0 24 page_views_orc_zlib MANAGED_TABLE NULL NULL27 1520078184 6 0 hadoop 0 27 page_views_lzo_index MANAGED_TABLE NULL NULL30 1520083461 6 0 hadoop 0 30 page_views_lzo_index1 MANAGED_TABLE NULL NULL31 1524370014 1 0 hadoop 0 31 t1 EXTERNAL_TABLE NULL NULL37 1524468636 1 0 hadoop 0 37 states_raw MANAGED_TABLE NULL NULL38 1524468678 1 0 hadoop 0 38 states_seq MANAGED_TABLE NULL NULLmysqlä¸­çš„tblsçš„æ•°æ®å·²ç»å­˜åœ¨jdbctableè¡¨ä¸­äº†ã€‚jdbcDF.show8 åˆ†åŒºæŽ¨æµ‹ï¼ˆPartition Discoveryï¼‰è¡¨åˆ†åŒºæ˜¯åœ¨åƒHiveè¿™æ ·çš„ç³»ç»Ÿä¸­ä½¿ç”¨çš„å¸¸è§ä¼˜åŒ–æ–¹æ³•ã€‚ åœ¨åˆ†åŒºè¡¨ä¸­ï¼Œæ•°æ®é€šå¸¸å­˜å‚¨åœ¨ä¸åŒçš„ç›®å½•ä¸­ï¼Œåˆ†åŒºåˆ—å€¼åœ¨æ¯ä¸ªåˆ†åŒºç›®å½•çš„è·¯å¾„ä¸­ç¼–ç ã€‚ æ‰€æœ‰å†…ç½®çš„æ–‡ä»¶æºï¼ˆåŒ…æ‹¬Text / CSV / JSON / ORC / Parquetï¼‰éƒ½èƒ½å¤Ÿè‡ªåŠ¨å‘çŽ°å’ŒæŽ¨æ–­åˆ†åŒºä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ›å»ºå¦‚ä¸‹çš„ç›®å½•ç»“æž„;123456789hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CNæ·»åŠ jsonæ–‡ä»¶ï¼špeople.json &#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CNæˆ‘ä»¬ä½¿ç”¨spark sqlè¯»å–å¤–éƒ¨æ•°æ®æºï¼š1234567891011121314151617val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+æˆ‘ä»¬æ”¹å˜è¯»å–çš„ç›®å½•12345678910111213141516val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)scala&gt; df.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true) |-- country: string (nullable = true)scala&gt; df.show+----+-------+-------+| age| name|country|+----+-------+-------+|null|Michael| CN|| 30| Andy| CN|| 19| Justin| CN|+----+-------+-------+å¤§å®¶æœ‰æ²¡æœ‰å‘çŽ°ä»€ä¹ˆå‘¢ï¼ŸSpark SQLå°†è‡ªåŠ¨ä»Žè·¯å¾„ä¸­æå–åˆ†åŒºä¿¡æ¯ã€‚æ³¨æ„ï¼Œåˆ†åŒºåˆ—çš„æ•°æ®ç±»åž‹æ˜¯è‡ªåŠ¨æŽ¨æ–­çš„ã€‚ç›®å‰æ”¯æŒæ•°å­—æ•°æ®ç±»åž‹ï¼Œæ—¥æœŸï¼Œæ—¶é—´æˆ³å’Œå­—ç¬¦ä¸²ç±»åž‹ã€‚æœ‰æ—¶ç”¨æˆ·å¯èƒ½ä¸æƒ³è‡ªåŠ¨æŽ¨æ–­åˆ†åŒºåˆ—çš„æ•°æ®ç±»åž‹ã€‚å¯¹äºŽè¿™äº›ç”¨ä¾‹ï¼Œè‡ªåŠ¨ç±»åž‹æŽ¨æ–­å¯ä»¥é€šè¿‡spark.sql.sources.partitionColumnTypeInference.enabledè¿›è¡Œé…ç½®ï¼Œé»˜è®¤ä¸ºtrueã€‚å½“ç¦ç”¨ç±»åž‹æŽ¨æ–­æ—¶ï¼Œå­—ç¬¦ä¸²ç±»åž‹å°†ç”¨äºŽåˆ†åŒºåˆ—ã€‚ä»ŽSpark 1.6.0å¼€å§‹ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ†åŒºå‘çŽ°ä»…åœ¨ç»™å®šè·¯å¾„ä¸‹æ‰¾åˆ°åˆ†åŒºã€‚å¯¹äºŽä¸Šé¢çš„ç¤ºä¾‹ï¼Œå¦‚æžœç”¨æˆ·å°†è·¯å¾„/table/gender=maleä¼ é€’ç»™SparkSession.read.parquetæˆ–SparkSession.read.loadï¼Œåˆ™ä¸ä¼šå°†æ€§åˆ«è§†ä¸ºåˆ†åŒºåˆ—ã€‚å¦‚æžœç”¨æˆ·éœ€è¦æŒ‡å®šå¯åŠ¨åˆ†åŒºå‘çŽ°çš„åŸºæœ¬è·¯å¾„ï¼Œåˆ™å¯ä»¥basePathåœ¨æ•°æ®æºé€‰é¡¹ä¸­è¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼Œå½“path/to/table/gender=maleæ˜¯æ•°æ®è·¯å¾„å¹¶ä¸”ç”¨æˆ·å°†basePathè®¾ç½®ä¸ºpath/to/table/æ—¶ï¼Œæ€§åˆ«å°†æ˜¯åˆ†åŒºåˆ—ã€‚]]></content>
      <categories>
        <category>Spark SQL</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming çŠ¶æ€ç®¡ç†å‡½æ•°çš„é€‰æ‹©æ¯”è¾ƒ]]></title>
    <url>%2F2018%2F06%2F06%2FSparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[ä¸€ã€updateStateByKeyå®˜ç½‘åŽŸè¯ï¼š1In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.ä¹Ÿå³æ˜¯è¯´å®ƒä¼šç»Ÿè®¡å…¨å±€çš„keyçš„çŠ¶æ€ï¼Œå°±ç®—æ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œå®ƒä¹Ÿä¼šåœ¨æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶å€™è¿”å›žä¹‹å‰çš„keyçš„çŠ¶æ€ã€‚ç¼ºç‚¹ï¼šè‹¥æ•°æ®é‡å¤ªå¤§çš„è¯ï¼Œéœ€è¦checkpointçš„æ•°æ®ä¼šå ç”¨è¾ƒå¤§çš„å­˜å‚¨ï¼Œæ•ˆçŽ‡ä½Žä¸‹ã€‚ç¨‹åºç¤ºä¾‹å¦‚ä¸‹ï¼š12345678910111213141516171819202122232425262728object StatefulWordCountApp &#123; def main(args: Array[String]) &#123; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulWordCountApp&quot;) .setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(10)) //æ³¨æ„ï¼šè¦ä½¿ç”¨updateStateByKeyå¿…é¡»è®¾ç½®checkpointç›®å½• ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;) val lines = ssc.socketTextStream(&quot;bda3&quot;,9999) lines.flatMap(_.split(&quot;,&quot;)).map((_,1)) .updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; /*çŠ¶æ€æ›´æ–°å‡½æ•° * @param currentValues keyç›¸åŒvalueå½¢æˆçš„åˆ—è¡¨ * @param preValues keyå¯¹åº”çš„valueï¼Œå‰ä¸€çŠ¶æ€ * */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val curr = currentValues.sum //seqåˆ—è¡¨ä¸­æ‰€æœ‰valueæ±‚å’Œ val pre = preValues.getOrElse(0) //èŽ·å–ä¸Šä¸€çŠ¶æ€å€¼ Some(curr + pre) &#125; &#125;äºŒã€mapWithStatemapWithStateï¼šä¹Ÿæ˜¯ç”¨äºŽå…¨å±€ç»Ÿè®¡keyçš„çŠ¶æ€ï¼Œä½†æ˜¯å®ƒå¦‚æžœæ²¡æœ‰æ•°æ®è¾“å…¥ï¼Œä¾¿ä¸ä¼šè¿”å›žä¹‹å‰çš„keyçš„çŠ¶æ€ï¼Œæœ‰ä¸€ç‚¹å¢žé‡çš„æ„Ÿè§‰ã€‚æ•ˆçŽ‡æ›´é«˜ï¼Œç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨å®˜æ–¹ä»£ç å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132object StatefulNetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() val sparkConf = new SparkConf() .setAppName(&quot;StatefulNetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.checkpoint(&quot;.&quot;) val initialRDD = ssc.sparkContext .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1))) val lines = ssc.socketTextStream(args(0), args(1).toInt) val words = lines.flatMap(_.split(&quot; &quot;)) val wordDstream = words.map(x =&gt; (x, 1)) val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val stateDstream = wordDstream.mapWithState( StateSpec.function(mappingFunc).initialState(initialRDD)) stateDstream.print() ssc.start() ssc.awaitTermination() &#125; &#125;ä¸‰ã€æºç åˆ†æžupateStateByKeyï¼šmapè¿”å›žçš„æ˜¯MappedDStreamï¼Œè€ŒMappedDStreamå¹¶æ²¡æœ‰updateStateByKeyæ–¹æ³•ï¼Œå¹¶ä¸”å®ƒçš„çˆ¶ç±»DStreamä¸­ä¹Ÿæ²¡æœ‰è¯¥æ–¹æ³•ã€‚ä½†æ˜¯DStreamçš„ä¼´ç”Ÿå¯¹è±¡ä¸­æœ‰ä¸€ä¸ªéšå¼è½¬æ¢å‡½æ•°ï¼š123456object DStream &#123; implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairDStreamFunctions[K, V] = &#123; new PairDStreamFunctions[K, V](stream) &#125;è·Ÿè¿›åŽ» PairDStreamFunctions ï¼Œå‘çŽ°æœ€ç»ˆè°ƒç”¨çš„æ˜¯è‡ªå·±çš„updateStateByKeyã€‚å…¶ä¸­updateFuncå°±è¦ä¼ å…¥çš„å‚æ•°ï¼Œä»–æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒSeq[V]è¡¨ç¤ºå½“å‰keyå¯¹åº”çš„æ‰€æœ‰å€¼ï¼Œ123456Option[S] æ˜¯å½“å‰keyçš„åŽ†å²çŠ¶æ€ï¼Œè¿”å›žçš„æ˜¯æ–°çš„çŠ¶æ€ã€‚def updateStateByKey[S: ClassTag]( updateFunc: (Seq[V], Option[S]) =&gt; Option[S] ): DStream[(K, S)] = ssc.withScope &#123; updateStateByKey(updateFunc, defaultPartitioner())&#125;æœ€ç»ˆè°ƒç”¨ï¼š12345678910def updateStateByKey[S: ClassTag]( updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)], partitioner: Partitioner, rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123; val cleanedFunc = ssc.sc.clean(updateFunc) val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123; cleanedFunc(it) &#125; new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)&#125;å†è·Ÿè¿›åŽ» new StateDStream:åœ¨è¿™é‡Œé¢newå‡ºäº†ä¸€ä¸ªStateDStreamå¯¹è±¡ã€‚åœ¨å…¶computeæ–¹æ³•ä¸­ï¼Œä¼šå…ˆèŽ·å–ä¸Šä¸€ä¸ªbatchè®¡ç®—å‡ºçš„RDDï¼ˆåŒ…å«äº†è‡³ç¨‹åºå¼€å§‹åˆ°ä¸Šä¸€ä¸ªbatchå•è¯çš„ç´¯è®¡è®¡æ•°ï¼‰ï¼Œç„¶åŽåœ¨èŽ·å–æœ¬æ¬¡batchä¸­StateDStreamçš„çˆ¶ç±»è®¡ç®—å‡ºçš„RDDï¼ˆæœ¬æ¬¡batchçš„å•è¯è®¡æ•°ï¼‰åˆ†åˆ«æ˜¯prevStateRDDå’ŒparentRDDï¼Œç„¶åŽåœ¨è°ƒç”¨ computeUsingPreviousRDD æ–¹æ³•ï¼š1234567891011121314151617181920private [this] def computeUsingPreviousRDD( batchTime: Time, parentRDD: RDD[(K, V)], prevStateRDD: RDD[(K, S)]) = &#123; // Define the function for the mapPartition operation on cogrouped RDD; // first map the cogrouped tuple to tuples of required type, // and then apply the update function val updateFuncLocal = updateFunc val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123; val i = iterator.map &#123; t =&gt; val itr = t._2._2.iterator val headOption = if (itr.hasNext) Some(itr.next()) else None (t._1, t._2._1.toSeq, headOption) &#125; updateFuncLocal(batchTime, i) &#125; val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner) val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning) Some(stateRDD)&#125;åœ¨è¿™é‡Œä¸¤ä¸ªRDDè¿›è¡Œcogroupç„¶åŽåº”ç”¨updateStateByKeyä¼ å…¥çš„å‡½æ•°ã€‚æˆ‘ä»¬çŸ¥é“cogroupçš„æ€§èƒ½æ˜¯æ¯”è¾ƒä½Žä¸‹ï¼Œå‚è€ƒhttp://lxw1234.com/archives/2015/07/384.htmã€‚mapWithState:123456789@Experimentaldef mapWithState[StateType: ClassTag, MappedType: ClassTag]( spec: StateSpec[K, V, StateType, MappedType] ): MapWithStateDStream[K, V, StateType, MappedType] = &#123; new MapWithStateDStreamImpl[K, V, StateType, MappedType]( self, spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]] )&#125;è¯´æ˜Žï¼šStateSpec å°è£…äº†çŠ¶æ€ç®¡ç†å‡½æ•°ï¼Œå¹¶åœ¨è¯¥æ–¹æ³•ä¸­åˆ›å»ºäº†MapWithStateDStreamImplå¯¹è±¡ã€‚MapWithStateDStreamImpl ä¸­åˆ›å»ºäº†ä¸€ä¸ªInternalMapWithStateDStreamç±»åž‹å¯¹è±¡internalStreamï¼Œåœ¨MapWithStateDStreamImplçš„computeæ–¹æ³•ä¸­è°ƒç”¨äº†internalStreamçš„getOrComputeæ–¹æ³•ã€‚12345678910111213141516private[streaming] class MapWithStateDStreamImpl[ KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag]( dataStream: DStream[(KeyType, ValueType)], spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType]) extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123; private val internalStream = new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec) override def slideDuration: Duration = internalStream.slideDuration override def dependencies: List[DStream[_]] = List(internalStream) override def compute(validTime: Time): Option[RDD[MappedType]] = &#123; internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125; &#125;InternalMapWithStateDStreamä¸­æ²¡æœ‰getOrComputeæ–¹æ³•ï¼Œè¿™é‡Œè°ƒç”¨çš„æ˜¯å…¶çˆ¶ç±» DStream çš„getOrCpmputeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸­æœ€ç»ˆä¼šè°ƒç”¨InternalMapWithStateDStreamçš„Computeæ–¹æ³•ï¼š12345678910111213141516171819202122232425262728293031323334/** Method that generates an RDD for the given time */override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123; // Get the previous state or create a new empty state RDD val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123; case Some(rdd) =&gt; if (rdd.partitioner != Some(partitioner)) &#123; // If the RDD is not partitioned the right way, let us repartition it using the // partition index as the key. This is to ensure that state RDD is always partitioned // before creating another state RDD using it MapWithStateRDD.createFromRDD[K, V, S, E]( rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime) &#125; else &#123; rdd &#125; case None =&gt; MapWithStateRDD.createFromPairRDD[K, V, S, E]( spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)), partitioner, validTime ) &#125; // Compute the new state RDD with previous state RDD and partitioned data RDD // Even if there is no data RDD, use an empty one to create a new state RDD val dataRDD = parent.getOrCompute(validTime).getOrElse &#123; context.sparkContext.emptyRDD[(K, V)] &#125; val partitionedDataRDD = dataRDD.partitionBy(partitioner) val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt; (validTime - interval).milliseconds &#125; Some(new MapWithStateRDD( prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))&#125;æ ¹æ®ç»™å®šçš„æ—¶é—´ç”Ÿæˆä¸€ä¸ªMapWithStateRDDï¼Œé¦–å…ˆèŽ·å–äº†å…ˆå‰çŠ¶æ€çš„RDDï¼špreStateRDDå’Œå½“å‰æ—¶é—´çš„RDD:dataRDDï¼Œç„¶åŽå¯¹dataRDDåŸºäºŽå…ˆå‰çŠ¶æ€RDDçš„åˆ†åŒºå™¨è¿›è¡Œé‡æ–°åˆ†åŒºèŽ·å–partitionedDataRDDã€‚æœ€åŽå°†preStateRDDï¼ŒpartitionedDataRDDå’Œç”¨æˆ·å®šä¹‰çš„å‡½æ•°mappingFunctionä¼ ç»™æ–°ç”Ÿæˆçš„MapWithStateRDDå¯¹è±¡è¿”å›žã€‚åŽç»­è‹¥æœ‰å…´è¶£å¯ä»¥ç»§ç»­è·Ÿè¿›MapWithStateRDDçš„computeæ–¹æ³•ï¼Œé™äºŽç¯‡å¹…ä¸å†å±•ç¤ºã€‚]]></content>
      <categories>
        <category>Spark Streaming</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxç³»ç»Ÿé‡è¦å‚æ•°è°ƒä¼˜ï¼Œä½ çŸ¥é“å—]]></title>
    <url>%2F2018%2F06%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%2F</url>
    <content type="text"><![CDATA[å½“å‰ä¼šè¯ç”Ÿæ•ˆulimit -u -&gt; æŸ¥çœ‹å½“å‰æœ€å¤§è¿›ç¨‹æ•°ulimit -n -&gt;æŸ¥çœ‹å½“å‰æœ€å¤§æ–‡ä»¶æ•°ulimit -u xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§è¿›ç¨‹æ•°ä¸ºxxxulimit -n xxx -&gt; ä¿®æ”¹å½“å‰æœ€å¤§æ–‡ä»¶æ•°ä¸ºxxxæ°¸ä¹…ç”Ÿæ•ˆ1.vi /etc/security/limits.confï¼Œæ·»åŠ å¦‚ä¸‹çš„è¡Œsoft noproc 11000hard noproc 11000soft nofile 4100hard nofile 4100 è¯´æ˜Žï¼šä»£è¡¨é’ˆå¯¹æ‰€æœ‰ç”¨æˆ·noproc æ˜¯ä»£è¡¨æœ€å¤§è¿›ç¨‹æ•°nofile æ˜¯ä»£è¡¨æœ€å¤§æ–‡ä»¶æ‰“å¼€æ•°2.è®© SSH æŽ¥å— Login ç¨‹å¼çš„ç™»å…¥ï¼Œæ–¹ä¾¿åœ¨ ssh å®¢æˆ·ç«¯æŸ¥çœ‹ ulimit -a èµ„æºé™åˆ¶ï¼š1)ã€vi /etc/ssh/sshd_configæŠŠ UserLogin çš„å€¼æ”¹ä¸º yesï¼Œå¹¶æŠŠ # æ³¨é‡ŠåŽ»æŽ‰2)ã€é‡å¯ sshd æœåŠ¡/etc/init.d/sshd restart3)ã€ä¿®æ”¹æ‰€æœ‰ linux ç”¨æˆ·çš„çŽ¯å¢ƒå˜é‡æ–‡ä»¶ï¼švi /etc/profileulimit -u 10000ulimit -n 4096ulimit -d unlimitedulimit -m unlimitedulimit -s unlimitedulimit -t unlimitedulimit -v unlimited4)ã€ç”Ÿæ•ˆsource /etc/profile]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkåŠ¨æ€å†…å­˜ç®¡ç†æºç è§£æžï¼]]></title>
    <url>%2F2018%2F06%2F03%2FSpark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81%2F</url>
    <content type="text"><![CDATA[ä¸€ã€Sparkå†…å­˜ç®¡ç†æ¨¡å¼Sparkæœ‰ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œé™æ€å†…å­˜ç®¡ç†(Static MemoryManager)å’ŒåŠ¨æ€ï¼ˆç»Ÿä¸€ï¼‰å†…å­˜ç®¡ç†ï¼ˆUnified MemoryManagerï¼‰ã€‚åŠ¨æ€å†…å­˜ç®¡ç†ä»ŽSpark1.6å¼€å§‹å¼•å…¥ï¼Œåœ¨SparkEnv.scalaä¸­çš„æºç å¯ä»¥çœ‹åˆ°ï¼ŒSparkç›®å‰é»˜è®¤é‡‡ç”¨åŠ¨æ€å†…å­˜ç®¡ç†æ¨¡å¼ï¼Œè‹¥å°†spark.memory.useLegacyModeè®¾ç½®ä¸ºtrueï¼Œåˆ™ä¼šæ”¹ä¸ºé‡‡ç”¨é™æ€å†…å­˜ç®¡ç†ã€‚12345678// SparkEnv.scala val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false) val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) &#125;äºŒã€SparkåŠ¨æ€å†…å­˜ç®¡ç†ç©ºé—´åˆ†é…ç›¸æ¯”äºŽStatic MemoryManageræ¨¡å¼ï¼ŒUnified MemoryManageræ¨¡åž‹æ‰“ç ´äº†å­˜å‚¨å†…å­˜å’Œè¿è¡Œå†…å­˜çš„ç•Œé™ï¼Œä½¿æ¯ä¸€ä¸ªå†…å­˜åŒºèƒ½å¤ŸåŠ¨æ€ä¼¸ç¼©ï¼Œé™ä½ŽOOMçš„æ¦‚çŽ‡ã€‚ç”±ä¸Šå›¾å¯çŸ¥ï¼Œexecutor JVMå†…å­˜ä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªåŒºåŸŸç»„æˆï¼šï¼ˆ1ï¼‰Reserved Memoryï¼ˆé¢„ç•™å†…å­˜ï¼‰ï¼šè¿™éƒ¨åˆ†å†…å­˜é¢„ç•™ç»™ç³»ç»Ÿä½¿ç”¨ï¼Œé»˜è®¤ä¸º300MBï¼Œå¯é€šè¿‡spark.testing.reservedMemoryè¿›è¡Œè®¾ç½®ã€‚12// UnifiedMemoryManager.scalaprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024å¦å¤–ï¼ŒJVMå†…å­˜çš„æœ€å°å€¼ä¹Ÿä¸Žreserved Memoryæœ‰å…³ï¼Œå³minSystemMemory = reserved Memory1.5ï¼Œå³é»˜è®¤æƒ…å†µä¸‹JVMå†…å­˜æœ€å°å€¼ä¸º300MB1.5=450MBã€‚12// UnifiedMemoryManager.scala val minSystemMemory = (reservedMemory * 1.5).ceil.toLongï¼ˆ2ï¼‰Spark Memeoy:åˆ†ä¸ºexecution Memoryå’Œstorage Memoryã€‚åŽ»é™¤æŽ‰reserved Memoryï¼Œå‰©ä¸‹usableMemoryçš„ä¸€éƒ¨åˆ†ç”¨äºŽexecutionå’Œstorageè¿™ä¸¤ç±»å †å†…å­˜ï¼Œé»˜è®¤æ˜¯0.6ï¼Œå¯é€šè¿‡spark.memory.fractionè¿›è¡Œè®¾ç½®ã€‚ä¾‹å¦‚ï¼šJVMå†…å­˜æ˜¯1Gï¼Œé‚£ä¹ˆç”¨äºŽexecutionå’Œstorageçš„é»˜è®¤å†…å­˜ä¸ºï¼ˆ1024-300ï¼‰*0.6=434MBã€‚1234// UnifiedMemoryManager.scala val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6) (usableMemory * memoryFraction).toLongä»–ä»¬çš„è¾¹ç•Œç”±spark.memory.storageFractionè®¾å®šï¼Œé»˜è®¤ä¸º0.5ã€‚å³é»˜è®¤çŠ¶æ€ä¸‹storage Memoryå’Œexecution Memoryä¸º1ï¼š1.1234// UnifiedMemoryManager.scala onHeapStorageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)ï¼ˆ3ï¼‰user Memory:å‰©ä½™å†…å­˜ï¼Œç”¨æˆ·æ ¹æ®éœ€è¦ä½¿ç”¨ï¼Œé»˜è®¤å usableMemoryçš„ï¼ˆ1-0.6ï¼‰=0.4.ä¸‰ã€å†…å­˜æŽ§åˆ¶è¯¦è§£é¦–å…ˆæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹Sparkå†…å­˜ç®¡ç†å®žçŽ°ç±»ä¹‹å‰çš„å…³ç³»ã€‚1.MemoryManagerä¸»è¦åŠŸèƒ½æ˜¯ï¼šï¼ˆ1ï¼‰è®°å½•ç”¨äº†å¤šå°‘StorageMemoryå’ŒExecutionMemoryï¼›ï¼ˆ2ï¼‰ç”³è¯·Storageã€Executionå’ŒUnroll Memoryï¼›ï¼ˆ3ï¼‰é‡Šæ”¾Stroageå’ŒExecution Memoryã€‚Executionå†…å­˜ç”¨æ¥æ‰§è¡Œshuffleã€joinsã€sortså’Œaggegationsæ“ä½œï¼ŒStorageå†…å­˜ç”¨äºŽç¼“å­˜å’Œå¹¿æ’­æ•°æ®ï¼Œæ¯ä¸€ä¸ªJVMä¸­éƒ½å­˜åœ¨ç€ä¸€ä¸ªMemoryManagerã€‚æž„é€ MemoryManageréœ€è¦æŒ‡å®šonHeapStorageMemoryå’ŒonHeapExecutionMemoryå‚æ•°ã€‚123456 // MemoryManager.scalaprivate[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging &#123;åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPoolå¯¹è±¡ï¼Œç”¨æ¥åˆ›å»ºå †å†…æˆ–å †å¤–çš„Storageå’ŒExecutionå†…å­˜æ± ï¼Œç®¡ç†Storageå’ŒExecutionçš„å†…å­˜åˆ†é…ã€‚123456789// MemoryManager.scala @GuardedBy(&quot;this&quot;) protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(&quot;this&quot;) protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(&quot;this&quot;) protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ä½¿ç”¨å †å¤–å†…å­˜ï¼Œå¯é€šè¿‡saprk.memory.offHeap.enabledè®¾ç½®ï¼Œé»˜è®¤å †å¤–å†…å­˜ä¸º0ï¼Œå¯ä½¿ç”¨spark.memory.offHeap.sizeå‚æ•°è®¾ç½®ã€‚123456789101112// All the code you will ever need final val tungstenMemoryMode: MemoryMode = &#123; if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123; require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0, &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;) require(Platform.unaligned(), &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;) MemoryMode.OFF_HEAP &#125; else &#123; MemoryMode.ON_HEAP &#125; &#125;12// MemoryManager.scala protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)é‡Šæ”¾numByteså­—èŠ‚çš„Executionå†…å­˜æ–¹æ³•12345678910// MemoryManager.scaladef releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) &#125; &#125;é‡Šæ”¾æŒ‡å®štaskçš„æ‰€æœ‰Executionå†…å­˜å¹¶å°†è¯¥taskæ ‡è®°ä¸ºinactiveã€‚12345// MemoryManager.scala private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123; onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) &#125;é‡Šæ”¾numByteså­—èŠ‚çš„Stoargeå†…å­˜æ–¹æ³•1234567// MemoryManager.scaladef releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123; memoryMode match &#123; case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes) &#125; &#125;é‡Šæ”¾æ‰€æœ‰Storageå†…å­˜æ–¹æ³•12345// MemoryManager.scalafinal def releaseAllStorageMemory(): Unit = synchronized &#123; onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() &#125;2.æŽ¥ä¸‹æ¥æˆ‘ä»¬äº†è§£ä¸€ä¸‹ï¼ŒUnifiedMemoryManageræ˜¯å¦‚ä½•å¯¹å†…å­˜è¿›è¡ŒæŽ§åˆ¶çš„ï¼ŸåŠ¨æ€å†…å­˜æ˜¯å¦‚ä½•å®žçŽ°çš„å‘¢ï¼ŸUnifiedMemoryManageç»§æ‰¿äº†MemoryManager1234567891011// UnifiedMemoryManage.scalaprivate[spark] class UnifiedMemoryManager private[memory] ( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) &#123;é‡å†™äº†maxOnHeapStorageMemoryæ–¹æ³•ï¼Œæœ€å¤§Storageå†…å­˜=æœ€å¤§å†…å­˜-æœ€å¤§Executionå†…å­˜ã€‚1234// UnifiedMemoryManage.scala override def maxOnHeapStorageMemory: Long = synchronized &#123; maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed &#125;æ ¸å¿ƒæ–¹æ³•acquireStorageMemoryï¼šç”³è¯·Storageå†…å­˜ã€‚12345678910111213141516171819202122232425262728293031// UnifiedMemoryManage.scalaoverride def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized &#123; assertInvariants() assert(numBytes &gt;= 0) val (executionPool, storagePool, maxMemory) = memoryMode match &#123; //æ ¹æ®ä¸åŒçš„å†…å­˜æ¨¡å¼åŽ»åˆ›å»ºStorageMemoryPoolå’ŒExecutionMemoryPool case MemoryMode.ON_HEAP =&gt; ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP =&gt; ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapMemory) &#125; if (numBytes &gt; maxMemory) &#123; // è‹¥ç”³è¯·å†…å­˜å¤§äºŽæœ€å¤§å†…å­˜ï¼Œåˆ™ç”³è¯·å¤±è´¥ logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; + s&quot;memory limit ($maxMemory bytes)&quot;) return false &#125; if (numBytes &gt; storagePool.memoryFree) &#123; // å¦‚æžœStorageå†…å­˜æ± æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œåˆ™å‘Executionå†…å­˜æ± å€Ÿç”¨ val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//å½“Executionå†…å­˜æœ‰ç©ºé—²æ—¶ï¼ŒStorageæ‰èƒ½å€Ÿåˆ°å†…å­˜ executionPool.decrementPoolSize(memoryBorrowedFromExecution)//ç¼©å°Executionå†…å­˜ storagePool.incrementPoolSize(memoryBorrowedFromExecution)//å¢žåŠ Storageå†…å­˜ &#125; storagePool.acquireMemory(blockId, numBytes)æ ¸å¿ƒæ–¹æ³•acquireExecutionMemoryï¼šç”³è¯·Executionå†…å­˜ã€‚123456789// UnifiedMemoryManage.scalaoverride private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized &#123;//ä½¿ç”¨äº†synchronizedå…³é”®å­—ï¼Œè°ƒç”¨acquireExecutionMemoryæ–¹æ³•å¯èƒ½ä¼šé˜»å¡žï¼Œç›´åˆ°Executionå†…å­˜æ± æœ‰è¶³å¤Ÿçš„å†…å­˜ã€‚ ... executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize) &#125;æ–¹æ³•æœ€åŽè°ƒç”¨äº†ExecutionMemoryPoolçš„acquireMemoryæ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„å‚æ•°éœ€è¦ä¸¤ä¸ªå‡½æ•°ï¼šmaybeGrowExecutionPool()å’ŒcomputeMaxExecutionPoolSize()ã€‚æ¯ä¸ªTaskèƒ½å¤Ÿä½¿ç”¨çš„å†…å­˜è¢«é™åˆ¶åœ¨pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasksã€‚å…¶ä¸­maxPoolSizeä»£è¡¨äº†execution poolçš„æœ€å¤§å†…å­˜ï¼ŒpoolSizeè¡¨ç¤ºå½“å‰è¿™ä¸ªpoolçš„å¤§å°ã€‚1234// ExecutionMemoryPool.scala val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks)maybeGrowExecutionPool()æ–¹æ³•å®žçŽ°äº†å¦‚ä½•åŠ¨æ€å¢žåŠ Executionå†…å­˜åŒºçš„å¤§å°ã€‚åœ¨æ¯æ¬¡ç”³è¯·executionå†…å­˜çš„åŒæ—¶ï¼Œexecutionå†…å­˜æ± ä¼šè¿›è¡Œå¤šæ¬¡å°è¯•ï¼Œæ¯æ¬¡å°è¯•éƒ½å¯èƒ½ä¼šå›žæ”¶ä¸€äº›å­˜å‚¨å†…å­˜ã€‚123456789101112131415// UnifiedMemoryManage.scala def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123; if (extraMemoryNeeded &gt; 0) &#123;//å¦‚æžœç”³è¯·çš„å†…å­˜å¤§äºŽ0 //è®¡ç®—executionå¯å€Ÿåˆ°çš„storageå†…å­˜ï¼Œæ˜¯storageå‰©ä½™å†…å­˜å’Œå¯å€Ÿå‡ºå†…å­˜çš„æœ€å¤§å€¼ val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage &gt; 0) &#123;//å¦‚æžœå¯ä»¥ç”³è¯·åˆ°å†…å­˜ val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//å®žé™…éœ€è¦çš„å†…å­˜ï¼Œå–å®žé™…éœ€è¦çš„å†…å­˜å’Œstorageå†…å­˜åŒºåŸŸå…¨éƒ¨å¯ç”¨å†…å­˜å¤§å°çš„æœ€å°å€¼ storagePool.decrementPoolSize(spaceToReclaim)//storageå†…å­˜åŒºåŸŸå‡å°‘ executionPool.incrementPoolSize(spaceToReclaim)//executionå†…å­˜åŒºåŸŸå¢žåŠ  &#125; &#125; &#125;]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
        <tag>æºç é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½å¤§æ•°æ®-é›¶åŸºç¡€å­¦å‘˜æ·±åœ³æŸå¸é«˜è–ªé¢è¯•é¢˜]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[å•¥ä¹Ÿä¸è¯´ï¼ç›´æŽ¥ä¸Šé¢˜é¢è¯•æ—¶é—´ï¼š20180531ç®€å•è¯´ä¸‹hdfsè¯»æ–‡ä»¶å’Œå†™æ–‡ä»¶çš„æµç¨‹æ¯å¤©æ•°æ®é‡æœ‰å¤šå¤§ï¼Ÿç”Ÿäº§é›†ç¾¤è§„æ¨¡æœ‰å¤šå¤§ï¼Ÿè¯´å‡ ä¸ªsparkå¼€å‘ä¸­é‡åˆ°çš„é—®é¢˜ï¼Œå’Œè§£å†³çš„æ–¹æ¡ˆé˜è¿°ä¸€ä¸‹æœ€è¿‘å¼€å‘çš„é¡¹ç›®ï¼Œä»¥åŠæ‹…ä»»çš„è§’è‰²ä½ç½®kafkaæœ‰åšè¿‡å“ªäº›è°ƒä¼˜æˆ‘ä»¬é¡¹ç›®ä¸­æ•°æ®å€¾æ–œçš„åœºæ™¯å’Œè§£å†³æ–¹æ¡ˆé›¶åŸºç¡€âž•å››ä¸ªæœˆç´§è·Ÿè‹¥æ³½å¤§æ•°æ®å­¦ä¹ ä¹‹åŽæ˜¯è¿™æ ·]]></content>
      <categories>
        <category>é¢è¯•é¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»ŽHiveä¸­çš„stored as file_foramtçœ‹hiveè°ƒä¼˜]]></title>
    <url>%2F2018%2F05%2F30%2F%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[ä¸€ã€è¡Œå¼æ•°æ®åº“å’Œåˆ—å¼æ•°æ®åº“çš„å¯¹æ¯”1ã€å­˜å‚¨æ¯”è¾ƒè¡Œå¼æ•°æ®åº“å­˜å‚¨åœ¨hdfsä¸Šå¼æŒ‰è¡Œè¿›è¡Œå­˜å‚¨çš„ï¼Œä¸€ä¸ªblockå­˜å‚¨ä¸€æˆ–å¤šè¡Œæ•°æ®ã€‚è€Œåˆ—å¼æ•°æ®åº“åœ¨hdfsä¸Šåˆ™æ˜¯æŒ‰ç…§åˆ—è¿›è¡Œå­˜å‚¨ï¼Œä¸€ä¸ªblockå¯èƒ½æœ‰ä¸€åˆ—æˆ–å¤šåˆ—æ•°æ®ã€‚2ã€åŽ‹ç¼©æ¯”è¾ƒå¯¹äºŽè¡Œå¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰è¡ŒåŽ‹ç¼©ï¼Œå½“ä¸€è¡Œä¸­æœ‰å¤šä¸ªå­—æ®µï¼Œå„ä¸ªå­—æ®µå¯¹åº”çš„æ•°æ®ç±»åž‹å¯èƒ½ä¸ä¸€è‡´ï¼ŒåŽ‹ç¼©æ€§èƒ½åŽ‹ç¼©æ¯”å°±æ¯”è¾ƒå·®ã€‚å¯¹äºŽåˆ—å¼æ•°æ®åº“ï¼Œå¿…ç„¶æŒ‰åˆ—åŽ‹ç¼©ï¼Œæ¯ä¸€åˆ—å¯¹åº”çš„æ˜¯ç›¸åŒæ•°æ®ç±»åž‹çš„æ•°æ®ï¼Œæ•…åˆ—å¼æ•°æ®åº“çš„åŽ‹ç¼©æ€§èƒ½è¦å¼ºäºŽè¡Œå¼æ•°æ®åº“ã€‚3ã€æŸ¥è¯¢æ¯”è¾ƒå‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect id,name from table_emp;å¯¹äºŽè¡Œå¼æ•°æ®åº“ï¼Œå®ƒè¦éåŽ†ä¸€æ•´å¼ è¡¨å°†æ¯ä¸€è¡Œä¸­çš„id,nameå­—æ®µæ‹¼æŽ¥å†å±•çŽ°å‡ºæ¥ï¼Œè¿™æ ·éœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°±æ¯”è¾ƒå¤§ï¼Œæ•ˆçŽ‡ä½Žã€‚å¯¹äºŽåˆ—å¼æ•°æ®åº“ï¼Œå®ƒåªéœ€æ‰¾åˆ°å¯¹åº”çš„id,nameå­—æ®µçš„åˆ—å±•çŽ°å‡ºæ¥å³å¯ï¼Œéœ€è¦æŸ¥è¯¢çš„æ•°æ®é‡å°ï¼Œæ•ˆçŽ‡é«˜ã€‚å‡è®¾æ‰§è¡Œçš„æŸ¥è¯¢æ“ä½œæ˜¯ï¼šselect * from table_emp;å¯¹äºŽè¿™ç§æŸ¥è¯¢æ•´ä¸ªè¡¨å…¨éƒ¨ä¿¡æ¯çš„æ“ä½œï¼Œç”±äºŽåˆ—å¼æ•°æ®åº“éœ€è¦å°†åˆ†æ•£çš„è¡Œè¿›è¡Œé‡æ–°ç»„åˆï¼Œè¡Œå¼æ•°æ®åº“æ•ˆçŽ‡å°±é«˜äºŽåˆ—å¼æ•°æ®åº“ã€‚ä½†æ˜¯ï¼Œåœ¨å¤§æ•°æ®é¢†åŸŸï¼Œè¿›è¡Œå…¨è¡¨æŸ¥è¯¢çš„åœºæ™¯å°‘ä¹‹åˆå°‘ï¼Œè¿›è€Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå¤šçš„è¿˜æ˜¯åˆ—å¼æ•°æ®åº“åŠåˆ—å¼å‚¨å­˜ã€‚äºŒã€stored as file_format è¯¦è§£1ã€å»ºä¸€å¼ è¡¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨â€œstored as file_formatâ€æ¥æŒ‡å®šè¯¥è¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œhiveä¸­ï¼Œè¡¨çš„é»˜è®¤å­˜å‚¨æ ¼å¼ä¸ºTextFileã€‚123456789101112131415161718192021CREATE TABLE tt (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;CREATE TABLE tt2 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;CREATE TABLE tt3 (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;#ä»¥ä¸Šä¸‰ç§æ–¹å¼å­˜å‚¨çš„æ ¼å¼éƒ½æ˜¯TEXTFILEã€‚2ã€TEXTFILEã€SEQUENCEFILEã€RCFILEã€ORCç­‰å››ç§å‚¨å­˜æ ¼å¼åŠå®ƒä»¬å¯¹äºŽhiveåœ¨å­˜å‚¨æ•°æ®å’ŒæŸ¥è¯¢æ•°æ®æ—¶æ€§èƒ½çš„ä¼˜åŠ£æ¯”è¾ƒ12345678file_format: | SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameTEXTFILE: åªæ˜¯hiveä¸­è¡¨æ•°æ®é»˜è®¤çš„å­˜å‚¨æ ¼å¼ï¼Œå®ƒå°†æ‰€æœ‰ç±»åž‹çš„æ•°æ®éƒ½å­˜å‚¨ä¸ºStringç±»åž‹ï¼Œä¸ä¾¿äºŽæ•°æ®çš„è§£æžï¼Œä½†å®ƒå´æ¯”è¾ƒé€šç”¨ã€‚ä¸å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒåŽ‹ç¼©ã€‚SEQUENCEFILE: è¿™ç§å‚¨å­˜æ ¼å¼æ¯”TEXTFILEæ ¼å¼å¤šäº†å¤´éƒ¨ã€æ ‡è¯†ã€ä¿¡æ¯é•¿åº¦ç­‰ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä½¿å¾—å…¶å…·å¤‡éšæœºè¯»å†™çš„èƒ½åŠ›ã€‚æ”¯æŒåŽ‹ç¼©ï¼Œä½†åŽ‹ç¼©çš„æ˜¯valueã€‚ï¼ˆå­˜å‚¨ç›¸åŒçš„æ•°æ®ï¼ŒSEQUENCEFILEæ¯”TEXTFILEç•¥å¤§ï¼‰RCFILEï¼ˆRecord Columnar Fileï¼‰: çŽ°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¾ˆå¤šä¸ªRow Group,æ¯ä¸ªRow Groupé»˜è®¤å¤§å°4MBï¼ŒRow Groupå†…éƒ¨å†æŒ‰åˆ—å­˜å‚¨ä¿¡æ¯ã€‚ç”±facebookå¼€æºï¼Œæ¯”æ ‡å‡†è¡Œå¼å­˜å‚¨èŠ‚çº¦10%çš„ç©ºé—´ã€‚ORC: ä¼˜åŒ–è¿‡åŽçš„RCFile,çŽ°åœ¨æ°´å¹³ä¸Šåˆ’åˆ†ä¸ºå¤šä¸ªStripes,å†åœ¨Stripeä¸­æŒ‰åˆ—å­˜å‚¨ã€‚æ¯ä¸ªStripeç”±ä¸€ä¸ªIndex Dataã€ä¸€ä¸ªRow Dataã€ä¸€ä¸ªStripe Footerç»„æˆã€‚æ¯ä¸ªStripesçš„å¤§å°ä¸º250MBï¼Œæ¯ä¸ªIndex Dataè®°å½•çš„æ˜¯æ•´åž‹æ•°æ®æœ€å¤§å€¼æœ€å°å€¼ã€å­—ç¬¦ä¸²æ•°æ®å‰åŽç¼€ä¿¡æ¯ï¼Œæ¯ä¸ªåˆ—çš„ä½ç½®ç­‰ç­‰è¯¸å¦‚æ­¤ç±»çš„ä¿¡æ¯ã€‚è¿™å°±ä½¿å¾—æŸ¥è¯¢ååˆ†å¾—é«˜æ•ˆï¼Œé»˜è®¤æ¯ä¸€ä¸‡è¡Œæ•°æ®å»ºç«‹ä¸€ä¸ªIndex Dataã€‚ORCå­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„40%å·¦å³ï¼Œä½¿ç”¨åŽ‹ç¼©åˆ™å¯ä»¥è¿›ä¸€æ­¥å°†è¿™ä¸ªæ•°å­—é™åˆ°10%~20%ã€‚ORCè¿™ç§æ–‡ä»¶æ ¼å¼å¯ä»¥ä½œç”¨äºŽè¡¨æˆ–è€…è¡¨çš„åˆ†åŒºï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç§æ–¹å¼è¿›è¡ŒæŒ‡å®šï¼š123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=OrcThe parameters are all placed in the TBLPROPERTIES (see Create Table). They are:Key|Default|Notes|-|-|-|orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)|orc.compress.size|262,144|number of bytes in each compression chunk|orc.stripe.size|67,108,864|number of bytes in each stripe|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)|orc.create.index|true|whether to create row indexes|orc.bloom.filter.columns |â€â€| comma separated list of column names for which bloom filter should be created|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)ç¤ºä¾‹ï¼šåˆ›å»ºå¸¦åŽ‹ç¼©çš„ORCå­˜å‚¨è¡¨1234567create table Addresses ( name string, street string, city string, state string, zip int) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);PARQUET: å­˜å‚¨å¤§å°ä¸ºTEXTFILEçš„60%~70%ï¼ŒåŽ‹ç¼©åŽåœ¨20%~30%ä¹‹é—´ã€‚æ³¨æ„ï¼šä¸åŒçš„å­˜å‚¨æ ¼å¼ä¸ä»…è¡¨çŽ°åœ¨å­˜å‚¨ç©ºé—´ä¸Šçš„ä¸åŒï¼Œå¯¹äºŽæ•°æ®çš„æŸ¥è¯¢ï¼Œæ•ˆçŽ‡ä¹Ÿä¸ä¸€æ ·ã€‚å› ä¸ºå¯¹äºŽä¸åŒçš„å­˜å‚¨æ ¼å¼ï¼Œæ‰§è¡Œç›¸åŒçš„æŸ¥è¯¢æ“ä½œï¼Œä»–ä»¬è®¿é—®çš„æ•°æ®é‡å¤§å°æ˜¯ä¸ä¸€æ ·çš„ã€‚å¦‚æžœè¦ä½¿ç”¨TEXTFILEä½œä¸ºhiveè¡¨æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼Œåˆ™å¿…é¡»å…ˆå­˜åœ¨ä¸€å¼ ç›¸åŒæ•°æ®çš„å­˜å‚¨æ ¼å¼ä¸ºTEXTFILEçš„è¡¨table_t0,ç„¶åŽåœ¨å»ºè¡¨æ—¶ä½¿ç”¨â€œinsert into table table_stored_file_ORC select from table_t0;â€åˆ›å»ºã€‚æˆ–è€…ä½¿ç”¨â€create table as select from table_t0;â€åˆ›å»ºã€‚]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkä¹‹åºåˆ—åŒ–åœ¨ç”Ÿäº§ä¸­çš„åº”ç”¨]]></title>
    <url>%2F2018%2F05%2F29%2FSpark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[åºåˆ—åŒ–åœ¨åˆ†å¸ƒå¼åº”ç”¨çš„æ€§èƒ½ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ ¼å¼åŒ–å¯¹è±¡ç¼“æ…¢ï¼Œæˆ–è€…æ¶ˆè€—å¤§é‡çš„å­—èŠ‚æ ¼å¼åŒ–ï¼Œä¼šå¤§å¤§é™ä½Žè®¡ç®—æ€§èƒ½ã€‚åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåˆ›å»ºå¤§é‡çš„è‡ªå®šä¹‰å®žä½“å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡åœ¨ç½‘ç»œä¼ è¾“æ—¶éœ€è¦åºåˆ—åŒ–ï¼Œè€Œä¸€ç§å¥½çš„åºåˆ—åŒ–æ–¹å¼å¯ä»¥è®©æ•°æ®æœ‰æ›´å¥½çš„åŽ‹ç¼©æ¯”ï¼Œä»Žè€Œæå‡ç½‘ç»œä¼ è¾“é€ŸçŽ‡ï¼Œæé«˜sparkä½œä¸šçš„è¿è¡Œé€Ÿåº¦ã€‚é€šå¸¸è¿™æ˜¯åœ¨sparkåº”ç”¨ä¸­ç¬¬ä¸€ä»¶éœ€è¦ä¼˜åŒ–çš„äº‹æƒ…ã€‚Sparkçš„ç›®æ ‡æ˜¯åœ¨ä¾¿åˆ©ä¸Žæ€§èƒ½ä¸­å–å¾—å¹³è¡¡ï¼Œæ‰€ä»¥æä¾›2ç§åºåˆ—åŒ–çš„é€‰æ‹©ã€‚Java serializationåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä¼šä½¿ç”¨Javaçš„ObjectOutputStreamæ¡†æž¶å¯¹å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä¸Žä»»ä½•å®žçŽ°java.io.Serializableçš„ç±»ä¸€èµ·å·¥ä½œã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡æ‰©å±•java.io.Externalizableæ¥æ›´ç´§å¯†åœ°æŽ§åˆ¶åºåˆ—åŒ–çš„æ€§èƒ½ã€‚Javaåºåˆ—åŒ–æ˜¯çµæ´»çš„ï¼Œä½†é€šå¸¸ç›¸å½“æ…¢ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´è®¸å¤šç±»çš„å¤§åž‹åºåˆ—åŒ–æ ¼å¼ã€‚æµ‹è¯•ä»£ç ï¼šæµ‹è¯•ç»“æžœï¼šKryo serializationSparkè¿˜å¯ä»¥ä½¿ç”¨Kryoåº“ï¼ˆç‰ˆæœ¬2ï¼‰æ¥æ›´å¿«åœ°åºåˆ—åŒ–å¯¹è±¡ã€‚Kryoæ¯”Javaä¸²è¡ŒåŒ–ï¼ˆé€šå¸¸å¤šè¾¾10å€ï¼‰è¦å¿«å¾—å¤šï¼Œä¹Ÿæ›´ç´§å‡‘ï¼Œä½†æ˜¯ä¸æ”¯æŒæ‰€æœ‰å¯ä¸²è¡ŒåŒ–ç±»åž‹ï¼Œå¹¶ä¸”è¦æ±‚æ‚¨æå‰æ³¨å†Œæ‚¨å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„ç±»ï¼Œä»¥èŽ·å¾—æœ€ä½³æ€§èƒ½ã€‚æµ‹è¯•ä»£ç ï¼šæµ‹è¯•ç»“æžœï¼šæµ‹è¯•ç»“æžœä¸­å‘çŽ°ï¼Œä½¿ç”¨ Kryo serialization çš„åºåˆ—åŒ–å¯¹è±¡ æ¯”ä½¿ç”¨ Java serializationçš„åºåˆ—åŒ–å¯¹è±¡è¦å¤§ï¼Œä¸Žæè¿°çš„ä¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼ŸæŸ¥æ‰¾å®˜ç½‘ï¼Œå‘çŽ°è¿™ä¹ˆä¸€å¥è¯ Finally, if you donâ€™t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.ã€‚ä¿®æ”¹ä»£ç åŽåœ¨æµ‹è¯•ä¸€æ¬¡ã€‚æµ‹è¯•ç»“æžœï¼šæ€»ç»“ï¼šKryo serialization æ€§èƒ½å’Œåºåˆ—åŒ–å¤§å°éƒ½æ¯”é»˜è®¤æä¾›çš„ Java serialization è¦å¥½ï¼Œä½†æ˜¯ä½¿ç”¨Kryoéœ€è¦å°†è‡ªå®šä¹‰çš„ç±»å…ˆæ³¨å†Œè¿›åŽ»ï¼Œä½¿ç”¨èµ·æ¥æ¯”Java serializationéº»çƒ¦ã€‚è‡ªä»ŽSpark 2.0.0ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ç®€å•ç±»åž‹ã€ç®€å•ç±»åž‹æ•°ç»„æˆ–å­—ç¬¦ä¸²ç±»åž‹çš„ç®€å•ç±»åž‹æ¥è°ƒæ•´RDDsæ—¶ï¼Œåœ¨å†…éƒ¨ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨ã€‚é€šè¿‡æŸ¥æ‰¾sparkcontextåˆå§‹åŒ–çš„æºç ï¼Œå¯ä»¥å‘çŽ°æŸäº›ç±»åž‹å·²ç»åœ¨sparkcontextåˆå§‹åŒ–çš„æ—¶å€™è¢«æ³¨å†Œè¿›åŽ»ã€‚]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹¥æ³½æ•°æ®å¸¦ä½ éšæ—¶äº†è§£ä¸šç•Œé¢è¯•é¢˜ï¼Œéšæ—¶è·³é«˜è–ª]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA%2F</url>
    <content type="text"><![CDATA[é“¾å®¶(ä¸€é¢ï¼ŒäºŒé¢)0.è‡ªæˆ‘ä»‹ç»1.å°è£…ç»§æ‰¿å¤šæ€æ¦‚å¿µ2.mvcè®¾è®¡æ€æƒ³3.çº¿ç¨‹æ± ,çœ‹è¿‡æºç å—4.sshæ¡†æž¶ä¸­åˆ†åˆ«å¯¹åº”mvcä¸­é‚£ä¸€å±‚5.shellå‘½ä»¤ï¼ˆæŸ¥è¯¢ä¸€ä¸ªæ–‡ä»¶æœ‰å¤šå°‘è¡Œã€‚ chown ä¿®æ”¹æ–‡ä»¶æƒé™ï¼Œ åªè®°å¾—é‚£ä¹ˆå¤šäº† ï¼‰6.spring ioc aop åŽŸç†7.å•åˆ©æ¨¡å¼8.SQLé¢˜ï¼Œæƒ³ä¸èµ·æ¥äº†ã€‚ã€‚9.jvm è¿è¡Œæ—¶æ•°æ®åŒºåŸŸ10.spring mvcçŸ¥é“å—ã€‚ã€‚11.å·¥åŽ‚æ¨¡å¼12.mr è®¡ç®—æµç¨‹13.hiveæŸ¥è¯¢è¯­å¥ï¼ˆè¡¨1ï¼šæ—¶é—´ é£Ÿå ‚æ¶ˆè´¹ è¡¨äºŒï¼šå„ä¸ªæ—¶é—´æ®µ ç”¨æˆ· æ¯ä¸ªé£Ÿå ‚æ¶ˆè´¹ æŸ¥è¯¢ç”¨æˆ·åœ¨æ¯ä¸ªæ—¶é—´å‡ºçŽ°åœ¨é‚£ä¸ªé£Ÿå ‚ç»Ÿè®¡æ¶ˆè´¹è®°å½• ï¼Œå¤§æ¦‚æ˜¯è¿™æ ·çš„ã€‚ã€‚ï¼‰14.gitçš„ä½¿ç”¨15.hadoopçš„ç†è§£16.hiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„åŒºåˆ«17.hiveå­˜å‚¨æ ¼å¼å’ŒåŽ‹ç¼©æ ¼å¼18.å¯¹sparkäº†è§£å—ï¼Ÿ å½“æ—¶é«˜çº§ç­è¿˜æ²¡å­¦ã€‚ã€‚19.hiveäºŽå…³ç³»åž‹æ•°æ®åº“çš„åŒºåˆ«20.å„ç§æŽ’åº æ‰‹å†™å †æŽ’åº,è¯´è¯´åŽŸç†21.é“¾è¡¨é—®é¢˜ï¼Œæµè§ˆå™¨è®¿é—®è®°å½•ï¼Œå‰è¿›åŽé€€å½¢æˆé“¾è¡¨ï¼Œæ–°åŠ ä¸€ä¸ªè®°å½•ï¼Œå¤šå‡ºä¸€ä¸ªåˆ†æ”¯ï¼Œåˆ é™¤ä»¥å‰çš„åˆ†æ”¯ã€‚è®¾è®¡ç»“æž„ï¼Œå¦‚æžœè¿™ä¸ªç»“æž„å†™åœ¨å‡½æ•°ä¸­æ€Žä¹ˆç»´æŠ¤ã€‚22ä¸­é—´ä¹Ÿç©¿æ’äº†é¡¹ç›®ã€‚æ— è®ºæ˜¯å·²ç»æ‰¾åˆ°å·¥ä½œçš„è¿˜æ˜¯æ­£åœ¨å·¥ä½œçš„ï¼Œæˆ‘çš„è§‰çš„é¢è¯•é¢˜éƒ½å¯ä»¥ç»™æ‚¨ä»¬å¸¦æ¥ä¸€äº›å¯å‘ã€‚å¯ä»¥äº†è§£å¤§æ•°æ®è¡Œä¸šéœ€è¦ä»€ä¹ˆæ ·çš„äººæ‰ï¼Œä»€ä¹ˆæŠ€èƒ½ï¼Œå¯¹åº”åŽ»è¡¥å……è‡ªå·±çš„ä¸è¶³ä¹‹å¤„ï¼Œä¸ºä¸‹ä¸€ä¸ªé«˜è–ªå·¥ä½œåšå‡†å¤‡ã€‚è‹¥æ³½å¤§æ•°æ®åŽé¢ä¼šéšæ—¶æ›´æ–°å­¦å‘˜é¢è¯•é¢˜ï¼Œè®©å¤§å®¶äº†è§£å¤§æ•°æ®è¡Œä¸šçš„å‘å±•è¶‹åŠ¿ï¼Œæ—¨åœ¨å¸®åŠ©æ­£åœ¨è‰°è¾›æ‰“æ‹¼çš„æ‚¨æŒ‡å‡ºä¸€æ¡åŒºç›´çš„æœªæ¥ä¹‹è·¯ï¼ï¼ˆå°‘èµ°å¼¯è·¯å™¢å™¢ã€‚ã€‚ï¼‰]]></content>
      <categories>
        <category>é¢è¯•é¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸€æ¬¡è·³æ§½ç»åŽ†ï¼ˆé˜¿é‡Œ/ç¾Žå›¢/å¤´æ¡/ç½‘æ˜“/æœ‰èµž...)]]></title>
    <url>%2F2018%2F05%2F24%2F%E6%9C%89%E8%B5%9E...)%2F</url>
    <content type="text"><![CDATA[ä¸ºå•¥è·³æ§½æ¯æ¬¡è¯´å› ä¸ºç”Ÿæ´»æˆæœ¬çš„æ—¶å€™é¢è¯•å®˜éƒ½ä¼šå¾ˆæƒŠå¥‡ï¼Œéš¾é“æœ‰æˆ‘ä»¬è¿™é‡Œè´µï¼Ÿå¥½æƒ³ç›´æŽ¥ç»™å‡ºä¸‹é¢è¿™å¼ å›¾ï¼ŒåŽ¦é—¨çš„æˆ¿ä»·çœŸçš„å¥½è´µå¥½è´µå¥½è´µã€‚ã€‚ã€‚é¢è¯•è¿‡ç¨‹ï¼ˆå…ˆæ‰“ä¸ªå¹¿å‘Šï¼Œæœ‰å…´è¶£åŠ å…¥é˜¿é‡Œçš„æ¬¢è¿Žå‘ç®€åŽ†è‡³ zhangzb2007@gmail.comï¼Œæˆ–ç®€ä¹¦ä¸Šç»™æˆ‘å‘ä¿¡æ¯ï¼‰é¢çš„æ˜¯Javaå²—ï¼Œæ€»å…±é¢äº†7å®¶å…¬å¸ï¼Œé€šè¿‡äº†6å®¶ã€‚æŒ‰è‡ªå·±çš„ä¿¡å¿ƒæå‡åº¦æˆ‘æŠŠé¢è¯•è¿‡ç¨‹åˆ†ä¸ºä¸ŠåŠåœºå’Œä¸‹åŠåœºã€‚ä¸ŠåŠåœºæ›¹æ“ä¸“è½¦è¿™æ˜¯å‰åˆ©é›†å›¢ä¸‹å±žå­å…¬å¸ï¼Œå·²ç»æ˜¯ä¸€å®¶ç‹¬è§’å…½ã€‚ä¸€é¢ä¸­è§„ä¸­çŸ©ï¼Œæ²¡å•¥ç‰¹åˆ«çš„ã€‚äºŒé¢å¥½åƒæ˜¯ä¸ªä¸»ç®¡ï¼Œéš”äº†å¥½å‡ å¤©ï¼ŒåŸºæœ¬æ²¡é—®æŠ€æœ¯é—®é¢˜ï¼Œåè€Œæ˜¯é—®èŒä¸šè§„åˆ’ï¼Œå¯¹åŠ ç­æœ‰å•¥çœ‹æ³•ï¼Œæœ‰ç‚¹æŽªæ‰‹ä¸åŠï¼Œæ„Ÿè§‰å›žç­”çš„ä¸å¥½ã€‚ä½†æ˜¯è¿‡å‡ å¤©è¿˜æ˜¯æ”¶åˆ°HRçš„çŽ°åœºé¢è¯•é€šçŸ¥ã€‚çŽ°åœºæ˜¯æŠ€æœ¯é¢åŠ HRé¢ï¼ŒæŠ€æœ¯é¢è¢«é—®äº†å‡ ä¸ªé—®é¢˜æœ‰ç‚¹æ‡µé€¼ï¼ša. zookeeperçš„watcherä¹è§‚é”æ€Žä¹ˆå®žçŽ° b. ä¸€ä¸ªé¡¹ç›®çš„æ•´ä¸ªæµç¨‹ c. è¯´å‡ºä¸€ä¸ªç©ºé—´æ¢æ—¶é—´çš„åœºæ™¯ d. centos7çš„å†…å­˜åˆ†é…æ–¹å¼å’Œ6æœ‰å•¥ä¸åŒ f. ä½ å¯¹å…¬å¸æœ‰ä»€ä¹ˆä»·å€¼ã€‚HRè·Ÿæˆ‘è¯´èŠ‚åŽï¼ˆé‚£ä¼šå†è¿‡ä¸¤å¤©å°±æ˜¯æ¸…æ˜Žï¼‰ä¼šç»™æˆ‘æ¶ˆæ¯ï¼Œç»“æžœè¿‡äº†åŠä¸ªæœˆçªç„¶æŽ¥åˆ°ä»–ä»¬çš„ç”µè¯ï¼Œè¯´æˆ‘é€šè¿‡äº†ï¼Œç»™æˆ‘è®²äº†ä»–ä»¬çš„è–ªèµ„æ–¹æ¡ˆï¼Œæ²¡å¤ªå¤§å¸å¼•åŠ›ï¼Œå†åŠ ä¸Šè¿™ç§èŽ«åå…¶å¦™çš„æ—¶é—´ç­‰å¾…ï¼Œç›´æŽ¥æ‹’äº†ã€‚ç¾ŽäºšæŸç§‘ä¼°è®¡å¾ˆå¤šäººæ²¡å¬è¯´è¿‡è¿™å®¶å…¬å¸ï¼Œè¿™æ˜¯ä¸€å®¶åŽ¦é—¨æœ¬åœŸå…¬å¸ï¼Œåšæ”¿åºœå®‰é˜²é¡¹ç›®çš„ï¼Œåœ¨åŽ¦é—¨ä¹Ÿè¿˜æ˜¯å°æœ‰åæ°”ã€‚ä½†æ˜¯é¢è¯•å®Œç›´æŽ¥é¢ è¦†äº†æˆ‘å¯¹è¿™å®¶å…¬å¸çš„è®¤çŸ¥ã€‚è¿›é—¨æœ€æ˜¾çœ¼çš„åœ°æ–¹æ˜¯å…šæ´»åŠ¨å®¤ï¼Œåœ¨ç­‰é¢è¯•å®˜çš„ä¸€å°æ®µæ—¶é—´é‡Œæœ‰å¥½å‡ æ‹¨äººåˆ°é‡Œé¢å‚è§‚ã€‚é¢è¯•å‰åšäº†ä¸€ä»½ç¬”è¯•é¢˜ï¼ŒåŸºæœ¬éƒ½æ˜¯web/æ•°æ®åº“æ–¹é¢çš„ã€‚ç¬¬ä¸€é¢ç®€å•é—®äº†å‡ ä¸ªredisçš„é—®é¢˜ä¹‹åŽé¢è¯•å®˜ä»‹ç»äº†ä»–ä»¬çš„é¡¹ç›®ï¼Œä»–ä»¬éƒ½æ˜¯åšCå’ŒC++çš„ï¼Œæƒ³æ‰¾ä¸€ä¸ªäººæ­ä¸€å¥—å¤§æ•°æ®é›†ç¾¤ï¼Œå¤„ç†ä»–ä»¬æ¯å¤©å‡ ç™¾Gçš„æ•°æ®ï¼Œç„¶åŽæœåŠ¡å™¨å…¨éƒ¨æ˜¯windowsï¼äºŒé¢æ˜¯å¦ä¸€ä¸ªéƒ¨é—¨çš„ï¼Œå°è±¡ä¸­å°±é—®äº†kafkaä¸ºä»€ä¹ˆæ€§èƒ½è¿™ä¹ˆå¥½ï¼Œç„¶åŽå°±å¼€å§‹é—®ä¹°æˆ¿äº†æ²¡æœ‰ï¼Œç»“å©šäº†æ²¡æœ‰ï¼Œä»–å¯¹æˆ‘çŽ°åœ¨çš„å…¬å¸æ¯”è¾ƒäº†è§£ï¼Œåˆæ‰¯äº†æŒºä¹…ã€‚ä¸‰é¢åº”è¯¥æ˜¯ä¸ªéƒ¨é—¨è€å¤§äº†ï¼Œæ²¡æœ‰é—®æŠ€æœ¯é—®é¢˜ï¼Œä¹Ÿæ˜¯é—®ä¹°æˆ¿äº†æ²¡ï¼Œç»“å©šæ²¡ï¼Œé—®å„ç§ç”Ÿæ´»é—®é¢˜ï¼Œæœ‰ç‚¹åƒäººå£æ™®æŸ¥ã€‚æˆ‘æœ‰ç‚¹å¥½å¥‡ï¼Œé—®ä»–ä»¬ä¸ºå•¥è¿™ä¹ˆå…³å¿ƒè¿™äº›é—®é¢˜ï¼Œä»–ç›´æŽ¥è¯´ä»–ä»¬æ›´å¼ºè°ƒå‘˜å·¥çš„ç¨³å®šæ€§ï¼Œé¡¹ç›®æ¯”è¾ƒç®€å•ï¼Œèƒ½åŠ›ä¸ç”¨è¦æ±‚å¤ªé«˜ï¼Œä¸è¦å¤ªå·®å°±è¡Œã€‚æ±—ï¼Œç›´æŽ¥æ‹’äº†ã€‚æœ‰èµžç»å¯¹æŽ¨èçš„ä¸€å®¶å…¬å¸ï¼Œæ•ˆçŽ‡è¶…é«˜ã€‚ä¸­åˆæ‰¾äº†ä¸€ä¸ªç½‘å‹å¸®å¿™å†…æŽ¨ï¼Œæ™šä¸Šå°±å¼€å§‹ä¸€é¢ï¼Œç¬¬äºŒå¤©æ—©ä¸ŠäºŒé¢ï¼Œç¬¬ä¸‰å¤©HRå°±çº¦çŽ°åœºé¢è¯•æ—¶é—´ï¼Œå¿«çš„è¶…ä¹Žæƒ³è±¡ã€‚çŽ°åœºé¢ä¹Ÿæ˜¯å…ˆä¸€ä¸ªæŠ€æœ¯é¢ï¼Œæœ€åŽæ‰HRé¢ã€‚é¢è¯•çš„æ•´ä½“éš¾åº¦ä¸­ç­‰ã€‚çŽ°åœ¨å°±è®°å¾—å‡ ä¸ªé—®é¢˜ï¼šG1å’ŒCMSçš„åŒºåˆ«ï¼ŒG1æœ‰å•¥åŠ£åŠ¿ï¼›Kafkaçš„æ•´ä½“æž¶æž„ï¼›Nettyçš„ä¸€æ¬¡è¯·æ±‚è¿‡ç¨‹ï¼›è‡ªæ—‹é”/åå‘é”/è½»é‡çº§é”ï¼ˆè¿™ä¸ªé—®é¢˜åœ¨å¤´æ¡çš„é¢è¯•é‡Œä¹Ÿå‡ºçŽ°äº†ä¸€æ¬¡ï¼‰ã€hbaseçº¿ä¸Šé—®é¢˜æŽ’æŸ¥ï¼ˆåˆšå¥½é‡åˆ°è¿‡NUMAæž¶æž„ä¸‹çš„ä¸€ä¸ªé—®é¢˜ï¼Œå€Ÿæ­¤æŠŠhbaseçš„å†…æ ¸ä»‹ç»äº†ä¸‹ï¼‰ã€‚è¿™é‡Œä¸å¾—ä¸è¯´ä¸‹æœ‰èµžçš„äººï¼ŒçœŸçš„å¾ˆèµžã€‚ç»ˆé¢çš„é¢è¯•å®˜æ˜¯ä¸€ä¸ªç ”å‘å›¢é˜Ÿçš„è´Ÿè´£äººï¼Œå…¨ç¨‹ä¸€ç›´å¾®ç¬‘ï¼Œä¸­é—´ç”µè¯å“äº†ä¸€æ¬¡ï¼Œä¸€ç›´è·Ÿæˆ‘é“æ­‰ã€‚é¢å®Œä¹‹åŽè¿˜æä¾›äº†å›¢é˜Ÿçš„ä¸‰ä¸ªç ”å‘æ–¹å‘è®©æˆ‘è‡ªå·±é€‰æ‹©ã€‚åŽé¢çœ‹ä»–çš„æœ‹å‹åœˆçŠ¶æ€ï¼Œä»–é‚£å¤©é«˜çƒ§ï¼Œé¢å®Œæˆ‘å°±åŽ»æ‰“ç‚¹æ»´äº†ï¼Œä½†æ˜¯æ•´ä¸ªè¿‡ç¨‹å®Œå…¨çœ‹ä¸å‡ºæ¥ã€‚å¸®æˆ‘å†…æŽ¨çš„ç½‘å‹æ˜¯åœ¨å¾®ä¿¡ç¾¤é‡Œæ‰¾åˆ°çš„ï¼ŒçŸ¥é“æˆ‘è¿‡äº†ä¹‹åŽä¸»åŠ¨æ‰¾æˆ‘ï¼Œè®©æˆ‘è¿‡åŽ»æ­å·žæœ‰å•¥é—®é¢˜éšæ—¶æ‰¾ä»–ã€‚è™½ç„¶æœ€ç»ˆæ²¡æœ‰åŽ»ï¼Œä½†è¿˜æ˜¯å¯ä»¥æ˜Žæ˜¾æ„Ÿå—åˆ°ä»–ä»¬çš„çƒ­æƒ…ã€‚å­—èŠ‚è·³åŠ¨(ä»Šæ—¥å¤´æ¡)HRç¾Žçœ‰æ‰“ç”µè¯è¿‡æ¥è¯´æ˜¯å­—èŠ‚è·³åŠ¨å…¬å¸ï¼Œæƒ³çº¦ä¸‹è§†é¢‘é¢è¯•æ—¶é—´ã€‚é‚£ä¼šæ˜¯æœ‰ç‚¹æ‡µçš„ï¼Œæˆ‘åªçŸ¥é“ä»Šæ—¥å¤´æ¡å’ŒæŠ–éŸ³ã€‚åŽé¢æƒ³åˆ°åŒ—äº¬çš„å·ç æ‰æƒ³èµ·æ¥ã€‚å¤´æ¡å¯ä»¥è¯´æ˜¯è¿™æ¬¡æ‰€æœ‰é¢è¯•é‡Œæµç¨‹æœ€è§„èŒƒçš„ï¼Œæ”¶åˆ°ç®€åŽ†åŽæœ‰é‚®ä»¶é€šçŸ¥ï¼Œé¢„çº¦é¢è¯•æ—¶é—´åŽé‚®ä»¶çŸ­ä¿¡é€šçŸ¥ï¼Œé¢è¯•å®ŒåŽä¸è¶…è¿‡ä¸€å¤©é€šçŸ¥é¢è¯•ç»“æžœï¼Œæ¯æ¬¡é¢è¯•æœ‰é¢è¯•åé¦ˆã€‚è¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒç‰¹åˆ«çš„ï¼Œå¤§éƒ¨åˆ†å…¬å¸çš„ç”µè¯æˆ–è€…è§†é¢‘é¢è¯•åŸºæœ¬æ˜¯ä¸‹ç­åŽï¼Œå¤´æ¡éƒ½æ˜¯ä¸Šç­æ—¶é—´ï¼Œè¿˜ä¸ç»™çº¦ä¸‹ç­æ—¶é—´ï¼ˆéš¾é“ä»–ä»¬ä¸åŠ ç­ï¼Ÿï¼‰ã€‚ä¸€é¢é¢è¯•å®˜åˆšä¸Šæ¥å°±è¯´ä»–ä»¬æ˜¯åšgoçš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰å…´è¶£ï¼Œä»–è‡ªå·±ä¹Ÿæ˜¯Javaè½¬çš„ã€‚æˆ‘è¯´æ²¡é—®é¢˜ï¼Œä»–å…ˆé—®äº†ä¸€äº›JavaåŸºç¡€é—®é¢˜ï¼Œç„¶åŽæœ‰ä¸€é“ç¼–ç¨‹é¢˜ï¼Œæ±‚ä¸€æ£µæ ‘ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€è¿‘çš„å…¬å…±çˆ¶èŠ‚ç‚¹ã€‚æ€è·¯åŸºæœ¬æ˜¯å¯¹çš„ï¼Œä½†æ˜¯æœ‰äº›ç»†èŠ‚æœ‰é—®é¢˜ï¼Œé¢è¯•å®˜äººå¾ˆå¥½ï¼Œè¾¹çœ‹è¾¹è·Ÿæˆ‘è®¨è®ºï¼Œæˆ‘è¾¹æ”¹è¿›ï¼Œå‰å‰åŽåŽä¼°è®¡ç”¨æ¥å¿«åŠå°æ—¶ã€‚ç„¶åŽåˆç»§ç»­é—®é—®é¢˜ï¼ŒHTTP 301 302æœ‰å•¥åŒºåˆ«ï¼Ÿè®¾è®¡ä¸€ä¸ªçŸ­é“¾æŽ¥ç®—æ³•ï¼›md5é•¿åº¦æ˜¯å¤šå°‘ï¼Ÿæ•´ä¸ªé¢è¯•è¿‡ç¨‹ä¸€ä¸ªå¤šå°æ—¶ï¼Œè‡ªæˆ‘æ„Ÿè§‰ä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä»¥ä¸ºè¿™æ¬¡åº”è¯¥æŒ‚äº†ï¼Œç»“æžœæ™šä¸Šæ”¶åˆ°é¢è¯•é€šè¿‡çš„é€šçŸ¥ã€‚äºŒé¢æ˜¯åœ¨ä¸€ä¸ªä¸Šåˆè¿›è¡Œçš„ï¼Œæˆ‘ä»¥ä¸ºzoomè§†é¢‘ç³»ç»Ÿä¼šè‡ªåŠ¨è¿žä¸Šï¼ˆä¸€é¢å°±æ˜¯è‡ªåŠ¨è¿žä¸Šï¼‰ï¼Œå°±åœ¨é‚£è¾¹ç­‰ï¼Œè¿‡äº†5åˆ†é’Ÿè¿˜æ˜¯ä¸è¡Œï¼Œæˆ‘å°±è”ç³»HRï¼ŒåŽŸæ¥è¦æ”¹idï¼Œç»ˆäºŽè¿žä¸ŠåŽé¢è¯•å®˜çš„è¡¨æƒ…ä¸æ˜¯å¾ˆå¥½çœ‹ï¼Œæœ‰ç‚¹ä¸è€çƒ¦çš„æ ·å­ï¼Œä¸æ‡‚æ˜¯ä¸æ˜¯å› ä¸ºæˆ‘è€½è¯¯äº†å‡ åˆ†é’Ÿï¼Œè¿™ç§è¡¨æƒ…å»¶ç»­äº†æ•´ä¸ªé¢è¯•è¿‡ç¨‹ï¼Œå…¨ç¨‹æœ‰ç‚¹åŽ‹æŠ‘ã€‚é—®çš„é—®é¢˜å¤§éƒ¨åˆ†å¿˜äº†ï¼Œåªè®°å¾—é—®äº†ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„é—®é¢˜ï¼ŒThreadLocalå¦‚æžœå¼•ç”¨ä¸€ä¸ªstaticå˜é‡æ˜¯ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Ÿé—®ç€é—®ç€çªç„¶è¯´ä»Šå¤©é¢è¯•åˆ°æ­¤ä¸ºæ­¢ï¼Œä¸€çœ‹æ—¶é—´æ‰è¿‡åŽ»äºŒåå‡ åˆ†é’Ÿã€‚ç¬¬äºŒå¤©å°±æ”¶åˆ°é¢è¯•æ²¡è¿‡çš„é€šçŸ¥ï¼Œæ„Ÿè§‰è‡ªå·±äºŒé¢ç­”çš„æ¯”ä¸€é¢å¥½å¤šäº†ï¼Œå®žåœ¨æƒ³ä¸é€šã€‚ä¸‹åŠåœºä¸€ç›´æ„Ÿè§‰è‡ªå·±å¤ªæ°´äº†ï¼Œä»£ç é‡ä¸å¤§ï¼Œä¸‰å¹´åŠçš„ITç»éªŒè¿˜æœ‰ä¸€å¹´åŽ»åšäº†äº§å“ï¼Œéƒ½ä¸æ•¢æŠ•å¤§åŽ‚ã€‚ä¸ŠåŠåœºçš„æŠ€æœ¯é¢åŸºæœ¬è¿‡äº†ä¹‹åŽè‡ªä¿¡å¿ƒå¤§å¤§æå‡ï¼Œå¼€å§‹æŒ‘æˆ˜æ›´é«˜éš¾åº¦çš„ã€‚ç¾Žå›¢è¿™ä¸ªæ˜¯åŽ¦é—¨ç¾Žå›¢ï¼Œä»–ä»¬åœ¨è¿™è¾¹åšäº†ä¸€ä¸ªå«æ¦›æžœæ°‘å®¿çš„APPï¼ŒåŠžå…¬åœ°ç‚¹åœ¨JFCé«˜æ¡£å†™å­—æ¥¼ï¼Œä¼‘æ¯åŒºå¯ä»¥é¢æœå¤§æµ·ï¼ŒçŽ¯å¢ƒæ˜¯å¾ˆä¸é”™ï¼Œé¢è¯•å°±æœ‰ç‚¹è™å¿ƒäº†ã€‚ä¸¤ç‚¹åŠè¿›åŽ»ã€‚ä¸€é¢ã€‚æˆ‘çš„ç®€åŽ†å¤§éƒ¨åˆ†æ˜¯å¤§æ•°æ®ç›¸å…³çš„ï¼Œä»–ä¸æ˜¯å¾ˆäº†è§£ï¼Œé—®äº†ä¸€äº›åŸºç¡€é—®é¢˜å’Œnettyçš„å†™æµç¨‹ï¼Œè¿˜é—®äº†ä¸€ä¸ªredisæ•°æ®ç»“æž„çš„å®žçŽ°ï¼Œç»“æž„ä»–é—®äº†é‡Œé¢å­—ç¬¦ä¸²æ˜¯æ€Žä¹ˆå®žçŽ°çš„ï¼Œæœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚ä¸€ç›´æ„Ÿè§‰è¿™ä¸ªå¤ªç®€å•ï¼Œæ²¡å¥½å¥½çœ‹ï¼Œåªè®°å¾—æœ‰æ ‡è®°é•¿åº¦ï¼Œå¯ä»¥ç›´æŽ¥å–ã€‚ç„¶åŽå°±æ¥ä¸¤é“ç¼–ç¨‹é¢˜ã€‚ç¬¬ä¸€é¢˜æ˜¯æ±‚ä¸€æ£µæ ‘æ‰€æœ‰å·¦å¶å­èŠ‚ç‚¹çš„å’Œï¼Œæ¯”è¾ƒç®€å•ï¼Œä¸€ä¸ªæ·±åº¦ä¼˜å…ˆå°±å¯ä»¥æžå®šã€‚ç¬¬äºŒé¢˜æ˜¯ç»™å®šä¸€ä¸ªå€¼Kï¼Œä¸€ä¸ªæ•°åˆ—ï¼Œæ±‚æ•°åˆ—ä¸­ä¸¤ä¸ªå€¼aå’Œbï¼Œä½¿å¾—a+b=kã€‚æˆ‘æƒ³åˆ°äº†ä¸€ä¸ªä½¿ç”¨æ•°ç»„ä¸‹æ ‡çš„æ–¹æ³•ï¼ˆæ„Ÿè§‰æ˜¯åœ¨å“ªé‡Œæœ‰è§è¿‡ï¼Œä¸ç„¶ä¼°è®¡æ˜¯æƒ³ä¸å‡ºæ¥ï¼‰ï¼Œè¿™ç§å¯æ˜¯è¾¾åˆ°O(n)çš„å¤æ‚åº¦ï¼›ä»–åˆåŠ äº†ä¸ªé™åˆ¶æ¡ä»¶ï¼Œä¸èƒ½ä½¿ç”¨æ›´å¤šå†…å­˜ï¼Œæˆ‘æƒ³åˆ°äº†å¿«æŽ’+éåŽ†ï¼Œä»–é—®æœ‰æ²¡æœ‰æ›´ä¼˜çš„ï¼Œå®žåœ¨æƒ³ä¸å‡ºæ¥ï¼Œä»–æäº†ä¸€ä¸ªå¯ä»¥ä¸¤ç«¯é€¼è¿‘ï¼Œæ„Ÿè§‰å¾ˆå·§å¦™ã€‚äºŒé¢ã€‚é¢è¯•å®˜é«˜é«˜ç˜¦ç˜¦çš„ï¼Œæˆ‘å¯¹è¿™ç§äººçš„å°è±¡éƒ½æ˜¯è‚¯å®šå¾ˆç‰›é€¼ï¼Œå¯èƒ½æ˜¯æºäºŽå¤§å­¦æ—¶ä»£é‚£äº›å¤§ç‰›éƒ½é•¿è¿™æ ·ã€‚å…ˆè®©æˆ‘è®²ä¸‹kafkaçš„ç»“æž„ï¼Œç„¶åŽæ€Žä¹ˆé˜²æ­¢è®¢å•é‡å¤æäº¤ï¼Œç„¶åŽå¼€å§‹å›´ç»•ç¼“å­˜åŒæ­¥é—®é¢˜å±•å¼€äº†é•¿è¾¾åŠå°æ—¶çš„è®¨è®ºï¼šå…ˆå†™æ•°æ®åº“ï¼Œå†å†™ç¼“å­˜æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå…ˆå†™ç¼“å­˜å†å†™æ•°æ®åº“æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå†™åº“æˆåŠŸç¼“å­˜æ›´æ–°å¤±è´¥æ€Žä¹ˆåŠžï¼Ÿç¼“å­˜æ›´æ–°æˆåŠŸå†™åº“å¤±è´¥æ€Žä¹ˆåŠžï¼Ÿä»–å’Œæˆ‘ä¸€èµ·åœ¨ä¸€å¼ çº¸ä¸Šå„ç§ç”»ï¼Œæ„Ÿè§‰ä¸æ˜¯é¢è¯•ï¼Œè€Œæ˜¯åœ¨è®¾è®¡æ–¹æ¡ˆã€‚ä¸‰é¢ã€‚è¿™æ˜¯åŽç«¯å›¢é˜Ÿè´Ÿè´£äººäº†ï¼Œå¾ˆå’Œè”¼ï¼Œä¸€ç›´ç¬‘å‘µå‘µã€‚é—®äº†æˆ‘ä¸€äº›å¾®æœåŠ¡çš„é—®é¢˜ï¼Œæˆ‘æåˆ°äº†istioï¼Œä»‹ç»äº†è®¾è®¡ç†å¿µï¼Œæ„Ÿè§‰ä»–æœ‰ç‚¹æ„å¤–ã€‚ç„¶åŽä»–é—®java8çš„æ–°ç‰¹æ€§ï¼Œé—®æˆ‘çŸ¥ä¸çŸ¥é“lambdaè¡¨è¾¾å¼æ€Žä¹ˆæ¥çš„ï¼Œæˆ‘ä»Žlambdaæ¼”ç®—è¯´åˆ°lispè¯´åˆ°scalaï¼Œæ„Ÿè§‰ä»–æ›´æ„å¤–ã€‚æ­¤å¤„æœ‰ç‚¹å¹ç‰›äº†ã€‚æˆ‘é—®äº†ä¸€äº›å›¢é˜Ÿçš„é—®é¢˜ï¼Œé¡¹ç›®æœªæ¥è§„åˆ’ç­‰ï¼Œæ„Ÿè§‰æ¦›æžœè¿˜æ˜¯æŒºä¸é”™çš„ã€‚å››é¢ã€‚è¿™ä¸ªåº”è¯¥æ˜¯æ¦›æžœåŽ¦é—¨çš„è´Ÿè´£äººäº†ï¼ŒæŠ€æœ¯é—®é¢˜é—®çš„ä¸å¤šï¼Œæ›´å¤šæ˜¯ä¸€äº›èŒä¸šè§„åˆ’ï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ç­‰ã€‚é¢è¯•ç»“æŸçš„æ—¶å€™ä»–å…ˆå‡ºåŽ»ï¼Œæˆ‘æ”¶æ‹¾ä¸‹ä¸œè¥¿ï¼Œå‡ºåŽ»çš„æ—¶å€™å‘çŽ°ä»–åœ¨ç”µæ¢¯æ—å¸®æˆ‘å¼€ç”µæ¢¯ï¼Œå¯¹å¾…é¢è¯•è€…çš„è¿™ç§æ€åº¦å®žåœ¨è®©äººå¾ˆæœ‰å¥½æ„Ÿã€‚å‡ºæ¥çš„æ—¶å€™å·²ç»æ˜¯å…­ç‚¹åŠã€‚ç½‘æ˜“é¢çš„æ˜¯ç½‘æ˜“äº‘éŸ³ä¹ï¼Œå¹³æ—¶ç»å¸¸ç”¨ï¼Œæ„Ÿè§‰å¦‚æžœå¯ä»¥å‚ä¸Žç ”å‘åº”è¯¥æ˜¯ç§æŒºç¾Žå¦™çš„æ„Ÿè§‰ã€‚ä¸€é¢ã€‚ä¸‹åˆæ‰“è¿‡æ¥çš„ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç©ºï¼Œæˆ‘è¯´æœ‰ï¼Œä»–è¯´ä½ ä¸ç”¨ä¸Šç­å—ï¼Ÿæœ‰æ€åº¦çœŸçš„å¯ä»¥ä¸ºæ‰€æ¬²ä¸ºï¼ˆè‹¦ç¬‘ï¼‰ã€‚ç„¶åŽé—®äº†ä¸ºä»€ä¹ˆç¦»èŒï¼ŒèŠäº†ä¼šæˆ¿ä»·ï¼Œé—®äº†å‡ ä¸ªnettyçš„é—®é¢˜ï¼Œgcçš„é—®é¢˜ï¼Œæœ€åŽé—®ä¸‹å¯¹ä¸šåŠ¡çš„çœ‹æ³•ã€‚ç„¶åŽçº¦äº†ä¸ªäºŒé¢çš„æ—¶é—´ï¼Œç»“æžœæ—¶é—´åˆ°äº†æ²¡äººè”ç³»æˆ‘ï¼Œç¬¬äºŒå¤©æ‰“ç”µè¯è·Ÿæˆ‘é“æ­‰é‡æ–°çº¦äº†æ—¶é—´ï¼Œä¸å¾—ä¸è¯´æ€åº¦è¿˜æ˜¯å¾ˆå¥½çš„ã€‚äºŒé¢é—®çš„åè€Œå¾ˆåŸºç¡€ï¼Œæ²¡å¤ªå¤šç‰¹åˆ«çš„ã€‚è®©æˆ‘æé—®çš„æ—¶å€™æˆ‘æŠŠç¾Žå›¢äºŒé¢é‡Œçš„ç¼“å­˜é—®é¢˜æ‹¿å‡ºæ¥é—®ä»–ï¼Œå¾ˆè€å¿ƒçš„ç»™æˆ‘è§£ç­”äº†å¥½å‡ åˆ†é’Ÿï¼Œäººå¾ˆå¥½ã€‚é˜¿é‡Œè¿™ä¸ªå…¶å®žä¸æ˜¯æœ€åŽé¢è¯•çš„ï¼Œä½†æ˜¯æ˜¯æœ€åŽç»“æŸçš„ï¼Œä¸å¾—ä¸è¯´é˜¿é‡ŒäººçœŸçš„å¥½å¿™ï¼Œå‘¨ä¸‰è·Ÿæˆ‘é¢„çº¦æ—¶é—´ï¼Œç„¶åŽå·²ç»æŽ’åˆ°ä¸‹ä¸€å‘¨çš„å‘¨ä¸€ã€‚æ€»ä½“ä¸Šæ„Ÿè§‰é˜¿é‡Œçš„é¢è¯•é£Žæ ¼æ˜¯å–œæ¬¢åœ¨æŸä¸ªç‚¹ä¸Šä¸æ–­æ·±å…¥ï¼Œç›´åˆ°ä½ è¯´ä¸çŸ¥é“ã€‚ä¸€é¢ã€‚è‡ªæˆ‘ä»‹ç»ï¼Œç„¶åŽä»‹ç»çŽ°åœ¨çš„é¡¹ç›®æž¶æž„ï¼Œç¬¬ä¸€éƒ¨åˆ†å°±æ˜¯æ—¥å¿—ä¸Šä¼ å’ŒæŽ¥æ”¶ï¼Œç„¶åŽå°±å¦‚ä½•ä¿è¯æ—¥å¿—ä¸Šä¼ çš„å¹‚ç­‰æ€§å¼€å§‹ä¸æ–­æ·±å…¥ï¼Œå…ˆè®©æˆ‘è®¾è®¡ä¸€ä¸ªæ–¹æ¡ˆï¼Œç„¶åŽé—®æœ‰æ²¡æœ‰ä»€ä¹ˆæ”¹è¿›çš„ï¼Œç„¶åŽå¦‚ä½•åœ¨ä¿è¯å¹‚ç­‰çš„å‰æä¸‹æé«˜æ€§èƒ½ï¼Œä¸­é—´ç©¿æ’åˆ†å¸ƒå¼é”ã€redisã€mqã€æ•°æ®åº“é”ç­‰å„ç§é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜è®¨è®ºäº†å·®ä¸å¤šåŠå°æ—¶ã€‚ç„¶åŽå°±é—®æˆ‘æœ‰æ²¡æœ‰ä»€ä¹ˆè¦äº†è§£çš„ï¼ŒèŠ±äº†åå‡ åˆ†é’Ÿä»‹ç»ä»–ä»¬çŽ°åœ¨åšçš„äº‹æƒ…ã€æŠ€æœ¯æ ˆã€æœªæ¥çš„ä¸€äº›è®¡åˆ’ï¼Œéžå¸¸è€å¿ƒã€‚äºŒé¢ã€‚ä¹Ÿæ˜¯ä»Žä»‹ç»é¡¹ç›®å¼€å§‹ï¼Œç„¶åŽæŠ“ä½ä¸€ä¸ªç‚¹ï¼Œç»“åˆç§’æ€çš„åœºæ™¯æ·±å…¥ï¼Œå¦‚ä½•å®žçŽ°åˆ†å¸ƒå¼é”ã€å¦‚ä½•ä¿è¯å¹‚ç­‰æ€§ã€åˆ†å¸ƒå¼äº‹åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚é—®æˆ‘åˆ†å¸ƒå¼é”çš„ç¼ºç‚¹ï¼Œæˆ‘è¯´æ€§èƒ½ä¼šå‡ºçŽ°ç“¶é¢ˆï¼Œä»–é—®æ€Žä¹ˆè§£å†³ï¼Œæˆ‘æƒ³äº†æ¯”è¾ƒä¹…ï¼Œä»–æç¤ºè¯´å‘æ•£ä¸‹æ€ç»´ï¼Œæˆ‘æœ€åŽæƒ³äº†ä¸ªç®€å•çš„æ–¹æ¡ˆï¼Œç›´æŽ¥ä¸ä½¿ç”¨åˆ†å¸ƒå¼é”ï¼Œä»–å¥½åƒæŒºæ»¡æ„ã€‚æ„Ÿè§‰ä»–ä»¬æ›´çœ‹é‡æ€è€ƒçš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯å…·ä½“æ–¹æ¡ˆã€‚è¿˜é—®äº†ä¸€è‡´æ€§hashå¦‚ä½•ä¿è¯è´Ÿè½½å‡è¡¡ï¼Œkafkaå’Œrocketmqå„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼Œdubboçš„ä¸€ä¸ªè¯·æ±‚è¿‡ç¨‹ã€åºåˆ—åŒ–æ–¹å¼ï¼Œåºåˆ—åŒ–æ¡†æž¶ã€PBçš„ç¼ºç‚¹ã€å¦‚ä½•ä»Žæ•°æ®åº“å¤§æ‰¹é‡å¯¼å…¥æ•°æ®åˆ°hbaseã€‚ä¸‰é¢ã€‚æ˜¯HRå’Œä¸»ç®¡çš„è”åˆè§†é¢‘é¢è¯•ã€‚è¿™ç§é¢è¯•è¿˜ç¬¬ä¸€æ¬¡é‡åˆ°ï¼Œæœ‰ç‚¹ç´§å¼ ã€‚ä¸»ç®¡å…ˆé¢ï¼Œä¹Ÿæ˜¯è®©æˆ‘å…ˆä»‹ç»é¡¹ç›®ï¼Œé—®æˆ‘æœ‰æ²¡æœ‰ç”¨è¿‡mqï¼Œå¦‚ä½•ä¿è¯æ¶ˆæ¯å¹‚ç­‰æ€§ã€‚æˆ‘å°±æŠŠkafka0.11ç‰ˆæœ¬çš„å¹‚ç­‰æ€§æ–¹æ¡ˆè¯´äº†ä¸‹ï¼Œå°±æ²¡å†é—®æŠ€æœ¯é—®é¢˜äº†ã€‚åŽé¢åˆé—®äº†ä¸ºå•¥ç¦»èŒï¼Œå¯¹ä¸šåŠ¡çš„çœ‹æ³•ä¹‹ç±»çš„ã€‚ç„¶åŽå°±äº¤ç»™HRï¼Œåªé—®äº†å‡ ä¸ªé—®é¢˜ï¼Œç„¶åŽå°±ç»“æŸäº†ï¼Œå…¨ç¨‹ä¸åˆ°åŠå°æ—¶ã€‚ä¸æ‡‚æ˜¯ä¸æ˜¯è·Ÿé¢è¯•çš„éƒ¨é—¨æœ‰å…³ï¼Œé˜¿é‡Œå¯¹å¹‚ç­‰æ€§è¿™ä¸ªé—®é¢˜å¾ˆæ‰§ç€ï¼Œä¸‰æ¬¡éƒ½é—®åˆ°ï¼Œè€Œä¸”è¿˜æ˜¯ä»Žä¸åŒè§’åº¦ã€‚æ€»ç»“ä»Žé¢è¯•çš„éš¾æ˜“ç¨‹åº¦çœ‹é˜¿é‡Œ &gt; ç¾Žå›¢ &gt; å¤´æ¡ &gt; æœ‰èµž &gt; ç½‘æ˜“ &gt; æ›¹æ“ä¸“è½¦ &gt; ç¾ŽäºšæŸç§‘ã€‚æ•´ä¸ªè¿‡ç¨‹çš„ä½“ä¼šæ˜¯åŸºç¡€çœŸçš„å¾ˆé‡è¦ï¼ŒåŸºç¡€å¥½äº†å¾ˆå¤šé—®é¢˜å³ä½¿æ²¡é‡åˆ°è¿‡ä¹Ÿå¯ä»¥ä¸¾ä¸€åä¸‰ã€‚ å¦å¤–å¯¹ä¸€æ ·æŠ€æœ¯ä¸€å®šè¦æ‡‚åŽŸç†ï¼Œè€Œä¸ä»…ä»…æ˜¯æ€Žä¹ˆä½¿ç”¨ï¼Œå°¤å…¶æ˜¯ç¼ºç‚¹ï¼Œå¯¹é€‰åž‹å¾ˆå…³é”®ï¼Œå¯ä»¥å¾ˆå¥½çš„ç”¨æ¥å›žç­”ä¸ºä»€ä¹ˆä¸é€‰xxxã€‚å¦å¤–å¯¹ä¸€äº›æ¯”è¾ƒæ–°çš„æŠ€æœ¯æœ‰æ‰€äº†è§£ä¹Ÿæ˜¯ä¸€ä¸ªåŠ åˆ†é¡¹ã€‚]]></content>
      <categories>
        <category>é¢è¯•é¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveä¸­è‡ªå®šä¹‰UDAFå‡½æ•°ç”Ÿäº§å°æ¡ˆä¾‹]]></title>
    <url>%2F2018%2F05%2F23%2FHive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[ä¸€ã€UDAF å›žé¡¾1.å®šä¹‰ï¼šUDAF(User Defined Aggregation Funcation ) ç”¨æˆ·è‡ªå®šä¹‰èšç±»æ–¹æ³•ï¼Œå’Œgroup byè”åˆä½¿ç”¨ï¼ŒæŽ¥å—å¤šä¸ªè¾“å…¥æ•°æ®è¡Œï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªè¾“å‡ºæ•°æ®è¡Œã€‚2.Hiveæœ‰ä¸¤ç§UDAFï¼šç®€å•å’Œé€šç”¨ç®€å•ï¼šåˆ©ç”¨æŠ½è±¡ç±»UDAFå’ŒUDAFEvaluatorï¼Œä½¿ç”¨Javaåå°„å¯¼è‡´æ€§èƒ½æŸå¤±ï¼Œä¸”æœ‰äº›ç‰¹æ€§ä¸èƒ½ä½¿ç”¨ï¼Œå¦‚å¯å˜é•¿åº¦å‚æ•°åˆ—è¡¨ ã€‚é€šç”¨ï¼šåˆ©ç”¨æŽ¥å£GenericUDAFResolver2ï¼ˆæˆ–æŠ½è±¡ç±»AbstractGenericUDAFResolverï¼‰å’ŒæŠ½è±¡ç±»GenericUDAFEvaluatorï¼Œå¯ä»¥ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½ï¼Œä½†æ¯”è¾ƒå¤æ‚ï¼Œä¸ç›´è§‚ã€‚3.ä¸€ä¸ªè®¡ç®—å‡½æ•°å¿…é¡»å®žçŽ°çš„5ä¸ªæ–¹æ³•çš„å…·ä½“å«ä¹‰å¦‚ä¸‹ï¼šinit()ï¼šä¸»è¦æ˜¯è´Ÿè´£åˆå§‹åŒ–è®¡ç®—å‡½æ•°å¹¶ä¸”é‡è®¾å…¶å†…éƒ¨çŠ¶æ€ï¼Œä¸€èˆ¬å°±æ˜¯é‡è®¾å…¶å†…éƒ¨å­—æ®µã€‚ä¸€èˆ¬åœ¨é™æ€ç±»ä¸­å®šä¹‰ä¸€ä¸ªå†…éƒ¨å­—æ®µæ¥å­˜æ”¾æœ€ç»ˆçš„ç»“æžœã€‚iterate()ï¼šæ¯ä¸€æ¬¡å¯¹ä¸€ä¸ªæ–°å€¼è¿›è¡Œèšé›†è®¡ç®—æ—¶å€™éƒ½ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œè®¡ç®—å‡½æ•°ä¼šæ ¹æ®èšé›†è®¡ç®—ç»“æžœæ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚å½“è¾“ å…¥å€¼åˆæ³•æˆ–è€…æ­£ç¡®è®¡ç®—äº†ï¼Œåˆ™å°±è¿”å›žtrueã€‚terminatePartial()ï¼šHiveéœ€è¦éƒ¨åˆ†èšé›†ç»“æžœçš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ï¼Œå¿…é¡»è¦è¿”å›žä¸€ä¸ªå°è£…äº†èšé›†è®¡ç®—å½“å‰çŠ¶æ€çš„å¯¹è±¡ã€‚merge()ï¼šHiveè¿›è¡Œåˆå¹¶ä¸€ä¸ªéƒ¨åˆ†èšé›†å’Œå¦ä¸€ä¸ªéƒ¨åˆ†èšé›†çš„æ—¶å€™ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚terminate()ï¼šHiveæœ€ç»ˆèšé›†ç»“æžœçš„æ—¶å€™å°±ä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚è®¡ç®—å‡½æ•°éœ€è¦æŠŠçŠ¶æ€ä½œä¸ºä¸€ä¸ªå€¼è¿”å›žç»™ç”¨æˆ·ã€‚äºŒã€éœ€æ±‚ä½¿ç”¨UDAFç®€å•æ–¹å¼å®žçŽ°ç»Ÿè®¡åŒºåŸŸäº§å“ç”¨æˆ·è®¿é—®æŽ’åä¸‰ã€è‡ªå®šä¹‰UDAFå‡½æ•°ä»£ç å®žçŽ°12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hive.org.ruozedata;import java.util.*;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import org.apache.log4j.Logger;public class UserClickUDAF extends UDAF &#123; // æ—¥å¿—å¯¹è±¡åˆå§‹åŒ– public static Logger logger = Logger.getLogger(UserClickUDAF.class); // é™æ€ç±»å®žçŽ°UDAFEvaluator public static class Evaluator implements UDAFEvaluator &#123; // è®¾ç½®æˆå‘˜å˜é‡ï¼Œå­˜å‚¨æ¯ä¸ªç»Ÿè®¡èŒƒå›´å†…çš„æ€»è®°å½•æ•° private static Map&lt;String, String&gt; courseScoreMap; private static Map&lt;String, String&gt; city_info; private static Map&lt;String, String&gt; product_info; private static Map&lt;String, String&gt; user_click; //åˆå§‹åŒ–å‡½æ•°,mapå’Œreduceå‡ä¼šæ‰§è¡Œè¯¥å‡½æ•°,èµ·åˆ°åˆå§‹åŒ–æ‰€éœ€è¦çš„å˜é‡çš„ä½œç”¨ public Evaluator() &#123; init(); &#125; // åˆå§‹åŒ–å‡½æ•°é—´ä¼ é€’çš„ä¸­é—´å˜é‡ public void init() &#123; courseScoreMap = new HashMap&lt;String, String&gt;(); city_info = new HashMap&lt;String, String&gt;(); product_info = new HashMap&lt;String, String&gt;(); &#125; //mapé˜¶æ®µï¼Œè¿”å›žå€¼ä¸ºbooleanç±»åž‹ï¼Œå½“ä¸ºtrueåˆ™ç¨‹åºç»§ç»­æ‰§è¡Œï¼Œå½“ä¸ºfalseåˆ™ç¨‹åºé€€å‡º public boolean iterate(String pcid, String pcname, String pccount) &#123; if (pcid == null || pcname == null || pccount == null) &#123; return true; &#125; if (pccount.equals(&quot;-1&quot;)) &#123; // åŸŽå¸‚è¡¨ city_info.put(pcid, pcname); &#125; else if (pccount.equals(&quot;-2&quot;)) &#123; // äº§å“è¡¨ product_info.put(pcid, pcname); &#125; else &#123; // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è” unionCity_Prod_UserClic1(pcid, pcname, pccount); &#125; return true; &#125; // å¤„ç†ç”¨æˆ·ç‚¹å‡»å…³è” private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123; if (product_info.containsKey(pcid)) &#123; if (city_info.containsKey(pcname)) &#123; String city_name = city_info.get(pcname); String prod_name = product_info.get(pcid); String cp_name = city_name + prod_name; // å¦‚æžœä¹‹å‰å·²ç»Putè¿‡Keyå€¼ä¸ºåŒºåŸŸä¿¡æ¯ï¼Œåˆ™æŠŠè®°å½•ç›¸åŠ å¤„ç† if (courseScoreMap.containsKey(cp_name)) &#123; int pcrn = 0; String strTemp = courseScoreMap.get(cp_name); String courseScoreMap_pn = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim(); pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn); courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn)); &#125; else &#123; courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount); &#125; &#125; &#125; &#125; /** * ç±»ä¼¼äºŽcombiner,åœ¨mapèŒƒå›´å†…åšéƒ¨åˆ†èšåˆï¼Œå°†ç»“æžœä¼ ç»™mergeå‡½æ•°ä¸­çš„å½¢å‚mapOutput * å¦‚æžœéœ€è¦èšåˆï¼Œåˆ™å¯¹iteratorè¿”å›žçš„ç»“æžœå¤„ç†ï¼Œå¦åˆ™ç›´æŽ¥è¿”å›žiteratorçš„ç»“æžœå³å¯ */ public Map&lt;String, String&gt; terminatePartial() &#123; return courseScoreMap; &#125; // reduce é˜¶æ®µï¼Œç”¨äºŽé€ä¸ªè¿­ä»£å¤„ç†mapå½“ä¸­æ¯ä¸ªä¸åŒkeyå¯¹åº”çš„ terminatePartialçš„ç»“æžœ public boolean merge(Map&lt;String, String&gt; mapOutput) &#123; this.courseScoreMap.putAll(mapOutput); return true; &#125; // å¤„ç†mergeè®¡ç®—å®ŒæˆåŽçš„ç»“æžœï¼Œå³å¯¹mergeå®ŒæˆåŽçš„ç»“æžœåšæœ€åŽçš„ä¸šåŠ¡å¤„ç† public String terminate() &#123; return courseScoreMap.toString(); &#125; &#125;&#125;å››ã€åˆ›å»ºhiveä¸­çš„ä¸´æ—¶å‡½æ•°123DROP TEMPORARY FUNCTION user_click;add jar /data/hive_udf-1.0.jar;CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;äº”ã€è°ƒç”¨è‡ªå®šä¹‰UDAFå‡½æ•°å¤„ç†æ•°æ®1234567891011121314151617insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from ( select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from ( select * from ( select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info union all select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info union all select count(1) as type, product_id as pcid, city_id as pcname from user_click where action_time=&apos;2016-05-05&apos; group by product_id,city_id ) a order by type) b) c ;å…­ã€åˆ›å»ºHiveä¸´æ—¶å¤–éƒ¨è¡¨1234567create external table tmp1(city_name string,product_name string,rn string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;location &apos;/works/tmp1&apos;;ä¸ƒã€ç»Ÿè®¡æœ€ç»ˆåŒºåŸŸå‰3äº§å“æŽ’å123456789select * from (select city_name, product_name, floor(sum(rn)) visit_num, row_number()over(partition by city_name order by sum(rn) desc) rn, &apos;2016-05-05&apos; action_time from tmp1 group by city_name,product_name) a where rn &lt;=3 ;å…«ã€æœ€ç»ˆç»“æžœ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark History Server Web UIé…ç½®]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.è¿›å…¥sparkç›®å½•å’Œé…ç½®æ–‡ä»¶12[root@hadoop000 ~]# cd /opt/app/spark/conf[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf2.åˆ›å»ºspark-historyçš„å­˜å‚¨æ—¥å¿—è·¯å¾„ä¸ºhdfsä¸Š(å½“ç„¶ä¹Ÿå¯ä»¥åœ¨linuxæ–‡ä»¶ç³»ç»Ÿä¸Š)12345678[root@hadoop000 conf]# hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - root root 0 2017-02-14 12:43 /sparkdrwxrwx--- - root root 0 2017-02-14 12:58 /tmpdrwxr-xr-x - root root 0 2017-02-14 12:58 /userYou have new mail in /var/spool/mail/root[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 itemsdrwxrwxrwx - root root 0 2017-02-15 21:44 /spark/checkpointdata[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylogåœ¨HDFSä¸­åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œç”¨äºŽä¿å­˜Sparkè¿è¡Œæ—¥å¿—ä¿¡æ¯ã€‚Spark History Serverä»Žæ­¤ç›®å½•ä¸­è¯»å–æ—¥å¿—ä¿¡æ¯3.é…ç½®12345[root@hadoop000 conf]# vi spark-defaults.confspark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs://nameservice1/spark/historylogspark.yarn.historyServer.address 172.16.101.55:18080spark.eventLog.dirä¿å­˜æ—¥å¿—ç›¸å…³ä¿¡æ¯çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯hdfs://å¼€å¤´çš„HDFSè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯file://å¼€å¤´çš„æœ¬åœ°è·¯å¾„ï¼Œéƒ½éœ€è¦æå‰åˆ›å»ºspark.yarn.historyServer.address : Spark history serverçš„åœ°å€(ä¸åŠ http://).è¿™ä¸ªåœ°å€ä¼šåœ¨Sparkåº”ç”¨ç¨‹åºå®ŒæˆåŽæäº¤ç»™YARN RMï¼Œç„¶åŽå¯ä»¥åœ¨RM UIä¸Šç‚¹å‡»é“¾æŽ¥è·³è½¬åˆ°history server UIä¸Š.4.æ·»åŠ SPARK_HISTORY_OPTSå‚æ•°12345[root@hadoop01 conf]# vi spark-env.sh#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \-Dspark.history.ui.port=18080 \-Dspark.history.retainedApplications=20&quot;5.å¯åŠ¨æœåŠ¡å’ŒæŸ¥çœ‹123456[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01 ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer[root@hadoop01 ~]# ps -ef|grep sparkroot 17283 16928 0 21:42 pts/2 00:00:00 grep sparkroot 28905 1 0 Feb16 ? 00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServerYou have new mail in /var/spool/mail/root[root@hadoop01 ~]# netstat -nlp|grep 28905tcp 0 0 0.0.0.0:18080 0.0.0.0:* LISTEN 28905/javaä»¥ä¸Šé…ç½®æ˜¯é’ˆå¯¹ä½¿ç”¨è‡ªå·±ç¼–è¯‘çš„Sparkéƒ¨ç½²åˆ°é›†ç¾¤ä¸­ä¸€åˆ°ä¸¤å°æœºå™¨ä¸Šä½œä¸ºæäº¤ä½œä¸šå®¢æˆ·ç«¯çš„ï¼Œå¦‚æžœä½ æ˜¯CDHé›†ç¾¤ä¸­é›†æˆçš„Sparké‚£ä¹ˆå¯ä»¥åœ¨ç®¡ç†ç•Œé¢ç›´æŽ¥æŸ¥çœ‹ï¼]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark åŸºæœ¬æ¦‚å¿µ]]></title>
    <url>%2F2018%2F05%2F21%2FSpark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[åŸºäºŽ Spark æž„å»ºçš„ç”¨æˆ·ç¨‹åºï¼ŒåŒ…å«äº† ä¸€ä¸ªdriver ç¨‹åºå’Œé›†ç¾¤ä¸Šçš„ executorsï¼›ï¼ˆèµ·äº†ä¸€ä¸ªä½œä¸šï¼Œå°±æ˜¯ä¸€ä¸ªApplicationï¼‰sparkåè¯è§£é‡ŠApplication jarï¼šåº”ç”¨ç¨‹åºjaråŒ…åŒ…å«äº†ç”¨æˆ·çš„ Spark ç¨‹åºçš„ä¸€ä¸ª jar åŒ…. åœ¨æŸäº›æƒ…å†µä¸‹ç”¨æˆ·å¯èƒ½æƒ³è¦åˆ›å»ºä¸€ä¸ªå›Šæ‹¬äº†åº”ç”¨åŠå…¶ä¾èµ–çš„ â€œèƒ–â€ jar åŒ…. ä½†å®žé™…ä¸Š, ç”¨æˆ·çš„ jar ä¸åº”è¯¥åŒ…æ‹¬ Hadoop æˆ–æ˜¯ Spark çš„åº“, è¿™äº›åº“ä¼šåœ¨è¿è¡Œæ—¶è¢«è¿›è¡ŒåŠ è½½ï¼›Driver Programï¼šè¿™ä¸ªè¿›ç¨‹è¿è¡Œåº”ç”¨ç¨‹åºçš„ main æ–¹æ³•å¹¶ä¸”æ–°å»º SparkContext ï¼›Cluster Managerï¼šé›†ç¾¤ç®¡ç†è€…åœ¨é›†ç¾¤ä¸ŠèŽ·å–èµ„æºçš„å¤–éƒ¨æœåŠ¡ (ä¾‹å¦‚:standalone,Mesos,Yarn)ï¼›ï¼ˆâ€“masterï¼‰Deploy modeï¼šéƒ¨ç½²æ¨¡å¼å‘Šè¯‰ä½ åœ¨å“ªé‡Œå¯åŠ¨driver program. åœ¨ â€œclusterâ€ æ¨¡å¼ä¸‹, æ¡†æž¶åœ¨é›†ç¾¤å†…éƒ¨è¿è¡Œ driver. åœ¨ â€œclientâ€ æ¨¡å¼ä¸‹, æäº¤è€…åœ¨é›†ç¾¤å¤–éƒ¨è¿è¡Œ driver.ï¼›Worker Nodeï¼šå·¥ä½œèŠ‚ç‚¹é›†ç¾¤ä¸­ä»»ä½•å¯ä»¥è¿è¡Œåº”ç”¨ä»£ç çš„èŠ‚ç‚¹ï¼›ï¼ˆyarnä¸Šå°±æ˜¯node managerï¼‰Executorï¼šåœ¨ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šä¸ºæŸåº”ç”¨å¯åŠ¨çš„ä¸€ä¸ªè¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹è´Ÿè´£è¿è¡Œä»»åŠ¡ï¼Œå¹¶ä¸”è´Ÿè´£å°†æ•°æ®å­˜åœ¨å†…å­˜æˆ–è€…ç£ç›˜ä¸Šã€‚æ¯ä¸ªåº”ç”¨éƒ½æœ‰å„è‡ªç‹¬ç«‹çš„ executorsï¼›Taskï¼šä»»åŠ¡è¢«é€åˆ°æŸä¸ª executor ä¸Šæ‰§è¡Œçš„å·¥ä½œå•å…ƒï¼›Jobï¼šåŒ…å«å¾ˆå¤šå¹¶è¡Œè®¡ç®—çš„taskã€‚ä¸€ä¸ª action å°±ä¼šäº§ç”Ÿä¸€ä¸ªjobï¼›Stageï¼šä¸€ä¸ª Job ä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªtaskçš„é›†åˆï¼Œæ¯ä¸ªtaské›†åˆè¢«ç§°ä¸º stageï¼Œstageä¹‹é—´æ˜¯ç›¸äº’ä¾èµ–çš„(å°±åƒ Mapreduce åˆ† mapå’Œ reduce stagesä¸€æ ·)ï¼Œå¯ä»¥åœ¨Driver çš„æ—¥å¿—ä¸Šçœ‹åˆ°ã€‚sparkå·¥ä½œæµç¨‹1ä¸ªactionä¼šè§¦å‘1ä¸ªjobï¼Œ1ä¸ªjobåŒ…å«nä¸ªstageï¼Œæ¯ä¸ªstageåŒ…å«nä¸ªtaskï¼Œnä¸ªtaskä¼šé€åˆ°nä¸ªexecutorä¸Šæ‰§è¡Œï¼Œä¸€ä¸ªApplicationæ˜¯ç”±ä¸€ä¸ªdriver ç¨‹åºå’Œnä¸ª executorç»„æˆã€‚æäº¤çš„æ—¶å€™ï¼Œé€šè¿‡Cluster Managerå’ŒDeploy modeæŽ§åˆ¶ã€‚sparkåº”ç”¨ç¨‹åºåœ¨é›†ç¾¤ä¸Šè¿è¡Œä¸€ç»„ç‹¬ç«‹çš„è¿›ç¨‹ï¼Œé€šè¿‡SparkContextåè°ƒçš„åœ¨mainæ–¹æ³•é‡Œé¢ã€‚å¦‚æžœè¿è¡Œåœ¨ä¸€ä¸ªé›†ç¾¤ä¹‹ä¸Šï¼ŒSparkContextèƒ½å¤Ÿè¿žæŽ¥å„ç§çš„é›†ç¾¤ç®¡ç†è€…ï¼ŒåŽ»èŽ·å–åˆ°ä½œä¸šæ‰€éœ€è¦çš„èµ„æºã€‚ä¸€æ—¦è¿žæŽ¥æˆåŠŸï¼Œsparkåœ¨é›†ç¾¤èŠ‚ç‚¹ä¹‹ä¸Šè¿è¡Œexecutorè¿›ç¨‹ï¼Œæ¥ç»™ä½ çš„åº”ç”¨ç¨‹åºè¿è¡Œè®¡ç®—å’Œå­˜å‚¨æ•°æ®ã€‚å®ƒä¼šå‘é€ä½ çš„åº”ç”¨ç¨‹åºä»£ç åˆ°executorsä¸Šã€‚æœ€åŽï¼ŒSparkContextå‘é€tasksåˆ°executorsä¸ŠåŽ»è¿è¡Œ1ã€æ¯ä¸ªApplicationéƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„executorè¿›ç¨‹ï¼Œè¿™äº›è¿›ç¨‹åœ¨è¿è¡Œå‘¨æœŸå†…éƒ½æ˜¯å¸¸é©»çš„ä»¥å¤šçº¿ç¨‹çš„æ–¹å¼è¿è¡Œtasksã€‚å¥½å¤„æ˜¯æ¯ä¸ªè¿›ç¨‹æ— è®ºæ˜¯åœ¨è°ƒåº¦è¿˜æ˜¯æ‰§è¡Œéƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ„å‘³ç€æ•°æ®ä¸èƒ½è·¨åº”ç”¨ç¨‹åºè¿›è¡Œå…±äº«ï¼Œé™¤éžå†™åˆ°å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿï¼ˆAlluxioï¼‰ã€‚2ã€sparkå¹¶ä¸å…³å¿ƒåº•å±‚çš„é›†ç¾¤ç®¡ç†ã€‚3ã€driver ç¨‹åºä¼šç›‘å¬å¹¶ä¸”æŽ¥æ”¶å¤–é¢çš„ä¸€äº›executorè¯·æ±‚ï¼Œåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸé‡Œé¢ã€‚æ‰€ä»¥ï¼Œdriver ç¨‹åºåº”è¯¥èƒ½è¢«Worker Nodeé€šè¿‡ç½‘ç»œè®¿é—®ã€‚4ã€å› ä¸ºdriver åœ¨é›†ç¾¤ä¸Šè°ƒåº¦Tasksï¼Œdriver å°±åº”è¯¥é è¿‘Worker Nodeã€‚]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sparkä¸å¾—ä¸ç†è§£çš„é‡è¦æ¦‚å¿µâ€”â€”ä»Žæºç è§’åº¦çœ‹RDD]]></title>
    <url>%2F2018%2F05%2F20%2FSpark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD%2F</url>
    <content type="text"><![CDATA[1.RDDæ˜¯ä»€ä¹ˆResilient Distributed Datasetï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ï¼Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¹¶è¡Œæ“ä½œä¸å¯å˜çš„åˆ†åŒºå…ƒç´ çš„é›†åˆ2.RDDäº”å¤§ç‰¹æ€§A list of partitionsæ¯ä¸ªrddæœ‰å¤šä¸ªåˆ†åŒºprotected def getPartitions: Array[Partition]A function for computing each splitè®¡ç®—ä½œç”¨åˆ°æ¯ä¸ªåˆ†åŒºdef compute(split: Partition, context: TaskContext): Iterator[T]A list of dependencies on other RDDsrddä¹‹é—´å­˜åœ¨ä¾èµ–ï¼ˆRDDçš„è¡€ç¼˜å…³ç³»ï¼‰å¦‚ï¼šRDDA=&gt;RDDB=&gt;RDDC=&gt;RDDDprotected def getDependencies: Seq[Dependency[_]] = depsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)å¯é€‰ï¼Œé»˜è®¤å“ˆå¸Œçš„åˆ†åŒº@transient val partitioner: Option[Partitioner] = NoneOptionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„æœ€ä¼˜æ‰§è¡Œä½ç½®ï¼Œå°½é‡å®žçŽ°æ•°æ®æœ¬åœ°åŒ–ï¼Œå‡å°‘IOï¼ˆè¿™å¾€å¾€æ˜¯ç†æƒ³çŠ¶æ€ï¼‰protected def getPreferredLocations(split: Partition): Seq[String] = Nilæºç æ¥è‡ªgithubã€‚3.å¦‚ä½•åˆ›å»ºRDDåˆ›å»ºRDDæœ‰ä¸¤ç§æ–¹å¼ parallelize() å’Œtextfile()ï¼Œå…¶ä¸­parallelizeå¯æŽ¥æ”¶é›†åˆç±»ï¼Œä¸»è¦ä½œä¸ºæµ‹è¯•ç”¨ã€‚textfileå¯è¯»å–æ–‡ä»¶ç³»ç»Ÿï¼Œæ˜¯å¸¸ç”¨çš„ä¸€ç§æ–¹å¼1234567891011121314151617parallelize() def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) &#125;textfileï¼ˆï¼‰ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;æºç æ€»ç»“ï¼š1ï¼‰.å–_2æ˜¯å› ä¸ºæ•°æ®ä¸ºï¼ˆkeyï¼ˆåç§»é‡ï¼‰ï¼Œvalueï¼ˆæ•°æ®ï¼‰ï¼‰4.å¸¸è§çš„transformationå’Œactionç”±äºŽæ¯”è¾ƒç®€å•ï¼Œå¤§æ¦‚è¯´ä¸€ä¸‹å¸¸ç”¨çš„ç”¨å¤„ï¼Œä¸åšä»£ç æµ‹è¯•transformationMapï¼šå¯¹æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œFlatMapï¼šå…ˆå¯¹æ•°æ®é›†è¿›è¡Œæ‰å¹³åŒ–å¤„ç†ï¼Œç„¶åŽå†MapFilterï¼šå¯¹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œä¸ºtrueåˆ™é€šè¿‡destinctï¼šåŽ»é‡æ“ä½œactionreduceï¼šå¯¹æ•°æ®è¿›è¡Œèšé›†reduceBykeyï¼šå¯¹keyå€¼ç›¸åŒçš„è¿›è¡Œæ“ä½œcollectï¼šæ²¡æœ‰æ•ˆæžœçš„actionï¼Œä½†æ˜¯å¾ˆæœ‰ç”¨saveAstextFileï¼šæ•°æ®å­˜å…¥æ–‡ä»¶ç³»ç»Ÿforeachï¼šå¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œfuncçš„æ“ä½œ]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç¾Žå‘³ä¸ç”¨ç­‰å¤§æ•°æ®é¢è¯•é¢˜(201804æœˆ)]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)%2F</url>
    <content type="text"><![CDATA[1.è‹¥æ³½å¤§æ•°æ®çº¿ä¸‹ç­ï¼ŒæŸæŸæŸçš„å°ä¼™ä¼´çŽ°åœºé¢è¯•é¢˜æˆªå›¾:2.åˆ†äº«å¦å¤–1å®¶çš„å¿˜è®°åå­—å…¬å¸çš„å¤§æ•°æ®é¢è¯•é¢˜ï¼š]]></content>
      <categories>
        <category>é¢è¯•é¢˜</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®é¢è¯•é¢˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDDã€DataFrameå’ŒDataSetçš„åŒºåˆ«]]></title>
    <url>%2F2018%2F05%2F19%2FSpark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[åœ¨Sparkä¸­ï¼ŒRDDã€DataFrameã€Datasetæ˜¯æœ€å¸¸ç”¨çš„æ•°æ®ç±»åž‹ï¼Œä»Šå¤©è°ˆè°ˆä»–ä»¬çš„åŒºåˆ«ï¼ä¸€ ã€å…±æ€§1ã€RDDã€DataFrameã€Datasetå…¨éƒ½æ˜¯sparkå¹³å°ä¸‹çš„åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†ï¼Œä¸ºå¤„ç†è¶…å¤§åž‹æ•°æ®æä¾›ä¾¿åˆ©2ã€ä¸‰è€…éƒ½æœ‰æƒ°æ€§æœºåˆ¶ï¼Œåœ¨è¿›è¡Œåˆ›å»ºã€è½¬æ¢ï¼Œå¦‚mapæ–¹æ³•æ—¶ï¼Œä¸ä¼šç«‹å³æ‰§è¡Œï¼Œåªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶ï¼Œä¸‰è€…æ‰ä¼šå¼€å§‹éåŽ†è¿ç®—ã€‚3ã€ä¸‰è€…éƒ½ä¼šæ ¹æ®sparkçš„å†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—ï¼Œè¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º4ã€ä¸‰è€…éƒ½æœ‰partitionçš„æ¦‚å¿µã€‚äºŒã€RDDä¼˜ç¼ºç‚¹ä¼˜ç‚¹ï¼š1ã€ç›¸æ¯”äºŽä¼ ç»Ÿçš„MapReduceæ¡†æž¶ï¼ŒSparkåœ¨RDDä¸­å†…ç½®å¾ˆå¤šå‡½æ•°æ“ä½œï¼Œgroupï¼Œmapï¼Œfilterç­‰ï¼Œæ–¹ä¾¿å¤„ç†ç»“æž„åŒ–æˆ–éžç»“æž„åŒ–æ•°æ®ã€‚2ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹é£Žæ ¼3ã€ç¼–è¯‘æ—¶ç±»åž‹å®‰å…¨ï¼Œç¼–è¯‘æ—¶å°±èƒ½æ£€æŸ¥å‡ºç±»åž‹é”™è¯¯ç¼ºç‚¹ï¼š1ã€åºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„æ€§èƒ½å¼€é”€2ã€GCçš„æ€§èƒ½å¼€é”€ï¼Œé¢‘ç¹çš„åˆ›å»ºå’Œé”€æ¯å¯¹è±¡, åŠ¿å¿…ä¼šå¢žåŠ GCä¸‰ã€DataFrame1ã€ä¸ŽRDDå’ŒDatasetä¸åŒï¼ŒDataFrameæ¯ä¸€è¡Œçš„ç±»åž‹å›ºå®šä¸ºRowï¼Œåªæœ‰é€šè¿‡è§£æžæ‰èƒ½èŽ·å–å„ä¸ªå­—æ®µçš„å€¼ã€‚å¦‚12345df.foreach&#123; x =&gt; val v1=x.getAs[String](&quot;v1&quot;) val v2=x.getAs[String](&quot;v2&quot;)&#125;2ã€DataFrameå¼•å…¥äº†schemaå’Œoff-heapschema : RDDæ¯ä¸€è¡Œçš„æ•°æ®, ç»“æž„éƒ½æ˜¯ä¸€æ ·çš„. è¿™ä¸ªç»“æž„å°±å­˜å‚¨åœ¨schemaä¸­. Sparké€šè¿‡schameå°±èƒ½å¤Ÿè¯»æ‡‚æ•°æ®, å› æ­¤åœ¨é€šä¿¡å’ŒIOæ—¶å°±åªéœ€è¦åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ•°æ®, è€Œç»“æž„çš„éƒ¨åˆ†å°±å¯ä»¥çœç•¥äº†.off-heap : æ„å‘³ç€JVMå †ä»¥å¤–çš„å†…å­˜, è¿™äº›å†…å­˜ç›´æŽ¥å—æ“ä½œç³»ç»Ÿç®¡ç†ï¼ˆè€Œä¸æ˜¯JVMï¼‰ã€‚Sparkèƒ½å¤Ÿä»¥äºŒè¿›åˆ¶çš„å½¢å¼åºåˆ—åŒ–æ•°æ®(ä¸åŒ…æ‹¬ç»“æž„)åˆ°off-heapä¸­, å½“è¦æ“ä½œæ•°æ®æ—¶, å°±ç›´æŽ¥æ“ä½œoff-heapå†…å­˜. ç”±äºŽSparkç†è§£schema, æ‰€ä»¥çŸ¥é“è¯¥å¦‚ä½•æ“ä½œ.off-heapå°±åƒåœ°ç›˜, schemaå°±åƒåœ°å›¾, Sparkæœ‰åœ°å›¾åˆæœ‰è‡ªå·±åœ°ç›˜äº†, å°±å¯ä»¥è‡ªå·±è¯´äº†ç®—äº†, ä¸å†å—JVMçš„é™åˆ¶, ä¹Ÿå°±ä¸å†æ”¶GCçš„å›°æ‰°äº†.3ã€ç»“æž„åŒ–æ•°æ®å¤„ç†éžå¸¸æ–¹ä¾¿ï¼Œæ”¯æŒAvro, CSV, Elasticsearchæ•°æ®ç­‰ï¼Œä¹Ÿæ”¯æŒHive, MySQLç­‰ä¼ ç»Ÿæ•°æ®è¡¨4ã€å…¼å®¹Hiveï¼Œæ”¯æŒHqlã€UDFæœ‰schemaå’Œoff-heapæ¦‚å¿µï¼ŒDataFrameè§£å†³äº†RDDçš„ç¼ºç‚¹, ä½†æ˜¯å´ä¸¢äº†RDDçš„ä¼˜ç‚¹. DataFrameä¸æ˜¯ç±»åž‹å®‰å…¨çš„ï¼ˆåªæœ‰ç¼–è¯‘åŽæ‰èƒ½çŸ¥é“ç±»åž‹é”™è¯¯ï¼‰, APIä¹Ÿä¸æ˜¯é¢å‘å¯¹è±¡é£Žæ ¼çš„.å››ã€DataSet1ã€DataSetæ˜¯åˆ†å¸ƒå¼çš„æ•°æ®é›†åˆã€‚DataSetæ˜¯åœ¨Spark1.6ä¸­æ·»åŠ çš„æ–°çš„æŽ¥å£ã€‚å®ƒé›†ä¸­äº†RDDçš„ä¼˜ç‚¹ï¼ˆå¼ºç±»åž‹ å’Œå¯ä»¥ç”¨å¼ºå¤§lambdaå‡½æ•°ï¼‰ä»¥åŠSpark SQLä¼˜åŒ–çš„æ‰§è¡Œå¼•æ“Žã€‚DataSetå¯ä»¥é€šè¿‡JVMçš„å¯¹è±¡è¿›è¡Œæž„å»ºï¼Œå¯ä»¥ç”¨å‡½æ•°å¼çš„è½¬æ¢ï¼ˆmap/flatmap/filterï¼‰è¿›è¡Œå¤šç§æ“ä½œã€‚2ã€DataSetç»“åˆäº†RDDå’ŒDataFrameçš„ä¼˜ç‚¹, å¹¶å¸¦æ¥çš„ä¸€ä¸ªæ–°çš„æ¦‚å¿µEncoderã€‚DataSet é€šè¿‡Encoderå®žçŽ°äº†è‡ªå®šä¹‰çš„åºåˆ—åŒ–æ ¼å¼ï¼Œä½¿å¾—æŸäº›æ“ä½œå¯ä»¥åœ¨æ— éœ€åºåˆ—åŒ–æƒ…å†µä¸‹è¿›è¡Œã€‚å¦å¤–Datasetè¿˜è¿›è¡Œäº†åŒ…æ‹¬Tungstenä¼˜åŒ–åœ¨å†…çš„å¾ˆå¤šæ€§èƒ½æ–¹é¢çš„ä¼˜åŒ–ã€‚3ã€Datasetç­‰åŒäºŽDataFrameï¼ˆSpark 2.Xï¼‰]]></content>
      <categories>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¤§æ•°æ®ä¹‹å®žæ—¶æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶--ç”Ÿäº§ä¸ŠCanalä¸ŽMaxwellé¢ å³°å¯¹å†³]]></title>
    <url>%2F2018%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3%2F</url>
    <content type="text"><![CDATA[ä¸€.æ•°æ®æºåŒæ­¥ä¸­é—´ä»¶ï¼šCanalhttps://github.com/alibaba/canalhttps://github.com/Hackeruncle/syncClientMaxwellhttps://github.com/zendesk/maxwelläºŒ.æž¶æž„ä½¿ç”¨MySQL â€”- ä¸­é—´ä»¶ mcp â€”&gt;KAFKAâ€”&gt;?â€”&gt;å­˜å‚¨HBASE/KUDU/Cassandra å¢žé‡çš„a.å…¨é‡ bootstrapb.å¢žé‡1.å¯¹æ¯”Canal(æœåŠ¡ç«¯)Maxwell(æœåŠ¡ç«¯+å®¢æˆ·ç«¯)è¯­è¨€JavaJavaæ´»è·ƒåº¦æ´»è·ƒæ´»è·ƒHAæ”¯æŒå®šåˆ¶ ä½†æ˜¯æ”¯æŒæ–­ç‚¹è¿˜åŽŸåŠŸèƒ½æ•°æ®è½åœ°å®šåˆ¶è½åœ°åˆ°kafkaåˆ†åŒºæ”¯æŒæ”¯æŒbootstrap(å¼•å¯¼)ä¸æ”¯æŒæ”¯æŒæ•°æ®æ ¼å¼æ ¼å¼è‡ªç”±json(æ ¼å¼å›ºå®š) spark jsonâ€“&gt;DFæ–‡æ¡£è¾ƒè¯¦ç»†è¾ƒè¯¦ç»†éšæœºè¯»æ”¯æŒæ”¯æŒä¸ªäººé€‰æ‹©Maxwella.æœåŠ¡ç«¯+å®¢æˆ·ç«¯ä¸€ä½“ï¼Œè½»é‡çº§çš„b.æ”¯æŒæ–­ç‚¹è¿˜åŽŸåŠŸèƒ½+bootstrap+jsonCan do SELECT * from table (bootstrapping) initial loads of a table.supports automatic position recover on master promotionflexible partitioning schemes for Kakfa - by database, table, primary key, or columnMaxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).2.å®˜ç½‘è§£è¯»Bç«™è§†é¢‘3.éƒ¨ç½²3.1 MySQL Installhttps://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txthttps://ke.qq.com/course/262452?tuin=11cffd503.2 ä¿®æ”¹12345678910111213141516171819202122$ vi /etc/my.cnf[mysqld]binlog_format=row$ service mysql start3.3 åˆ›å»ºMaxwellçš„dbå’Œç”¨æˆ·mysql&gt; create database maxwell;Query OK, 1 row affected (0.03 sec)mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt;3.4è§£åŽ‹1[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz3.5æµ‹è¯•STDOUT:123bin/maxwell --user=&apos;maxwell&apos; \--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \--producer=stdoutæµ‹è¯•1ï¼šinsert sqlï¼š12mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);Query OK, 1 row affected (0.03 sec)maxwellè¾“å‡ºï¼š123456789101112131415161718&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;insert&quot;, &quot;ts&quot;: 1525959044, &quot;xid&quot;: 201, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 18, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;updateuser&quot;: null &#125;&#125;æµ‹è¯•1ï¼šupdate sql:1mysql&gt; update ruozedata set age=29 where id=999;é—®é¢˜: ROWï¼Œä½ è§‰å¾—binlogæ›´æ–°å‡ ä¸ªå­—æ®µï¼Ÿmaxwellè¾“å‡ºï¼š12345678910111213141516171819202122&#123; &quot;database&quot;: &quot;ruozedb&quot;, &quot;table&quot;: &quot;ruozedata&quot;, &quot;type&quot;: &quot;update&quot;, &quot;ts&quot;: 1525959208, &quot;xid&quot;: 255, &quot;commit&quot;: true, &quot;data&quot;: &#123; &quot;id&quot;: 999, &quot;name&quot;: &quot;jepson&quot;, &quot;age&quot;: 29, &quot;address&quot;: &quot;www.ruozedata.com&quot;, &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;, &quot;creuser&quot;: null, &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;, &quot;updateuser&quot;: null &#125;, &quot;old&quot;: &#123; &quot;age&quot;: 18, &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot; &#125;&#125;4.å…¶ä»–æ³¨æ„ç‚¹å’Œæ–°ç‰¹æ€§4.1 kafka_version ç‰ˆæœ¬Using kafka version: 0.11.0.1 0.10jar:12345678[root@hadoop000 kafka-clients]# lltotal 4000-rw-r--r--. 1 ruoze games 746207 May 8 06:34 kafka-clients-0.10.0.1.jar-rw-r--r--. 1 ruoze games 951041 May 8 06:35 kafka-clients-0.10.2.1.jar-rw-r--r--. 1 ruoze games 1419544 May 8 06:35 kafka-clients-0.11.0.1.jar-rw-r--r--. 1 ruoze games 324016 May 8 06:34 kafka-clients-0.8.2.2.jar-rw-r--r--. 1 ruoze games 641408 May 8 06:34 kafka-clients-0.9.0.1.jar[root@hadoop000 kafka-clients]#]]></content>
      <categories>
        <category>å…¶ä»–ç»„ä»¶</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>maxwell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«]]></title>
    <url>%2F2018%2F05%2F12%2FSpark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[ä¸€. YARN-Clusterå’ŒYARN-Clientçš„åŒºåˆ«ï¼ˆ1ï¼‰SparkContextåˆå§‹åŒ–ä¸åŒï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Driveræ‰€åœ¨ä½ç½®çš„ä¸åŒï¼ŒYarnClusterçš„Driveræ˜¯åœ¨é›†ç¾¤çš„æŸä¸€å°NMä¸Šï¼Œä½†æ˜¯Yarn-Clientå°±æ˜¯åœ¨driveræ‰€åœ¨çš„æœºå™¨ä¸Šï¼›ï¼ˆ2ï¼‰è€ŒDriverä¼šå’ŒExecutorsè¿›è¡Œé€šä¿¡ï¼Œè¿™ä¹Ÿå¯¼è‡´äº†Yarn_clusteråœ¨æäº¤Appä¹‹åŽå¯ä»¥å…³é—­Clientï¼Œè€ŒYarn-Clientä¸å¯ä»¥ï¼›ï¼ˆ3ï¼‰æœ€åŽå†æ¥è¯´åº”ç”¨åœºæ™¯ï¼ŒYarn-Clusteré€‚åˆç”Ÿäº§çŽ¯å¢ƒï¼ŒYarn-Clienté€‚åˆäº¤äº’å’Œè°ƒè¯•ã€‚äºŒ. yarn client æ¨¡å¼yarn-client æ¨¡å¼çš„è¯ ï¼ŒæŠŠ å®¢æˆ·ç«¯å…³æŽ‰çš„è¯ ï¼Œæ˜¯ä¸èƒ½æäº¤ä»»åŠ¡çš„ ã€‚ä¸‰.yarn cluster æ¨¡å¼yarn-cluster æ¨¡å¼çš„è¯ï¼Œ client å…³é—­æ˜¯å¯ä»¥æäº¤ä»»åŠ¡çš„ ï¼Œæ€»ç»“:1.spark-shell/spark-sql åªæ”¯æŒ yarn-clientæ¨¡å¼ï¼›2.spark-submitå¯¹äºŽä¸¤ç§æ¨¡å¼éƒ½æ”¯æŒã€‚]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
        <tag>æž¶æž„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§æ”¹é€ Spark1.6æºä»£ç ï¼Œcreate tableè¯­æ³•æ”¯æŒOracleåˆ—è¡¨åˆ†åŒº]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1.éœ€æ±‚é€šè¿‡Spark SQL JDBC æ–¹æ³•ï¼ŒæŠ½å–Oracleè¡¨æ•°æ®ã€‚2.é—®é¢˜å¤§æ•°æ®å¼€å‘äººå‘˜åæ˜ ï¼Œä½¿ç”¨æ•ˆæžœä¸Šåˆ—è¡¨åˆ†åŒºä¼˜äºŽæ•£åˆ—åˆ†åŒºã€‚ä½†Spark SQL JDBCæ–¹æ³•åªæ”¯æŒæ•°å­—ç±»åž‹åˆ†åŒºï¼Œè€Œä¸šåŠ¡è¡¨çš„åˆ—è¡¨åˆ†åŒºå­—æ®µæ˜¯ä¸ªå­—ç¬¦ä¸²ã€‚ç›®å‰Oracleè¡¨ä½¿ç”¨åˆ—è¡¨åˆ†åŒºï¼Œå¯¹çœçº§ä»£ç åˆ† åŒºã€‚å‚è€ƒ http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases3.Oracleçš„åˆ†åŒº3.1åˆ—è¡¨åˆ†åŒº:è¯¥åˆ†åŒºçš„ç‰¹ç‚¹æ˜¯æŸåˆ—çš„å€¼åªæœ‰å‡ ä¸ªï¼ŒåŸºäºŽè¿™æ ·çš„ç‰¹ç‚¹æˆ‘ä»¬å¯ä»¥é‡‡ç”¨åˆ—è¡¨åˆ†åŒºã€‚ä¾‹ä¸€:1234567891011CREATE TABLE PROBLEM_TICKETS(PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,STATUS VARCHAR2(20))PARTITION BY LIST (STATUS)(PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02)3.2æ•£åˆ—åˆ†åŒº:è¿™ç±»åˆ†åŒºæ˜¯åœ¨åˆ—å€¼ä¸Šä½¿ç”¨æ•£åˆ—ç®—æ³•ï¼Œä»¥ç¡®å®šå°†è¡Œæ”¾å…¥å“ªä¸ªåˆ†åŒºä¸­ã€‚å½“åˆ—çš„å€¼æ²¡æœ‰åˆé€‚çš„æ¡ä»¶æ—¶ï¼Œå»ºè®®ä½¿ç”¨æ•£åˆ—åˆ†åŒºã€‚ æ•£åˆ—åˆ†åŒºä¸ºé€šè¿‡æŒ‡å®šåˆ†åŒºç¼–å·æ¥å‡åŒ€åˆ†å¸ƒæ•°æ®çš„ä¸€ç§åˆ†åŒºç±»åž‹ï¼Œå› ä¸ºé€šè¿‡åœ¨I/Oè®¾å¤‡ä¸Šè¿›è¡Œæ•£åˆ—åˆ†åŒºï¼Œä½¿å¾—è¿™äº›åˆ†åŒºå¤§å°ä¸€è‡´ã€‚ä¾‹ä¸€:1234567891011CREATE TABLE HASH_TABLE(COL NUMBER(8),INF VARCHAR2(100) )PARTITION BY HASH (COL)(PARTITION PART01 TABLESPACE HASH_TS01, PARTITION PART02 TABLESPACE HASH_TS02, PARTITION PART03 TABLESPACE HASH_TS03)4.æ”¹é€ è“è‰²ä»£ç æ˜¯æ”¹é€ Sparkæºä»£ç ,åŠ è¯¾ç¨‹é¡¾é—®é¢†å–PDFã€‚1) Spark SQL JDBCçš„å»ºè¡¨è„šæœ¬ä¸­éœ€è¦åŠ å…¥åˆ—è¡¨åˆ†åŒºé…ç½®é¡¹ã€‚12345678910111213CREATE TEMPORARY TABLE TBLS_INUSING org.apache.spark.sql.jdbc OPTIONS (driver &quot;com.mysql.jdbc.Driver&quot;,url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,fetchSize &quot;1000&quot;,partitionColumn &quot;TBL_ID&quot;,numPartitions &quot;null&quot;,lowerBound &quot;null&quot;,upperBound &quot;null&quot;,user &quot;hive2user&quot;,password &quot;hive2user&quot;,partitionInRule &quot;1|15,16,18,19|20,21&quot;);2)ç¨‹åºå…¥å£org.apache.spark.sql.execution.datasources.jdbc.DefaultSourceï¼Œæ–¹æ³•createRelation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748override def createRelation(sqlContext: SQLContext,parameters: Map[String, String]): BaseRelation = &#123;val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)// add partition in ruleval partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)// validind all the partition in rule if (partitionColumn != null&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)&amp;&amp; partitionInRule == null )&#123; sys.error(&quot;Partitioning incompletely specified&quot;) &#125;val partitionInfo = if (partitionColumn == null) &#123; null&#125; else &#123; val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123; val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot; upperBound = &quot;0&quot; inGroups &#125; else&#123; Array[String]() &#125; JDBCPartitioningInfo( partitionColumn, lowerBound.toLong, upperBound.toLong, numPartitions.toInt, inPartitions)&#125;val parts = JDBCRelation.columnPartition(partitionInfo)val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))// parameters is immutableif(numPartitions != null)&#123;properties.put(&quot;numPartitions&quot; , numPartitions) &#125;JDBCRelation(url, table, parts, properties)(sqlContext) &#125; &#125;3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelationï¼Œæ–¹æ³•columnPartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))val column = partitioning.columnvar i: Int = 0var ans = new ArrayBuffer[Partition]()// partition by long if(partitioning.inPartitions.length == 0)&#123;val numPartitions = partitioning.numPartitionsif (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.// Here we get a little roundoff, but that&apos;s (hopefully) OK.val stride: Long = (partitioning.upperBound / numPartitions- partitioning.lowerBound / numPartitions)var currentValue: Long = partitioning.lowerBoundwhile (i &lt; numPartitions) &#123;val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else nullcurrentValue += strideval upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =if (upperBound == null) &#123; lowerBound&#125; else if (lowerBound == null) &#123; upperBound&#125; else &#123; s&quot;$lowerBound AND $upperBound&quot; &#125; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125;&#125;// partition by in else&#123; while(i &lt; partitioning.inPartitions.length)&#123; val inContent = partitioning.inPartitions(i) val whereClause = s&quot;$column in ($inContent)&quot; ans += JDBCPartition(whereClause, i) i= i+ 1 &#125; &#125; ans.toArray &#125;4)å¯¹å¤–æ–¹æ³•org.apache.spark.sql.SQLContext , æ–¹æ³•jdbc123456789101112def jdbc(url: String,table: String,columnName: String,lowerBound: Long,upperBound: Long,numPartitions: Int,inPartitions: Array[String] = Array[String]()): DataFrame = &#123;read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
        <tag>æºç é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿäº§ä¸­Hiveé™æ€å’ŒåŠ¨æ€åˆ†åŒºè¡¨ï¼Œè¯¥æ€Žæ ·æŠ‰æ‹©å‘¢ï¼Ÿ]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[ä¸€.éœ€æ±‚æŒ‰ç…§ä¸åŒéƒ¨é—¨ä½œä¸ºåˆ†åŒºï¼Œå¯¼æ•°æ®åˆ°ç›®æ ‡è¡¨äºŒ.ä½¿ç”¨é™æ€åˆ†åŒºè¡¨æ¥å®Œæˆ71.åˆ›å»ºé™æ€åˆ†åŒºè¡¨ï¼š12345678910create table emp_static_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;2.æ’å…¥æ•°æ®ï¼š12hive&gt;insert into table emp_static_partition partition(deptno=10) select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;3.æŸ¥è¯¢æ•°æ®ï¼š1hive&gt;select * from emp_static_partition;ä¸‰.ä½¿ç”¨åŠ¨æ€åˆ†åŒºè¡¨æ¥å®Œæˆ1.åˆ›å»ºåŠ¨æ€åˆ†åŒºè¡¨ï¼š123456789create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;ã€æ³¨æ„ã€‘åŠ¨æ€åˆ†åŒºè¡¨ä¸Žé™æ€åˆ†åŒºè¡¨çš„åˆ›å»ºï¼Œåœ¨è¯­æ³•ä¸Šæ˜¯æ²¡æœ‰ä»»ä½•åŒºåˆ«çš„2.æ’å…¥æ•°æ®ï¼š12hive&gt;insert into table emp_dynamic_partition partition(deptno) select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;ã€æ³¨æ„ã€‘åˆ†åŒºçš„å­—æ®µåç§°ï¼Œå†™åœ¨æœ€åŽï¼Œæœ‰å‡ ä¸ªå°±å†™å‡ ä¸ª ä¸Žé™æ€åˆ†åŒºç›¸æ¯”ï¼Œä¸éœ€è¦whereéœ€è¦è®¾ç½®å±žæ€§çš„å€¼ï¼š1hive&gt;set hive.exec.dynamic.partition.mode=nonstrictï¼›å‡å¦‚ä¸è®¾ç½®ï¼ŒæŠ¥é”™å¦‚ä¸‹:3.æŸ¥è¯¢æ•°æ®ï¼š1hive&gt;select * from emp_dynamic_partition;åˆ†åŒºåˆ—ä¸ºdeptnoï¼Œå®žçŽ°äº†åŠ¨æ€åˆ†åŒºå››.æ€»ç»“åœ¨ç”Ÿäº§ä¸Šæˆ‘ä»¬æ›´å€¾å‘æ˜¯é€‰æ‹©åŠ¨æ€åˆ†åŒºï¼Œæ— éœ€æ‰‹å·¥æŒ‡å®šæ•°æ®å¯¼å…¥çš„å…·ä½“åˆ†åŒºï¼Œè€Œæ˜¯ç”±selectçš„å­—æ®µ(å­—æ®µå†™åœ¨æœ€åŽï¼Œæœ‰å‡ ä¸ªå†™å‡ ä¸ª)è‡ªè¡Œå†³å®šå¯¼å‡ºåˆ°å“ªä¸€ä¸ªåˆ†åŒºä¸­ï¼Œ å¹¶è‡ªåŠ¨åˆ›å»ºç›¸åº”çš„åˆ†åŒºï¼Œä½¿ç”¨ä¸Šæ›´åŠ æ–¹ä¾¿å¿«æ· ï¼Œåœ¨ç”Ÿäº§å·¥ä½œä¸­ç”¨çš„éžå¸¸å¤šå¤šã€‚]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5minæŽŒæ¡ï¼ŒHiveçš„HiveServer2 å’ŒJDBCå®¢æˆ·ç«¯&ä»£ç çš„ç”Ÿäº§ä½¿ç”¨]]></title>
    <url>%2F2018%2F05%2F04%2F5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF%26%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. ä»‹ç»ï¼šä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œé€šè¿‡HiveServeræˆ–è€…HiveServer2ï¼Œå®¢æˆ·ç«¯å¯ä»¥åœ¨ä¸å¯åŠ¨CLIçš„æƒ…å†µä¸‹å¯¹Hiveä¸­çš„æ•°æ®è¿›è¡Œæ“ä½œï¼Œä¸¤è€…éƒ½å…è®¸è¿œç¨‹å®¢æˆ·ç«¯ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€å¦‚javaï¼Œpythonç­‰å‘hiveæäº¤è¯·æ±‚ï¼Œå–å›žç»“æžœï¼ˆä»Žhive0.15èµ·å°±ä¸å†æ”¯æŒhiveserveräº†ï¼‰ï¼Œä½†æ˜¯åœ¨è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯è¦è¯´ä¸€ä¸‹HiveServerã€‚HiveServeræˆ–è€…HiveServer2éƒ½æ˜¯åŸºäºŽThriftçš„ï¼Œä½†HiveSeveræœ‰æ—¶è¢«ç§°ä¸ºThrift serverï¼Œè€ŒHiveServer2å´ä¸ä¼šã€‚æ—¢ç„¶å·²ç»å­˜åœ¨HiveServerï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦HiveServer2å‘¢ï¼Ÿè¿™æ˜¯å› ä¸ºHiveServerä¸èƒ½å¤„ç†å¤šäºŽä¸€ä¸ªå®¢æˆ·ç«¯çš„å¹¶å‘è¯·æ±‚ï¼Œè¿™æ˜¯ç”±äºŽHiveServerä½¿ç”¨çš„ThriftæŽ¥å£æ‰€å¯¼è‡´çš„é™åˆ¶ï¼Œä¸èƒ½é€šè¿‡ä¿®æ”¹HiveServerçš„ä»£ç ä¿®æ­£ã€‚å› æ­¤åœ¨Hive-0.11.0ç‰ˆæœ¬ä¸­é‡å†™äº†HiveServerä»£ç å¾—åˆ°äº†HiveServer2ï¼Œè¿›è€Œè§£å†³äº†è¯¥é—®é¢˜ã€‚HiveServer2æ”¯æŒå¤šå®¢æˆ·ç«¯çš„å¹¶å‘å’Œè®¤è¯ï¼Œä¸ºå¼€æ”¾APIå®¢æˆ·ç«¯å¦‚é‡‡ç”¨jdbcã€odbcã€beelineçš„æ–¹å¼è¿›è¡Œè¿žæŽ¥ã€‚2.é…ç½®å‚æ•°Hiveserver2å…è®¸åœ¨é…ç½®æ–‡ä»¶hive-site.xmlä¸­è¿›è¡Œé…ç½®ç®¡ç†ï¼Œå…·ä½“çš„å‚æ•°ä¸ºï¼šå‚æ•° | å«ä¹‰ |-|-|hive.server2.thrift.min.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º5ã€‚hive.server2.thrift.max.worker.threads| æœ€å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º500ã€‚hive.server2.thrift.port| TCP çš„ç›‘å¬ç«¯å£ï¼Œé»˜è®¤ä¸º10000ã€‚hive.server2.thrift.bind.host| TCPç»‘å®šçš„ä¸»æœºï¼Œé»˜è®¤ä¸ºlocalhosté…ç½®ç›‘å¬ç«¯å£å’Œè·¯å¾„123456789vi hive-site.xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.48.130&lt;/value&gt;&lt;/property&gt;3. å¯åŠ¨hiveserver2ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨123[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/[hadoop@hadoop001 bin]$ hiveserver2 which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)4. é‡æ–°å¼€ä¸ªçª—å£ï¼Œä½¿ç”¨beelineæ–¹å¼è¿žæŽ¥-n æŒ‡å®šæœºå™¨ç™»é™†çš„åå­—ï¼Œå½“å‰æœºå™¨çš„ç™»é™†ç”¨æˆ·å-u æŒ‡å®šä¸€ä¸ªè¿žæŽ¥ä¸²æ¯æˆåŠŸè¿è¡Œä¸€ä¸ªå‘½ä»¤ï¼Œhiveserver2å¯åŠ¨çš„é‚£ä¸ªçª—å£ï¼Œåªè¦åœ¨å¯åŠ¨beelineçš„çª—å£ä¸­æ‰§è¡ŒæˆåŠŸä¸€æ¡å‘½ä»¤ï¼Œå¦å¤–ä¸ªçª—å£éšå³æ‰“å°ä¸€ä¸ªOKå¦‚æžœå‘½ä»¤é”™è¯¯ï¼Œhiveserver2é‚£ä¸ªçª—å£å°±ä¼šæŠ›å‡ºå¼‚å¸¸ä½¿ç”¨hadoopç”¨æˆ·å¯åŠ¨123456789[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoopwhich: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)scan complete in 4msConnecting to jdbc:hive2://localhost:10000/defaultConnected to: Apache Hive (version 1.1.0-cdh5.7.0)Driver: Hive JDBC (version 1.1.0-cdh5.7.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.7.0 by Apache Hive0: jdbc:hive2://localhost:10000/default&gt;ä½¿ç”¨SQL123456789101112131415160: jdbc:hive2://localhost:10000/default&gt; show databases;INFO : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)INFO : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 secondsINFO : Concurrency mode is disabled, not creating a lock managerINFO : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databasesINFO : Starting task [Stage-0:DDL] in serial modeINFO : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 secondsINFO : OK+----------------+--+| database_name |+----------------+--+| default |+----------------+--+1 row selected5.ä½¿ç”¨ç¼–å†™javaä»£ç æ–¹å¼è¿žæŽ¥5.1ä½¿ç”¨mavenæž„å»ºé¡¹ç›®ï¼Œpom.xmlæ–‡ä»¶å¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;5.2JdbcApp.javaæ–‡ä»¶ä»£ç :123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class JdbcApp &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws Exception &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); //select table:ename String tableName = &quot;emp&quot;; String sql = &quot;select ename from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); while(res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è°ˆè°ˆæˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜åŠå…¥é—¨]]></title>
    <url>%2F2018%2F05%2F01%2F%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[&#8195;å½“å¹´æˆ‘æ˜¯åšC#+Javaè½¯ä»¶å¼€å‘ï¼Œç„¶åŽè€ƒå–OCPæ¥äº†ä¸Šæµ·ï¼Œç«‹å¿—è¦åšä¸€åDBAã€‚åªè®°å¾—å½“å¹´è¯•ç”¨æœŸåˆšè¿‡æ—¶ï¼Œé˜´å·®é˜³é”™è½®åˆ°æˆ‘è´Ÿè´£å…¬å¸çš„å¤§æ•°æ®å¹³å°è¿™å—ï¼Œåˆšå¼€å§‹å¾ˆç—›è‹¦ï¼Œä¸€ä¸ªé™Œç”Ÿçš„è¡Œä¸šï¼Œä¸€ä¸ªè®¨è®ºçš„å°ä¼™ä¼´éƒ½æ²¡æœ‰ï¼Œä¸€ä»½çŽ°æˆèµ„æ–™éƒ½æ²¡æœ‰ï¼Œå¿ƒæƒ…ç„¦è™‘ã€‚åŽæ¥æˆ‘è°ƒæ•´å¿ƒæ€ï¼Œä»ŽDBè½¬ç§»åˆ°å¯¹å¤§æ•°æ®çš„ç ”ç©¶ï¼Œå†³å®šå•ƒä¸‹è¿™å—ç¡¬éª¨å¤´ï¼ŒæŠŠå®ƒåš¼ç¢Žï¼ŒæŠŠå®ƒæ¶ˆåŒ–å¸æ”¶ã€‚&#8195;ç”±äºŽå½“æ—¶å…¬å¸éƒ½æ˜¯CDHçŽ¯å¢ƒï¼Œåˆšå¼€å§‹å®‰è£…å¡äº†å¾ˆä¹…éƒ½è¿‡ä¸åŽ»ï¼ŒåŽé¢é€‰æ‹©åœ¨çº¿å®‰è£…ï¼Œå¾ˆæ…¢ï¼Œæœ‰æ—¶éœ€è¦1å¤©ã€‚åŽæ¥å®‰è£…HDFS ,YARN,HIVEç»„ä»¶ï¼Œä¸è¿‡å¯¹å®ƒä»¬ä¸ç†è§£ï¼Œä¸æ˜Žç™½ï¼Œæœ‰æ—¶å¾ˆå›°æƒ‘ã€‚è¿™æ ·çš„è¿‡ç¨‹å¤§æ¦‚æŒç»­ä¸‰ä¸ªæœˆäº†ã€‚&#8195;åŽæ¥çœ‹äº†å¾ˆå¤šåšæ–‡ï¼Œéƒ½æ˜¯Apache Hadoopç‰ˆæœ¬æ­å»ºï¼ŒäºŽæ˜¯æˆ‘å…ˆè¯•è¯•ç”¨Apache Hadoopæ­å»ºéƒ¨ç½²å•èŠ‚ç‚¹å’Œé›†ç¾¤ï¼Œç„¶åŽé…ç½®HAï¼Œæœ€åŽæˆ‘å‘çŽ°è‡ªå·±æ¯”è¾ƒå–œæ¬¢è¿™ç§æ–¹å¼ï¼Œå› ä¸ºæˆ‘èƒ½äº†è§£å…¶é…ç½®å‚æ•°ï¼Œé…ç½®æ–‡ä»¶å’Œå¸¸è§„å‘½ä»¤ç­‰ç­‰ï¼Œå†å›žå¤´åŽ»å¯¹æ¯”CDHå®‰è£…HDFSæœåŠ¡ï¼ŒçœŸæ˜¯å¤ªçˆ½äº†ï¼Œå› ä¸ºApache Hadoopç‰ˆæœ¬æœ‰çœŸæ­£ä½“éªŒæ„Ÿï¼Œè¿™æ—¶æˆ‘å°±è¿…é€Ÿè°ƒæ•´æ–¹å‘ : å…ˆApacheç‰ˆæœ¬ï¼Œå†CDHã€‚&#8195;ç”±äºŽå…¬å¸é¡¹ç›®çŽ¯å¢ƒï¼ŒæŽ¨è¿›è‡ªå·±å®žåœ¨å¤ªæ…¢ï¼ŒäºŽæ˜¯æˆ‘åœ¨ç½‘ä¸Šçœ‹å„ç§ç›¸å…³è§†é¢‘æ•™ç¨‹ï¼›åŠ nç§ç¾¤ï¼Œåœ¨ç¾¤é‡Œæ½œæ°´ï¼Œçœ‹æ°´å‹ä»¬æçš„é—®é¢˜è‡ªå·±ä¼šä¸ä¼šï¼Œä¸ä¼šå°±åŽ»æŸ¥èµ„æ–™ï¼Œä¼šå°±å¸®åŠ©ä»–ä»¬ä¸€èµ·ç ”ç©¶å­¦ä¹ è¿›æ­¥ã€‚&#8195;åŽæ¥è¿™æ ·çš„è¿›åº¦å¤ªæ…¢äº†ï¼Œå› ä¸ºå¾ˆå¤šç¾¤éƒ½æ˜¯æ‰“å¹¿å‘Šï¼Œæ½œæ°´ï¼Œæ²¡æœ‰çœŸæ­£çš„æŠ€æœ¯è®¨è®ºæ°›å›´ï¼ŒäºŽæ˜¯æˆ‘è¿…é€Ÿè°ƒæ•´æ–¹å‘ï¼Œè‡ªå·±å»ºä¸ªQQç¾¤ï¼Œæ…¢æ…¢æ‹›å…µä¹°é©¬ï¼Œå’Œç®¡ç†å‘˜ä»¬ä¸€èµ·åŽ»ç®¡ç†ï¼Œåœ¨è¿‡åŽ»çš„ä¸¤å¹´é‡Œæˆ‘ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šçŸ¥è¯†å’Œè®¤è¯†å’Œæˆ‘ä¸€æ ·å‰è¿›çš„å°ä¼™ä¼´ä»¬ï¼ŒçŽ°åœ¨ä¹Ÿæœ‰å¾ˆå¤šå·²æˆä¸ºfriendsã€‚&#8195;æ¯å½“å¤œæ™šï¼Œæˆ‘å°±ä¼šæ·±æ·±æ€è€ƒä»…å‡­å…¬å¸é¡¹ç›®,ç½‘ä¸Šå…è´¹è¯¾ç¨‹è§†é¢‘ï¼ŒQQç¾¤ç­‰ï¼Œè¿˜æ˜¯ä¸å¤Ÿçš„ï¼ŒäºŽæ˜¯æˆ‘å¼€å§‹å’¨è¯¢åŸ¹è®­æœºæž„çš„è¯¾ç¨‹ï¼Œåœ¨è¿™é‡Œæé†’å„ä½å°ä¼™ä¼´ä»¬ï¼ŒæŠ¥ç­ä¸€å®šè¦æ“¦äº®çœ¼ç›ï¼Œé€‰æ‹©è€å¸ˆå¾ˆé‡è¦ï¼ŒçœŸå¿ƒå¾ˆé‡è¦ï¼Œè®¸å¤šåŸ¹è®­æœºæž„çš„è€å¸ˆéƒ½æ˜¯Javaè½¬çš„ï¼Œè®²çš„æ˜¯å…¨æ˜¯åŸºç¡€ï¼Œæ ¹æœ¬æ²¡æœ‰ä¼ä¸šé¡¹ç›®å®žæˆ˜ç»éªŒï¼›è¿˜æœ‰ä¸è¦è·Ÿé£Žï¼Œä¸€å®šçœ‹ä»”ç»†çœ‹æ¸…æ¥šè¯¾ç¨‹æ˜¯å¦ç¬¦åˆå½“å‰çš„ä½ ã€‚&#8195;è¿™æ—¶è¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼ŒäºŽæ˜¯æˆ‘å¼€å§‹æ¯å¤©ä¸Šä¸‹ç­åœ°é“ä¸Šçœ‹æŠ€æœ¯åšå®¢ï¼Œç§¯æžåˆ†äº«ã€‚ç„¶åŽå†ç”³è¯·åšå®¢ï¼Œå†™åšæ–‡ï¼Œå†™æ€»ç»“ï¼ŒåšæŒæ¯æ¬¡åšå®Œä¸€æ¬¡å®žéªŒå°±å°†åšæ–‡ï¼Œæ¢³ç†å¥½ï¼Œå†™å¥½ï¼Œè¿™æ ·ä¹…è€Œä¹…ä¹‹ï¼ŒçŸ¥è¯†ç‚¹å°±æ…¢æ…¢å¤¯å®žç§¯ç´¯äº†ã€‚&#8195;å†ç€åŽé¢å°±å¼€å§‹å—é‚€å‡ å¤§åŸ¹è®­æœºæž„åšå…¬å¼€è¯¾ï¼Œå†ä¸€æ¬¡å°†çŸ¥è¯†ç‚¹æ¢³ç†äº†ï¼Œä¹Ÿè®¤è¯†äº†æ–°çš„å°ä¼™ä¼´ä»¬ï¼Œæˆ‘ä»¬æœ‰ç€ç›¸åŒçš„æ–¹å‘å’Œç›®æ ‡ï¼Œæˆ‘ä»¬å°½æƒ…çš„è®¨è®ºç€å¤§æ•°æ®çš„çŸ¥è¯†ç‚¹ï¼Œæ…¢æ…¢æœç€æˆ‘ä»¬å¿ƒç›®ä¸­çš„ç›®æ ‡è€ŒåŠªåŠ›ç€ï¼ä»¥ä¸ŠåŸºæœ¬å°±æ˜¯æˆ‘å’Œå¤§æ•°æ®çš„æƒ…ç¼˜ï¼Œä¸‹é¢æˆ‘æ¥è°ˆè°ˆæˆ‘å¯¹å¤§æ•°æ®å…¥é—¨çš„æ„Ÿæ‚Ÿã€‚1. å¿ƒæ€è¦ç«¯æ­£ã€‚æ—¢ç„¶æƒ³è¦ä»Žäº‹è¿™è¡Œï¼Œé‚£ä¹ˆä¸€å®šè¦ä¸‹å®šå†³å¿ƒï¼Œå½“ç„¶ä»˜å‡ºæ˜¯è‚¯å®šå¤§å¤§çš„ï¼Œä¸å…‰å…‰æ˜¯æ¯›çˆ·çˆ·ï¼Œè€Œæ›´å¤šçš„ä»˜å‡ºæ˜¯è‡ªå·±çš„é‚£ä¸€ä»½åšæŒï¼Œå‡¡äº‹è´µåœ¨åšæŒï¼ŒçœŸçœŸä½“çŽ°åœ¨è¿™é‡Œã€‚åŽæ¥æˆ‘å°†æˆ‘è€å©†ä»ŽåŒ–å·¥å®žéªŒå®¤åˆ†æžå‘˜è½¬è¡Œï¼ŒåšPythonçˆ¬è™«å’Œæ•°æ®åˆ†æžï¼Œå½“ç„¶è¿™ä¸ªä¸»è¦è¿˜æ˜¯é å¥¹çš„é‚£ä»½åšæŒã€‚2. å¿ƒç›®ä¸­è¦æœ‰è®¡åˆ’ã€‚å…ˆå­¦ä¹ Linuxå’ŒShellï¼Œå†å­¦ä¹ æ•°æ®åº“å’ŒSQLï¼Œå†å­¦ä¹ Javaå’ŒScalaï¼Œç„¶åŽå­¦ä¹ Apache Haoopã€Hiveã€Kafkaã€Sparkï¼Œæœå¤§æ•°æ®ç ”å‘æˆ–å¼€å‘è€ŒåŠªåŠ›ç€ã€‚3. å„ç§æ–¹å¼å­¦ä¹ ã€‚QQç¾¤ï¼Œåšå®¢ï¼Œä¸Šä¸‹ç­çœ‹æŠ€æœ¯æ–‡ç« ï¼Œé€‰æ‹©å¥½çš„è€å¸ˆå’Œè¯¾ç¨‹åŸ¹è®­ï¼Œ(æ“¦äº®çœ¼ç›ï¼Œå¾ˆå¤šè§†é¢‘ï¼Œå¾ˆå¤šå¤§æ•°æ®è€å¸ˆéƒ½æ˜¯çžŽæ‰¯çš„ï¼Œæœ€ç»ˆæ€»ç»“ä¸€å¥è¯ï¼Œä¸åœ¨ä¼ä¸šä¸Šç­çš„æ•™å¤§æ•°æ®éƒ½æ˜¯è€æµæ°“çš„ã€‚)å¯ä»¥åŠ é€Ÿè‡ªå·±å‰è¿›çš„é©¬æ‹‰æ¾é‡Œç¨‹ï¼Œå…¶å®žä¸€èˆ¬éƒ½è¦çœ‹å¤§å®¶æ€Žä¹ˆè¡¡é‡åŸ¹è®­è¿™ä¸ªäº‹çš„ï¼Œtimeå’Œmoneyçš„æŠ‰æ‹©ï¼Œä»¥åŠå¿«é€ŸjumpåŽçš„é«˜è–ªã€‚4. é¡¹ç›®ç»éªŒã€‚å¾ˆå¤šå°ç™½éƒ½æ²¡æœ‰é¡¹ç›®ç»éªŒä¹Ÿæ²¡æœ‰é¢è¯•ç»éªŒå’ŒæŠ€å·§ï¼Œå±¡å±¡é¢è¯•ä»¥å¤±è´¥å‘Šç»ˆï¼Œè¿™æ—¶å¤§å®¶å¯ä»¥æ‰¾ä½ ä»¬ç†Ÿæ‚‰çš„å°ä¼™ä¼´ä»¬çš„ï¼Œè®©ä»–ç»™ä½ åŸ¹è®­ä»–çš„é¡¹ç›®ï¼Œè¿™æ ·å°±æœ‰äº†ï¼Œå½“ç„¶å¯ä»¥ç›´æŽ¥äº’è”ç½‘æœç´¢ä¸€ä¸ªå°±è¡Œï¼Œä¸è¿‡ä¸€èˆ¬å¾ˆéš¾æœ‰å®Œæ•´çš„ã€‚è€Œé¢è¯•ï¼Œå°±çœ‹çœ‹å…¶ä»–äººé¢è¯•åˆ†äº«ï¼Œå­¦ä¹ ä»–äººã€‚æœ€åŽï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚æœ€åŽï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚æœ€åŽï¼Œæ€»ç»“ä¸€å¥è¯ï¼ŒåšæŒæ‰æ˜¯æœ€é‡è¦çš„ã€‚]]></content>
      <categories>
        <category>æ„Ÿæƒ³</category>
      </categories>
      <tags>
        <tag>äººç”Ÿæ„Ÿæ‚Ÿ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2minå¿«é€Ÿäº†è§£ï¼ŒHiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨]]></title>
    <url>%2F2018%2F05%2F01%2F2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[åœ¨äº†è§£å†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨åŒºåˆ«å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸‹Hiveæž¶æž„ ï¼šå¤§å®¶å¯ä»¥ç®€å•çœ‹ä¸€ä¸‹è¿™ä¸ªæž¶æž„å›¾ï¼Œæˆ‘ä»‹ç»å…¶ä¸­è¦ç‚¹ï¼šHiveçš„æ•°æ®åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§ä¸ºæ™®é€šæ•°æ®ï¼Œä¸€ç§ä¸ºå…ƒæ•°æ®ã€‚å…ƒæ•°æ®å­˜å‚¨ç€è¡¨çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¢žåˆ æ”¹æŸ¥è®°å½•ï¼Œç±»ä¼¼äºŽHadoopæž¶æž„ä¸­çš„namespaceã€‚æ™®é€šæ•°æ®å°±æ˜¯è¡¨ä¸­çš„è¯¦ç»†æ•°æ®ã€‚Hiveçš„å…ƒæ•°æ®é»˜è®¤å­˜å‚¨åœ¨derbyä¸­ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹å­˜å‚¨åœ¨MySQLä¸­ã€‚æ™®é€šæ•°æ®å¦‚æž¶æž„å›¾æ‰€ç¤ºå­˜å‚¨åœ¨hdfsä¸­ã€‚ä¸‹é¢æˆ‘ä»¬æ¥ä»‹ç»è¡¨çš„ä¸¤ç§ç±»åž‹ï¼šå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨å†…éƒ¨è¡¨ï¼ˆMANAGEDï¼‰ï¼šhiveåœ¨hdfsä¸­å­˜åœ¨é»˜è®¤çš„å­˜å‚¨è·¯å¾„ï¼Œå³defaultæ•°æ®åº“ã€‚ä¹‹åŽåˆ›å»ºçš„æ•°æ®åº“åŠè¡¨ï¼Œå¦‚æžœæ²¡æœ‰æŒ‡å®šè·¯å¾„åº”éƒ½åœ¨/user/hive/warehouseä¸‹ï¼Œæ‰€ä»¥åœ¨è¯¥è·¯å¾„ä¸‹çš„è¡¨ä¸ºå†…éƒ¨è¡¨ã€‚å¤–éƒ¨è¡¨ï¼ˆEXTERNALï¼‰ï¼šæŒ‡å®šäº†/user/hive/warehouseä»¥å¤–è·¯å¾„æ‰€åˆ›å»ºçš„è¡¨è€Œå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„ä¸»è¦åŒºåˆ«å°±æ˜¯å†…éƒ¨è¡¨ï¼šå½“åˆ é™¤å†…éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®å’ŒHDFSä¸Šçš„æ™®é€šæ•°æ®éƒ½ä¼šåˆ é™¤ ï¼›å¤–éƒ¨è¡¨ï¼šå½“åˆ é™¤å¤–éƒ¨è¡¨æ—¶ï¼ŒMySQLçš„å…ƒæ•°æ®ä¼šè¢«åˆ é™¤ï¼ŒHDFSä¸Šçš„æ•°æ®ä¸ä¼šè¢«åˆ é™¤ï¼›1.å‡†å¤‡æ•°æ®: æŒ‰tabé”®åˆ¶è¡¨ç¬¦ä½œä¸ºå­—æ®µåˆ†å‰²ç¬¦cat /tmp/ruozedata.txt 1 jepson 32 110 2 ruoze 22 112 3 www.ruozedata.com 18 120 2.å†…éƒ¨è¡¨æµ‹è¯•ï¼šåœ¨Hiveé‡Œé¢åˆ›å»ºä¸€ä¸ªè¡¨ï¼š123456789hive&gt; create table ruozedata(id int, &gt; name string, &gt; age int, &gt; tele string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\t&apos; &gt; STORED AS TEXTFILE;OKTime taken: 0.759 secondsè¿™æ ·æˆ‘ä»¬å°±åœ¨Hiveé‡Œé¢åˆ›å»ºäº†ä¸€å¼ æ™®é€šçš„è¡¨ï¼ŒçŽ°åœ¨ç»™è¿™ä¸ªè¡¨å¯¼å…¥æ•°æ®ï¼š1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;å†…éƒ¨è¡¨åˆ é™¤1hive&gt; drop table ruozedata;3.å¤–éƒ¨è¡¨æµ‹è¯•:åˆ›å»ºå¤–éƒ¨è¡¨å¤šäº†externalå…³é”®å­—è¯´æ˜Žä»¥åŠhdfsä¸Šlocation â€˜/hive/externalâ€™12345678hive&gt; create external table exter_ruozedata( &gt; id int, &gt; name string, &gt; age int, &gt; tel string) &gt; location &apos;/hive/external&apos;;OKTime taken: 0.098 secondsåˆ›å»ºå¤–éƒ¨è¡¨ï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨çš„æ—¶å€™åŠ ä¸Šexternalå…³é”®å­—ï¼ŒåŒæ—¶æŒ‡å®šå¤–éƒ¨è¡¨å­˜æ”¾æ•°æ®çš„è·¯å¾„ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä¸æŒ‡å®šå¤–éƒ¨è¡¨çš„å­˜æ”¾è·¯å¾„ï¼Œè¿™æ ·Hiveå°† åœ¨HDFSä¸Šçš„/user/hive/warehouse/æ–‡ä»¶å¤¹ä¸‹ä»¥å¤–éƒ¨è¡¨çš„è¡¨ååˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå¹¶å°†å±žäºŽè¿™ä¸ªè¡¨çš„æ•°æ®å­˜æ”¾åœ¨è¿™é‡Œï¼‰å¤–éƒ¨è¡¨å¯¼å…¥æ•°æ®å’Œå†…éƒ¨è¡¨ä¸€æ ·ï¼š1load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;åˆ é™¤å¤–éƒ¨è¡¨1hive&gt; drop table exter_ruozedata;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F27%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„éƒ¨ç½²ä½¿ç”¨ï¼Œä½ ä¼šå—ï¼Œä¸‰ç§æ–¹å¼ï¼ä¸€.ä¸´æ—¶å‡½æ•°ideaç¼–å†™udfæ‰“åŒ…Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Buildrzä¸Šä¼ è‡³æœåŠ¡å™¨æ·»åŠ jaråŒ…hive&gt;add xxx.jar jar_filepath;æŸ¥çœ‹jaråŒ…hive&gt;list jars;åˆ›å»ºä¸´æ—¶å‡½æ•°hive&gt;create temporary function my_lower as â€˜com.example.hive.udf.Lowerâ€™;äºŒ.æŒä¹…å‡½æ•°ideaç¼–å†™udfæ‰“åŒ…Maven Projects â€”-&gt;Lifecycle â€”-&gt;package â€”-&gt; å³å‡» Run Maven Buildrzä¸Šä¼ è‡³æœåŠ¡å™¨ä¸Šä¼ åˆ°HDFS$ hdfs dfs -put xxx.jar hdfs:///path/to/xxx.jaråˆ›å»ºæŒä¹…å‡½æ•°hive&gt;CREATE FUNCTION myfunc AS â€˜myclassâ€™ USING JAR â€˜hdfs:///path/to/xxx.jarâ€™;æ³¨æ„ç‚¹ï¼šæ­¤æ–¹æ³•åœ¨show functionsæ—¶æ˜¯çœ‹ä¸åˆ°çš„ï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨éœ€è¦ä¸Šä¼ è‡³hdfsä¸‰.æŒä¹…å‡½æ•°ï¼Œå¹¶æ³¨å†ŒçŽ¯å¢ƒä»‹ç»ï¼šCentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9ä¸‹è½½æºç hive-1.1.0-cdh5.7.0-src.tar.gzhttp://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gzè§£åŽ‹æºç tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/cd /home/hadoop/hive-1.1.0-cdh5.7.0å°†HelloUDF.javaæ–‡ä»¶å¢žåŠ åˆ°HIVEæºç ä¸­cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/ä¿®æ”¹FunctionRegistry.java æ–‡ä»¶1234cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/vi FunctionRegistry.javaåœ¨importä¸­å¢žåŠ ï¼šimport org.apache.hadoop.hive.ql.udf.HelloUDF;åœ¨æ–‡ä»¶å¤´éƒ¨ static å—ä¸­æ·»åŠ ï¼šsystem.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);é‡æ–°ç¼–è¯‘cd /home/hadoop/hive-1.1.0-cdh5.7.0mvn clean package -DskipTests -Phadoop-2 -Pdistç¼–è¯‘ç»“æžœå…¨éƒ¨ä¸ºï¼šBUILD SUCCESSæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼š/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/targeté…ç½®hiveçŽ¯å¢ƒé…ç½®hiveçŽ¯å¢ƒæ—¶ï¼Œå¯ä»¥å…¨æ–°é…ç½®æˆ–å°†ç¼–è¯‘åŽå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveçŽ¯å¢ƒä¸­ï¼š7.1. å…¨éƒ¨é…ç½®ï¼šå‚ç…§ä¹‹å‰æ–‡æ¡£ Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²7.2. å°†ç¼–è¯‘åŽå¸¦UDFå‡½æ•°çš„åŒ…å¤åˆ¶åˆ°æ—§hiveçŽ¯å¢ƒåˆ°/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libä¸‹ï¼Œæ‰¾åˆ°hive-exec-1.1.0-cdh5.7.0.jaråŒ…ï¼Œå¹¶å°†æ—§çŽ¯å¢ƒä¸­å¯¹ç…§çš„åŒ…æ›¿æ¢æŽ‰å‘½ä»¤ï¼š1234cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/libmv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bakcd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/libcp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/libæœ€ç»ˆå¯åŠ¨hiveæµ‹è¯•ï¼š123hivehive (default)&gt; show functions ; -- èƒ½æŸ¥çœ‹åˆ°æœ‰ helloudfhive(default)&gt;select deptno,dname,helloudf(dname) from dept; -- helloudfå‡½æ•°ç”Ÿæ•ˆ]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveè‡ªå®šä¹‰å‡½æ•°(UDF)çš„ç¼–ç¨‹å¼€å‘ï¼Œä½ ä¼šå—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[æœ¬åœ°å¼€å‘çŽ¯å¢ƒï¼šIntelliJ IDEA+Maven3.3.91. åˆ›å»ºå·¥ç¨‹æ‰“å¼€IntelliJ IDEAFileâ€“&gt;Newâ€“&gt;Projectâ€¦â€“&gt;Mavené€‰æ‹©Create from archetyeâ€“&gt;org.apache.maven.archety:maven-archetype-quitkstart2. é…ç½®åœ¨å·¥ç¨‹ä¸­æ‰¾åˆ°pom.xmlæ–‡ä»¶ï¼Œæ·»åŠ hadoopã€hiveä¾èµ–3. åˆ›å»ºç±»ã€å¹¶ç¼–å†™ä¸€ä¸ªHelloUDF.javaï¼Œä»£ç å¦‚ä¸‹ï¼šé¦–å…ˆä¸€ä¸ªUDFå¿…é¡»æ»¡è¶³ä¸‹é¢ä¸¤ä¸ªæ¡ä»¶:ä¸€ä¸ªUDFå¿…é¡»æ˜¯org.apache.hadoop.hive.ql.exec.UDFçš„å­ç±»ï¼ˆæ¢å¥è¯è¯´å°±æ˜¯æˆ‘ä»¬ä¸€èˆ¬éƒ½æ˜¯åŽ»ç»§æ‰¿è¿™ä¸ªç±»ï¼‰ä¸€ä¸ªUDFå¿…é¡»è‡³å°‘å®žçŽ°äº†evaluate()æ–¹æ³•4. æµ‹è¯•ï¼Œå³å‡»è¿è¡Œrun â€˜HelloUDF.main()â€™5. æ‰“åŒ…åœ¨IDEAèœå•ä¸­é€‰æ‹©viewâ€“&gt;Tool Windowsâ€“&gt;Maven Projectsï¼Œç„¶åŽåœ¨Maven Projectsçª—å£ä¸­é€‰æ‹©ã€å·¥ç¨‹åã€‘â€“&gt;Lifecycleâ€“&gt;packageï¼Œåœ¨packageä¸­å³é”®é€‰æ‹©Run Maven Buildå¼€å§‹æ‰“åŒ…æ‰§è¡ŒæˆåŠŸåŽåœ¨æ—¥å¿—ä¸­æ‰¾ï¼š[INFO] Building jar: (è·¯å¾„)/hive-1.0.jar]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDLï¼Œä½ çœŸçš„äº†è§£å—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ å…¨é¢å‰–æžHive DDLï¼æ¦‚å¿µDatabaseHiveä¸­åŒ…å«äº†å¤šä¸ªæ•°æ®åº“ï¼Œé»˜è®¤çš„æ•°æ®åº“ä¸ºdefaultï¼Œå¯¹åº”äºŽHDFSç›®å½•æ˜¯/user/hadoop/hive/warehouseï¼Œå¯ä»¥é€šè¿‡hive.metastore.warehouse.dirå‚æ•°è¿›è¡Œé…ç½®ï¼ˆhive-site.xmlä¸­é…ç½®ï¼‰TableHiveä¸­çš„è¡¨åˆåˆ†ä¸ºå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨ ,Hive ä¸­çš„æ¯å¼ è¡¨å¯¹åº”äºŽHDFSä¸Šçš„ä¸€ä¸ªç›®å½•ï¼ŒHDFSç›®å½•ä¸ºï¼š/user/hadoop/hive/warehouse/[databasename.db]/tablePartitionåˆ†åŒºï¼Œæ¯å¼ è¡¨ä¸­å¯ä»¥åŠ å…¥ä¸€ä¸ªåˆ†åŒºæˆ–è€…å¤šä¸ªï¼Œæ–¹ä¾¿æŸ¥è¯¢ï¼Œæé«˜æ•ˆçŽ‡ï¼›å¹¶ä¸”HDFSä¸Šä¼šæœ‰å¯¹åº”çš„åˆ†åŒºç›®å½•ï¼š/user/hadoop/hive/warehouse/[databasename.db]/tableDDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];IF NOT EXISTSï¼šåŠ ä¸Šè¿™å¥è¯ä»£è¡¨åˆ¤æ–­æ•°æ®åº“æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±ä¼šåˆ›å»ºï¼Œå­˜åœ¨å°±ä¸ä¼šåˆ›å»ºã€‚COMMENTï¼šæ•°æ®åº“çš„æè¿°LOCATIONï¼šåˆ›å»ºæ•°æ®åº“çš„åœ°å€ï¼Œä¸åŠ é»˜è®¤åœ¨/user/hive/warehouse/è·¯å¾„ä¸‹WITH DBPROPERTIESï¼šæ•°æ®åº“çš„å±žæ€§Drop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICTï¼šé»˜è®¤æ˜¯restrictï¼Œå¦‚æžœè¯¥æ•°æ®åº“è¿˜æœ‰è¡¨å­˜åœ¨åˆ™æŠ¥é”™ï¼›CASCADEï¼šçº§è”åˆ é™¤æ•°æ®åº“(å½“æ•°æ®åº“è¿˜æœ‰è¡¨æ—¶ï¼Œçº§è”åˆ é™¤è¡¨åŽåœ¨åˆ é™¤æ•°æ®åº“)ã€‚Alter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)Use Database12USE database_name;USE DEFAULT;Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;â€œ | â€ï¼šå¯ä»¥é€‰æ‹©å…¶ä¸­ä¸€ç§â€œ[ ]â€ï¼šå¯é€‰é¡¹LIKE â€˜identifier_with_wildcardsâ€™ï¼šæ¨¡ç³ŠæŸ¥è¯¢æ•°æ®åº“Describe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_nameï¼šæŸ¥çœ‹æ•°æ®åº“çš„æè¿°ä¿¡æ¯å’Œæ–‡ä»¶ç›®å½•ä½ç½®è·¯å¾„ä¿¡æ¯ï¼›EXTENDEDï¼šåŠ ä¸Šæ•°æ®åº“é”®å€¼å¯¹çš„å±žæ€§ä¿¡æ¯ã€‚hive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s)Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type1: ARRAY &lt; data_type &gt;map_type1: MAP &lt; primitive_type, data_type &gt;struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classnameconstraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARYï¼ˆä¸´æ—¶è¡¨ï¼‰Hiveä»Ž0.14.0å¼€å§‹æä¾›åˆ›å»ºä¸´æ—¶è¡¨çš„åŠŸèƒ½ï¼Œè¡¨åªå¯¹å½“å‰sessionæœ‰æ•ˆï¼Œsessioné€€å‡ºåŽï¼Œè¡¨è‡ªåŠ¨åˆ é™¤ã€‚è¯­æ³•ï¼šCREATE TEMPORARY TABLE â€¦æ³¨æ„ï¼šå¦‚æžœåˆ›å»ºçš„ä¸´æ—¶è¡¨è¡¨åå·²å­˜åœ¨ï¼Œé‚£ä¹ˆå½“å‰sessionå¼•ç”¨åˆ°è¯¥è¡¨åæ—¶å®žé™…ç”¨çš„æ˜¯ä¸´æ—¶è¡¨ï¼Œåªæœ‰dropæˆ–renameä¸´æ—¶è¡¨åæ‰èƒ½ä½¿ç”¨åŽŸå§‹è¡¨ä¸´æ—¶è¡¨é™åˆ¶ï¼šä¸æ”¯æŒåˆ†åŒºå­—æ®µå’Œåˆ›å»ºç´¢å¼•EXTERNALï¼ˆå¤–éƒ¨è¡¨ï¼‰Hiveä¸Šæœ‰ä¸¤ç§ç±»åž‹çš„è¡¨ï¼Œä¸€ç§æ˜¯Managed Table(é»˜è®¤çš„)ï¼Œå¦ä¸€ç§æ˜¯External Tableï¼ˆåŠ ä¸ŠEXTERNALå…³é”®å­—ï¼‰ã€‚å®ƒä¿©çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šå½“æˆ‘ä»¬dropè¡¨æ—¶ï¼ŒManaged Tableä¼šåŒæ—¶åˆ åŽ»dataï¼ˆå­˜å‚¨åœ¨HDFSä¸Šï¼‰å’Œmeta dataï¼ˆå­˜å‚¨åœ¨MySQLï¼‰ï¼Œè€ŒExternal Tableåªä¼šåˆ meta dataã€‚1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; );PARTITIONED BYï¼ˆåˆ†åŒºè¡¨ï¼‰äº§ç”ŸèƒŒæ™¯ï¼šå¦‚æžœä¸€ä¸ªè¡¨ä¸­æ•°æ®å¾ˆå¤šï¼Œæˆ‘ä»¬æŸ¥è¯¢æ—¶å°±å¾ˆæ…¢ï¼Œè€—è´¹å¤§é‡æ—¶é—´ï¼Œå¦‚æžœè¦æŸ¥è¯¢å…¶ä¸­éƒ¨åˆ†æ•°æ®è¯¥æ€Žä¹ˆåŠžå‘¢ï¼Œè¿™æ˜¯æˆ‘ä»¬å¼•å…¥åˆ†åŒºçš„æ¦‚å¿µã€‚å¯ä»¥æ ¹æ®PARTITIONED BYåˆ›å»ºåˆ†åŒºè¡¨ï¼Œä¸€ä¸ªè¡¨å¯ä»¥æ‹¥æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºä»¥æ–‡ä»¶å¤¹çš„å½¢å¼å•ç‹¬å­˜åœ¨è¡¨æ–‡ä»¶å¤¹çš„ç›®å½•ä¸‹ï¼›åˆ†åŒºæ˜¯ä»¥å­—æ®µçš„å½¢å¼åœ¨è¡¨ç»“æž„ä¸­å­˜åœ¨ï¼Œé€šè¿‡describe tableå‘½ä»¤å¯ä»¥æŸ¥çœ‹åˆ°å­—æ®µå­˜åœ¨ï¼Œä½†æ˜¯è¯¥å­—æ®µä¸å­˜æ”¾å®žé™…çš„æ•°æ®å†…å®¹ï¼Œä»…ä»…æ˜¯åˆ†åŒºçš„è¡¨ç¤ºã€‚åˆ†åŒºå»ºè¡¨åˆ†ä¸º2ç§ï¼Œä¸€ç§æ˜¯å•åˆ†åŒºï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨è¡¨æ–‡ä»¶å¤¹ç›®å½•ä¸‹åªæœ‰ä¸€çº§æ–‡ä»¶å¤¹ç›®å½•ã€‚å¦å¤–ä¸€ç§æ˜¯å¤šåˆ†åŒºï¼Œè¡¨æ–‡ä»¶å¤¹ä¸‹å‡ºçŽ°å¤šæ–‡ä»¶å¤¹åµŒå¥—æ¨¡å¼ã€‚å•åˆ†åŒºï¼š123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OKå¤šåˆ†åŒºï¼š123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMATå®˜ç½‘è§£é‡Šï¼š1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]DELIMITEDï¼šåˆ†éš”ç¬¦ï¼ˆå¯ä»¥è‡ªå®šä¹‰åˆ†éš”ç¬¦ï¼‰ï¼›FIELDS TERMINATED BY char:æ¯ä¸ªå­—æ®µä¹‹é—´ä½¿ç”¨çš„åˆ†å‰²ï¼›ä¾‹ï¼š-FIELDS TERMINATED BY â€˜\nâ€™ å­—æ®µä¹‹é—´çš„åˆ†éš”ç¬¦ä¸º\n;COLLECTION ITEMS TERMINATED BY char:é›†åˆä¸­å…ƒç´ ä¸Žå…ƒç´ ï¼ˆarrayï¼‰ä¹‹é—´ä½¿ç”¨çš„åˆ†éš”ç¬¦ï¼ˆcollectionå•ä¾‹é›†åˆçš„è·ŸæŽ¥å£ï¼‰ï¼›MAP KEYS TERMINATED BY charï¼šå­—æ®µæ˜¯K-Vå½¢å¼æŒ‡å®šçš„åˆ†éš”ç¬¦ï¼›LINES TERMINATED BY charï¼šæ¯æ¡æ•°æ®ä¹‹é—´ç”±æ¢è¡Œç¬¦åˆ†å‰²ï¼ˆé»˜è®¤[ \n ]ï¼‰ä¸€èˆ¬æƒ…å†µä¸‹LINES TERMINATED BY charæˆ‘ä»¬å°±ä½¿ç”¨é»˜è®¤çš„æ¢è¡Œç¬¦\nï¼Œåªéœ€è¦æŒ‡å®šFIELDS TERMINATED BY charã€‚åˆ›å»ºdemo1è¡¨ï¼Œå­—æ®µä¸Žå­—æ®µä¹‹é—´ä½¿ç”¨\tåˆ†å¼€ï¼Œæ¢è¡Œç¬¦ä½¿ç”¨é»˜è®¤\nï¼š123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OKåˆ›å»ºdemo2è¡¨ï¼Œå¹¶æŒ‡å®šå…¶ä»–å­—æ®µï¼š123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED ASï¼ˆå­˜å‚¨æ ¼å¼ï¼‰Create Table As Selectåˆ›å»ºè¡¨ï¼ˆæ‹·è´è¡¨ç»“æž„åŠæ•°æ®ï¼Œå¹¶ä¸”ä¼šè¿è¡ŒMapReduceä½œä¸šï¼‰12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;åŠ è½½æ•°æ®1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;å¤åˆ¶æ•´å¼ è¡¨12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt;å¤åˆ¶è¡¨ä¸­çš„ä¸€äº›å­—æ®µ1create table emp3 as select empno,ename from emp;LIKEä½¿ç”¨likeåˆ›å»ºè¡¨æ—¶ï¼Œåªä¼šå¤åˆ¶è¡¨çš„ç»“æž„ï¼Œä¸ä¼šå¤åˆ¶è¡¨çš„æ•°æ®1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt;å¹¶æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®desc formatted table_nameæŸ¥è¯¢è¡¨çš„è¯¦ç»†ä¿¡æ¯12hive&gt; desc formatted emp;OKcol_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno intDetailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt;é€šè¿‡æŸ¥è¯¢å¯ä»¥åˆ—å‡ºåˆ›å»ºè¡¨æ—¶çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥åœ¨mysqlä¸­æŸ¥è¯¢å‡ºè¿™äº›ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰select * from table_params;æŸ¥è¯¢æ•°æ®åº“ä¸‹çš„æ‰€æœ‰è¡¨1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt;æŸ¥è¯¢åˆ›å»ºè¡¨çš„è¯­æ³•123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later)æŒ‡å®šPURGEåŽï¼Œæ•°æ®ä¸ä¼šæ”¾åˆ°å›žæ”¶ç®±ï¼Œä¼šç›´æŽ¥åˆ é™¤DROP TABLEåˆ é™¤æ­¤è¡¨çš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚å¦‚æžœé…ç½®äº†åžƒåœ¾ç®±ï¼ˆå¹¶ä¸”æœªæŒ‡å®šPURGEï¼‰ï¼Œåˆ™å®žé™…å°†æ•°æ®ç§»è‡³.Trash / Currentç›®å½•ã€‚å…ƒæ•°æ®å®Œå…¨ä¸¢å¤±åˆ é™¤EXTERNALè¡¨æ—¶ï¼Œè¡¨ä¸­çš„æ•°æ®ä¸ä¼šä»Žæ–‡ä»¶ç³»ç»Ÿä¸­åˆ é™¤Alter Tableé‡å‘½å1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)ç”¨æˆ·å¯ä»¥ç”¨ ALTER TABLE ADD PARTITION æ¥å‘ä¸€ä¸ªè¡¨ä¸­å¢žåŠ åˆ†åŒºã€‚åˆ†åŒºåæ˜¯å­—ç¬¦ä¸²æ—¶åŠ å¼•å·ã€‚æ³¨ï¼šæ·»åŠ åˆ†åŒºæ—¶å¯èƒ½å‡ºçŽ°FAILED: SemanticException table is not partitioned but partition spec existsé”™è¯¯ã€‚åŽŸå› æ˜¯ï¼Œä½ åœ¨åˆ›å»ºè¡¨æ—¶å¹¶æ²¡æœ‰æ·»åŠ åˆ†åŒºï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨æ—¶åˆ›å»ºåˆ†åŒºï¼Œå†æ·»åŠ åˆ†åŒºã€‚hive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 secondsæŸ¥è¯¢ç»“æžœ123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);æŸ¥çœ‹åˆ†åŒºè¯­å¥12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s)æŒ‰åˆ†åŒºæŸ¥è¯¢1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveç”Ÿäº§ä¸Šï¼ŒåŽ‹ç¼©å’Œå­˜å‚¨ç»“åˆä½¿ç”¨æ¡ˆä¾‹]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[ä½ ä»¬Hiveç”Ÿäº§ä¸Šï¼ŒåŽ‹ç¼©å’Œå­˜å‚¨ï¼Œç»“åˆä½¿ç”¨äº†å—ï¼Ÿæ¡ˆä¾‹ï¼šåŽŸæ–‡ä»¶å¤§å°ï¼š19M1. ORC+Zlipç»“åˆ12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views;ç”¨ORC+Zlipä¹‹åŽçš„æ–‡ä»¶ä¸º2.8Mç”¨ORC+Zlipä¹‹åŽçš„æ–‡ä»¶ä¸º2.8M###### 2. Parquet+gzipç»“åˆ12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;ç”¨Parquet+gzipä¹‹åŽçš„æ–‡ä»¶ä¸º3.9M3. Parquet+Lzoç»“åˆ3.1 å®‰è£…Lzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile3.2 å®‰è£…Lzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile3.3 è½¯è¿žæŽ¥1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop3.4 æµ‹è¯•lzoplzop xxx.logè‹¥ç”Ÿæˆxxx.log.lzoæ–‡ä»¶ï¼Œåˆ™è¯´æ˜ŽæˆåŠŸ3.5 å®‰è£…Hadoop-LZO12345 gitæˆ–svn ä¸‹è½½https://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/3.6 é…ç½®åœ¨core-site.xmlé…ç½®1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;åœ¨mapred-site.xmlä¸­é…ç½® &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;åœ¨hadoop-env.shä¸­é…ç½®export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib3.7 æµ‹è¯•12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views;ç”¨Parquet+Lzo(æœªå»ºç«‹ç´¢å¼•)ä¹‹åŽçš„æ–‡ä»¶ä¸º5.9M]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>æ¡ˆä¾‹</tag>
        <tag>åŽ‹ç¼©æ ¼å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveå­˜å‚¨æ ¼å¼çš„ç”Ÿäº§åº”ç”¨]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ç›¸åŒæ•°æ®ï¼Œåˆ†åˆ«ä»¥TextFileã€SequenceFileã€RcFileã€ORCå­˜å‚¨çš„æ¯”è¾ƒã€‚åŽŸå§‹å¤§å°: 19M1. TextFile(é»˜è®¤) æ–‡ä»¶å¤§å°ä¸º18.1M2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY â€œ\tâ€ STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views;ç”¨SequenceFileå­˜å‚¨åŽçš„æ–‡ä»¶ä¸º19.6M3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views;ç”¨RcFileå­˜å‚¨åŽçš„æ–‡ä»¶ä¸º17.9M4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views;ç”¨ORCFileå­˜å‚¨åŽçš„æ–‡ä»¶ä¸º7.7M5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; ç”¨ORCFileå­˜å‚¨åŽçš„æ–‡ä»¶ä¸º13.1Mæ€»ç»“ï¼šç£ç›˜ç©ºé—´å ç”¨å¤§å°æ¯”è¾ƒORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>åŽ‹ç¼©æ ¼å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¤§æ•°æ®åŽ‹ç¼©ï¼Œä½ ä»¬çœŸçš„äº†è§£å—ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œå¸¦ä½ ä»¬å‰–æžå¤§æ•°æ®ä¹‹åŽ‹ç¼©ï¼1. åŽ‹ç¼©çš„å¥½å¤„å’Œåå¤„å¥½å¤„å‡å°‘å­˜å‚¨ç£ç›˜ç©ºé—´é™ä½ŽIO(ç½‘ç»œçš„IOå’Œç£ç›˜çš„IO)åŠ å¿«æ•°æ®åœ¨ç£ç›˜å’Œç½‘ç»œä¸­çš„ä¼ è¾“é€Ÿåº¦ï¼Œä»Žè€Œæé«˜ç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦åå¤„ç”±äºŽä½¿ç”¨æ•°æ®æ—¶ï¼Œéœ€è¦å…ˆå°†æ•°æ®è§£åŽ‹ï¼ŒåŠ é‡CPUè´Ÿè·2. åŽ‹ç¼©æ ¼å¼åŽ‹ç¼©æ¯”åŽ‹ç¼©æ—¶é—´å¯ä»¥çœ‹å‡ºï¼ŒåŽ‹ç¼©æ¯”è¶Šé«˜ï¼ŒåŽ‹ç¼©æ—¶é—´è¶Šé•¿ï¼ŒåŽ‹ç¼©æ¯”ï¼šSnappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2åŽ‹ç¼©æ ¼å¼ä¼˜ç‚¹ç¼ºç‚¹gzipåŽ‹ç¼©æ¯”åœ¨å››ç§åŽ‹ç¼©æ–¹å¼ä¸­è¾ƒé«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œåœ¨åº”ç”¨ä¸­å¤„ç†gzipæ ¼å¼çš„æ–‡ä»¶å°±å’Œç›´æŽ¥å¤„ç†æ–‡æœ¬ä¸€æ ·ï¼›æœ‰hadoop nativeåº“ï¼›å¤§éƒ¨åˆ†linuxç³»ç»Ÿéƒ½è‡ªå¸¦gzipå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿ä¸æ”¯æŒsplitlzoåŽ‹ç¼©/è§£åŽ‹é€Ÿåº¦ä¹Ÿæ¯”è¾ƒå¿«ï¼Œåˆç†çš„åŽ‹ç¼©çŽ‡ï¼›æ”¯æŒsplitï¼Œæ˜¯hadoopä¸­æœ€æµè¡Œçš„åŽ‹ç¼©æ ¼å¼ï¼›æ”¯æŒhadoop nativeåº“ï¼›éœ€è¦åœ¨linuxç³»ç»Ÿä¸‹è‡ªè¡Œå®‰è£…lzopå‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿åŽ‹ç¼©çŽ‡æ¯”gzipè¦ä½Žï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›lzoè™½ç„¶æ”¯æŒsplitï¼Œä½†éœ€è¦å¯¹lzoæ–‡ä»¶å»ºç´¢å¼•ï¼Œå¦åˆ™hadoopä¹Ÿæ˜¯ä¼šæŠŠlzoæ–‡ä»¶çœ‹æˆä¸€ä¸ªæ™®é€šæ–‡ä»¶ï¼ˆä¸ºäº†æ”¯æŒsplitéœ€è¦å»ºç´¢å¼•ï¼Œéœ€è¦æŒ‡å®šinputformatä¸ºlzoæ ¼å¼ï¼‰snappyåŽ‹ç¼©é€Ÿåº¦å¿«ï¼›æ”¯æŒhadoop nativeåº“ä¸æ”¯æŒsplitï¼›åŽ‹ç¼©æ¯”ä½Žï¼›hadoopæœ¬èº«ä¸æ”¯æŒï¼Œéœ€è¦å®‰è£…ï¼›linuxç³»ç»Ÿä¸‹æ²¡æœ‰å¯¹åº”çš„å‘½ä»¤d. bzip2bzip2æ”¯æŒsplitï¼›å…·æœ‰å¾ˆé«˜çš„åŽ‹ç¼©çŽ‡ï¼Œæ¯”gzipåŽ‹ç¼©çŽ‡éƒ½é«˜ï¼›hadoopæœ¬èº«æ”¯æŒï¼Œä½†ä¸æ”¯æŒnativeï¼›åœ¨linuxç³»ç»Ÿä¸‹è‡ªå¸¦bzip2å‘½ä»¤ï¼Œä½¿ç”¨æ–¹ä¾¿åŽ‹ç¼©/è§£åŽ‹é€Ÿåº¦æ…¢ï¼›ä¸æ”¯æŒnativeæ€»ç»“ï¼šä¸åŒçš„åœºæ™¯é€‰æ‹©ä¸åŒçš„åŽ‹ç¼©æ–¹å¼ï¼Œè‚¯å®šæ²¡æœ‰ä¸€ä¸ªä¸€åŠ³æ°¸é€¸çš„æ–¹æ³•ï¼Œå¦‚æžœé€‰æ‹©é«˜åŽ‹ç¼©æ¯”ï¼Œé‚£ä¹ˆå¯¹äºŽcpuçš„æ€§èƒ½è¦æ±‚è¦é«˜ï¼ŒåŒæ—¶åŽ‹ç¼©ã€è§£åŽ‹æ—¶é—´è€—è´¹ä¹Ÿå¤šï¼›é€‰æ‹©åŽ‹ç¼©æ¯”ä½Žçš„ï¼Œå¯¹äºŽç£ç›˜ioã€ç½‘ç»œioçš„æ—¶é—´è¦å¤šï¼Œç©ºé—´å æ®è¦å¤šï¼›å¯¹äºŽæ”¯æŒåˆ†å‰²çš„ï¼Œå¯ä»¥å®žçŽ°å¹¶è¡Œå¤„ç†ã€‚åº”ç”¨åœºæ™¯ï¼šä¸€èˆ¬åœ¨HDFS ã€Hiveã€HBaseä¸­ä¼šä½¿ç”¨ï¼›å½“ç„¶ä¸€èˆ¬è¾ƒå¤šçš„æ˜¯ç»“åˆSpark æ¥ä¸€èµ·ä½¿ç”¨ã€‚]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>åŽ‹ç¼©æ ¼å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå¸¸ç”¨å‘½ä»¤å¤§å…¨1. å•ç‹¬å¯åŠ¨å’Œå…³é—­hadoopæœåŠ¡åŠŸèƒ½å‘½ä»¤å¯åŠ¨åç§°èŠ‚ç‚¹hadoop-daemon.sh start namenodeå¯åŠ¨æ•°æ®èŠ‚ç‚¹hadoop-daemons.sh start datanode slaveå¯åŠ¨secondarynamenodehadoop-daemon.sh start secondarynamenodeå¯åŠ¨resourcemanageryarn-daemon.sh start resourcemanagerå¯åŠ¨nodemanagerbin/yarn-daemons.sh start nodemanageråœæ­¢æ•°æ®èŠ‚ç‚¹hadoop-daemons.sh stop datanode2. å¸¸ç”¨çš„å‘½ä»¤åŠŸèƒ½å‘½ä»¤åˆ›å»ºç›®å½•hdfs dfs -mkdir /inputæŸ¥çœ‹hdfs dfs -lsé€’å½’æŸ¥çœ‹hdfs dfs ls -Rä¸Šä¼ hdfs dfs -putä¸‹è½½hdfs dfs -getåˆ é™¤hdfs dfs -rmä»Žæœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ°hdfshdfs fs -moveFromLocal /input/xx.txt /input/xx.txtä»Žhdfså‰ªåˆ‡ç²˜è´´åˆ°æœ¬åœ°hdfs fs -moveToLocal /input/xx.txt /input/xx.txtè¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å¦ä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾hdfs fs -appedToFile ./hello.txt /input/hello.txtæŸ¥çœ‹æ–‡ä»¶å†…å®¹hdfs fs -cat /input/hello.txtæ˜¾ç¤ºä¸€ä¸ªæ–‡ä»¶åˆ°æœ«å°¾hdfs fs -tail /input/hello.txtä»¥å­—ç¬¦ä¸²çš„å½¢å¼æ‰“å°æ–‡ä»¶çš„å†…å®¹hdfs fs -text /input/hello.txtä¿®æ”¹æ–‡ä»¶æƒé™hdfs fs -chmod 666 /input/hello.txtä¿®æ”¹æ–‡ä»¶æ‰€å±žhdfs fs -chown ruoze.ruoze /input/hello.txtä»Žæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‹·è´åˆ°hdfsé‡Œhdfs fs -copyFromLocal /input/hello.txt /input/ä»Žhdfsæ‹·è´åˆ°æœ¬åœ°hdfs fs -copyToLocal /input/hello.txt /input/ä»Žhdfsåˆ°ä¸€ä¸ªè·¯å¾„æ‹·è´åˆ°å¦ä¸€ä¸ªè·¯å¾„hdfs fs -cp /input/xx.txt /output/xx.txtä»Žhdfsåˆ°ä¸€ä¸ªè·¯å¾„ç§»åŠ¨åˆ°å¦ä¸€ä¸ªè·¯å¾„hdfs fs -mv /input/xx.txt /output/xx.txtç»Ÿè®¡æ–‡ä»¶ç³»ç»Ÿçš„å¯ç”¨ç©ºé—´ä¿¡æ¯hdfs fs -df -h /ç»Ÿè®¡æ–‡ä»¶å¤¹çš„å¤§å°ä¿¡æ¯hdfs fs -du -s -h /ç»Ÿè®¡ä¸€ä¸ªæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶èŠ‚ç‚¹æ•°é‡hadoop fs -count /aaaè®¾ç½®hdfsçš„æ–‡ä»¶å‰¯æœ¬æ•°é‡hadoop fs -setrep 3 /input/xx.txtæ€»ç»“ï¼šä¸€å®šè¦å­¦ä¼šæŸ¥çœ‹å‘½ä»¤å¸®åŠ©1.hadoopå‘½ä»¤ç›´æŽ¥å›žè½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©2.hdfså‘½ä»¤ã€hdfs dfså‘½ä»¤ç›´æŽ¥å›žè½¦æŸ¥çœ‹å‘½ä»¤å¸®åŠ©3.hadoop fs ç­‰ä»· hdfs dfså‘½ä»¤ï¼Œå’ŒLinuxçš„å‘½ä»¤å·®ä¸å¤šã€‚]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒSpark2.2.0 å…¨ç½‘æœ€è¯¦ç»†çš„æºç ç¼–è¯‘çŽ¯å¢ƒå‡†å¤‡JDKï¼š Spark 2.2.0åŠä»¥ä¸Šç‰ˆæœ¬åªæ”¯æŒJDK1.8Mavenï¼š3.3.9è®¾ç½®mavençŽ¯å¢ƒå˜é‡æ—¶ï¼Œéœ€è®¾ç½®mavenå†…å­˜ï¼šexport MAVEN_OPTS=â€-Xmx2g -XX:ReservedCodeCacheSize=512mâ€Scalaï¼š2.11.8Gitç¼–è¯‘ä¸‹è½½sparkçš„taråŒ…ï¼Œå¹¶è§£åŽ‹12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgzç¼–è¾‘dev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.shæ³¨é‡Šä»¥ä¸‹å†…å®¹ï¼š#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n)æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1ç¼–è¾‘pom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xmlæ·»åŠ åœ¨repositoryså†…&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;å®‰è£…1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarnç¨å¾®ç­‰å¾…å‡ å°æ—¶ï¼Œç½‘ç»œè¾ƒå¥½çš„è¯ï¼Œéžå¸¸å¿«ã€‚ä¹Ÿå¯ä»¥å‚è€ƒJå“¥åšå®¢ï¼šåŸºäºŽCentOS6.4çŽ¯å¢ƒç¼–è¯‘Spark-2.1.0æºç  http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>çŽ¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnæ¨¡å¼ï¼Ÿ]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ç”Ÿäº§ä¸Šè¦é€‰æ‹©Spark On Yarnï¼Ÿå¼€å‘ä¸Šæˆ‘ä»¬é€‰æ‹©local[2]æ¨¡å¼ç”Ÿäº§ä¸Šè·‘ä»»åŠ¡Jobï¼Œæˆ‘ä»¬é€‰æ‹©Spark On Yarnæ¨¡å¼ ï¼Œå°†Spark Applicationéƒ¨ç½²åˆ°yarnä¸­ï¼Œæœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š1.éƒ¨ç½²Applicationå’ŒæœåŠ¡æ›´åŠ æ–¹ä¾¿åªéœ€è¦yarnæœåŠ¡ï¼ŒåŒ…æ‹¬Sparkï¼ŒStormåœ¨å†…çš„å¤šç§åº”ç”¨ç¨‹åºä¸è¦è¦è‡ªå¸¦æœåŠ¡ï¼Œå®ƒä»¬ç»ç”±å®¢æˆ·ç«¯æäº¤åŽï¼Œç”±yarnæä¾›çš„åˆ†å¸ƒå¼ç¼“å­˜æœºåˆ¶åˆ†å‘åˆ°å„ä¸ªè®¡ç®—èŠ‚ç‚¹ä¸Šã€‚2.èµ„æºéš”ç¦»æœºåˆ¶yarnåªè´Ÿè´£èµ„æºçš„ç®¡ç†å’Œè°ƒåº¦ï¼Œå®Œå…¨ç”±ç”¨æˆ·å’Œè‡ªå·±å†³å®šåœ¨yarné›†ç¾¤ä¸Šè¿è¡Œå“ªç§æœåŠ¡å’ŒApplicatioinï¼Œæ‰€ä»¥åœ¨yarnä¸Šæœ‰å¯èƒ½åŒæ—¶è¿è¡Œå¤šä¸ªåŒç±»çš„æœåŠ¡å’ŒApplicationã€‚Yarnåˆ©ç”¨Cgroupså®žçŽ°èµ„æºçš„éš”ç¦»ï¼Œç”¨æˆ·åœ¨å¼€å‘æ–°çš„æœåŠ¡æˆ–è€…Applicationæ—¶ï¼Œä¸ç”¨æ‹…å¿ƒèµ„æºéš”ç¦»æ–¹é¢çš„é—®é¢˜ã€‚3.èµ„æºå¼¹æ€§ç®¡ç†Yarnå¯ä»¥é€šè¿‡é˜Ÿåˆ—çš„æ–¹å¼ï¼Œç®¡ç†åŒæ—¶è¿è¡Œåœ¨yarné›†ç¾¤ç§çš„å¤šä¸ªæœåŠ¡ï¼Œå¯æ ¹æ®ä¸åŒç±»åž‹çš„åº”ç”¨ç¨‹åºåŽ‹åŠ›æƒ…å†µï¼Œè°ƒæ•´å¯¹åº”çš„èµ„æºä½¿ç”¨é‡ï¼Œå®žçŽ°èµ„æºå¼¹æ€§ç®¡ç†ã€‚Spark On Yarnæœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯clusteræ¨¡å¼ï¼Œä¸€ç§æ˜¯clientæ¨¡å¼ã€‚è¿è¡Œclientæ¨¡å¼ï¼šâ€œ./spark-shell â€“master yarnâ€â€œ./spark-shell â€“master yarn-clientâ€â€œ./spark-shell â€“master yarn â€“deploy-mode clientâ€è¿è¡Œçš„æ˜¯clusteræ¨¡å¼â€œ./spark-shell â€“master yarn-clusterâ€â€œ./spark-shell â€“master yarn â€“deploy-mode clusterâ€clientå’Œclusteræ¨¡å¼çš„ä¸»è¦åŒºåˆ«ï¼ša. clientçš„driveræ˜¯è¿è¡Œåœ¨å®¢æˆ·ç«¯è¿›ç¨‹ä¸­b. clusterçš„driveræ˜¯è¿è¡Œåœ¨Application Masterä¹‹ä¸­]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>é«˜çº§</tag>
        <tag>spark</tag>
        <tag>æž¶æž„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHiveå…¨ç½‘æœ€è¯¦ç»†çš„ç¼–è¯‘åŠéƒ¨ç½²ä¸€ã€éœ€è¦å®‰è£…çš„è½¯ä»¶ç›¸å…³çŽ¯å¢ƒï¼šjdk-7u80hadoop-2.6.0-cdh5.7.1 ä¸æ”¯æŒjdk1.8ï¼Œå› æ­¤æ­¤å¤„ä¹Ÿå»¶ç»­jdk1.7apache-maven-3.3.9mysql5.1hadoopä¼ªåˆ†å¸ƒé›†ç¾¤å·²å¯åŠ¨äºŒã€å®‰è£…jdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profileä¸‰ã€å®‰è£…maven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profileå››ã€å®‰è£…mysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges;äº”ã€ä¸‹è½½hiveæºç åŒ…ï¼šè¾“å…¥ï¼šhttp://archive.cloudera.com/cdh5/cdh/5/æ ¹æ®cdhç‰ˆæœ¬é€‰æ‹©å¯¹åº”hiveè½¯ä»¶åŒ…ï¼šhive-1.1.0-cdh5.7.1-src.tar.gzè§£åŽ‹åŽä½¿ç”¨mavenå‘½ä»¤ç¼–è¯‘æˆå®‰è£…åŒ…å…­ã€ç¼–è¯‘:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# ç¼–è¯‘ç”Ÿæˆçš„åŒ…åœ¨ä»¥ä¸‹ä½ç½®ï¼š# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzä¸ƒã€å®‰è£…ç¼–è¯‘ç”Ÿæˆçš„HiveåŒ…ï¼Œç„¶åŽæµ‹è¯•12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profileå…«ã€æ›´æ”¹çŽ¯å¢ƒå˜é‡12345su - hadoopcd /usr/local/hivecd conf1ã€hive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop2ã€hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ä¹ã€æ‹·è´mysqlé©±åŠ¨åŒ…åˆ°$HIVE_HOME/libä¸Šæ–¹çš„hive-site.xmlä½¿ç”¨äº†javaçš„mysqlé©±åŠ¨åŒ…éœ€è¦å°†è¿™ä¸ªåŒ…ä¸Šä¼ åˆ°hiveçš„libç›®å½•ä¹‹ä¸‹è§£åŽ‹ mysql-connector-java-5.1.45.zip å¯¹åº”çš„æ–‡ä»¶åˆ°ç›®å½•å³å¯1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/æœªæ‹·è´æœ‰ç›¸å…³æŠ¥é”™ï¼šThe specified datastore driver (â€œcom.mysql.jdbc.Driverâ€) was not found in the CLASSPATH.Please check your CLASSPATH specification,and the name of the driver.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>çŽ¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[è‹¥æ³½å¤§æ•°æ®ï¼ŒHadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(MapReduce+Yarn)ä¿®æ”¹mapred-site.xml1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ä¿®æ”¹yarn-site.xml12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;å¯åŠ¨123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.shå…³é—­1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>çŽ¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[Hadoopå…¨ç½‘æœ€è¯¦ç»†çš„ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²(HDFS)1.æ·»åŠ hadoopç”¨æˆ·123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# æ‰¾åˆ°root ALL=(ALL) ALLï¼Œæ·»åŠ hadoop ALL=(ALL) NOPASSWD:ALL2.ä¸Šä¼ å¹¶è§£åŽ‹123[root@hadoop-01 software]# rz #ä¸Šä¼ hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz3.è½¯è¿žæŽ¥1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop4.è®¾ç½®çŽ¯å¢ƒå˜é‡1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile5.è®¾ç½®ç”¨æˆ·ã€ç”¨æˆ·ç»„123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt6.åˆ‡æ¢hadoopç”¨æˆ·1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: å¯æ‰§è¡Œæ–‡ä»¶# etc: é…ç½®æ–‡ä»¶# sbin: shellè„šæœ¬ï¼Œå¯åŠ¨å…³é—­hdfs,yarnç­‰7.é…ç½®æ–‡ä»¶12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # é…ç½®è‡ªå·±æœºå™¨çš„IP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;8.é…ç½®hadoopç”¨æˆ·çš„sshä¿¡ä»»å…³ç³»8.1å…¬é’¥/å¯†é’¥ é…ç½®æ— å¯†ç ç™»å½•12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys8.2 æŸ¥çœ‹æ—¥æœŸï¼Œçœ‹æ˜¯å¦é…ç½®æˆåŠŸ1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # ç¬¬ä¸€æ¬¡å›žè½¦è¾“å…¥yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #ä¸éœ€è¦å›žè½¦è¾“å…¥yes,å³OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # ç¬¬ä¸€æ¬¡å›žè½¦è¾“å…¥yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #ä¸éœ€è¦å›žè½¦è¾“å…¥yes,å³OKSun Aug 20 14:22:29 CST 20179.æ ¼å¼åŒ–å’Œå¯åŠ¨123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found.9.1è§£å†³æ–¹æ³•:æ·»åŠ çŽ¯å¢ƒå˜é‡12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# å°†export JAVA_HOME=$&#123;JAVA_HOME&#125;æ”¹ä¸ºexport JAVA_HOME=/usr/java/jdk1.8.0_4512345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied9.2è§£å†³æ–¹æ³•:æ·»åŠ æƒé™123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop9.3 ç»§ç»­å¯åŠ¨1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh9.4æ£€æŸ¥æ˜¯å¦æˆåŠŸ123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode9.5è®¿é—®ï¼š http://192.168.137.130:500709.6ä¿®æ”¹dfså¯åŠ¨çš„è¿›ç¨‹ï¼Œä»¥hadoop-01å¯åŠ¨å¯åŠ¨çš„ä¸‰ä¸ªè¿›ç¨‹ï¼šnamenode: hadoop-01 bin/hdfs getconf -namenodesdatanode: localhost datanodes (using default slaves file) etc/hadoop/slavessecondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt;9.7é‡å¯123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>çŽ¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸€ï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®žæˆ˜å‘½ä»¤ï¼ˆä¸€ï¼‰æŸ¥çœ‹å½“å‰ç›®å½• pwdæŸ¥çœ‹IPifconfig æŸ¥çœ‹è™šæ‹Ÿæœºiphostname ä¸»æœºåå­—i æŸ¥çœ‹ä¸»æœºåæ˜ å°„çš„IPåˆ‡æ¢ç›®å½• cdcd ~ åˆ‡æ¢å®¶ç›®å½•ï¼ˆrootä¸º/rootï¼Œæ™®é€šç”¨æˆ·ä¸º/home/ç”¨æˆ·åï¼‰cd /filename ä»¥ç»å¯¹è·¯å¾„åˆ‡æ¢ç›®å½•cd - è¿”å›žä¸Šä¸€æ¬¡æ“ä½œè·¯å¾„ï¼Œå¹¶è¾“å‡ºè·¯å¾„cd ../ è¿”å›žä¸Šä¸€å±‚ç›®å½•æ¸…ç†æ¡Œé¢ clearæ˜¾ç¤ºå½“å‰ç›®å½•æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ lsls -l(ll) æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ls -la æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´ï¼Œä¾‹ï¼š.sshï¼‰ls -lh æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æ–‡ä»¶å¤§å°ls -lrt æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯+æŒ‰æ—¶é—´æŽ’åºæŸ¥çœ‹æ–‡ä»¶å¤¹å¤§å° du -shå‘½ä»¤å¸®åŠ©man å‘½ä»¤å‘½ä»¤ â€“helpåˆ›å»ºæ–‡ä»¶å¤¹ mkdirmkdir -p filename1/filename2 é€’å½’åˆ›å»ºæ–‡ä»¶å¤¹åˆ›å»ºæ–‡ä»¶ touch/vi/echo xx&gt;filenameæŸ¥çœ‹æ–‡ä»¶å†…å®¹cat filename ç›´æŽ¥æ‰“å°æ‰€æœ‰å†…å®¹more filename æ ¹æ®çª—å£å¤§å°è¿›è¡Œåˆ†é¡µæ˜¾ç¤ºæ–‡ä»¶ç¼–è¾‘ viviåˆ†ä¸ºå‘½ä»¤è¡Œæ¨¡å¼ï¼Œæ’å…¥æ¨¡å¼ï¼Œå°¾è¡Œæ¨¡å¼å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;æ’å…¥æ¨¡å¼ï¼šæŒ‰iæˆ–aé”®æ’å…¥æ¨¡å¼â€”&gt;å‘½ä»¤è¡Œæ¨¡å¼ï¼šæŒ‰Escé”®å‘½ä»¤è¡Œæ¨¡å¼â€”&gt;å°¾è¡Œæ¨¡å¼ï¼šæŒ‰Shiftå’Œ:é”®æ’å…¥æ¨¡å¼dd åˆ é™¤å…‰æ ‡æ‰€åœ¨è¡Œn+dd åˆ é™¤å…‰æ ‡ä»¥ä¸‹çš„nè¡ŒdG åˆ é™¤å…‰æ ‡ä»¥ä¸‹è¡Œgg ç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯G æœ€åŽä¸€è¡Œç¬¬ä¸€ä¸ªå­—æ¯shift+$ è¯¥è¡Œæœ€åŽä¸€ä¸ªå­—æ¯å°¾è¡Œæ¨¡å¼q! å¼ºåˆ¶é€€å‡ºqw å†™å…¥å¹¶é€€å‡ºqw! å¼ºåˆ¶å†™å…¥é€€å‡ºx é€€å‡ºï¼Œå¦‚æžœå­˜åœ¨æ”¹åŠ¨ï¼Œåˆ™ä¿å­˜å†é€€å‡º]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxå¸¸ç”¨å‘½ä»¤ï¼ˆä¸‰ï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®žæˆ˜å‘½ä»¤ï¼ˆä¸‰ï¼‰ç”¨æˆ·ã€ç”¨æˆ·ç»„ç”¨æˆ·useradd ç”¨æˆ·å æ·»åŠ ç”¨æˆ·userdel ç”¨æˆ·å åˆ é™¤ç”¨æˆ·id ç”¨æˆ·å æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯passwd ç”¨æˆ·å ä¿®æ”¹ç”¨æˆ·å¯†ç su - ç”¨æˆ·å åˆ‡æ¢ç”¨æˆ·ll /home/ æŸ¥çœ‹å·²æœ‰çš„ç”¨æˆ·ç”¨æˆ·ç»„groupadd ç”¨æˆ·ç»„ æ·»åŠ ç”¨æˆ·ç»„cat /etc/group ç”¨æˆ·ç»„çš„æ–‡ä»¶usermod -a -G ç”¨æˆ·ç»„ ç”¨æˆ· å°†ç”¨æˆ·æ·»åŠ åˆ°ç”¨æˆ·ç»„ä¸­ç»™ä¸€ä¸ªæ™®é€šç”¨æˆ·æ·»åŠ sudoæƒé™123vi /etc/sudoers #åœ¨root ALL=(ALL) ALL ä¸‹é¢æ·»åŠ ä¸€è¡Œ ç”¨æˆ· ALL=(ALL) NOPASSWD:ALLä¿®æ”¹æ–‡ä»¶æƒé™chown ä¿®æ”¹æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„æ‰€å±žç”¨æˆ·å’Œç”¨æˆ·ç»„chown -R ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶å¤¹å -R ä¸ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹chown ç”¨æˆ·:ç”¨æˆ·ç»„ æ–‡ä»¶åchmod: ä¿®æ”¹æ–‡ä»¶å¤¹æˆ–è€…æ–‡ä»¶çš„æƒé™chmod -R 700 æ–‡ä»¶å¤¹åchmod 700 æ–‡ä»¶å¤¹år =&gt; 4 w =&gt; 2 x =&gt; 1 åŽå°æ‰§è¡Œå‘½ä»¤&amp;nohupscreenå¤šäººåˆä½œ screenscreen -list æŸ¥çœ‹ä¼šè¯screen -S å»ºç«‹ä¸€ä¸ªåŽå°çš„ä¼šè¯screen -r è¿›å…¥ä¼šè¯ctrl+a+d é€€å‡ºä¼šè¯]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linuxå¸¸ç”¨å‘½ä»¤ï¼ˆäºŒï¼‰]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linuxæœ€å¸¸ç”¨å®žæˆ˜å‘½ä»¤ï¼ˆäºŒï¼‰å®žæ—¶æŸ¥çœ‹æ–‡ä»¶å†…å®¹ tail filenametail -f filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åŽï¼Œä¸èƒ½ç›‘è§†æ–‡ä»¶å†…å®¹tail -F filename å½“æ–‡ä»¶(å)è¢«ä¿®æ”¹åŽï¼Œä¾ç„¶å¯ä»¥ç›‘è§†æ–‡ä»¶å†…å®¹å¤åˆ¶ã€ç§»åŠ¨æ–‡ä»¶cp oldfilename newfilename å¤åˆ¶mv oldfilename newfilename ç§»åŠ¨/é‡å‘½åechoecho â€œxxxâ€ è¾“å‡ºecho â€œxxxâ€ &gt; filename è¦†ç›–echo â€œxxxâ€ &gt;&gt; filename è¿½åŠ åˆ é™¤ rmrm -f å¼ºåˆ¶åˆ é™¤rm -rf å¼ºåˆ¶åˆ é™¤æ–‡ä»¶å¤¹ï¼Œr è¡¨ç¤ºé€’å½’å‚æ•°ï¼ŒæŒ‡é’ˆå¯¹æ–‡ä»¶å¤¹åŠæ–‡ä»¶å¤¹é‡Œé¢æ–‡ä»¶åˆ«å aliasalias x=â€xxxxxxâ€ ä¸´æ—¶å¼•ç”¨åˆ«åalias x=â€xxxxxxâ€ é…ç½®åˆ°çŽ¯å¢ƒå˜é‡ä¸­å³ä¸ºæ°¸ä¹…ç”Ÿæ•ˆæŸ¥çœ‹åŽ†å²å‘½ä»¤ historyhistory æ˜¾ç¤ºå‡ºæ‰€æœ‰åŽ†å²è®°å½•history n æ˜¾ç¤ºå‡ºnæ¡è®°å½•!n æ‰§è¡Œç¬¬næ¡è®°å½•ç®¡é“å‘½ä»¤ ï¼ˆ | ï¼‰ç®¡é“çš„ä¸¤è¾¹éƒ½æ˜¯å‘½ä»¤ï¼Œå·¦è¾¹çš„å‘½ä»¤å…ˆæ‰§è¡Œï¼Œæ‰§è¡Œçš„ç»“æžœä½œä¸ºå³è¾¹å‘½ä»¤çš„è¾“å…¥æŸ¥çœ‹è¿›ç¨‹ã€æŸ¥çœ‹idã€ç«¯å£ps -ef ï½œgrep è¿›ç¨‹å æŸ¥çœ‹è¿›ç¨‹åŸºæœ¬ä¿¡æ¯netstat -nplï½œgrep è¿›ç¨‹åæˆ–è¿›ç¨‹id æŸ¥çœ‹æœåŠ¡idå’Œç«¯å£æ€æ­»è¿›ç¨‹ killkill -9 è¿›ç¨‹å/pid å¼ºåˆ¶åˆ é™¤kill -9 $(pgrep è¿›ç¨‹å)ï¼šæ€æ­»ä¸Žè¯¥è¿›ç¨‹ç›¸å…³çš„æ‰€æœ‰è¿›ç¨‹rpm æœç´¢ã€å¸è½½rpm -qa | grep xxx æœç´¢xxxrpm â€“nodeps -e xxx åˆ é™¤xxxâ€“nodeps ä¸éªŒè¯åŒ…çš„ä¾èµ–æ€§æŸ¥è¯¢find è·¯å¾„ -name xxx (æŽ¨è)which xxxlocal xxxæŸ¥çœ‹ç£ç›˜ã€å†…å­˜ã€ç³»ç»Ÿçš„æƒ…å†µdf -h æŸ¥çœ‹ç£ç›˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µfree -m æŸ¥çœ‹å†…å­˜å¤§å°åŠå…¶ä½¿ç”¨æƒ…å†µtop æŸ¥çœ‹ç³»ç»Ÿæƒ…å†µè½¯è¿žæŽ¥ln -s åŽŸå§‹ç›®å½• ç›®æ ‡ç›®å½•åŽ‹ç¼©ã€è§£åŽ‹tar -czf åŽ‹ç¼© tar -xzvf è§£åŽ‹zip åŽ‹ç¼© unzip è§£åŽ‹]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFSæž¶æž„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFSæž¶æž„è®¾è®¡åŠå‰¯æœ¬æ”¾ç½®ç­–ç•¥HDFSä¸»è¦ç”±3ä¸ªç»„ä»¶æž„æˆï¼Œåˆ†åˆ«æ˜¯NameNodeã€SecondaryNameNodeå’ŒDataNodeï¼ŒHSFSæ˜¯ä»¥master/slaveæ¨¡å¼è¿è¡Œçš„ï¼Œå…¶ä¸­NameNodeã€SecondaryNameNode è¿è¡Œåœ¨masterèŠ‚ç‚¹ï¼ŒDataNodeè¿è¡ŒslaveèŠ‚ç‚¹ã€‚NameNodeå’ŒDataNodeæž¶æž„å›¾NameNode(åç§°èŠ‚ç‚¹)å­˜å‚¨ï¼šå…ƒä¿¡æ¯çš„ç§ç±»ï¼ŒåŒ…å«:æ–‡ä»¶åç§°æ–‡ä»¶ç›®å½•ç»“æž„æ–‡ä»¶çš„å±žæ€§[æƒé™,åˆ›å»ºæ—¶é—´,å‰¯æœ¬æ•°]æ–‡ä»¶å¯¹åº”å“ªäº›æ•°æ®å—â€“&gt;æ•°æ®å—å¯¹åº”å“ªäº›datanodeèŠ‚ç‚¹ä½œç”¨ï¼šç®¡ç†ç€æ–‡ä»¶ç³»ç»Ÿå‘½åç©ºé—´ç»´æŠ¤è¿™æ–‡ä»¶ç³»ç»Ÿæ ‘åŠæ ‘ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•ç»´æŠ¤æ‰€æœ‰è¿™äº›æ–‡ä»¶æˆ–ç›®å½•çš„æ‰“å¼€ã€å…³é—­ã€ç§»åŠ¨ã€é‡å‘½åç­‰æ“ä½œDataNode(æ•°æ®èŠ‚ç‚¹)å­˜å‚¨ï¼šæ•°æ®å—ã€æ•°æ®å—æ ¡éªŒã€ä¸ŽNameNodeé€šä¿¡ä½œç”¨ï¼šè¯»å†™æ–‡ä»¶çš„æ•°æ®å—NameNodeçš„æŒ‡ç¤ºæ¥è¿›è¡Œåˆ›å»ºã€åˆ é™¤ã€å’Œå¤åˆ¶ç­‰æ“ä½œé€šè¿‡å¿ƒè·³å®šæœŸå‘NameNodeå‘é€æ‰€å­˜å‚¨æ–‡ä»¶å—åˆ—è¡¨ä¿¡æ¯Scondary NameNode(ç¬¬äºŒåç§°èŠ‚ç‚¹)å­˜å‚¨: å‘½åç©ºé—´é•œåƒæ–‡ä»¶fsimage+ç¼–è¾‘æ—¥å¿—editlogä½œç”¨: å®šæœŸåˆå¹¶fsimage+editlogæ–‡ä»¶ä¸ºæ–°çš„fsimageæŽ¨é€ç»™NamenNodeå‰¯æœ¬æ”¾ç½®ç­–ç•¥ç¬¬ä¸€å‰¯æœ¬ï¼šæ”¾ç½®åœ¨ä¸Šä¼ æ–‡ä»¶çš„DataNodeä¸Šï¼›å¦‚æžœæ˜¯é›†ç¾¤å¤–æäº¤ï¼Œåˆ™éšæœºæŒ‘é€‰ä¸€å°ç£ç›˜ä¸å¤ªæ…¢ã€CPUä¸å¤ªå¿™çš„èŠ‚ç‚¹ä¸Šç¬¬äºŒå‰¯æœ¬ï¼šæ”¾ç½®åœ¨ä¸Žç¬¬ä¸€ä¸ªå‰¯æœ¬ä¸åŒçš„æœºæž¶çš„èŠ‚ç‚¹ä¸Šç¬¬ä¸‰å‰¯æœ¬ï¼šä¸Žç¬¬äºŒä¸ªå‰¯æœ¬ç›¸åŒæœºæž¶çš„ä¸åŒèŠ‚ç‚¹ä¸Šå¦‚æžœè¿˜æœ‰æ›´å¤šçš„å‰¯æœ¬ï¼šéšæœºæ”¾åœ¨èŠ‚ç‚¹ä¸­]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>æž¶æž„</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é…ç½®å¤šå°è™šæ‹Ÿæœºä¹‹é—´çš„SSHä¿¡ä»»]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[æœ¬æœºçŽ¯å¢ƒ3å°æœºå™¨æ‰§è¡Œå‘½ä»¤ssh-keygené€‰å–ç¬¬ä¸€å°,ç”Ÿæˆauthorized_keysæ–‡ä»¶hadoop002 hadoop003ä¼ è¾“id_rsa.pubæ–‡ä»¶åˆ°hadoop001hadoop001æœºå™¨ åˆå¹¶id_rsa.pub2ã€id_rsa.pub3åˆ°authorized_keysè®¾ç½®æ¯å°æœºå™¨çš„æƒé™12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keyså°†authorized_keysåˆ†å‘åˆ°hadoop002ã€hadoop003æœºå™¨éªŒè¯(æ¯å°æœºå™¨ä¸Šæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œåªè¾“å…¥yesï¼Œä¸è¾“å…¥å¯†ç ï¼Œè¯´æ˜Žé…ç½®æˆåŠŸ)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>çŽ¯å¢ƒæ­å»º</tag>
        <tag>åŸºç¡€</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
