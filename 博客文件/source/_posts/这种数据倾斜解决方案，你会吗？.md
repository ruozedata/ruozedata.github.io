---
layout: post
title: "这种数据倾斜解决方案，你会吗？"
date: 2019-04-02
comments: true
tags: 
    - 数据倾斜
categories: 
    - 案例

---

<!--more--> 

### 概述

有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。

### 产生原因

- 方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。
- 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。
- 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。
- 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。
- 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。

![mark](http://pucwi7op1.bkt.clouddn.com/blog/20190724/DIOJ70yxFLMm.png?imageslim)

### 代码

```
import org.apache.spark.{SparkConf, SparkContext}
import scala.util.Random

object Demo {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("Demo").setMaster("local[2]")
    val sc=new SparkContext(conf)

    //准备数据
    val array=new Array[Int](10000)
    for (i <-0 to 9999){
      array(i)=new Random().nextInt(10)
    }
    //array.foreach(x=>print(x+","))
    //生成一个rdd
    val rdd=sc.parallelize(array)
    //数据量很大就先取样
    //rdd.sample(false,0.1)
    //所有key加一操作
    val maprdd=rdd.map((_,1))
    //没有加随机前缀的结果
      maprdd.countByKey.foreach(print)
    //(0,976)(5,997)(1,966)(6,959)(9,1004)(2,1051)(7,973)(3,1036)(8,1022)(4,1016)

    //val wc=rdd.map(x=>(x,1)).reduceByKey(_+_)
    //wc.foreach(print)
    //(4,1016)(0,976)(6,959)(8,1022)(2,1051)(1,966)(3,1036)(7,973)(9,1004)(5,997)

    //两阶段聚合（局部聚合+全局聚合）处理数据倾斜

    //加随机前缀,文章评论有正确代码
    val prifix=new Random().nextInt(10)
    val prifixrdd=maprdd.map(x=>(prifix+"_"+x._1,x._2))

    //加上随机前缀的key进行局部聚合
    val tmprdd=prifixrdd.reduceByKey(_+_)
    //去除随机前缀
    val newrdd=tmprdd.map(x=> (x._1.split("_")(1),x._2))
    //最终聚合
    newrdd.reduceByKey(_+_).foreach(print)
    //(4,1016)(7,973)(5,997)(9,1004)(8,1022)(6,959)(0,976)(3,1036)(2,1051)(1,966)
  }
}
```