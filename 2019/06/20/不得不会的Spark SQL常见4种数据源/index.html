<!-- build time:Tue Jul 09 2019 11:25:38 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/bitbug_favicon.ico?v=7.1.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/bitbug_favicon (2).ico?v=7.1.0"><link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"7.1.0",sidebar:{position:"left",display:"post",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="keywords" content="高级,Spark,SQL"><meta property="og:type" content="article"><meta property="og:title" content="不得不会的Spark SQL常见4种数据源"><meta property="og:url" content="http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/index.html"><meta property="og:site_name" content="若泽大数据 www.ruozedata.com"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2019-06-20T02:02:18.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="不得不会的Spark SQL常见4种数据源"><link rel="alternate" href="/atom.xml" title="若泽大数据 www.ruozedata.com" type="application/atom+xml"><link rel="canonical" href="http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>不得不会的Spark SQL常见4种数据源 | 若泽大数据 www.ruozedata.com</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">若泽大数据 www.ruozedata.com</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">ruozedata</p></div><div class="site-nav-toggle"><button aria-label="Basculer la barre de navigation"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-主页"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>主页</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-发展历史"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>发展历史</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Recherche</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="Recherche..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="post-meta"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="ruozedata"><meta itemprop="description" content="若泽数据优秀博客汇总"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="若泽大数据 www.ruozedata.com"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">不得不会的Spark SQL常见4种数据源</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posté le</span> <time title="Article créé le: 2019-06-20 00:00:00 / Mise à jour le: 10:02:18" itemprop="dateCreated datePublished" datetime="2019-06-20T00:00:00+08:00">2019-06-20</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Spark-SQL/" itemprop="url" rel="index"><span itemprop="name">Spark SQL</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> Vues: <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><a id="more"></a><h3 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h3><h4 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h4><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。</p><p>修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          </span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><h4 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h3><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Encoders for most common types are automatically provided by importing spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// DataFrames can be saved as Parquet files, maintaining the schema information</span><br><span class="line">peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Read in the parquet file created above</span><br><span class="line">// Parquet files are self-describing so the schema is preserved</span><br><span class="line">// The result of loading a Parquet file is also a DataFrame</span><br><span class="line">val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet files can also be used to create a temporary view and then used in SQL statements</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">// +------------+</span><br><span class="line">// |       value|</span><br><span class="line">// +------------+</span><br><span class="line">// |Name: Justin|</span><br><span class="line">// +------------+</span><br></pre></td></tr></table></figure><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet</p><p>或SQLContext.read.load，Spark SQL将自动解析分区信息。</p><p>返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项：</p><p><code>spark.sql.parquet.mergeSchema</code>为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sqlContext from the previous example is used in this example.</span><br><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, stored into a partition directory</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths.</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">.config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><h4 id="内嵌Hive应用"><a href="#内嵌Hive应用" class="headerlink" title="内嵌Hive应用"></a>内嵌Hive应用</h4><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.warehouse.dir=</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive应用"><a href="#外部Hive应用" class="headerlink" title="外部Hive应用"></a>外部Hive应用</h4><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;rddtable2&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br></pre></td></tr></table></figure></div><div><div id="reward-container"><div></div><button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">加群</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/assets/img/weixin.jpg" alt="ruozedata WeChat Pay"></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/高级/" rel="tag"># 高级</a> <a href="/tags/Spark/" rel="tag"># Spark</a> <a href="/tags/SQL/" rel="tag"># SQL</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/06/19/JVM快速调优手册之一_内存结构(堆内存和非堆内存)/" rel="next" title="JVM快速调优手册之一: 内存结构(堆内存和非堆内存)"><i class="fa fa-chevron-left"></i> JVM快速调优手册之一: 内存结构(堆内存和非堆内存)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2019/06/28/Docker实践之常用命令及自定义Web首页/" rel="prev" title="Docker实践之常用命令及自定义Web首页">Docker实践之常用命令及自定义Web首页 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table Des Matières</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Ensemble</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">ruozedata</p><div class="site-description motion-element" itemprop="description">若泽数据优秀博客汇总</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">133</span> <span class="site-state-item-name">articles</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">catégories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">51</span> <span class="site-state-item-name">mots clé</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ruozedata" title="GitHub &rarr; https://github.com/ruozedata" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/356836323?spm_id_from=333.788.b_765f7570696e666f.2" title="B站学习视频 &rarr; https://space.bilibili.com/356836323?spm_id_from=333.788.b_765f7570696e666f.2" rel="noopener" target="_blank"><i class="fa fa-fw fa-film"></i>B站学习视频</a> </span><span class="links-of-author-item"><a href="https://ruoze.ke.qq.com/#tab=1&category=-1" title="腾讯课堂学习视频 &rarr; https://ruoze.ke.qq.com/#tab=1&category=-1" rel="noopener" target="_blank"><i class="fa fa-fw fa-youtube"></i>腾讯课堂学习视频</a> </span><span class="links-of-author-item"><a href="http://www.ruozedata.com" title="官网 &rarr; http://www.ruozedata.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>官网</a></span></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#通用加载-保存方法"><span class="nav-number">1.</span> <span class="nav-text">通用加载/保存方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#手动指定选项"><span class="nav-number">1.1.</span> <span class="nav-text">手动指定选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#文件保存选项"><span class="nav-number">1.2.</span> <span class="nav-text">文件保存选项</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parquet文件"><span class="nav-number">2.</span> <span class="nav-text">Parquet文件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Parquet读写"><span class="nav-number">2.1.</span> <span class="nav-text">Parquet读写</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析分区信息"><span class="nav-number">2.2.</span> <span class="nav-text">解析分区信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Schema合并"><span class="nav-number">2.3.</span> <span class="nav-text">Schema合并</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive数据源"><span class="nav-number">3.</span> <span class="nav-text">Hive数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#内嵌Hive应用"><span class="nav-number">3.1.</span> <span class="nav-text">内嵌Hive应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#外部Hive应用"><span class="nav-number">3.2.</span> <span class="nav-text">外部Hive应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JSON数据集"><span class="nav-number">4.</span> <span class="nav-text">JSON数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JDBC"><span class="nav-number">5.</span> <span class="nav-text">JDBC</span></a></li></ol></div></div></div></div></aside></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"></div><span class="post-meta-divider">|</span><div class="theme-info"><a href="http://www.ruozedata.com">若泽数据</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total visiteurs"><span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Vues totales"><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/utils.js?v=7.1.0"></script><script src="/js/motion.js?v=7.1.0"></script><script src="/js/schemes/muse.js?v=7.1.0"></script><script src="/js/scrollspy.js?v=7.1.0"></script><script src="/js/post-details.js?v=7.1.0"></script><script src="/js/next-boot.js?v=7.1.0"></script><script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script><script>var gitalk=new Gitalk({clientID:"53baf2a5ee1c5d4e827f",clientSecret:"e4d5b4c40ee409e495d0a94c31ee721c0a404bbe",repo:"blog_issues",owner:"ruozedata",admin:["ruozedata"],id:md5(location.pathname),language:"zh-CN",distractionFreeMode:"true"});gitalk.render("gitalk-container")</script><script>function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var e=$("#local-search-input");e.attr("autocapitalize","none"),e.attr("autocorrect","off"),e.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(e){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(e,t,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:e,dataType:isXml?"xml":"json",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():e,r=document.getElementById(t),s=document.getElementById(o),a=function(){var e=r.value.trim().toLowerCase(),t=e.split(/[\s\-]+/);t.length>1&&t.push(e);var o=[];if(e.length>0&&n.forEach(function(n){function r(t,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===e&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(e,t){var o="",n=t.start;return t.hits.forEach(function(t){o+=e.substring(n,t.position);var r=t.position+t.length;o+='<b class="search-keyword">'+e.substring(t.position,r)+"</b>",n=r}),o+=e.substring(n,t.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url).replace(/\/{2,}/g,"/"),d=[],g=[];if(""!=l&&(t.forEach(function(e){function t(e,t,o){var n=e.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(t=t.toLowerCase(),e=e.toLowerCase());(s=t.indexOf(e,r))>-1;)a.push({position:s,word:e}),r=s+n;return a}d=d.concat(t(e,h,!1)),g=g.concat(t(e,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(e){e.sort(function(e,t){return t.position!==e.position?t.position-e.position:e.word.length-t.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hits.length!==t.hits.length?t.hits.length-e.hits.length:e.start-t.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(e){b+="<a href='"+f+'\'><p class="search-result">'+s(p,e)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===t.length&&""===t[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x"></i></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>';else{o.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hitCount!==t.hitCount?t.hitCount-e.hitCount:t.id-e.id});var a='<ul class="search-result-list">';o.forEach(function(e){a+=e.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(e){e.stopPropagation()}),$(document).on("keyup",function(e){var t=27===e.which&&$(".search-popup").is(":visible");t&&onPopupClose()})</script></body></html><!-- rebuild by neat -->