<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-24T02:21:20.245Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人人都应该会的ZooKeeper实战操作</title>
    <link href="http://yoursite.com/2019/07/23/%E4%BA%BA%E4%BA%BA%E9%83%BD%E5%BA%94%E8%AF%A5%E4%BC%9A%E7%9A%84ZooKeeper%E5%AE%9E%E6%88%98%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/07/23/人人都应该会的ZooKeeper实战操作/</id>
    <published>2019-07-22T16:00:00.000Z</published>
    <updated>2019-07-24T02:21:20.245Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="ZooKeeper数据结构"><a href="#ZooKeeper数据结构" class="headerlink" title="ZooKeeper数据结构"></a>ZooKeeper数据结构</h3><p>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。</p><p>很显然zookeeper集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个znode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/kUEByXevaAuM.png?imageslim" alt="mark"></p><h3 id="Zookeeper节点类型"><a href="#Zookeeper节点类型" class="headerlink" title="Zookeeper节点类型"></a>Zookeeper节点类型</h3><ol><li><p>Znode有两种类型</p><ul><li>短暂(ephemeral):客户端和服务器端断开连接后，创建的节点自己删除</li><li>持久(persistent):客户端和服务器端断开连接后，创建的节点不删除</li></ul></li><li><p>ZNode有四种形式的目录节点(默认是persistent)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(1)持久化目录节点（PERSISTENT）</span><br><span class="line">   客户端与zookeeper断开连接后，该节点依旧存在</span><br><span class="line">(2)持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL）</span><br><span class="line">   客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号</span><br><span class="line">(3)临时目录节点（EPHEMERAL）</span><br><span class="line">客户端与zookeeper断开连接后，该节点被删除</span><br><span class="line">(4)临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）</span><br><span class="line">客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</p></li><li><p>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</p></li></ol><h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><ul><li>Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。</li><li>Leader负责进行投票的发起和决议，更新系统状态。</li><li>Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。</li><li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。</li><li>全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。</li><li>更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</li><li>数据更新原子性，一次数据更新要么成功，要么失败。</li><li>实时性，在一定时间范围内，client能读到最新数据。</li></ul><h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><table><thead><tr><th style="text-align:center">命令基本语法</th><th style="text-align:center">功能描述</th></tr></thead><tbody><tr><td style="text-align:center">help</td><td style="text-align:center">显示所有操作命令</td></tr><tr><td style="text-align:center">ls path [watch]</td><td style="text-align:center">使用 ls 命令来查看当前znode中所包含的内容</td></tr><tr><td style="text-align:center">ls2 path [watch]</td><td style="text-align:center">查看当前节点数据并能看到更新次数等数据</td></tr><tr><td style="text-align:center">create</td><td style="text-align:center">普通创建 -s 含有序列 -e 临时（重启或者超时消失）</td></tr><tr><td style="text-align:center">get path [watch]</td><td style="text-align:center">获得节点的值</td></tr><tr><td style="text-align:center">set</td><td style="text-align:center">设置节点的具体值</td></tr><tr><td style="text-align:center">stat</td><td style="text-align:center">查看节点状态</td></tr><tr><td style="text-align:center">delete</td><td style="text-align:center">删除节点</td></tr><tr><td style="text-align:center">rmr</td><td style="text-align:center">递归删除节点</td></tr></tbody></table><ol><li><p>启动客户端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop bin]$ ./zkCli.sh</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>查看当前znode中所包含的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, config]</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>查看当前节点数据并能看到更新次数等数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls2 /</span><br><span class="line">[controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, config]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Wed Dec 31 16:00:00 GMT-08:00 1969</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Wed Dec 31 16:00:00 GMT-08:00 1969</span><br><span class="line">pZxid = 0x658</span><br><span class="line">cversion = 45</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create /test test</span><br><span class="line">Created /test</span><br><span class="line">[zk: localhost:2181(CONNECTED) 10] create /test/t1 t1</span><br><span class="line">Created /test/t1</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>获取节点的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get /test</span><br><span class="line">test</span><br><span class="line">cZxid = 0x67b</span><br><span class="line">ctime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">mZxid = 0x67b</span><br><span class="line">mtime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67b</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 0</span><br><span class="line">[zk: localhost:2181(CONNECTED) 11] get /test/t1</span><br><span class="line">t1</span><br><span class="line">cZxid = 0x67c</span><br><span class="line">ctime = Sun Apr 29 21:12:47 GMT-08:00 2018</span><br><span class="line">mZxid = 0x67c</span><br><span class="line">mtime = Sun Apr 29 21:12:47 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67c</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 2</span><br></pre></td></tr></table></figure></li></ol><pre><code>这里获取节点信息可以看到一堆信息，那这些代表什么意思呢？下面我们来看看具体的含义：- czxid - 引起这个znode创建的zxid，创建节点的事务的zxid（ZooKeeper Transaction Id）。每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于    zxid2，那么zxid1在zxid2之前发生。- ctime - znode被创建的毫秒数(从1970年开始)- mzxid - znode最后更新的zxid- mtime - znode最后修改的毫秒数(从1970年开始)- pZxid-znode最后更新的子节点zxid- cversion - znode子节点变化号，znode子节点修改次数- dataversion - znode数据变化号- aclVersion - znode访问控制列表的变化号- ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。- dataLength- znode的数据长度- numChildren - znode子节点数量</code></pre><ol start="6"><li><p>创建临时节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] create -e /tmp_test tmp_test</span><br><span class="line">Created /tmp_test</span><br><span class="line">[zk: localhost:2181(CONNECTED) 13] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, test, tmp_test, admin, isr_change_notification, consumers, config]</span><br><span class="line">退出</span><br><span class="line">[zk: localhost:2181(CONNECTED) 14] quit</span><br><span class="line">Quitting...</span><br><span class="line">2018-04-29 21:17:58,754 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x1630235a6260042 closed</span><br><span class="line">2018-04-29 21:17:58,755 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@512] - EventThread shut down</span><br><span class="line">重启</span><br><span class="line">[hadoop@hadoop bin]$ ./zkCli.sh</span><br><span class="line">再次查看</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, test, admin, isr_change_notification, consumers, config]</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>创建带序号的节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /num_test num_test</span><br><span class="line">Created /num_test0000000028</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /num_test1 num_test1</span><br><span class="line">Created /num_test10000000029</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /num_test2 num_test2</span><br><span class="line">Created /num_test20000000030</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>修改节点值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /test change_test</span><br><span class="line">cZxid = 0x67b</span><br><span class="line">ctime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">mZxid = 0x689</span><br><span class="line">mtime = Sun Apr 29 21:26:53 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67c</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure></li><li><p>删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] delete /num_test0000000028</span><br></pre></td></tr></table></figure></li><li><p>递归删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] rmr /test</span><br></pre></td></tr></table></figure></li><li><p>查看节点状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] stat /consumers</span><br><span class="line">cZxid = 0x2</span><br><span class="line">ctime = Wed Apr 25 01:41:56 GMT-08:00 2018</span><br><span class="line">mZxid = 0x2</span><br><span class="line">mtime = Wed Apr 25 01:41:56 GMT-08:00 2018</span><br><span class="line">pZxid = 0x3f5</span><br><span class="line">cversion = 37</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="ZooKeeper" scheme="http://yoursite.com/categories/ZooKeeper/"/>
    
    
      <category term="ZooKeeper" scheme="http://yoursite.com/tags/ZooKeeper/"/>
    
      <category term="实战操作" scheme="http://yoursite.com/tags/%E5%AE%9E%E6%88%98%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Kafka之数据迁移</title>
    <link href="http://yoursite.com/2019/07/18/Kafka%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    <id>http://yoursite.com/2019/07/18/Kafka之数据迁移/</id>
    <published>2019-07-17T16:00:00.000Z</published>
    <updated>2019-07-24T02:19:13.265Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>当Kafka 减少Broker节点后，需要把数据分区迁移到其他节点上，以下将介绍我的一次迁移验证过程。</p><p>前3步为环境准备，实际数据操作看第4步即可</p><p>增加Broker节点，也可以采用步骤4相同的方法进行重新分区</p><p>方案思想：<code>使用kafka-reassign-partitions命令，把partition重新分配到指定的Broker上</code></p><h3 id="创建测试topic，具体为3个分区，2个副本"><a href="#创建测试topic，具体为3个分区，2个副本" class="headerlink" title="创建测试topic，具体为3个分区，2个副本"></a>创建测试topic，具体为3个分区，2个副本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics --create --topic test-topic \</span><br><span class="line">--zookeeper cdh-002/kafka \</span><br><span class="line">--replication-factor 2 --partitions 3</span><br></pre></td></tr></table></figure><h3 id="查看创建的topic"><a href="#查看创建的topic" class="headerlink" title="查看创建的topic"></a>查看创建的topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics --describe --zookeeper cdh-002/kafka --topic test-topic</span><br></pre></td></tr></table></figure><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/TJHSXayn2rpq.png?imageslim" alt="mark"></p><h3 id="产生若干条测试数据"><a href="#产生若干条测试数据" class="headerlink" title="产生若干条测试数据"></a>产生若干条测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer --topic test-topic \</span><br><span class="line">--broker-list cdh-004:9092</span><br></pre></td></tr></table></figure><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/XaBgiGo83Dhh.png?imageslim" alt="mark"></p><h3 id="使用命令进行重分区"><a href="#使用命令进行重分区" class="headerlink" title="使用命令进行重分区"></a>使用命令进行重分区</h3><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/UAX7PCNdJsRp.png?imageslim" alt="mark"></p><ol><li><p>新建文件topic-to-move.json ，比如加入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;:&quot;test-topic&quot;&#125;], &quot;version&quot;: 1&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用–generate生成迁移计划，broker-list根据自己环境设置，我的环境由于broker 75挂掉了，只剩下76和77</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--topics-to-move-json-file /opt/lb/topic-to-move.json \</span><br><span class="line">--broker-list &quot;76,77&quot; --generate</span><br></pre></td></tr></table></figure><p>输出日志：</p><p>（<font color="red">从日志可知各个分区副本所在的Broker节点，以及建议的副本分布</font>）</p><p>Current partition replica assignment (<font color="red">当前分区副本分布</font>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:0,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:2,</span><br><span class="line">&quot;replicas&quot;:[75,76]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:1,</span><br><span class="line">&quot;replicas&quot;:[77,75]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Proposed partition reassignment configuration (<font color="red">建议分区副本分布</font>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:0,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:2,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:1,</span><br><span class="line">&quot;replicas&quot;:[77,76]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>新建文件kafka-reassign-execute.json，并把建议的分区副本分布配置拷贝到新建文件中。(生产上一般会保留当前分区副本分布，仅更改下线的分区，这样数据移动更少)</p></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/5b90JllUjzyl.png?imageslim" alt="mark"></p><ol start="4"><li><p>使用–execute执行迁移计划 (有数据移动，broker 75上的数据会移到broker 76和77上，如果数据量大，执行的时间会比较久，耐心等待即可)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--reassignment-json-file /opt/lb/kafka-reassign-execute.json \</span><br><span class="line">--execute</span><br></pre></td></tr></table></figure></li><li><p>使用-verify查看迁移进度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--reassignment-json-file /opt/lb/kafka-reassign-execute.json \</span><br><span class="line">--verify</span><br></pre></td></tr></table></figure></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/NRvEyHQV56rh.png?imageslim" alt="mark"></p><ol start="6"><li><p>通过消费者验证，可知，并未丢失数据。注意需要加–from-beginning。(此时broker 75和77同时宕机，也不会丢失数据，因为76上有了所有分区的副本)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer --topic test-topic --from-beginning --zookeeper cdh-002/kafka</span><br></pre></td></tr></table></figure></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/0B6vo0NKWXaj.png?imageslim" alt="mark"></p><pre><code>&lt;font color=&quot;red&quot; size=4&gt;另外一种验证方法是:（生产最佳实践）&lt;/font&gt;另外一种验证方法就是通过查看Kafka存储路径来确认，是否有迁移数据</code></pre><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/Bs9N0qEqie1Y.png?imageslim" alt="mark"></p><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cdh-003 ~]# cd /var/local/kafka/data/</span><br><span class="line">[root@cdh-003 data]# ll</span><br><span class="line">rwxr-xr-x 2 kafka kafka  110 Oct 23 14:21 test-topic-0</span><br><span class="line">drwxr-xr-x 2 kafka kafka  110 Oct 23 14:52 test-topic-1</span><br><span class="line">drwxr-xr-x 2 kafka kafka  110 Oct 23 14:21 test-topic-2</span><br></pre></td></tr></table></figure></code></pre><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://yoursite.com/tags/Kafka/"/>
    
      <category term="数据迁移" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>知遇若泽数据，成就自己未来</title>
    <link href="http://yoursite.com/2019/07/16/%E7%9F%A5%E9%81%87%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%88%90%E5%B0%B1%E8%87%AA%E5%B7%B1%E6%9C%AA%E6%9D%A5/"/>
    <id>http://yoursite.com/2019/07/16/知遇若泽数据，成就自己未来/</id>
    <published>2019-07-15T16:00:00.000Z</published>
    <updated>2019-07-24T02:24:47.066Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><center>不吹不擂，不自不黑，</center><br><center>你愿意花3min，阅读刚毕业的我吗？</center><br><center>以下所有内容真实，禁得住考验！</center><a id="more"></a><h2 id="应届的我"><a href="#应届的我" class="headerlink" title="应届的我"></a>应届的我</h2><p>我是一名应届毕业生，虽然是计算机专业，但是大学里过于理论与老旧的知识无法满足我的求知欲，听说大数据是热门，便通过各种方法学习。最后结缘若泽数据，也多亏若老和J哥，让我成功掌握大数据，端午节线下班第14期，一结束的第二天，我便拿到了自己满意的offer，坐标 长沙。(底部有截图)</p><h2 id="遇见你不容易"><a href="#遇见你不容易" class="headerlink" title="遇见你不容易"></a>遇见你不容易</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;哈哈哈，若泽数据唯一的缺点，就是广告不到位，对于一个啥也不懂的小白，甚至是有几年工作经验的码农，要找到它是真的不容易。相信很多小伙伴都有过类似的经历，<strong>百度一下大数据培训，那广告是琳琅满目，百花齐放</strong>，口号那是一个比一个响亮，简直是让人热血沸腾，看了都感觉自己好像已经达到了人生巅峰，可真正进去之后，就发现梦想很丰满，现实很骨感。别问我为什么，因为我就是。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;去年年底，因为我在某机构，实在学不懂spark，整整蒙蔽徘徊一个多星期后，决定咬牙另起炉灶自学，尝试过重新看机构录下来的视频，看网上五花八门的博客，可仍然是迷茫，一度怀疑自己的智商不适合学大数据了，直到偶然间在慕课上浏览并观看了若泽PK的Spark相关视频，我才找回了自己。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;就在看完慕课网的课程没多久，马上幸运女神又再次眷属我，当得知<font color="blue">若泽数据要办2019年上半年第六期高级班</font>，我就马上报名参加，在此也十分感谢我的家人，因为他们一直都支持我学习，提供第二次学习的经济支柱，为了不辜负他们的期望，便天天拿着小板凳盼着开班、开班、开班。</p><h2 id="高级班全力冲刺的我"><a href="#高级班全力冲刺的我" class="headerlink" title="高级班全力冲刺的我"></a>高级班全力冲刺的我</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;高级班的学习氛围非常浓厚，在这里也认识了许许多多有经验有梦想的大佬和小伙伴们，无论是班群还是小组群，都是<font color="blue">激情澎湃</font>的，我虽然大多数时间潜水，但围观大家的讨论，也学到了很多解决问题的办法，也避免了自己学习上可能会遇到的很多坑。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;老师们讲的内容都是直接跟<font color="red">企业生产</font>上挂钩的，就拿Spark来说，从零到有，深入浅出，结合具体案例和相关性能调优的方方面面细火慢炖，结合官网讲解，完美地将大数据的原滋原味呈现出来。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;除了课堂内容无可挑剔以外，老师为了保证我们的吸收效果，课下严格要求我们完成相应布置的作业，每周上课都会随机抽取2名同学讲解作业，再结合作业去理解老师的课堂内容就<font color="blue">如鱼得水</font>了。</p><h2 id="线下班使出吃奶劲的我"><a href="#线下班使出吃奶劲的我" class="headerlink" title="线下班使出吃奶劲的我"></a>线下班使出吃奶劲的我</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;线下班是含金量十足而且十分紧凑的，一群小伙伴早上九点学到晚上12点，<font color="blue">高端的食材，往往只需要最朴素的烹饪</font>，线下班所有的知识点都是平时高级班一点一点做好铺垫和前置的，我们只管做好笔记，就每一个生产可能会遇到的问题和具体的解决办法，细嚼慢咽，理解到骨髓里，那种成就感和迫不及待迎接offer的自信便油然而生了。</p><h2 id="希望有缘人，能够成为我的学弟"><a href="#希望有缘人，能够成为我的学弟" class="headerlink" title="希望有缘人，能够成为我的学弟"></a>希望有缘人，能够成为我的学弟</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;若泽数据不仅仅传授的是知识，更多的是获取知识的方法，及时写博客记录，结合官网学习，遇见错误如何定位解决等等，都是我现在入职后受益很大的财富，希望越来越多的小伙伴能够认识若泽数据，找到自己的方向，走向人生巅峰。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;也祝愿若泽数据能够越办越好，力争中国第一大数据培训机构，为我国大数据的繁荣昌盛，输送更多的人才。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<font color="blue">学弟们，我是2019届刚毕业的高级班第6期某学员，也是你们的学长哟，长沙等你哟！</font></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/DwjTCTeHvKhr.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/0hUTrfwVQxjm.png?imageslim" alt="mark"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;&lt;center&gt;不吹不擂，不自不黑，&lt;/center&gt;&lt;br&gt;&lt;center&gt;你愿意花3min，阅读刚毕业的我吗？&lt;/center&gt;&lt;br&gt;&lt;center&gt;以下所有内容真实，禁得住考验！&lt;/center&gt;
    
    </summary>
    
      <category term="有缘大数据" scheme="http://yoursite.com/categories/%E6%9C%89%E7%BC%98%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://yoursite.com/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="企业在职" scheme="http://yoursite.com/tags/%E4%BC%81%E4%B8%9A%E5%9C%A8%E8%81%8C/"/>
    
      <category term="感悟" scheme="http://yoursite.com/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>Kudu与Spark集成</title>
    <link href="http://yoursite.com/2019/07/12/Kudu%E4%B8%8ESpark%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2019/07/12/Kudu与Spark集成/</id>
    <published>2019-07-11T16:00:00.000Z</published>
    <updated>2019-07-23T08:59:20.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">    &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;</span><br><span class="line">    &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object KuduApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">     //Read a table from Kudu</span><br><span class="line">    val df = spark.read</span><br><span class="line">          .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;))</span><br><span class="line">          .format(&quot;kudu&quot;).load</span><br><span class="line">        df.schema.printTreeString()</span><br><span class="line">//    // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">//    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    // The schema is encoded in a string</span><br><span class="line">//    val schemalString=&quot;id,age,name&quot;</span><br><span class="line">//</span><br><span class="line">//    // Generate the schema based on the string of schema</span><br><span class="line">//    val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))</span><br><span class="line">//    val schema=StructType(fields)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    val KuduTable = kuduContext.createTable(</span><br><span class="line">//     &quot;test_table&quot;, schema, Seq(&quot;id&quot;),</span><br><span class="line">//     new CreateTableOptions()</span><br><span class="line">//       .setNumReplicas(1)</span><br><span class="line">//       .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema</span><br><span class="line">//</span><br><span class="line">//    val  id  = KuduTable.getColumn(&quot;id&quot;)</span><br><span class="line">//    print(id)</span><br><span class="line">//</span><br><span class="line">//    kuduContext.tableExists(&quot;test_table&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font size="3"><b>现象:通过spark sql 操作报如下错误:</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)</span><br><span class="line">    at cn.zhangyu.KuduApp$.main(KuduApp.scala:18)</span><br><span class="line">    at cn.zhangyu.KuduApp.main(KuduApp.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try.orElse(Try.scala:84)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure><font size="3"><b>而通过KuduContext是可以操作的没有报错,代码为上面注解部分</b></font><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>查询<a href="https://kudu.apache.org/" target="_blank" rel="noopener">kudu官网</a>:</p><p>官网中说出了版本的问题<br><br>如果将Spark 2与Scala 2.11一起使用，请使用kudu-spark2_2.11控件。<br><br>kudu-spark版本1.8.0及更低版本的语法略有不同。有关有效示例，请参阅您的版本的文档。可以在发布页面上找到版本化文档。</p><ul><li>spark-shell –packages org.apache.kudu:kudu-spark2_2.11:1.9.0 看到了 官网使用的是1.9.0的版本.</li></ul><font size="3"><b>但是但是但是</b></font><p>官网下面说到了下面几个集成问题</p><ul><li>Spark 2.2+在运行时需要Java 8，即使Kudu Spark 2.x集成与Java 7兼容。Spark 2.2是Kudu 1.5.0的默认依赖版本。</li><li>当注册为临时表时，必须为名称包含大写或非ascii字符的Kudu表分配备用名称。</li><li>包含大写或非ascii字符的列名的Kudu表不能与SparkSQL一起使用。可以在Kudu中重命名列以解决此问题。</li><li>并且OR谓词不会被推送到Kudu，而是由Spark任务进行评估。只有LIKE带有后缀通配符的谓词才会被推送到Kudu，这意味着它LIKE “FOO%”被推下但LIKE “FOO%BAR”不是。</li><li>Kudu不支持Spark SQL支持的每种类型。例如， Date不支持复杂类型。</li><li>Kudu表只能在SparkSQL中注册为临时表。使用HiveContext可能无法查询Kudu表。</li></ul><font size="3"><b>那就很奇怪了我用的1.5.0版本报错为:找不到类,数据源有问题</b></font><p>但是把Kudu改成<code>1.9.0</code> 问题解决</p><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = false)</span><br><span class="line"> |-- age: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Spark集成最佳实践"><a href="#Spark集成最佳实践" class="headerlink" title="Spark集成最佳实践"></a>Spark集成最佳实践</h3><ul><li><p>每个群集避免多个Kudu客户端。</p><p>一个常见的Kudu-Spark编码错误是实例化额外的KuduClient对象。在kudu-spark中，a KuduClient属于KuduContext。Spark应用程序代码不应创建另一个KuduClient连接到同一群集。相反，应用程序代码应使用KuduContext访问KuduClient使用 KuduContext#syncClient。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">   val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">   val list = kuduContext.syncClient.getTablesList.getTablesList</span><br><span class="line">   if (list.iterator().hasNext)&#123;</span><br><span class="line">     print(list.iterator().next())</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li>要诊断KuduClientSpark作业中的多个实例，请查看主服务器的日志中的符号，这些符号会被来自不同客户端的许多GetTableLocations或 GetTabletLocations请求过载，通常大约在同一时间。这种症状特别适用于Spark Streaming代码，其中创建KuduClient每个任务将导致来自新客户端的主请求的周期性波。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
      <category term="集成" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>生产上Flume如何源码编译and远程Debug</title>
    <link href="http://yoursite.com/2019/07/11/%E7%94%9F%E4%BA%A7%E4%B8%8AFlume%E5%A6%82%E4%BD%95%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91and%E8%BF%9C%E7%A8%8BDebug/"/>
    <id>http://yoursite.com/2019/07/11/生产上Flume如何源码编译and远程Debug/</id>
    <published>2019-07-10T16:00:00.000Z</published>
    <updated>2019-07-24T02:22:50.338Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h3><ol><li>apache-flume-1.8.0-src （官网下载源码，或者git下载）</li><li>jdk1.8</li></ol><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><ol><li>用Inteallij IDEA 导入已下载的flume工程</li><li><p>修改<code>flume-parent</code>下的 pom.xml 添加 aliyun的仓库（加快下载，有些包直接从maven repository上下载很慢 ）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;&lt;!-- 代码库 --&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;maven-ali&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">        &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">        &lt;/releases&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">            &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;</span><br><span class="line">            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;</span><br><span class="line">        &lt;/snapshots&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure></li><li><p>开始漫长的编译过程</p><p>如果是第一次的话，可能下载包要花2个多小时，中间可能会报错（报错主要是某些包没下载成功，此时可以手动从仓库中手动下载到本地，然后放在本地 的maven 包路径下，默认的本地的包路径是 C:\Users\你的用户名.m2\repository 下面）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn clean</span><br><span class="line">mvn install -DskipTests -U -Dtar</span><br></pre></td></tr></table></figure></li><li><p>由于整个项目是用pom管理包和模块，十分方便，如果在整个编译过程中，某些模块你需要编译，或者编译耗时，或者编译失败，并且你暂时用不到整个模块，可以从pom中注释掉这个模块，不做编译，具体做法如下图所示（具体的根据你的需求操作即可）<br><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/9xjUKMaIiAlm.png?imageslim" alt="mark"></p></li></ol><h3 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h3><ol><li><p>修改服务器上的 bin/flume-ng 中的JAVA_OPTS变量，支持远程调试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS=&quot;-Xmx20m -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y&quot;</span><br></pre></td></tr></table></figure></li></ol><pre><code>具体如下：</code></pre><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/3LFwnuvEtCmR.png?imageslim" alt="mark"></p><ol start="2"><li>Inteallij IDEA配置 ，远程调试</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/kIM6h3SrtMSb.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/xq1oTGBX3gIa.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/IjVaRSdXrHWJ.png?imageslim" alt="mark"></p><ol start="3"><li>在任意代码出打上断点</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/zyyRQQtEuqLu.png?imageslim" alt="mark"></p><ol start="4"><li><p>启动flume-ng（按实际情况修改下面命令）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file ./conf/flume-custom.properties --name hd1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>启动后日志如下：</p></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/vrL4ekeR7sdi.png?imageslim" alt="mark"></p><ol start="5"><li>Inteallij IDEA 开始debug，可以发现在断点处停止，debug流程成功了</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/hSlp85lEX5Dy.png?imageslim" alt="mark"></p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul><li>源码编译耗时费力，需要耐心，熬过去了，会有很大收获，同样也是更好理解源码的开始，万事开头难</li><li>remote debug可以更方便的了解执行流程，学习源码的捷径</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://yoursite.com/categories/Flume/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
      <category term="源码编译" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Docker实践之常用命令及自定义Web首页</title>
    <link href="http://yoursite.com/2019/06/28/Docker%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89Web%E9%A6%96%E9%A1%B5/"/>
    <id>http://yoursite.com/2019/06/28/Docker实践之常用命令及自定义Web首页/</id>
    <published>2019-06-27T16:00:00.000Z</published>
    <updated>2019-07-09T03:05:32.043Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker --help</span><br><span class="line">//常用命令：</span><br><span class="line">--------------------------------------------</span><br><span class="line">  exec        Run a command in a running container</span><br><span class="line">  history     Show the history of an image</span><br><span class="line">  images      List images</span><br><span class="line">  kill        Kill one or more running containers</span><br><span class="line">  logs        Fetch the logs of a container</span><br><span class="line">  ps          List containers</span><br><span class="line">  pull        Pull an image or a repository from a registry</span><br><span class="line">  push        Push an image or a repository to a registry</span><br><span class="line">  rename      Rename a container</span><br><span class="line">  restart     Restart one or more containers</span><br><span class="line">  rm          Remove one or more containers</span><br><span class="line">  rmi         Remove one or more images</span><br><span class="line">  run         Run a command in a new container</span><br><span class="line">  search      Search the Docker Hub for images</span><br><span class="line">  start       Start one or more stopped containers</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics</span><br><span class="line">  stop        Stop one or more running containers</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span><br><span class="line">  top         Display the running processes of a container</span><br><span class="line">  version     Show the Docker version information</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker search nginx</span><br><span class="line">NAME                                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">nginx                                                  Official build of Nginx.                        10179               [OK]</span><br><span class="line">jwilder/nginx-proxy                                    Automated Nginx reverse proxy for docker con…   1454                                    [OK]</span><br><span class="line">richarvey/nginx-php-fpm                                Container running Nginx + PHP-FPM capable of…   645                                     [OK]</span><br><span class="line">jrcs/letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as p…   436                                     [OK]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker pull nginx   //拉取官方版本的nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">f17d81b4b692: Pull complete</span><br><span class="line">82dca86e04c3: Downloading  11.24MB/22.2MB</span><br><span class="line">82dca86e04c3: Pull complete</span><br><span class="line">046ccb106982: Pull complete</span><br><span class="line">Digest: sha256:d59a1aa7866258751a261bae525a1842c7ff0662d4f34a355d5f36826abc0341</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>docker相当于一个小型的linux系统，但是它又只是一个单一的进程，可以不对外暴露端口号，如果对外暴露端口号，那也只能有一个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \     //运行一个实例</span><br><span class="line">--name huluwa-niginx-v1 \            //自定义一个名字</span><br><span class="line">-d \                                 //后台运行</span><br><span class="line">-p 8080:80 \                         //对外暴露的端口号，对应linux的8080端口号</span><br><span class="line">nginx:latest                         //运行的镜像名及版本</span><br><span class="line">d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps     //查看当前活动的实例</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   2 minutes ago</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# ps -ef | grep docker</span><br><span class="line">root     23182     1  0 22:00 ?        00:00:07 /usr/bin/dockerd</span><br><span class="line">root     23189 23182  0 22:00 ?        00:00:03 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root     25014 23182  0 22:27 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8080 -container-ip 172.17.0.2 -container-port 80</span><br><span class="line">root     25021 23189  0 22:27 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root     25492 10525  0 22:35 pts/0    00:00:00 grep --color=auto docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker-proxy -proto tcp&lt;br&gt;</span><br><span class="line">-host-ip 0.0.0.0   //&lt;br&gt;</span><br><span class="line">-host-port 8080   //linux系统的端口号&lt;br&gt;</span><br><span class="line">-container-ip 172.17.0.2  //docker相当于一个小型的linux系统，这就是小型系统的IP地址&lt;br&gt;</span><br><span class="line">-container-port 80  //docker内部的一个端口号&lt;br&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# netstat -nlp |grep 8080</span><br><span class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      25014/docker-proxy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker images    //查看所有的镜像</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              62f816a209e6        7 days ago          109MB</span><br><span class="line">mysql               5.6                 a46c2a2722b9        2 weeks ago         256MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        2 months ago        1.84kB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps -a   //查看所有实例，不论什么状态</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                    NAMES</span><br><span class="line">9883abaaad85        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour               0.0.0.0:3308-&gt;3306/tcp   huluwa-mysql-v5</span><br><span class="line">34fb53521694        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Created                                                 huluwa-mysql-v4</span><br><span class="line">2a5c95f3c043        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v3</span><br><span class="line">84e65fd24271        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v2</span><br><span class="line">b3c12bcb28eb        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysqlv1</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   12 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   13 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 13 hours ago        Exited (0) 13 hours ago                                 elastic_ritchie</span><br></pre></td></tr></table></figure><p>正在运行的status就是Up，已经关闭的status就是Exited</p><h3 id="自定义首页"><a href="#自定义首页" class="headerlink" title="自定义首页"></a>自定义首页</h3><ol><li>登录初始的nginx Web页面</li></ol><p><img src="/assets/blogImg/2019-06-28-1.png" alt="enter description here"></p><ol start="2"><li><p>通过index.html配置一个自定义的首页</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 html]# pwd</span><br><span class="line">/root/docker/nginx/html</span><br><span class="line">[root@hadoop004 html]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 92 Nov 13 23:09 index.html</span><br></pre></td></tr></table></figure><p>在windows中打开index.html页面是这样的：<br><img src="/assets/blogImg/2019-06-28-2.png" alt="enter description here"></p></li><li><p>将本地的html文件挂载到container中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \</span><br><span class="line">--name huluwa-niginx-v2 \</span><br><span class="line">-v /root/docker/nginx/html:/usr/share/nginx/html:ro \ //本地的/root/docker/nginx/html和容器里的/usr/share/nginx/html建立一个映射，将本地的文件夹挂载到容器里</span><br><span class="line">-d \</span><br><span class="line">-p 8082:80 \</span><br><span class="line">nginx:latest</span><br><span class="line">c7937fd855963c7cca831d495436881a16e7e9befa61288cb28e2ab8b986decf</span><br><span class="line">[root@hadoop004 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                         PORTS                  NAMES</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About a minute ago   Up About a minute              0.0.0.0:8082-&gt;80/tcp   huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About an hour ago    Up About an hour               0.0.0.0:8080-&gt;80/tcp   huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 About an hour ago    Exited (0) About an hour ago                          elastic_ritchie</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>打开ip:8082页面查看</li></ol><p><img src="/assets/blogImg/2019-06-28-3.png" alt="enter description here"></p><p>发现首页已经被置换为本地文件中的index.html文件<br><br>-v 把本地文件或文件夹挂载到容器中<br><br>挂载的目的，就是把容器中的数据保存在本地，容器进程移除后之后，数据不会丢失，如果不挂载的话，容器进程挂掉之后，数据就全没有了<br><br>ro：可读<br><br>rw：可读写<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch部署</title>
    <link href="http://yoursite.com/2019/06/21/Elasticsearch%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/06/21/Elasticsearch部署/</id>
    <published>2019-06-20T16:00:00.000Z</published>
    <updated>2019-07-15T08:23:48.910Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><p>上传解压elasticsearch的tar包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 elasticsearch]# ll</span><br><span class="line">total 236</span><br><span class="line">drwxr-xr-x  2 root root   4096 Apr 22 11:28 bin</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 config</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 lib</span><br><span class="line">-rw-r--r--  1 root root  11358 Aug 14  2017 LICENSE.txt</span><br><span class="line">drwxr-xr-x 13 root root   4096 Aug 14  2017 modules</span><br><span class="line">-rw-r--r--  1 root root 194187 Aug 14  2017 NOTICE.txt</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 plugins</span><br><span class="line">-rw-r--r--  1 root root   9549 Aug 14  2017 README.textile</span><br><span class="line">[root@hadoop001 elasticsearch]# cd config/</span><br><span class="line">[root@hadoop001 config]# ll</span><br><span class="line">total 16</span><br><span class="line">-rw-rw---- 1 root root 2854 Aug 14  2017 elasticsearch.yml</span><br><span class="line">-rw-rw---- 1 root root 3064 Aug 14  2017 jvm.options</span><br><span class="line">-rw-rw---- 1 root root 4456 Aug 14  2017 log4j2.properties</span><br><span class="line">[root@hadoop001 config]# vi elasticsearch.yml</span><br><span class="line">cluster.name: HLWCluster</span><br><span class="line">node.name: hadoop001</span><br><span class="line">path.data: /root/app/elasticsearch/data</span><br><span class="line">path.logs: /root/app/elasticsearch/logs</span><br><span class="line">network.host: 172.26.183.103</span><br><span class="line">[root@hadoop001 config]# cd ../</span><br><span class="line">[root@hadoop001 elasticsearch]# cd bin</span><br><span class="line">[root@hadoop001 bin]# ll</span><br><span class="line">total 348</span><br><span class="line">-rwxr-xr-x 1 root root   8075 Aug 14  2017 elasticsearch</span><br><span class="line">-rw-r--r-- 1 root root   3343 Aug 14  2017 elasticsearch.bat</span><br><span class="line">-rw-r--r-- 1 root root   1023 Aug 14  2017 elasticsearch.in.bat</span><br><span class="line">-rwxr-xr-x 1 root root    367 Aug 14  2017 elasticsearch.in.sh</span><br><span class="line">-rwxr-xr-x 1 root root   2550 Aug 14  2017 elasticsearch-keystore</span><br><span class="line">-rw-r--r-- 1 root root    743 Aug 14  2017 elasticsearch-keystore.bat</span><br><span class="line">-rwxr-xr-x 1 root root   2540 Aug 14  2017 elasticsearch-plugin</span><br><span class="line">-rw-r--r-- 1 root root    731 Aug 14  2017 elasticsearch-plugin.bat</span><br><span class="line">-rw-r--r-- 1 root root  11239 Aug 14  2017 elasticsearch-service.bat</span><br><span class="line">-rw-r--r-- 1 root root 104448 Aug 14  2017 elasticsearch-service-mgr.exe</span><br><span class="line">-rw-r--r-- 1 root root 103936 Aug 14  2017 elasticsearch-service-x64.exe</span><br><span class="line">-rw-r--r-- 1 root root  80896 Aug 14  2017 elasticsearch-service-x86.exe</span><br><span class="line">-rwxr-xr-x 1 root root    223 Aug 14  2017 elasticsearch-systemd-pre-exec</span><br><span class="line">-rwxr-xr-x 1 root root   2514 Aug 14  2017 elasticsearch-translog</span><br><span class="line">-rw-r--r-- 1 root root   1435 Aug 14  2017 elasticsearch-translog.bat</span><br><span class="line">[root@hadoop001 bin]# ./elasticsearch</span><br><span class="line">[2019-04-22T11:47:17,068][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [hadoop001] uncaught exception in thread [main]</span><br><span class="line">org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:127) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">Caused by: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        ... 6 more</span><br><span class="line">[root@hadoop001 bin]# cd ../logs</span><br><span class="line">[root@hadoop001 logs]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_deprecation.log</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_index_indexing_slowlog.log</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_index_search_slowlog.log</span><br><span class="line">-rw-r--r-- 1 root root 2691 Apr 22 11:47 HLWCluster.log</span><br><span class="line">[root@hadoop001 logs]# cat HLWCluster.log</span><br><span class="line">[2019-04-22T11:47:17,058][ERROR][o.e.b.Bootstrap          ] Exception</span><br><span class="line">java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">[2019-04-22T11:47:17,068][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [hadoop001] uncaught exception in thread [main]</span><br><span class="line">org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:127) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">Caused by: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        ... 6 more</span><br></pre></td></tr></table></figure><p>第一个报错：不能用root用户去运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 app]# useradd esuser</span><br><span class="line">[root@hadoop001 app]# chown -R esuser:esuser elasticsearch-5.5.2</span><br><span class="line">[root@hadoop001 app]# mv ./elasticsearch-5.5.2 /home/esuser/elasticsearch-5.5.2</span><br><span class="line">[root@hadoop001 app]# su - esuser</span><br><span class="line">[esuser@hadoop001 ~]$ cd elasticsearch</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ pwd</span><br><span class="line">/home/esuser/elasticsearch</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ ll</span><br><span class="line">total 244</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:28 bin</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:46 config</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:42 data</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Aug 14  2017 lib</span><br><span class="line">-rw-r--r--  1 esuser esuser  11358 Aug 14  2017 LICENSE.txt</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:47 logs</span><br><span class="line">drwxr-xr-x 13 esuser esuser   4096 Aug 14  2017 modules</span><br><span class="line">-rw-r--r--  1 esuser esuser 194187 Aug 14  2017 NOTICE.txt</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Aug 14  2017 plugins</span><br><span class="line">-rw-r--r--  1 esuser esuser   9549 Aug 14  2017 README.textile</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ cd config</span><br><span class="line">[esuser@hadoop001 config]$ vi elasticsearch.yml</span><br><span class="line">path.data: /home/esuser/elasticsearch/data</span><br><span class="line">path.logs: /home/esuser/elasticsearch/logs</span><br><span class="line">[esuser@hadoop001 config]$ cd ../bin</span><br><span class="line">[esuser@hadoop001 bin]$ ./elasticsearch</span><br><span class="line">[2019-04-22T12:16:04,747][INFO ][o.e.n.Node               ] [hadoop001] initializing ...</span><br><span class="line">[2019-04-22T12:16:04,822][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [17.8gb], net total_space [39.2gb], spins? [unknown], types [rootfs]</span><br><span class="line">[2019-04-22T12:16:04,823][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] heap size [1.9gb], compressed ordinary object pointers [true]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] node name [hadoop001], node ID [oKhZUG8wTl2bz38DZ_5rHA]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] version[5.5.2], pid[4894], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_45/25.45-b02]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/home/esuser/elasticsearch]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [aggs-matrix-stats]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [ingest-common]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-expression]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-groovy]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-mustache]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-painless]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [parent-join]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [percolator]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [reindex]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty3]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty4]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] no plugins loaded</span><br><span class="line">[2019-04-22T12:16:08,200][INFO ][o.e.d.DiscoveryModule    ] [hadoop001] using discovery type [zen]</span><br><span class="line">[2019-04-22T12:16:08,882][INFO ][o.e.n.Node               ] [hadoop001] initialized</span><br><span class="line">[2019-04-22T12:16:08,883][INFO ][o.e.n.Node               ] [hadoop001] starting ...</span><br><span class="line">[2019-04-22T12:16:09,051][INFO ][o.e.t.TransportService   ] [hadoop001] publish_address &#123;172.26.183.103:9300&#125;, bound_addresses &#123;172.26.183.103:9300&#125;</span><br><span class="line">[2019-04-22T12:16:09,063][INFO ][o.e.b.BootstrapChecks    ] [hadoop001] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</span><br><span class="line">ERROR: [2] bootstrap checks failed</span><br><span class="line">[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]</span><br><span class="line">[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span><br><span class="line">[2019-04-22T12:16:09,080][INFO ][o.e.n.Node               ] [hadoop001] stopping ...</span><br><span class="line">[2019-04-22T12:16:09,110][INFO ][o.e.n.Node               ] [hadoop001] stopped</span><br><span class="line">[2019-04-22T12:16:09,110][INFO ][o.e.n.Node               ] [hadoop001] closing ...</span><br><span class="line">[2019-04-22T12:16:09,128][INFO ][o.e.n.Node               ] [hadoop001] closed</span><br></pre></td></tr></table></figure><p>第二次报错：<br><br>ERROR: [2] bootstrap checks failed<br><br>[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]<br><br>[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072 </span><br><span class="line">[root@hadoop001 ~]# vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">[root@hadoop001 etc]# sysctl -p</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time = 120</span><br><span class="line">net.ipv4.conf.all.rp_filter = 0</span><br><span class="line">net.ipv4.conf.default.rp_filter = 0</span><br><span class="line">net.ipv4.conf.default.arp_announce = 2</span><br><span class="line">net.ipv4.conf.lo.arp_announce = 2</span><br><span class="line">net.ipv4.conf.all.arp_announce = 2</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 1024</span><br><span class="line">net.ipv4.tcp_synack_retries = 2</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br><span class="line">kernel.sysrq = 1</span><br><span class="line">vm.max_map_count = 262144</span><br></pre></td></tr></table></figure><p>以上设置永久生效需要重启reboot</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 etc]# reboot</span><br></pre></td></tr></table></figure><p>查看配置有没有生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# su - esuser</span><br><span class="line">[esuser@hadoop001 ~]$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 63461</span><br><span class="line">max locked memory       (kbytes, -l) 64</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 65536</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 4096</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br><span class="line">[root@hadoop001 config]# vi elasticsearch.yml</span><br><span class="line">bootstrap.memory_lock: false</span><br><span class="line">bootstrap.system_call_filter: false</span><br><span class="line">[esuser@hadoop001 bin]$ ./elasticsearch</span><br><span class="line">[2019-04-22T12:58:13,817][INFO ][o.e.n.Node               ] [hadoop001] initializing ...</span><br><span class="line">[2019-04-22T12:58:13,896][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [17.8gb], net total_space [39.2gb], spins? [unknown], types [rootfs]</span><br><span class="line">[2019-04-22T12:58:13,897][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] heap size [1.9gb], compressed ordinary object pointers [true]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] node name [hadoop001], node ID [oKhZUG8wTl2bz38DZ_5rHA]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] version[5.5.2], pid[3085], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_45/25.45-b02]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/home/esuser/elasticsearch]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [aggs-matrix-stats]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [ingest-common]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-expression]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-groovy]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-mustache]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-painless]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [parent-join]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [percolator]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [reindex]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty3]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty4]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] no plugins loaded</span><br><span class="line">[2019-04-22T12:58:17,555][INFO ][o.e.d.DiscoveryModule    ] [hadoop001] using discovery type [zen]</span><br><span class="line">[2019-04-22T12:58:18,182][INFO ][o.e.n.Node               ] [hadoop001] initialized</span><br><span class="line">[2019-04-22T12:58:18,183][INFO ][o.e.n.Node               ] [hadoop001] starting ...</span><br><span class="line">[2019-04-22T12:58:18,357][INFO ][o.e.t.TransportService   ] [hadoop001] publish_address &#123;172.26.183.103:9300&#125;, bound_addresses &#123;172.26.183.103:9300&#125;</span><br><span class="line">[2019-04-22T12:58:18,368][INFO ][o.e.b.BootstrapChecks    ] [hadoop001] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</span><br><span class="line">[2019-04-22T12:58:21,428][INFO ][o.e.c.s.ClusterService   ] [hadoop001] new_master &#123;hadoop001&#125;&#123;oKhZUG8wTl2bz38DZ_5rHA&#125;&#123;aY3CSi1XTOCq29GPw3cFqA&#125;&#123;172.26.183.103&#125;&#123;172.26.183.103:9300&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)</span><br><span class="line">[2019-04-22T12:58:21,471][INFO ][o.e.h.n.Netty4HttpServerTransport] [hadoop001] publish_address &#123;172.26.183.103:9200&#125;, bound_addresses &#123;172.26.183.103:9200&#125;</span><br><span class="line">[2019-04-22T12:58:21,471][INFO ][o.e.n.Node               ] [hadoop001] started</span><br><span class="line">[2019-04-22T12:58:21,473][INFO ][o.e.g.GatewayService     ] [hadoop001] recovered [0] indices into cluster_state</span><br></pre></td></tr></table></figure><p>需要后台运行的话加个 -d 参数：./elasticsearch -d</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[esuser@hadoop001 ~]$ curl -XGET &apos;172.26.183.103:9200/?pretty&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;hadoop001&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;HLWCluster&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;jb0pPZNBTwmQj6iNBWtvzg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;5.5.2&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;b2f0c09&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2017-08-14T12:33:14.154Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;6.6.0&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://yoursite.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://yoursite.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>不得不会的Spark SQL常见4种数据源</title>
    <link href="http://yoursite.com/2019/06/20/%E4%B8%8D%E5%BE%97%E4%B8%8D%E4%BC%9A%E7%9A%84Spark%20SQL%E5%B8%B8%E8%A7%814%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/</id>
    <published>2019-06-19T16:00:00.000Z</published>
    <updated>2019-06-20T02:02:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h3><h4 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h4><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。</p><p>修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          </span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><h4 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h3><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Encoders for most common types are automatically provided by importing spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// DataFrames can be saved as Parquet files, maintaining the schema information</span><br><span class="line">peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Read in the parquet file created above</span><br><span class="line">// Parquet files are self-describing so the schema is preserved</span><br><span class="line">// The result of loading a Parquet file is also a DataFrame</span><br><span class="line">val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet files can also be used to create a temporary view and then used in SQL statements</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">// +------------+</span><br><span class="line">// |       value|</span><br><span class="line">// +------------+</span><br><span class="line">// |Name: Justin|</span><br><span class="line">// +------------+</span><br></pre></td></tr></table></figure><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet</p><p>或SQLContext.read.load，Spark SQL将自动解析分区信息。</p><p>返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项：</p><p><code>spark.sql.parquet.mergeSchema</code>为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sqlContext from the previous example is used in this example.</span><br><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, stored into a partition directory</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths.</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">.config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><h4 id="内嵌Hive应用"><a href="#内嵌Hive应用" class="headerlink" title="内嵌Hive应用"></a>内嵌Hive应用</h4><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.warehouse.dir=</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive应用"><a href="#外部Hive应用" class="headerlink" title="外部Hive应用"></a>外部Hive应用</h4><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;rddtable2&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之八: GC插件&amp;错误not_supported_for_this_jvm&amp;命令jstatd</title>
    <link href="http://yoursite.com/2019/06/19/JVM_8/"/>
    <id>http://yoursite.com/2019/06/19/JVM_8/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:54:00.202Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a> <font size="4"><b>1.插件安装</b></font><p>tools-&gt;plugin-&gt;Available Plugin 会有值得安装的插件，如：VisualGC</p><p><img src="/assets/pic/2019-06-19-8-1.png" alt="插件安装"></p><p>插件列表: <a href="https://visualvm.dev.java.net/plugins.html" target="_blank" rel="noopener">https://visualvm.dev.java.net/plugins.html</a></p><p>注意：上面提供的端口配置有些麻烦，不如直接这样做:</p><font size="4"><b>2.要使用 VisualGC 必须在远程机上启动jstatd代理程序，否则会显示<font color="#FF4500">“not supported for this jvm” </font>错误</b></font> <font color="blue">而启动 jstatd 时会有一个权限问题，需要做如下修改：</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@xxx-01 ~]# java -version</span><br><span class="line">java version &quot;1.7.0_55&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_55-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)</span><br><span class="line">[root@xxx-01 ~]# jstatd </span><br><span class="line">Could not create remote object</span><br><span class="line">access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">java.security.AccessControlException: access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)</span><br><span class="line">        at java.security.AccessController.checkPermission(AccessController.java:559)</span><br><span class="line">        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)</span><br><span class="line">        at java.lang.System.setProperty(System.java:783)</span><br><span class="line">        at sun.tools.jstatd.Jstatd.main(Jstatd.java:139)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@xxx-01 ~]# cd  /usr/java/jdk1.7.0_55</span><br><span class="line">[root@xxx-01 ~]# vi /usr/java/jdk1.7.0_55/jstatd.all.policy</span><br><span class="line">    grant codebase &quot;file:$&#123;JAVA_HOME&#125;/lib/tools.jar&quot; &#123;  </span><br><span class="line">     permission java.security.AllPermission;  </span><br><span class="line">    &#125;;  </span><br><span class="line">[root@xxx-01 jdk1.7.0_55]# jstatd -J-Djava.security.policy=/usr/java/jdk1.7.0_55/jstatd.all.policy  &amp;</span><br></pre></td></tr></table></figure><font color="blue">然后后台模式启动 jstatd命令</font> <font color="blue">主机面GC:</font><p><img src="/assets/pic/2019-06-19-8-2.png" alt="主机面GC"></p><font color="blue">Threads:</font><p><img src="/assets/pic/2019-06-19-8-3.png" alt="Threads"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之五: ParNew收集器+CMS收集器的产品案例分析(响应时间优先)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%94_ParNew%E6%94%B6%E9%9B%86%E5%99%A8+CMS%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BA%A7%E5%93%81%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90(%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之五_ParNew收集器+CMS收集器的产品案例分析(响应时间优先)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:55.585Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><font color="green" size="3"><b>双核,4个cores; 16G memory</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@alish2-cassandra-01 ~]# cat /proc/cpuinfo | grep &quot;cpu cores&quot;</span><br><span class="line">cpu cores       : 2</span><br><span class="line">cpu cores       : 2</span><br></pre></td></tr></table></figure><h3 id="公式简述"><a href="#公式简述" class="headerlink" title="公式简述"></a>公式简述</h3><p>响应时间优先的并发收集器，主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。</p><ol><li><font size="3" color="red">ParNew收集器</font><p>ParNew收集器是Serial收集器的多线程版本，许多运行在Server模式下的虚拟机中首选的新生代收集器，除Serial外，<font color="blue">只有它能与CMS收集器配合工作。</font></p></li><li><font size="3" color="red">CMS收集器</font><p>CMS， 全称Concurrent Low Pause Collector，是jdk1.4后期版本开始引入的新gc算法，在jdk5和jdk6中得到了进一步改进，它的主要适合场景是对响应时间的重要性需求 大于对吞吐量的要求，能够承受垃圾回收线程和应用线程共享处理器资源，并且应用中存在比较多的长生命周期的对象的应用。CMS是用于对tenured generation的回收，也就是年老代的回收，目标是尽量减少应用的暂停时间，减少FullGC发生的几率，利用和应用程序线程并发的垃圾回收线程来 标记清除年老代。<br>CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期是这样：<br><br><br><font size="3" color="blue">初始标记(CMS-initial-mark) -&gt; 并发标记(CMS-concurrent-mark) -&gt; 重新标记(CMS-remark) -&gt; 并发清除(CMS-concurrent-sweep) -&gt;并发重设状态等待下次CMS的触发(CMS-concurrent-reset)</font><br><br><br>其中的1，3两个步骤需要暂停所有的应用程序线程的。第一次暂停从root对象开始标记存活的对象，这个阶段称为初始标记；第二次暂停是在并发标记之后，暂停所有应用程序线程，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致）。第一次暂停会比较短，第二次暂停通常会比较长，并且remark这个阶段可以并行标记。<br><br><br>而并发标记、并发清除、并发重设阶段的所谓并发，是指一个或者多个垃圾回收线程和应用程序线程并发地运行，垃圾回收线程不会暂停应用程序的执行，如果你有多于一个处理器，那么并发收集线程将与应用线程在不同的处理器上运行，显然，这样的开销就是会降低应用的吞吐量。Remark阶段的并行，是指暂停了所有应用程序后，启动一定数目的垃圾回收进程进行并行标记，此时的应用线程是暂停的。</p></li></ol><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>($TOMCAT_HOME/bin/catalina.sh)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_OPTS=&quot;-server -Xmx10240m -Xms10240m -Xmn3840m -XX:PermSize=256m</span><br><span class="line"></span><br><span class="line">-XX:MaxPermSize=256m -Denv=denalicnprod</span><br><span class="line"></span><br><span class="line">-XX:SurvivorRatio=8  -XX:PretenureSizeThreshold=1048576</span><br><span class="line"></span><br><span class="line">-XX:+DisableExplicitGC  </span><br><span class="line"></span><br><span class="line">-XX:+UseParNewGC  -XX:ParallelGCThreads=10</span><br><span class="line"></span><br><span class="line">-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled</span><br><span class="line"></span><br><span class="line">-XX:+CMSScavengeBeforeRemark -XX:ParallelCMSThreads=10</span><br><span class="line"></span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</span><br><span class="line"></span><br><span class="line">-XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled</span><br><span class="line"></span><br><span class="line">-XX:+UseFastAccessorMethods</span><br><span class="line"></span><br><span class="line">-XX:LargePageSizeInBytes=128M</span><br><span class="line"></span><br><span class="line">-XX:SoftRefLRUPolicyMSPerMB=0</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDateStamps -Xloggc:gc.log -verbose:gc&quot;</span><br></pre></td></tr></table></figure><h3 id="公式解析"><a href="#公式解析" class="headerlink" title="公式解析"></a>公式解析</h3><table><thead><tr><th>参 数</th><th>含 义</th></tr></thead><tbody><tr><td>-server</td><td>一定要作为第一个参数，启用JDK的server版本，在多个CPU时性能佳</td></tr><tr><td>-Xms</td><td>java Heap初始大小。 默认是物理内存的1/64。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。</td></tr><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存。</td></tr><tr><td>-Xmn</td><td>设置年轻代大小，一般设置为Xmx的2/8~3/8,等同于-XX:NewSize 和 -XX:MaxNewSize 。</td></tr><tr><td>-XX:PermSize</td><td>设定内存的永久保存区初始大小，缺省值为64M</td></tr><tr><td>-XX:MaxPermSize</td><td>设定内存的永久保存区最大大小，缺省值为64M</td></tr><tr><td>-Denv</td><td>指定tomcat运行哪个project</td></tr><tr><td>-XX:SurvivorRatio</td><td>Eden区与Survivor区的大小比值, 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10</td></tr><tr><td>-XX:PretenureSizeThreshold</td><td>晋升年老代的对象大小。默认为0，比如设为1048576(1M)，则超过1M的对象将不在eden区分配，而直接进入年老代。</td></tr><tr><td>-XX:+DisableExplicitGC</td><td>关闭System.gc()</td></tr><tr><td><font color="#1E90FF">-XX:+UseParNewGC</font></td><td><font color="#1E90FF">设置年轻代为并发收集。可与CMS收集同时使用。</font></td></tr><tr><td>-XX:ParallelGCThreads</td><td></td></tr><tr><td><font color="#1E90FF">-XX:+UseConcMarkSweepGC</font></td><td><font color="#1E90FF">设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了。所以，此时年轻代大小最好用-Xmn设置。</font></td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>开启并行remark</td></tr><tr><td>-XX:+CMSScavengeBeforeRemark</td><td>这个参数还蛮重要的，它的意思是在执行CMS remark之前进行一次youngGC，这样能有效降低remark的时间</td></tr><tr><td>-XX:ParallelCMSThreads</td><td>CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4) ，如果你需要明确设定，可以通过-XX:ParallelCMSThreads=20来设定,其中ParallelGCThreads是年轻代的并行收集线程数</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction</td><td><font color="#3CB371">使用cms作为垃圾回收使用70％后开始CMS收集</font></td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td></tr><tr><td>-XX:+UseCMSCompactAtFullCollection</td><td>打开对年老代的压缩。可能会影响性能，但是可以消除内存碎片。</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此参数设置运行次FullGC以后对内存空间进行压缩、整理。</td></tr><tr><td>-XX:+CMSPermGenSweepingEnabled</td><td>为了避免Perm区满引起的full gc，<font color="#3CB371">建议开启CMS回收Perm区选项</font></td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr><tr><td>-XX:+UseFastAccessorMethods</td><td>原始类型的快速优化</td></tr><tr><td>-XX:LargePageSizeInBytes</td><td>内存页的大小，不可设置过大， 会影响Perm的大小</td></tr><tr><td>-XX:SoftRefLRUPolicyMSPerMB</td><td>“软引用”的对象在最后一次被访问后能存活0毫秒（默认为1秒）。</td></tr><tr><td>-XX:+PrintGCDetails</td><td>记录 GC 运行时的详细数据信息，包括新生成对象的占用内存大小以及耗费时间等</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td>打印垃圾收集的时间戳</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td></tr><tr><td>-XX:+PrintGCDateStamps</td><td>之前打印gc日志的时候使用是：-XX:+PrintGCTimeStamps，这个选项记录的是jvm启动时间为起点的相对时间，可读性较差，不利于定位问题，使用PrintGCDateStamps记录的是系统时间，更humanreadable</td></tr><tr><td>-Xloggc</td><td>与上面几个配合使用，把相关日志信息记录到文件以便分析</td></tr><tr><td>-verbose:gc</td><td>记录 GC 运行以及运行时间，一般用来查看 GC 是否是应用的瓶颈</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之四: 堆内存分配的CMS公式解析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%9B%9B_%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之四_堆内存分配的CMS公式解析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:36.520Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="JVM-堆内存组成"><a href="#JVM-堆内存组成" class="headerlink" title="JVM 堆内存组成"></a>JVM 堆内存组成</h3><p>Java堆由Perm区和Heap区组成，Heap区由Old区和New区（也叫Young区）组成，New区由Eden区、From区和To区（Survivor）组成。</p><p><img src="/assets/pic/2019-06-19-4-1.png" alt="JVM 堆内存组成"></p><p>Eden区用于存放新生成的对象。Eden中的对象生命不会超过一次Minor GC。Survivor Space 有两个，存放每次垃圾回收后存活的对象，即图的S0和S1。Old Generation Old区，也称老生代，主要存放应用程序中生命周期长的存活对象</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：</p><p>eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况。</p><p>即：</p><font color="blue">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2))</font><p>进而推断出：</p><font color="blue">CMSInitiatingOccupancyFraction &lt;= ((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100</font><table><thead><tr><th style="text-align:left">参数</th><th>含义</th></tr></thead><tbody><tr><td style="text-align:left">Xmx-Xmn</td><td>Old区大小</td></tr><tr><td style="text-align:left">CMSInitiatingOccupancyFraction/100</td><td>Old区百分之多少时,cms开始gc</td></tr><tr><td style="text-align:left">1-CMSInitiatingOccupancyFraction/100</td><td>Old区开始gc回收时剩余空间百分比</td></tr><tr><td style="text-align:left">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)</td><td>Old区开始gc回收时剩余空间大小</td></tr><tr><td style="text-align:left">(Xmn-Xmn/(SurvivorRatior+2))</td><td>eden+from survivor区的大小</td></tr></tbody></table><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存</td></tr><tr><td>-Xmn</td><td>java heap最小值，一般设置为Xmx的3、4分之一,等同于-XX:NewSize 和 -XX:MaxNewSize ,其实为<font color="blue">young区大小</font></td></tr><tr><td>-XX</td><td>CMSInitiatingOccupancyFraction=70 :使用cms作为垃圾回收使用70％后开始CMS收集</td></tr><tr><td>-XX</td><td>SurvivorRatio=2: 生还者池的大小，默认是2</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="堆内存分配的CMS公式解析" scheme="http://yoursite.com/tags/%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之三: 内存分配策略</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%89_%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之三_内存分配策略/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:31.567Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><p><img src="/assets/pic/2019-06-19-3-1.png" alt="内存分配策略"></p><p>了解GC其中很重要一点就是了解JVM的内存分配策略：<font color="red">即对象在哪里分配和对象什么时候回收。</font><br><br><br>Java技术体系中所提倡的自动内存管理可以归结于两个部分：给对象分配内存以及回收分配给对象的内存。<br><br><br>我们都知道，Java对象分配，都是在Java堆上进行分配的，虽然存在JIT编译后被拆分为标量类型并简介地在栈上进行分配。如果采用分代算法，那么新生的对象是分配在新生代的Eden区上的。如果启动了本地线程分配缓冲，将按线程优先在TLAB上进行分配。<br><br><br>事实上，Java的分配规则不是百分百固定的，其取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。<br><br><br>简单来说，对象内存分配主要是在堆中分配。但是分配的规则并不是固定的，取决于使用的收集器组合以及JVM内存相关参数的设定。<br><br><br>下面Serial和Serial Old收集器做一个内存分配和回收的策略总结。</p><h3 id="对象优先在新生代Eden分配"><a href="#对象优先在新生代Eden分配" class="headerlink" title="对象优先在新生代Eden分配"></a>对象优先在新生代Eden分配</h3><p>首先，让我们来看一下新生代的内存分配情况<br><br><br>内存分配情况：将JVM内存划分为一块较大的Eden空间（80%）和两块小的Servivor（各占10%）。当回收时，将Eden和Survivor中还存活的对象一次性采用复制算法直接复制到另外一块Servivor空间上，最后清理到院Eden空间和原先的Survivor空间中的数据。<br><br><br>大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，JVM将发起一次Minor GC。<br><br><br>在这里先说明两个概念：</p><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多是具有朝生夕灭的特性，所以Minor GC非常频繁，而且该速度也比较快。</li><li><strong>老年代GC（Major GC/Full GC）</strong>：指发生在老年代的GC，出现了Major GC，一般可能也会伴随着一次Minor GC，但是与Minor GC不同的是，Major GC的速度慢十倍以上。</li></ul><h3 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h3><p>我们先对所谓的大对象做一个定义：大对象，这里指的是需要大量连续内存空间的Java对象。最典型的大对象可以是很长的字符串和数组。<br><br><br>JVM对大对象的态度：大对象对于JVM的内存分配来说是十分麻烦的，如果我们将大对象分配在新生代中，那样子的话很容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。<br><br><br>为了避免上述情况的经常发生而导致不需要的GC活动所浪费的资源和时间，可采用的分配策略是将大对象直接分配到老年代中去，虚拟机中也提供了<strong>-XX:PretenureSizeThreshold</strong>参数，令大于这个设置值的对象直接在老年代里面分配内容。</p><p><code>-XX:PretenureSizeThreshold只对Serial和ParNew收集器有效。</code></p><h3 id="长期存活的对象将进入老年代"><a href="#长期存活的对象将进入老年代" class="headerlink" title="长期存活的对象将进入老年代"></a>长期存活的对象将进入老年代</h3><p>当JVM采用分代收集的思想来管理内存时，为了识别哪些对象应该放在新生代、哪些对象应该放在老年代，JVM给每个对象定义了一个对象年龄计数器。<br><br><br>对象年龄计数器：如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，便可以被移动到Survivor空间中，年龄计数器将设置该对象的年龄为1.对于对象在Survivor区每经过一次Minor GC，年龄便增加1岁，当它的年龄增加到一定程度（可通过参数-XX:MaxTenuringThreshold设置）默认15，该对象便会进入到老年代中。成为老年代的对象。</p><h3 id="动态对象年龄判定"><a href="#动态对象年龄判定" class="headerlink" title="动态对象年龄判定"></a>动态对象年龄判定</h3><p>事实上，有的虚拟机并不永远地要求对象的年龄必须达到MaxTeruringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Surivior空间的一半，年龄大于或等于该年龄的对象就可以直接进行老年代，无须等到MaxTeruringThreshold中所要求的年龄。</p><h3 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h3><p>在发生Minor GC之前，虚拟机会先检查老年代中最大的可用的连续空间是否大于新生代中所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的，如果不成立，则虚拟机会查看HandlePromotionFaiure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次GC是有风险的；如果小于，或者HandlePromotionFaiure设置不允许冒险，那么这时就要改为进行一次Full GC。</p><p>所谓冒险：也就是说当用来轮转的Survivor区无法承受新生代中所存活的对象内存时，需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代中，前提是老年代中。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="内存分配策略" scheme="http://yoursite.com/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之七: Java程序性能分析工具Java VisualVM(Visual GC)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%83_Java%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7Java%20VisualVM(Visual%20GC)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之七_Java程序性能分析工具Java VisualVM(Visual GC)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:24.927Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><p>VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。</p><p>在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。</p><a id="more"></a><h3 id="Visual-GC-监控垃圾回收器"><a href="#Visual-GC-监控垃圾回收器" class="headerlink" title="Visual GC(监控垃圾回收器)"></a>Visual GC(监控垃圾回收器)</h3><p>Java VisualVM默认没有安装Visual GC插件，需要手动安装，JDK的安装目录的bin目露下双击jvisualvm.exe，即可打开Java VisualVM，点击菜单栏 工具-&gt;插件 安装Visual GC</p><p><img src="/assets/pic/2019-06-19-7-1.png" alt="Visual GC(监控垃圾回收器)1"></p><p>安装完成后重启Java VisualVM，Visual GC界面自动打开，即可看到JVM中堆内存的分代情况</p><p><img src="/assets/pic/2019-06-19-7-2.png" alt="Visual GC(监控垃圾回收器)2"></p><p>被监控的程序运行一段时间后Visual GC显示如下</p><p><img src="/assets/pic/2019-06-19-7-3.png" alt="Visual GC(监控垃圾回收器)3"></p><p>要看懂上面的图必须理解Java虚拟机的一些基本概念：</p><font color="blue" size="3"><b>堆(Heap)</b></font>：<font size="3">JVM管理的内存叫堆</font><p><strong>分代</strong>：根据对象的生命周期长短，把堆分为3个代：Young，Old和Permanent，根据不同代的特点采用不同的收集算法，扬长避短也。</p><ul><li><font color="blue">Young（年轻代）</font>年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor复制过来的对象。而且，Survivor区总有一个是空的。</li><li><font color="blue">Tenured（年老代）</font>年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。</li><li><font color="blue">Perm（持久代）</font>用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。</li></ul><font color="blue" size="3"><b>GC的基本概念</b></font><p>gc分为full gc 跟 minor gc，当每一块区满的时候都会引发gc。</p><ul><li><font color="blue">Scavenge GC</font><p>一般情况下，当新对象生成，并且在Eden申请空间失败时，就触发了Scavenge GC，堆Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。</p></li><li><font color="blue">Full GC</font><p>对整个堆进行整理，包括Young、Tenured和Perm。Full GC比Scavenge GC要慢，因此应该尽可能减少Full GC。有如下原因可能导致Full GC:</p><ul><li>上一次GC之后Heap的各域分配策略动态变化</li><li>System.gc()被显示调用</li><li>Perm域被写满</li><li>Tenured被写满</li></ul></li></ul><font color="blue" size="3"><b>内存溢出 out of memory</b></font><p>是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。</p><font color="blue" size="3"><b>内存泄露 memory leak</b></font><p>是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。<strong>其实说白了就是该内存空间使用完毕之后未回收。</strong></p><h3 id="Java-VisualVM的其他功能"><a href="#Java-VisualVM的其他功能" class="headerlink" title="Java VisualVM的其他功能"></a>Java VisualVM的其他功能</h3><ol><li><p>监视界面（cpu，类，堆，线程）</p><p><img src="/assets/pic/2019-06-19-7-4.png" alt="监视界面"></p></li><li><p>线程界面</p><p><img src="/assets/pic/2019-06-19-7-5.png" alt="线程界面"></p></li></ol><ol start="3"><li><p>Profile界面（性能剖析）</p><p>点击CPU按钮执行cpu分析查看方法</p><p><img src="/assets/pic/2019-06-19-7-6.png" alt="Profile界面"></p><p>点击内存按钮执行内存分析查看类</p><p><img src="/assets/pic/2019-06-19-7-7.png" alt="Profile界面"></p></li><li><p>堆dump和线程dump操作</p><p>Dump文件是进程的内存镜像，可以把程序的执行状态通过调试器保存到dump文件中，堆dump的dump文件内容如下图所示</p><p><img src="/assets/pic/2019-06-19-7-8.png" alt="Dump"></p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。&lt;/p&gt;&lt;p&gt;在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。&lt;/p&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之六: JVM参数设置及分析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%85%AD_JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之六_JVM参数设置及分析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:28.434Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><p>不管是YGC还是Full GC,GC过程中都会对导致程序运行中中断,正确的选择不同的GC策略,调整JVM、GC的参数，可以极大的减少由于GC工作，而导致的程序运行中断方面的问题，进而适当的提高Java程序的工作效率。但是调整GC是以个极为复杂的过程，由于各个程序具备不同的特点，如：web和GUI程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要cup个数，内存不同），所以使用的GC种类也会不同(如何选择见GC种类及如何选择)。本文将注重介绍JVM、GC的一些重要参数的设置来提高系统的性能。</p><p>JVM内存组成及GC相关内容请见之前的文章:JVM内存组成 GC策略&amp;内存申请</p><font size="3"><strong>JVM参数的含义</strong></font>实例见实例分析<br><br>参数名称|含义|默认值| |<br>——|—–|—–|—<br>-Xms|初始堆大小|物理内存的1/64(&lt;1GB)|默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.<br>-Xmx|最大堆大小|物理内存的1/4(&lt;1GB)|默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制<br>-Xmn|年轻代大小(1.4or lator)||注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。<br>整个堆大小=年轻代大小 + 年老代大小 + 持久代大小.<br>增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8<br>-XX:NewSize|设置年轻代大小(for 1.3/1.4)|<br>-XX:MaxNewSize|年轻代最大值(for 1.3/1.4)|<br>-XX:PermSize|设置持久代(perm gen)初始值|物理内存的1/64<br>-XX:MaxPermSize|设置持久代最大值|物理内存的1/4<br>-Xss|每个线程的堆栈大小||JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右。<br>一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。<br>-XX:ThreadStackSize|Thread Stack Size||(0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.]<br>-XX:NewRatio|年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)||-XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5<br>Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。<br>-XX:SurvivorRatio|Eden区与Survivor区的大小比值||设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10<br>-XX:LargePageSizeInBytes|内存页的大小不可设置过大， 会影响Perm的大小||=128m<br>-XX:+UseFastAccessorMethods|原始类型的快速优化<br>-XX:+DisableExplicitGC|关闭System.gc()||这个参数需要严格的测试<br>-XX:MaxTenuringThreshold|垃圾最大年龄||如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率<br>该参数只有在串行GC时才有效.<br>-XX:+AggressiveOpts|加快编译<br>-XX:+UseBiasedLocking|锁机制的性能改善<br>-Xnoclassgc|禁用垃圾回收<br>-XX:SoftRefLRUPolicyMSPerMB|每兆堆空闲空间中SoftReference的存活时间|1s|softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap<br>-XX:PretenureSizeThreshold|对象超过多大是直接在旧生代分配|0|单位字节 新生代采用Parallel Scavenge GC时无效<br>另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.<br>-XX:TLABWasteTargetPercent|TLAB占eden区的百分比|1%<br>-XX:+CollectGen0First|FullGC时是否先YGC|false<br><br><font size="3"><strong>并行收集器相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseParallelGC</td><td>Full GC采用parallel MSC<br>(此项待验证)</td><td></td><td>选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证)</td></tr><tr><td>-XX:+UseParNewGC</td><td>设置年轻代为并行收集</td><td></td><td>可与CMS收集同时使用<br>JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值</td></tr><tr><td>-XX:ParallelGCThreads</td><td>并行收集器的线程数</td><td></td><td>此值最好配置与处理器数目相等 同样适用于CMS</td></tr><tr><td>-XX:+UseParallelOldGC</td><td>年老代垃圾收集方式为并行收集(Parallel Compacting)</td><td></td><td>这个是JAVA 6出现的参数选项</td></tr><tr><td>-XX:MaxGCPauseMillis</td><td>每次年轻代垃圾回收的最长时间(最大暂停时间)</td><td></td><td>如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.</td></tr></tbody></table><p>-XX:+UseAdaptiveSizePolicy 自动选择年轻代区大小和相应的Survivor区比例<br>设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开.<br>-XX:GCTimeRatio|设置垃圾回收时间占程序运行时间的百分比||公式为1/(1+n)<br>-XX:+ScavengeBeforeFullGC|Full GC前调用YGC|true|Do young generation GC prior to a full GC. (Introduced in 1.4.1.)</p><font size="3"><strong>CMS相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseConcMarkSweepGC</td><td>使用CMS内存收集</td><td></td><td>测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.???</td></tr><tr><td>-XX:+AggressiveHeap</td><td></td><td></td><td>试图是使用大量的物理内存，长时间大内存使用的优化，能检查计算资源（内存， 处理器数量），至少需要256MB内存，大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升）</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>多少次后进行内存压缩</td><td>由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生”碎片”,使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理.</td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>降低标记停顿</td></tr><tr><td>-XX+UseCMSCompactAtFullCollection</td><td>在FULL GC的时候， 对年老代的压缩</td><td></td><td>CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。可能会影响性能,但是可以消除碎片</td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td><td>禁止hostspot自行触发CMS GC</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction=70</td><td>使用cms作为垃圾回收，使用70％后开始CMS收集</td><td>92</td><td>为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式<strong>CMSInitiatingOccupancyFraction</strong>计算公式</td></tr><tr><td>-XX:CMSInitiatingPermOccupancyFraction</td><td>设置Perm Gen使用到达多少比率时触发</td><td>92</td></tr><tr><td>-XX:+CMSIncrementalMode</td><td>设置为增量模式</td><td></td><td>用于单CPU情况</td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr></tbody></table><font size="3"><strong>辅助信息</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+PrintGC</td><td></td><td></td><td>输出形式:<br>[GC 118250K-&gt;113543K(130112K), 0.0094143 secs]<br>[Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs]</td></tr><tr><td>-XX:+PrintGCDetails</td><td></td><td></td><td>输出形式:<br>[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs]<br>[GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs]</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td></td></tr><tr><td>-XX:+PrintGC:PrintGCTimeStamps</td><td></td><td></td><td>可与-XX:+PrintGC -XX:+PrintGCDetails混合使用<br>输出形式:11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs]</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td><td></td><td>输出形式:Total time for which application threads were stopped: 0.0468229 seconds</td></tr><tr><td>-XX:+PrintGCApplicationConcurrentTime</td><td>打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用</td><td></td><td>输出形式:Application time: 0.5291524 seconds</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td><td></td></tr><tr><td>-Xloggc:filename</td><td>把相关日志信息记录到文件以便分析.<br>与上面几个配合使用</td><td></td></tr><tr><td>-XX:+PrintClassHistogram</td><td>garbage collects before printing the histogram.</td><td></td></tr><tr><td>-XX:+PrintTLAB</td><td>查看TLAB空间的使用情况</td><td></td></tr><tr><td>XX:+PrintTenuringDistribution</td><td>查看每次minor GC后新的存活周期的阈值</td><td></td><td>Desired survivor size 1048576 bytes, new threshold 7 (max 15)</td></tr></tbody></table><p>new threshold 7即标识新的存活周期的阈值为7。</p><font size="3" color="#FF4500"><strong>GC性能方面的考虑</strong></font><p>对于GC的性能主要有2个方面的指标：吞吐量throughput（工作时间不算gc的时间占总的时间比）和暂停pause（gc发生时app对外显示的无法响应）</p><ol><li><p>Total Heap</p><p>默认情况下，vm会增加/减少heap大小以维持free space在整个vm中占的比例，这个比例由MinHeapFreeRatio和MaxHeapFreeRatio指定。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>对vm分配尽可能多的memory；</li><li>将Xms和Xmx设为一样的值。如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。</li><li>处理器核数增加，内存也跟着增大。</li></ul></li><li><p>The Young Generation</p><p>另外一个对于app流畅性运行影响的因素是young generation的大小。young generation越大，minor collection越少；但是在固定heap size情况下，更大的young generation就意味着小的tenured generation，就意味着更多的major collection(major collection会引发minor collection)。<br><br>NewRatio反映的是young和tenured generation的大小比例。NewSize和MaxNewSize反映的是young generation大小的下限和上限，将这两个值设为一样就固定了young generation的大小（同Xms和Xmx设为一样）。<br><br>如果希望，SurvivorRatio也可以优化survivor的大小，不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>首先决定能分配给vm的最大的heap size，然后设定最佳的young generation的大小；</li><li>如果heap size固定后，增加young generation的大小意味着减小tenured generation大小。让tenured generation在任何时候够大，能够容纳所有live的data（留10%-20%的空余）。</li></ul></li></ol><font size="3" color="#FF4500"><strong>经验&amp;&amp;规则</strong></font><ul><li><p>年轻代大小选择</p><ul><li>响应时间优先的应用:尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象.</li><li>吞吐量优先的应用:尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用.</li><li>避免设置过小.当新生代设置过小时会导致:1.YGC次数更加频繁 2.可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC.</li></ul></li><li><p>年老代大小选择</p><ul><li>响应时间优先的应用:年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎 片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得:<br>并发垃圾收集信息、持久代并发收集次数、传统GC信息、花在年轻代和年老代回收上的时间比例。</li><li>吞吐量优先的应用:一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.原因是,这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象.</li></ul></li><li><p>较小堆引起的碎片问题</p><p>因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置:<br><br>-XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩.<br><br>-XX:CMSFullGCsBeforeCompaction=0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩</p></li><li><p>用64位操作系统，Linux下64位的jdk比32位jdk要慢一些，但是吃得内存更多，吞吐量更大</p></li><li>XMX和XMS设置一样大，MaxPermSize和MinPermSize设置一样大，这样可以减轻伸缩堆大小带来的压力</li><li>使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 然后老生代利用CMS并行收集， 这样能保证系统低延迟的吞吐效率。 实际上cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间</li><li>系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。(相关工具的使用方法将在后面的blog中介绍)</li><li>仔细了解自己的应用，如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。</li><li>采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿</li><li>JVM参数的设置(特别是 –Xmx –Xms –Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。</li></ul><p><strong>promotion failed</strong></p><p>垃圾回收时promotion failed是个很头痛的问题，一般可能是两种原因产生，第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间；第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。</p><p><strong>解决方方案一</strong></p><p>第一个原因我的最终解决办法是去掉救助空间，设置-XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0即可，第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。</p><p><strong>解决方案一的改进方案</strong></p><p>又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。</p><p>-Xmx4000M -Xms4000M -Xmn600M -XX:PermSize=500M -XX:MaxPermSize=500M -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log</p><font size="3" color="#FF4500">CMSInitiatingOccupancyFraction值与Xmn的关系公式</font><p>上面介绍了promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即：<br>(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2)) 进而推断出：</p><pre><code>CMSInitiatingOccupancyFraction &lt;=((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100 </code></pre><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当xmx=128 xmn=36 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-36)-(36-36/(1+2)))/(128-36)*100 =73.913 </span><br><span class="line"></span><br><span class="line">当xmx=128 xmn=24 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-24)-(24-24/(1+2)))/(128-24)*100=84.615… </span><br><span class="line"></span><br><span class="line">当xmx=3000 xmn=600 SurvivorRatior=1时  CMSInitiatingOccupancyFraction&lt;=((3000.0-600)-(600-600/(1+2)))/(3000-600)*100=83.33</span><br></pre></td></tr></table></figure><p>CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。</p><p>令：</p><p>网上一童鞋推断出的公式是：:(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn 这个公式个人认为不是很严谨，在内存小的时候会影响xmn的计算。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之二: 常见的垃圾收集器</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%8C_%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之二_常见的垃圾收集器/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:25:25.799Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><p>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<br><br><br>Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。</p><p><img src="/assets/pic/2019-06-19-2-1.png" alt="收集器"></p><font size="3"><b>HotSpot虚拟机的垃圾回收器</b></font><p>图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。</p><font size="3"><b>概念理解</b></font><ul><li><font size="3"><b>并发和并行</b></font><p>这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，它们可以解释如下</p><ul><li><strong>并行（Parallel）</strong>：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</li><li><strong>并发（Concurrent）</strong>：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。</li></ul></li><li><font size="3"><b>Minor GC 和 Full GC</b></font><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。</li><li><strong>老年代GC（Major GC / Full GC）</strong>：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。</li></ul></li><li><font size="3"><b>吞吐量</b></font><p>吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。<br><br><br>虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。</p></li></ul><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经（在JDK 1.3.1之前）是虚拟机新生代收集的唯一选择。</p><p><img src="/assets/pic/2019-06-19-2-2.png" alt="Serial收集器"></p><ul><li><p><strong>特性</strong></p><p>这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。Stop The World</p></li><li><p><strong>应用场景</strong></p><p>Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。</p></li><li><p><strong>优势</strong></p><p>简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。</p></li></ul><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p><img src="/assets/pic/2019-06-19-2-3.png" alt="ParNew收集器"></p><ul><li><p><strong>特性</strong></p><p>ParNew收集器其实就是Serial收集器的<strong>多线程版本</strong>，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。</p></li><li><p><strong>应用场景</strong></p><p>ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器。<br><br><br>很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器配合工作。<br><br><br>在JDK 1.5时期，HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器——CMS收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。<br><br><br>不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。</p></li><li><p><strong>Serial收集器 VS ParNew收集器</strong></p><p>ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。</p><p>然而，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。</p></li></ul><h3 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h3><ul><li><p><strong>特性</strong></p><p>Parallel Scavenge收集器是一个<strong>新生代收集器</strong>，它也是使用<strong>复制算法</strong>的收集器，又是并行的多线程收集器。</p></li><li><p><strong>应用场景</strong></p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。</p></li><li><p><strong>对比分析</strong></p><ul><li><p><strong>Parallel Scavenge收集器 VS CMS等收集器</strong></p><p>Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个<strong>可控制的吞吐量（Throughput）</strong>。</p><p>由于与吞吐量关系密切，Parallel Scavenge收集器也经常称为“吞吐量优先”收集器。</p></li><li><p><strong>Parallel Scavenge收集器 VS ParNew收集器</strong></p><p>Parallel Scavenge收集器与ParNew收集器的一个重要区别是它具有自适应调节策略。</p><p><strong>GC自适应的调节策略</strong></p><p>Parallel Scavenge收集器有一个参数-XX:+UseAdaptiveSizePolicy。当这个参数打开之后，就不需要手工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。</p></li></ul></li></ul><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-4.png" alt="Serial Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Serial Old是Serial收集器的<strong>老年代版本</strong>，它同样是一个<strong>单线程收集器</strong>，使用<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><ul><li><p><strong>Client模式</strong></p><p>Serial Old收集器的主要意义也是在于给Client模式下的虚拟机使用。</p></li><li><p><strong>Server模式</strong></p><p>如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。</p></li></ul></li></ul><h3 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-5.png" alt="Parallel Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Parallel Old是Parallel Scavenge收集器的<strong>老年代版本</strong>，使用<strong>多线程</strong>和<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><p>在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。</p><p>这个收集器是在JDK 1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择（Parallel Scavenge收集器无法与CMS收集器配合工作）。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合。</p></li></ul><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p><img src="/assets/pic/2019-06-19-2-6.png" alt="CMS收集器"></p><ul><li><p><strong>特性</strong></p><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。</p><p>CMS收集器是基于<strong>“标记—清除”算法</strong>实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤:</p><ol><li><p><strong>初始标记（CMS initial mark）</strong></p><p>初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。</p></li><li><p><strong>并发标记（CMS concurrent mark）</strong></p><p>并发标记阶段就是进行GC Roots Tracing的过程。</p></li><li><p><strong>重新标记（CMS remark）</strong></p><p>重新标记阶段是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，仍然需要“Stop The World”。</p></li><li><p><strong>并发清除（CMS concurrent sweep）</strong></p><p>并发清除阶段会清除对象。</p><p>由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。</p></li></ol></li><li><p><strong>优点</strong></p><p>CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：<strong>并发收集、低停顿</strong>。</p></li><li><p><strong>缺点</strong></p><ul><li><p><strong>CMS收集器对CPU资源非常敏感</strong></p><p>其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。</p><p>CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得很大。</p></li><li><p><strong>CMS收集器无法处理浮动垃圾</strong></p><p>CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。</p><p>由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。<br>也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。</p></li><li><p><strong>CMS收集器会产生大量空间碎片</strong></p><p>CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。</p><p>空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。</p></li></ul></li></ul><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p><img src="/assets/pic/2019-06-19-2-7.png" alt="G1收集器"></p><ul><li><p><strong>特性</strong></p><p>G1（Garbage-First）是一款<strong>面向服务端应用</strong>的垃圾收集器。HotSpot开发团队赋予它的使命是未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</p><p>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。</p><p>G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。</p><ul><li><p><strong>并行与并发</strong></p><p>G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</p></li><li><p><strong>分代收集</strong></p><p>与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。</p></li><li><p><strong>空间整合</strong></p><p>与CMS的“标记—清理”算法不同，G1从<strong>整体来看是基于“标记—整理”算法</strong>实现的收集器，从<strong>局部（两个Region之间）上来看是基于“复制”算法</strong>实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</p></li><li><p><strong>可预测的停顿</strong></p><p>这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。</p></li></ul></li><li><p><strong>执行过程</strong></p><p>G1收集器的运作大致可划分为以下几个步骤：</p><ul><li><p><strong>初始标记（Initial Marking）</strong></p><p>初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。</p></li><li><p><strong>并发标记（Concurrent Marking）</strong></p><p>并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。</p></li><li><p><strong>最终标记（Final Marking）</strong></p><p>最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。</p></li><li><p><strong>筛选回收（Live Data Counting and Evacuation）</strong></p><p>筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</p></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>虽然我们是在对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的收集器出现，更加没有万能的收集器，所以我们选择的只是对具体应用最合适的收集器。这点不需要多加解释就能证明：如果有一种放之四海皆准、任何场景下都适用的完美收集器存在，那HotSpot虚拟机就没必要实现那么多不同的收集器了。</p><p><img src="/assets/pic/2019-06-19-2-8.png" alt="总结"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="垃圾收集器" scheme="http://yoursite.com/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之一: 内存结构(堆内存和非堆内存)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%80_%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84(%E5%A0%86%E5%86%85%E5%AD%98%E5%92%8C%E9%9D%9E%E5%A0%86%E5%86%85%E5%AD%98)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之一_内存结构(堆内存和非堆内存)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:47.558Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><p><strong>图为Java虚拟机运行时的数据区:</strong></p><p><img src="/assets/pic/2019-06-19-1-1.png" alt="数据区"></p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>也称”永久代” 、“非堆”， 它用于存储虚拟机加载的类信息、常量、静态变量、是<font color="red">各个线程共享的内存区域</font>。<font color="blue">默认最小值为16MB，最大值为64MB（未验证）</font>，可以通过-XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。<br><br><br>运行时常量池：是方法区的一部分，Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种符号引用，这部分内容将在类加载后放到方法区的运行时常量池中。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>描述的是java 方法执行的内存模型：每个方法被执行的时候 都会创建一个“栈帧”用于存储局部变量表(包括参数)、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。声明周期与线程相同，是<font color="red">线程私有</font>的。<br><br><br>局部变量表存放了编译器可知的各种基本数据类型(boolean、byte、char、short、int、float、long、 double)、对象引用(引用指针，并非对象本身)，其中64位长度的long和double类型的数据会占用2个局部变量的空间，其余数据类型只占1 个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量是完全确定的，在运行期间栈帧不会改变局部 变量表的大小空间。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>与虚拟机栈基本类似，区别在于虚拟机栈为虚拟机执行的java方法服务，而本地方法栈则是为Native方法服务。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>也叫做java 堆、GC堆，是java虚拟机所管理的内存中最大的一块内存区域，也是<font color="red">被各个线程共享的内存区域</font>，在JVM启动时创建。该内存区域存放了对象实例及数组(所有new的对象)。其大小通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，-Xmx为JVM可申请的最大内存。在JVM启动时，最大内存会被保留下来。为对象内存而保留的地址空间可以被分成年轻代和老年代。<br><br><br>默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过-XX:MinHeapFreeRation=来指定这个比列；当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过XX:MaxHeapFreeRation=来指定这个比列，对于运行系统，为避免在运行时频繁调整Heap的大小，通常-Xms与-Xmx的值设成一样。</p><table><thead><tr><th style="text-align:center">Parameter</th><th style="text-align:center">Default Value</th></tr></thead><tbody><tr><td style="text-align:center">MinHeapFreeRatio</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">MaxHeapFreeRatio</td><td style="text-align:center">70</td></tr><tr><td style="text-align:center">-Xms</td><td style="text-align:center">3670k</td></tr><tr><td style="text-align:center">-Xmx</td><td style="text-align:center">64m</td></tr></tbody></table><font color="red">注：如果是64位系统，这些值一般需要扩张30％，来容纳在64位系统下变大的对象。</font><p>从J2SE 1.2开始，JVM使用分代收集算法，在不同年代的区域里使用不同的算法。堆被划分为新生代和老年代。新生代主要存储新创建的对象和尚未进入老年代的对象。老年代存储经过多次新生代GC(MinorGC)任然存活的对象。</p><p><img src="/assets/pic/2019-06-19-1-2.png" alt="堆"></p><font color="red"><b><br>注1：图中的Perm不是堆内存，是永久代<br><br>注2：图中的Virtaul则是各区域还未被分配的内存，即最大内存-当前分配的内存<br></b></font><p><strong>新生代：</strong></p><p>新生代包括一块eden（伊甸园）和2块survivor(通常又称S0和S1或From和To)。大多数对象都是在eden中初始化。而对于2块survivor来说，总有一块是空的，它会在下一个复制收集过程中作为eden中的活跃对象和另一块survivor的目的地。在对象衰老之前（也就是被复制到tenured之前），它们会在两块survivor区域之间以这样的方式复制。可通过-Xmn参数来指定新生代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及Survivor Space的大小。</p><p><strong>老年代：</strong></p><p>用于存放经过多次新生代Minor GC依然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：</p><ol><li>大对象，可通过启动参数设置-XX:PretenureSizeThreshold=1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。</li><li>大的数组对象，即数组中无引用外部对象。</li></ol><p>老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。</p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>是最小的一块内存区域，它的作用是当前线程所执行的字节码的行号指示器，在虚拟机的模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="内存结构" scheme="http://yoursite.com/tags/%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>捷报:连续5周若泽数据第20-21名学员喜捷offer(含面试题)</title>
    <link href="http://yoursite.com/2019/06/18/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD5%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC20-21%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/"/>
    <id>http://yoursite.com/2019/06/18/捷报_连续5周若泽数据第20-21名学员喜捷offer(含面试题)/</id>
    <published>2019-06-17T16:00:00.000Z</published>
    <updated>2019-06-18T11:02:01.857Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第20个小伙伴，<font color="red">25K</font></b></font><p><img src="/assets/blogImg/2019-06-18-1.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.第21个小伙伴，<font color="red">2.4W*14</font></b></font><p><img src="/assets/blogImg/2019-06-18-2.png" alt="就业2"></p><font color="blue" size="4"><b>接下来是现场技术面试题:</b></font><ol><li>谈谈Spark RDD 的几大特性，并深入讲讲体现在哪</li><li>说说你参与过的项目，和一些业务场景</li><li>请说说Spark的宽窄依赖</li><li>Spark的stage划分，task跟分区的关系</li><li>详细讲讲Spark的内存管理，计算与存储是如何协调的</li><li>rdd df ds 之间的区别 ，什么时候使用ds</li><li>聊聊kafka消费如何保证不会重复消费</li><li>你项目里说到了数据延迟和数据重跑，请你说说当时是怎么解决的，如何保障幂等性！</li></ol><font color="#00CD00">（ps: 这面试题，若泽数据高级班&amp;线下班的小伙伴，so easy!）</font><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#63B8FF&quot; size=&quot;4&quot;&gt;&lt;br&gt;我们不做过多宣传，因为我们是若泽数据，企业在职。&lt;br&gt;&lt;br&gt;（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）&lt;br&gt;&lt;/font&gt;
    
    </summary>
    
      <category term="高薪就业" scheme="http://yoursite.com/categories/%E9%AB%98%E8%96%AA%E5%B0%B1%E4%B8%9A/"/>
    
    
      <category term="高薪" scheme="http://yoursite.com/tags/%E9%AB%98%E8%96%AA/"/>
    
      <category term="就业" scheme="http://yoursite.com/tags/%E5%B0%B1%E4%B8%9A/"/>
    
      <category term="面试题" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>生产Flume源码导入IDEA方式</title>
    <link href="http://yoursite.com/2019/06/17/%E7%94%9F%E4%BA%A7Flume%E6%BA%90%E7%A0%81%E5%AF%BC%E5%85%A5IDEA%E6%96%B9%E5%BC%8F/"/>
    <id>http://yoursite.com/2019/06/17/生产Flume源码导入IDEA方式/</id>
    <published>2019-06-16T16:00:00.000Z</published>
    <updated>2019-06-20T09:37:18.312Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz"><a href="#下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz" class="headerlink" title="下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz"></a>下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz</h4><p>下载地址:<a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz</a></p><h4 id="win安装好maven-3-3-9"><a href="#win安装好maven-3-3-9" class="headerlink" title="win安装好maven-3.3.9"></a>win安装好maven-3.3.9</h4><h4 id="解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径"><a href="#解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径" class="headerlink" title="解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径"></a>解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径</h4><h4 id="编译：mvn-clean-compile"><a href="#编译：mvn-clean-compile" class="headerlink" title="编译：mvn clean compile"></a>编译：mvn clean compile</h4><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.0:enforce (clean) on project flume-parent: Some Enforcer rules have failed. Look above for specific messages explaining</span><br><span class="line">why the rule failed. -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p>换成以下编译命令，跳过enforcer</p><p><code>mvn clean compile validate -Denforcer.skip=true</code></p><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flume-ng-morphline-solr-sink: Could not resolve dependencies for project org.apache.flume.flume-ng-sinks:flume-ng-morphline-solr-sink:jar:1.6.0-cdh5.7.0: Fail</span><br><span class="line">ed to collect dependencies at org.kitesdk:kite-morphlines-all:pom:1.0.0-cdh5.7.0 -&gt; org.kitesdk:kite-morphlines-useragent:jar:1.0.0-cdh5.7.0 -&gt; ua_parser:ua-parser:jar:1.3.0: Failed to read artifact d</span><br><span class="line">escriptor for ua_parser:ua-parser:jar:1.3.0: Could not transfer artifact ua_parser:ua-parser:pom:1.3.0 from/to maven-twttr (http://maven.twttr.com): Connect to maven.twttr.com:80 [maven.twttr.com/31.1</span><br><span class="line">3.83.8] failed: Connection timed out: connect -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><code>flume-ng-morphline-solr-sink</code>我们用不到，可以直接注释掉，在<code>flume-ng-sinks</code>下的pom中找到并注释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;modules&gt;</span><br><span class="line">    &lt;module&gt;flume-hdfs-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-irc-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-hbase-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-elasticsearch-sink&lt;/module&gt;</span><br><span class="line">    &lt;!--&lt;module&gt;flume-ng-morphline-solr-sink&lt;/module&gt; --&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-kafka-sink&lt;/module&gt;</span><br><span class="line">&lt;/modules&gt;</span><br></pre></td></tr></table></figure><p>然后重新编译<code>mvn clean compile validate -Denforcer.skip=true</code>，成功</p><p><img src="/assets/pic/2019-06-17-1.png" alt="编译"></p><h4 id="导入IDEA"><a href="#导入IDEA" class="headerlink" title="导入IDEA"></a>导入IDEA</h4><p><img src="/assets/pic/2019-06-17-2.png" alt="1"></p><p><img src="/assets/pic/2019-06-17-3.png" alt="2"></p><p><img src="/assets/pic/2019-06-17-4.png" alt="3"></p><p><img src="/assets/pic/2019-06-17-5.png" alt="4"></p><p><img src="/assets/pic/2019-06-17-6.png" alt="5"></p><p><img src="/assets/pic/2019-06-17-7.png" alt="6"></p><p>然后等到导入完毕！</p><p><img src="/assets/pic/2019-06-17-8.png" alt="7"></p><p>导入后没有任何报错，这时我们就可以对源码进行修改了！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://yoursite.com/categories/Flume/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
    <link href="http://yoursite.com/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
    <id>http://yoursite.com/2019/06/14/生产SparkStreaming数据零丢失最佳实践(含代码)/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-14T05:14:17.936Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use test</span><br><span class="line">mysql&gt; create table hlw_offset(</span><br><span class="line">        topic varchar(32),</span><br><span class="line">        groupid varchar(50),</span><br><span class="line">        partitions int,</span><br><span class="line">        fromoffset bigint,</span><br><span class="line">        untiloffset bigint,</span><br><span class="line">        primary key(topic,groupid,partitions)</span><br><span class="line">        );</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;</span><br><span class="line">&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;</span><br><span class="line">--------------------------------------------------</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.27&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;config&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）StreamingContext</span><br><span class="line">2）从kafka中获取数据(从外部存储获取offset--&gt;根据offset获取kafka中的数据)</span><br><span class="line">3）根据业务进行逻辑处理</span><br><span class="line">4）将处理结果存到外部存储中--保存offset</span><br><span class="line">5）启动程序，等待程序结束</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import kafka.common.TopicAndPartition</span><br><span class="line">import kafka.message.MessageAndMetadata</span><br><span class="line">import kafka.serializer.StringDecoder</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">import scalikejdbc._</span><br><span class="line">import scalikejdbc.config._</span><br><span class="line">object JDBCOffsetApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建SparkStreaming入口</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)</span><br><span class="line">    val ssc = new StreamingContext(conf,Seconds(5))</span><br><span class="line">    //kafka消费主题</span><br><span class="line">    val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet</span><br><span class="line">    //kafka参数</span><br><span class="line">    //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改</span><br><span class="line">    val kafkaParams = Map[String,String](</span><br><span class="line">      &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),</span><br><span class="line">      &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),</span><br><span class="line">      &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)</span><br><span class="line">    )</span><br><span class="line">    //先使用scalikejdbc从MySQL数据库中读取offset信息</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来</span><br><span class="line">    //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到</span><br><span class="line">    DBs.setup()</span><br><span class="line">    val fromOffset = DB.readOnly( implicit session =&gt; &#123;</span><br><span class="line">      SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123;</span><br><span class="line">        (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))</span><br><span class="line">      &#125;).list().apply()</span><br><span class="line">    &#125;).toMap</span><br><span class="line">    //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费</span><br><span class="line">      val messages = if (fromOffset.isEmpty) &#123;</span><br><span class="line">        println(&quot;从头开始消费...&quot;)</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        println(&quot;从已存在记录开始消费...&quot;)</span><br><span class="line">        val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)</span><br><span class="line">      &#125;</span><br><span class="line">      messages.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">        if(!rdd.isEmpty())&#123;</span><br><span class="line">          //输出rdd的数据量</span><br><span class="line">          println(&quot;数据统计记录为：&quot;+rdd.count())</span><br><span class="line">          //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组</span><br><span class="line">//          trait HasOffsetRanges &#123;</span><br><span class="line">//            def offsetRanges: Array[OffsetRange]</span><br><span class="line">//          &#125;</span><br><span class="line">          val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">          offsetRanges.foreach(x =&gt; &#123;</span><br><span class="line">            //输出每次消费的主题，分区，开始偏移量和结束偏移量</span><br><span class="line">            println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;)</span><br><span class="line">           //将最新的偏移量信息保存到MySQL表中</span><br><span class="line">            DB.autoCommit( implicit session =&gt; &#123;</span><br><span class="line">              SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)</span><br><span class="line">            .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)</span><br><span class="line">              .update().apply()</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义的ValueUtils工具类如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import com.typesafe.config.ConfigFactory</span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">object ValueUtils &#123;</span><br><span class="line">val load = ConfigFactory.load()</span><br><span class="line">  def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;</span><br><span class="line">val value = load.getString(key)</span><br><span class="line">    if(StringUtils.isNotEmpty(value)) &#123;</span><br><span class="line">      value</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      defaultValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>application.conf内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">metadata.broker.list = &quot;192.168.137.251:9092&quot;</span><br><span class="line">auto.offset.reset = &quot;smallest&quot;</span><br><span class="line">group.id = &quot;hlw_offset_group&quot;</span><br><span class="line">kafka.topics = &quot;hlw_offset&quot;</span><br><span class="line">serializer.class = &quot;kafka.serializer.StringEncoder&quot;</span><br><span class="line">request.required.acks = &quot;1&quot;</span><br><span class="line"># JDBC settings</span><br><span class="line">db.default.driver = &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;</span><br><span class="line">db.default.user=&quot;root&quot;</span><br><span class="line">db.default.password=&quot;123456&quot;</span><br></pre></td></tr></table></figure></li><li><p>自定义kafka producer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.&#123;Date, Properties&#125;</span><br><span class="line">import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;</span><br><span class="line">object KafkaProducer &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val properties = new Properties()</span><br><span class="line">    properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))</span><br><span class="line">    properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))</span><br><span class="line">    properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))</span><br><span class="line">    val producerConfig = new ProducerConfig(properties)</span><br><span class="line">    val producer = new Producer[String,String](producerConfig)</span><br><span class="line">    val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)</span><br><span class="line">    //每次产生100条数据</span><br><span class="line">    var i = 0</span><br><span class="line">    for (i &lt;- 1 to 100) &#123;</span><br><span class="line">      val runtimes = new Date().toString</span><br><span class="line">     val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)</span><br><span class="line">      producer.send(messages)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;数据发送完毕...&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</span><br></pre></td></tr></table></figure></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台输出结果：</span><br><span class="line">从头开始消费...</span><br><span class="line">数据统计记录为：500</span><br><span class="line">---hlw_offset,0,0,500---</span><br></pre></td></tr></table></figure></li></ol><pre><code>查看MySQL表，offset记录成功<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |          0 |         500 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></code></pre><ol start="5"><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台结果输出：</span><br><span class="line">从已存在记录开始消费...</span><br><span class="line">数据统计记录为：300</span><br><span class="line">---hlw_offset,0,500,800---</span><br></pre></td></tr></table></figure></li><li><p>查看更新后的offset MySQL数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |        500 |         800 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;MySQL创建存储offset的表格&quot;&gt;&lt;a href=&quot;#MySQL创建存储offset的表格&quot; class=&quot;headerlink&quot; title=&quot;MySQL创建存储offset的表格&quot;&gt;&lt;/a&gt;MySQL创建存储offset的表格&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mysql&amp;gt; use test&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mysql&amp;gt; create table hlw_offset(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        topic varchar(32),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        groupid varchar(50),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        partitions int,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fromoffset bigint,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        untiloffset bigint,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        primary key(topic,groupid,partitions)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        );&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Spark Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark中Cache与Persist的巅峰对决</title>
    <link href="http://yoursite.com/2019/06/14/Spark%E4%B8%ADCache%E4%B8%8EPersist%E7%9A%84%E5%B7%85%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
    <id>http://yoursite.com/2019/06/14/Spark中Cache与Persist的巅峰对决/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-20T09:37:01.640Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="Cache的产生背景"><a href="#Cache的产生背景" class="headerlink" title="Cache的产生背景"></a>Cache的产生背景</h2><p>我们先做一个简单的测试读取一个本地文件做一次collect操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br></pre></td></tr></table></figure><p>上面我们进行了两次相同的操作，观察日志我们发现这样一句话<code>Submitting ResultStage 0 (file:///home/hadoop/data/input.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25), which has no missing parents</code>，每次都要去本地读取input.txt文件，这里大家能想到存在什么问题吗? 如果我的文件很大，每次都对相同的RDD进行同一个action操作，那么每次都要到本地读取文件，得到相同的结果。不断进行这样的重复操作，耗费资源浪费时间啊。这时候我们可能想到能不能把RDD保存在内存中呢？答案是可以的，这就是我们所要学习的cache。</p><h2 id="Cache的作用"><a href="#Cache的作用" class="headerlink" title="Cache的作用"></a>Cache的作用</h2><p>通过上面的讲解我们知道, 有时候很多地方都会用到同一个RDD, 那么每个地方遇到Action操作的时候都会对同一个算子计算多次, 这样会造成效率低下的问题。通过cache操作可以把RDD持久化到内存或者磁盘。</p><p>现在我们利用上面说的例子，把rdd进行cache操作</p><p>rdd.cache这时候我们打开192.168.137.130:4040界面查看storage界面中是否有我们的刚才cache的文件，发现并没有。这时候我们进行一个action操作rdd.count。继续查看storage是不是有东西了哈</p><p><img src="/assets/pic/2019-06-14-1.png" alt="Cache"></p><p>并且给我们列出了很多信息，存储级别（后面详解），大小（会发现要比源文件大，这也是一个调优点）等等。</p><p>说到这里小伙伴能能想到什么呢？ cacha是一个Tranformation还是一个Action呢？相信大伙应该知道了。</p><p>cache这个方法也是个Tranformation,当第一次遇到Action算子的时才会进行持久化，所以说我们第一次进行了cache操作在ui中并没有看到结果，进行了count操作才有。</p><h2 id="源码详细解析"><a href="#源码详细解析" class="headerlink" title="源码详细解析"></a>源码详细解析</h2><p><strong>Spark版本：2.2.0</strong></p><p>源码分析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">  */</span><br><span class="line"> def cache(): this.type = persist()</span><br></pre></td></tr></table></figure><p>从源码中可以明显看出cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：（这里注释cache的storage level）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，是不是和上面对上了哈，这里我们能够得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><p>我相信小伙伴们肯定很好奇这个缓存级别到底有多少种呢？我们继续怼源码看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: DeveloperApi ::</span><br><span class="line"> * Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,</span><br><span class="line"> * or ExternalBlockStore, whether to drop the RDD to disk if it falls out of memory or</span><br><span class="line"> * ExternalBlockStore, whether to keep the data in memory in a serialized format, and whether</span><br><span class="line"> * to replicate the RDD partitions on multiple nodes.</span><br><span class="line"> *</span><br><span class="line"> * The [[org.apache.spark.storage.StorageLevel]] singleton object contains some static constants</span><br><span class="line"> * for commonly useful storage levels. To create your own storage level object, use the</span><br><span class="line"> * factory method of the singleton object (`StorageLevel(...)`).</span><br><span class="line"> */</span><br><span class="line">@DeveloperApi</span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure><p>我们先来看看存储类型，源码中我们可以看出有五个参数，分别代表：</p><p><code>useDisk</code>:使用硬盘（外存）;</p><p><code>useMemory</code>:使用内存;</p><p><code>useOffHeap</code>:使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。这部分内存也会被频繁的使用而且也可能导致OOM，它是通过存储在堆中的DirectByteBuffer对象进行引用，可以避免堆和堆外数据进行来回复制；</p><p><code>deserialized</code>:反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象;</p><p><code>replication</code>:备份数（在多个节点上备份，默认为1）。</p><p>我们接着看看缓存级别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Various [[org.apache.spark.storage.StorageLevel]] defined and utility functions for creating</span><br><span class="line"> * new storage levels.</span><br><span class="line"> */</span><br><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，<strong>但这些有什么区别呢？</strong>可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，和上面说的存储类型是相对应的，四个参数是因为有一个是有默认值的。</p><p>好吧这里我又想问小伙伴们一个问题了，这几种存储方式什么意思呢？该如何选择呢？</p><p>官网上进行了详细的解释。我这里介绍一个有兴趣的同学可以去官网看看哈。</p><p><strong>MEMORY_ONLY</strong></p><blockquote><p>使用反序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，某些分区将不会被缓存，并且将在需要时重新计算。这是默认级别。</p></blockquote><p><strong>MEMORY_AND_DISK</strong></p><blockquote><p>使用反序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</p></blockquote><p><strong>MEMORY_ONLY_SER（(Java and Scala)）</strong></p><blockquote><p>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，但是会加大cpu负担。</p></blockquote><p>一个简单的案例感官行的认识存储级别的差别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19M     page_views.dat</span><br><span class="line"></span><br><span class="line">val rdd1=sc.textFile(&quot;file:///home/hadoop/data/page_views.dat&quot;)</span><br><span class="line">rdd1.persist().count</span><br></pre></td></tr></table></figure><p>ui查看缓存大小：</p><p><img src="/assets/pic/2019-06-14-2.png" alt="ui查看缓存大小"></p><p>是不是明显变大了，我们先删除缓存<font color="red">rdd1.unpersist()</font></p><p>使用MEMORY_ONLY_SER级别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">rdd1.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">rdd1.count</span><br></pre></td></tr></table></figure><p><img src="/assets/pic/2019-06-14-3.png" alt="MEMORY_ONLY_SER"></p><p>这里我就用这两种方式进行对比，大家可以试试其他方式。</p><p>那如何选择呢？哈哈官网也说了。</p><p>你可以在内存使用和CPU效率之间来做出不同的选择不同的权衡。</p><p>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</p><p>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</p><p>不要泄漏到磁盘，除非你在内存中计算需要很大的花费，或者可以过滤大量数据，保存部分相对重要的在内存中。否则存储在磁盘中计算速度会很慢，性能急剧降低。</p><p>后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p><h2 id="删除缓存中的数据"><a href="#删除缓存中的数据" class="headerlink" title="删除缓存中的数据"></a>删除缓存中的数据</h2><p>spark自动监视每个节点上的缓存使用，并以最近最少使用的（LRU）方式丢弃旧数据分区。如果您想手动删除RDD，而不是等待它从缓存中掉出来，请使用 RDD.unpersist()方法。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jul 24 2019 10:28:30 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Cache" scheme="http://yoursite.com/tags/Cache/"/>
    
      <category term="Persist" scheme="http://yoursite.com/tags/Persist/"/>
    
  </entry>
  
</feed>
