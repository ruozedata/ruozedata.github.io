<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-13T08:00:08.535Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>若泽数据课程一览</title>
    <link href="http://yoursite.com/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/"/>
    <id>http://yoursite.com/2019/05/08/若泽数据课程一览/</id>
    <published>2019-05-07T16:00:00.000Z</published>
    <updated>2019-05-13T08:00:08.535Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h1 id="若泽数据课程系列"><a href="#若泽数据课程系列" class="headerlink" title="若泽数据课程系列"></a>若泽数据课程系列</h1><h2 id="基础班"><a href="#基础班" class="headerlink" title="基础班"></a>基础班</h2><h3 id="Liunx"><a href="#Liunx" class="headerlink" title="Liunx"></a>Liunx</h3><ul><li>VM虚拟机安装</li><li>Liunx常用命令（重点）</li><li>开发环境搭</li></ul><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ul><li>源码安装&amp;yum安装</li><li>CRUD编写</li><li>权限控制</li></ul><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>架构介绍&amp;&amp;源码编译</li><li>伪分布式安装&amp;&amp;企业应用</li><li><p>HDFS（重点）</p><ul><li>架构设计</li><li>副本放置策略</li><li>读写流程</li></ul></li><li><p>YARN（重点）</p><ul><li>架构设计</li><li>工作流程</li><li>调度管理&amp;&amp;常见参数配置（调优）</li></ul></li><li><p>MapReduce</p><ul><li>架构设计</li><li>wordcount原理&amp;&amp;join原理和案例<a id="more"></a><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3></li></ul></li><li><p>架构设计</p></li><li>Hive DDL&amp;DML</li><li>join在大数据中的使用</li><li>使用自带UDF和开发自定义UDF</li></ul><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><ul><li>架构设计</li><li>RDBMS导入导出</li></ul><h3 id="整合项目将所有组件合作使用。"><a href="#整合项目将所有组件合作使用。" class="headerlink" title="整合项目将所有组件合作使用。"></a>整合项目将所有组件合作使用。</h3><h3 id="人工智能基础"><a href="#人工智能基础" class="headerlink" title="人工智能基础"></a>人工智能基础</h3><ul><li>python基础</li><li>常用库——pandas、numpy、sklearn、keras</li></ul><h2 id="高级班"><a href="#高级班" class="headerlink" title="高级班"></a>高级班</h2><h3 id="scala编程（重点）"><a href="#scala编程（重点）" class="headerlink" title="scala编程（重点）"></a>scala编程（重点）</h3><h3 id="Spark（五星重点）"><a href="#Spark（五星重点）" class="headerlink" title="Spark（五星重点）"></a>Spark（五星重点）</h3><h3 id="Hadoop高级"><a href="#Hadoop高级" class="headerlink" title="Hadoop高级"></a>Hadoop高级</h3><h3 id="Hive高级"><a href="#Hive高级" class="headerlink" title="Hive高级"></a>Hive高级</h3><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h3 id="调度平台"><a href="#调度平台" class="headerlink" title="调度平台"></a>调度平台</h3><h2 id="线下班"><a href="#线下班" class="headerlink" title="线下班"></a>线下班</h2><p><img src="/assets/blogImg/若泽数据.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;若泽数据课程系列&quot;&gt;&lt;a href=&quot;#若泽数据课程系列&quot; class=&quot;headerlink&quot; title=&quot;若泽数据课程系列&quot;&gt;&lt;/a&gt;若泽数据课程系列&lt;/h1&gt;&lt;h2 id=&quot;基础班&quot;&gt;&lt;a href=&quot;#基础班&quot; class=&quot;headerlink&quot; title=&quot;基础班&quot;&gt;&lt;/a&gt;基础班&lt;/h2&gt;&lt;h3 id=&quot;Liunx&quot;&gt;&lt;a href=&quot;#Liunx&quot; class=&quot;headerlink&quot; title=&quot;Liunx&quot;&gt;&lt;/a&gt;Liunx&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;VM虚拟机安装&lt;/li&gt;&lt;li&gt;Liunx常用命令（重点）&lt;/li&gt;&lt;li&gt;开发环境搭&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;MySQL&quot;&gt;&lt;a href=&quot;#MySQL&quot; class=&quot;headerlink&quot; title=&quot;MySQL&quot;&gt;&lt;/a&gt;MySQL&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;源码安装&amp;amp;yum安装&lt;/li&gt;&lt;li&gt;CRUD编写&lt;/li&gt;&lt;li&gt;权限控制&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;Hadoop&quot;&gt;&lt;a href=&quot;#Hadoop&quot; class=&quot;headerlink&quot; title=&quot;Hadoop&quot;&gt;&lt;/a&gt;Hadoop&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;架构介绍&amp;amp;&amp;amp;源码编译&lt;/li&gt;&lt;li&gt;伪分布式安装&amp;amp;&amp;amp;企业应用&lt;/li&gt;&lt;li&gt;&lt;p&gt;HDFS（重点）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;副本放置策略&lt;/li&gt;&lt;li&gt;读写流程&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;YARN（重点）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;工作流程&lt;/li&gt;&lt;li&gt;调度管理&amp;amp;&amp;amp;常见参数配置（调优）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;MapReduce&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;wordcount原理&amp;amp;&amp;amp;join原理和案例
    
    </summary>
    
      <category term="课程" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B/"/>
    
    
      <category term="课程" scheme="http://yoursite.com/tags/%E8%AF%BE%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>docker常用命令以及安装mysql</title>
    <link href="http://yoursite.com/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/"/>
    <id>http://yoursite.com/2019/05/08/docker常用命令以及安装mysql/</id>
    <published>2019-05-07T16:00:00.000Z</published>
    <updated>2019-05-13T08:00:04.117Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>Docker是一个开源的应用容器引擎；是一个轻量级容器技术；</p><p>Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；</p><p>运行中的这个镜像称为容器，容器启动是非常快速的。<br><a id="more"></a></p><h3 id="2-核心概念"><a href="#2-核心概念" class="headerlink" title="2.核心概念"></a>2.核心概念</h3><p>docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；</p><p>docker客户端(Client)：连接docker主机进行操作；</p><p>docker仓库(Registry)：用来保存各种打包好的软件镜像；</p><p>docker镜像(Images)：软件打包好的镜像；放在docker仓库中；</p><p>docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用</p><h3 id="3-安装环境"><a href="#3-安装环境" class="headerlink" title="3.安装环境"></a>3.安装环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VM ware Workstation10</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">uname -r</span><br><span class="line">3.10.0-862.el7.x86_64</span><br></pre></td></tr></table></figure><p><strong>检查内核版本，必须是3.10及以上</strong> 查看命令：uname -r</p><h3 id="4-在linux虚拟机上安装docker"><a href="#4-在linux虚拟机上安装docker" class="headerlink" title="4.在linux虚拟机上安装docker"></a>4.在linux虚拟机上安装docker</h3><p>步骤：</p><p>1、检查内核版本，必须是3.10及以上<br>uname -r</p><p>2、安装docker<br>yum install docker</p><p>3、输入y确认安装<br>Dependency Updated:<br>audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1</p><p>Complete!<br>(成功标志)</p><p>4、启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl start docker</span><br><span class="line">[root@hadoop000 ~]# docker -v</span><br><span class="line">Docker version 1.13.1, build 8633870/1.13.1</span><br></pre></td></tr></table></figure><p></p><p>5、开机启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><p></p><p>6、停止docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl stop docker</span><br><span class="line">``` </span><br><span class="line">### 5.常用命令</span><br><span class="line"></span><br><span class="line">镜像操作</span><br><span class="line">|操作|命令|说明|</span><br><span class="line">|---|---|---|</span><br><span class="line">检索|docker search 关键字 eg：docker search redis|我们经常去docker hub上检索镜像的详细信息，如镜像的TAG。|</span><br><span class="line">拉取|docker pull 镜像名:tag|:tag是可选的，tag表示标签，多为软件的版本，默认是latest</span><br><span class="line">列表|docker images|查看所有本地镜像</span><br><span class="line">删除|docker rmi image-id|删除指定的本地镜像</span><br><span class="line"></span><br><span class="line">当然大家也可以在官网查找：https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">容器操作</span><br><span class="line">软件镜像（QQ安装程序）----运行镜像----产生一个容器（正在运行的软件，运行的QQ）；</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line"></span><br><span class="line">- 1、搜索镜像</span><br><span class="line">[root@localhost ~]# docker search tomcat</span><br><span class="line">- 2、拉取镜像</span><br><span class="line">[root@localhost ~]# docker pull tomcat</span><br><span class="line">- 3、根据镜像启动容器</span><br><span class="line">docker run --name mytomcat -d tomcat:latest</span><br><span class="line">- 4、docker ps  </span><br><span class="line">查看运行中的容器</span><br><span class="line">- 5、 停止运行中的容器</span><br><span class="line">docker stop  容器的id</span><br><span class="line">- 6、查看所有的容器</span><br><span class="line">docker ps -a</span><br><span class="line">- 7、启动容器</span><br><span class="line">docker start 容器id</span><br><span class="line">- 8、删除一个容器</span><br><span class="line"> docker rm 容器id</span><br><span class="line">- 9、启动一个做了端口映射的tomcat</span><br><span class="line">[root@localhost ~]# docker run -d -p 8888:8080 tomcat</span><br><span class="line">-d：后台运行</span><br><span class="line">-p: 将主机的端口映射到容器的一个端口    主机端口:容器内部的端口</span><br><span class="line"></span><br><span class="line">- 10、为了演示简单关闭了linux的防火墙</span><br><span class="line">service firewalld status ；查看防火墙状态</span><br><span class="line">service firewalld stop：关闭防火墙</span><br><span class="line">systemctl disable firewalld.service #禁止firewall开机启动</span><br><span class="line">- 11、查看容器的日志</span><br><span class="line">docker logs container-name/container-id</span><br><span class="line"></span><br><span class="line">更多命令参看</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/docker/</span><br><span class="line">可以参考镜像文档</span><br><span class="line"></span><br><span class="line">### 6.使用docker安装mysql</span><br><span class="line"></span><br><span class="line">- docker pull mysql</span><br></pre></td></tr></table></figure><p></p><p>docker pull mysql<br>Using default tag: latest<br>Trying to pull repository docker.io/library/mysql …<br>latest: Pulling from docker.io/library/mysql<br>a5a6f2f73cd8: Pull complete<br>936836019e67: Pull complete<br>283fa4c95fb4: Pull complete<br>1f212fb371f9: Pull complete<br>e2ae0d063e89: Pull complete<br>5ed0ae805b65: Pull complete<br>0283dc49ef4e: Pull complete<br>a7e1170b4fdb: Pull complete<br>88918a9e4742: Pull complete<br>241282fa67c2: Pull complete<br>b0fecf619210: Pull complete<br>bebf9f901dcc: Pull complete<br>Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8<br>Status: Downloaded newer image for docker.io/mysql:latest<br>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 启动</span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br>[root@hadoop000 ~]# docker run –name mysql01 -d mysql<br>756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a<br>[root@hadoop000 ~]# docker ps -a<br>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES<br>756620c8e583 mysql “docker-entrypoint…” 9 seconds ago Exited (1) 7 seconds ago mysql01<br>[root@hadoop000 ~]# docker logs 756620c8e583<br>error: database is uninitialized and password option is not specified<br>You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以看到上面启动的方式是错误的，提示我们要带上具体的密码</span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker run -p 3306:3306 –name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysql<br>eae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc06<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">–name：给新创建的容器命名，此处命名为pwc-mysql</span><br><span class="line">-e：配置信息，此处配置mysql的root用户的登陆密码</span><br><span class="line">-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口</span><br><span class="line">-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...</span><br></pre></td></tr></table></figure><p></p><ul><li><p>查看是否启动成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES</span><br><span class="line">eae86796e132        mysql               &quot;docker-entrypoint...&quot;   8 minutes ago       Up 8 minutes        0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql02</span><br></pre></td></tr></table></figure></li><li><p>登陆MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql04 /bin/bash</span><br><span class="line">root@e34aba02c0c3:/# mysql -uroot -p123456 </span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 80</span><br><span class="line">Server version: 8.0.13 MySQL Community Server - GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></li><li><p>其他的高级操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br><span class="line">把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面</span><br><span class="line">改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）</span><br><span class="line"></span><br><span class="line">docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br><span class="line">指定mysql的一些配置参数</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h3&gt;&lt;p&gt;Docker是一个开源的应用容器引擎；是一个轻量级容器技术；&lt;/p&gt;&lt;p&gt;Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；&lt;/p&gt;&lt;p&gt;运行中的这个镜像称为容器，容器启动是非常快速的。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>spark2.4.2详细介绍</title>
    <link href="http://yoursite.com/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/04/23/spark2.4.2详细介绍/</id>
    <published>2019-04-22T16:00:00.000Z</published>
    <updated>2019-05-13T08:58:29.254Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Spark发布了最新的版本spark-2.4.2<br>根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的</p><h4 id="版本介绍"><a href="#版本介绍" class="headerlink" title="版本介绍"></a>版本介绍</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。<font color="#FF4500"> <strong>我们强烈建议所有2.4用户升级到此稳定版本。</strong></font><br><a id="more"></a></p><h4 id="显著的变化"><a href="#显著的变化" class="headerlink" title="显著的变化"></a>显著的变化</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。</li><li>还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。</li></ul><h4 id="详细更改"><a href="#详细更改" class="headerlink" title="详细更改"></a>详细更改</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>在Spark Driver中发现Java死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>使用选项logConf = true时密码将以conf的明文形式记录</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>用Snappy 1.1.7.1解压、压缩空序列化数据时失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferTo中的潜在损坏</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointData因文件系统已缓存而无法清理</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>在仅使用空值列的AggregateEstimation之后的错误outputRows估计</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>修复包名称不匹配</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>修复updateTableStats以使用新统计信息或无更新表统计信息</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1静默删除DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>松开在ExpressionInfo的’examples’字段中换行断言条件</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>将jquery更新为1.12.x以获取安全修复程序</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>隐藏“org.apache.spark.util.kvstore”的API文档</td></tr></tbody></table><h6 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Spark发布了最新的版本spark-2.4.2&lt;br&gt;根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的&lt;/p&gt;&lt;h4 id=&quot;版本介绍&quot;&gt;&lt;a href=&quot;#版本介绍&quot; class=&quot;headerlink&quot; title=&quot;版本介绍&quot;&gt;&lt;/a&gt;版本介绍&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/spark2.4.2_1.jpg&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。&lt;font color=&quot;#FF4500&quot;&gt; &lt;strong&gt;我们强烈建议所有2.4用户升级到此稳定版本。&lt;/strong&gt;&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我司Kafka+Flink+MySQL生产完整案例代码</title>
    <link href="http://yoursite.com/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2018/12/20/我司Kafka+Flink+MySQL生产完整案例代码/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-05-03T01:24:05.187Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h6 id="1-版本信息："><a href="#1-版本信息：" class="headerlink" title="1.版本信息："></a>1.版本信息：</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flink实战</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="3-工程pom-xml"><a href="#3-工程pom-xml" class="headerlink" title="3.工程pom.xml"></a>3.工程pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConf类 定义与MySQL连接的JDBC的参数<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="5-MySQLSlink类"><a href="#5-MySQLSlink类" class="headerlink" title="5.MySQLSlink类"></a>5.MySQLSlink类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-数据清洗日期工具类"><a href="#6-数据清洗日期工具类" class="headerlink" title="6.数据清洗日期工具类"></a>6.数据清洗日期工具类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //测试一下</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQL建表"><a href="#7-MySQL建表" class="headerlink" title="7.MySQL建表"></a>7.MySQL建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-主程序："><a href="#8-主程序：" class="headerlink" title="8.主程序："></a>8.主程序：</h6><p>主要是将time的格式转成yyyyMMddHHmmss,</p><p>还有取URL中的课程ID，将不是/class开头的过滤掉。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="9-启动主程序，查看MySQL表数据在递增"><a href="#9-启动主程序，查看MySQL表数据在递增" class="headerlink" title="9.启动主程序，查看MySQL表数据在递增"></a>9.启动主程序，查看MySQL表数据在递增</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafka过来的消息是我模拟的，一分钟产生100条。</p><p>以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;h6 id=&quot;1-版本信息：&quot;&gt;&lt;a href=&quot;#1-版本信息：&quot; class=&quot;headerlink&quot; title=&quot;1.版本信息：&quot;&gt;&lt;/a&gt;1.版本信息：&lt;/h6&gt;&lt;p&gt;Flink Version:1.6.2&lt;br&gt;Kafka Version:0.9.0.0&lt;br&gt;MySQL Version:5.6.21&lt;/p&gt;&lt;h6 id=&quot;2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot;&gt;&lt;a href=&quot;#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot; class=&quot;headerlink&quot; title=&quot;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&quot;&gt;&lt;/a&gt;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&lt;/h6&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1.74.103.143    2018-12-20 18:12:00  &amp;quot;GET /class/130.html HTTP/1.1&amp;quot;     404 https://search.yahoo.com/search?p=Flink实战&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)</title>
    <link href="http://yoursite.com/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
    <id>http://yoursite.com/2018/11/10/最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)/</id>
    <published>2018-11-09T16:00:00.000Z</published>
    <updated>2019-05-03T11:57:01.078Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="1-下载Flink安装包"><a href="#1-下载Flink安装包" class="headerlink" title="1.下载Flink安装包"></a>1.下载Flink安装包</h5><p>flink下载地址</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可</p><p>上传至机器的/opt目录下<br><a id="more"></a></p><h5 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-配置master节点"><a href="#3-配置master节点" class="headerlink" title="3.配置master节点"></a>3.配置master节点</h5><p>选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。</p><p>jobmanager.rpc.address: node1</p><p>(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)</p><p>rest.port: 8088</p><p>本次安装 master节点为node1，因为单机，slave节点也为node1</p><h5 id="4-配置slaves"><a href="#4-配置slaves" class="headerlink" title="4.配置slaves"></a>4.配置slaves</h5><p>将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。</p><h5 id="5-启动flink集群"><a href="#5-启动flink集群" class="headerlink" title="5.启动flink集群"></a>5.启动flink集群</h5><p>bin/start-cluster.sh</p><p>打开 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 查看web页面<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managers代表当前的flink只有一个节点，每个task还有两个slots</p><h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6.测试"></a>6.测试</h5><p><strong>依赖</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p></p><h5 id="7-Socket测试代码"><a href="#7-Socket测试代码" class="headerlink" title="7.Socket测试代码"></a>7.Socket测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)</p><p>在命令行set MAVEN_OPTS= -Xms128m -Xmx512m</p><p>继续执行mvn clean install</p><p>生成FlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>找到打成的jar，并upload，开始上传<br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>运行参数介绍<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个</p><p>发送数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>点开running的job，你可以看见接收的字节数等信息</p><p>到log目录下可以清楚的看见输出<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure><p></p><p>除了可以在界面提交，还可以将jar上传的linux中进行提交任务</p><p>运行flink上传的jar<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure><p></p><p>其他步骤一致。</p><h5 id="8-使用kafka作为source"><a href="#8-使用kafka作为source" class="headerlink" title="8.使用kafka作为source"></a>8.使用kafka作为source</h5><p>加上依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //可以通过正则表达式来匹配合适的topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //配置从最新的地方开始消费</span><br><span class="line">        kafkaSource.setStartFromLatest();        //使用addsource，将kafka的输入转变为datastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-使用mysql作为sink"><a href="#9-使用mysql作为sink" class="headerlink" title="9.使用mysql作为sink"></a>9.使用mysql作为sink</h5><p>flink本身并没有提供datastream输出到mysql，需要我们自己去实现</p><p>首先，导入依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。</p><p>使用这个mysqlsink也非常简单<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure><p></p><h5 id="10-总结"><a href="#10-总结" class="headerlink" title="10.总结"></a>10.总结</h5><p>本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h5 id=&quot;1-下载Flink安装包&quot;&gt;&lt;a href=&quot;#1-下载Flink安装包&quot; class=&quot;headerlink&quot; title=&quot;1.下载Flink安装包&quot;&gt;&lt;/a&gt;1.下载Flink安装包&lt;/h5&gt;&lt;p&gt;flink下载地址&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/flink/flink-1.5.0/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://archive.apache.org/dist/flink/flink-1.5.0/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可&lt;/p&gt;&lt;p&gt;上传至机器的/opt目录下&lt;br&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>生产开发必用-Spark RDD转DataFrame的两种方法</title>
    <link href="http://yoursite.com/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/06/14/生产开发必用-Spark RDD转DataFrame的两种方法/</id>
    <published>2018-06-13T16:00:00.000Z</published>
    <updated>2019-05-13T10:20:37.694Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:<br><a href="https://blog.csdn.net/lemonzhaotao/article/details/80211231" target="_blank" rel="noopener">https://blog.csdn.net/lemonzhaotao/article/details/80211231</a></p><p>在本篇文章中，将介绍RDD转换为DataFrame的2种方式</p><p>官网之RDD转DF:<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds</a></p><p>DataFrame 与 RDD 的交互</p><p>Spark SQL它支持两种不同的方式转换已经存在的RDD到DataFrame</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>第一种方式是使用反射的方式，用反射去推倒出来RDD里面的schema。这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。<br>对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常非常多的，是不可能只有20来个字段的。<br>示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 1</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runInferSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  val df = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">              .map(x =&gt; People(x(0), x(1).trim.toInt))  //将rdd的每一行都转换成了一个people</span><br><span class="line">              .toDF         //必须先导入import spark.implicits._  不然这个方法会报错</span><br><span class="line">  df.show()</span><br><span class="line">  df.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">  // 这个DF包含了两个字段name和age</span><br><span class="line">  val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">  // teenager(0)代表第一个字段</span><br><span class="line">  // 取值的第一种方式：index from zero</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line">  // 取值的第二种方式：byName</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()</span><br><span class="line">&#125;</span><br><span class="line">// 注意：case class必须定义在main方法之外；否则会报错</span><br><span class="line">case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p></p><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>创建一个DataFrame，使用编程的方式 这个方式用的非常多。通过编程方式指定schema ，对于第一种方式的schema其实定义在了case class里面了。<br>官网解读：<br>当我们的case class不能提前定义(因为业务处理的过程当中，你的字段可能是在变化的),因此使用case class很难去提前定义。<br>使用该方式创建DF的三大步骤：</p><ul><li>Create an RDD of Rows from the original RDD;</li><li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li><li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 2</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  // 1.转成RDD</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  // 2.定义schema，带有StructType的</span><br><span class="line">  // 定义schema信息</span><br><span class="line">  val schemaString = &quot;name age&quot;</span><br><span class="line">  // 对schema信息按空格进行分割</span><br><span class="line">  // 最终fileds里包含了2个StructField</span><br><span class="line">  val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">                            // 字段类型，字段名称判断是不是为空</span><br><span class="line">                           .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">  val schema = StructType(fields)</span><br><span class="line">  // 3.把我们的schema信息作用到RDD上</span><br><span class="line">  //   这个RDD里面包含了一些行</span><br><span class="line">  // 形成Row类型的RDD</span><br><span class="line">  val rowRDD = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">                  .map(x =&gt; Row(x(0), x(1).trim))</span><br><span class="line">  // 通过SparkSession创建一个DataFrame</span><br><span class="line">  // 传进来一个rowRDD和schema，将schema作用到rowRDD上</span><br><span class="line">  val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="扩展-生产上创建DataFrame的代码举例"><a href="#扩展-生产上创建DataFrame的代码举例" class="headerlink" title="[扩展]生产上创建DataFrame的代码举例"></a>[扩展]生产上创建DataFrame的代码举例</h4><p>在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，这里将展示部分代码：</p><h4 id="Schema的定义"><a href="#Schema的定义" class="headerlink" title="Schema的定义"></a>Schema的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">object AccessConvertUtil &#123;</span><br><span class="line">  val struct = StructType(</span><br><span class="line">    Array(</span><br><span class="line">      StructField(&quot;url&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsType&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsId&quot;,LongType),</span><br><span class="line">      StructField(&quot;traffic&quot;,LongType),</span><br><span class="line">      StructField(&quot;ip&quot;,StringType),</span><br><span class="line">      StructField(&quot;city&quot;,StringType),</span><br><span class="line">      StructField(&quot;time&quot;,StringType),</span><br><span class="line">      StructField(&quot;day&quot;,StringType)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  /**</span><br><span class="line">    * 根据输入的每一行信息转换成输出的样式</span><br><span class="line">    */</span><br><span class="line">  def parseLog(log:String) = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val splits = log.split(&quot;\t&quot;)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      val traffic = splits(2).toLong</span><br><span class="line">      val ip = splits(3)</span><br><span class="line">      val domain = &quot;http://www.imooc.com/&quot;</span><br><span class="line">      val cms = url.substring(url.indexOf(domain) + domain.length)</span><br><span class="line">      val cmsTypeId = cms.split(&quot;/&quot;)</span><br><span class="line">      var cmsType = &quot;&quot;</span><br><span class="line">      var cmsId = 0l</span><br><span class="line">      if (cmsTypeId.length &gt; 1) &#123;</span><br><span class="line">        cmsType = cmsTypeId(0)</span><br><span class="line">        cmsId = cmsTypeId(1).toLong</span><br><span class="line">      &#125;</span><br><span class="line">      val city = IpUtils.getCity(ip)</span><br><span class="line">      val time = splits(0)</span><br><span class="line">      val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;)</span><br><span class="line">      //这个Row里面的字段要和struct中的字段对应上</span><br><span class="line">      Row(url, cmsType, cmsId, traffic, ip, city, time, day)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt; Row(0)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object SparkStatCleanJob &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;)</span><br><span class="line">    //accessRDD.take(10).foreach(println)</span><br><span class="line">    //RDD ==&gt; DF，创建生成DataFrame</span><br><span class="line">    val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)),</span><br><span class="line">      AccessConvertUtil.struct)</span><br><span class="line">    accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite)</span><br><span class="line">            .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark 技术团队开源机器学习平台 MLflow</title>
    <link href="http://yoursite.com/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/"/>
    <id>http://yoursite.com/2018/06/12/Apache Spark 技术团队开源机器学习平台 MLflow/</id>
    <published>2018-06-11T16:00:00.000Z</published>
    <updated>2019-05-13T08:56:44.124Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。<br><a id="more"></a><br><img src="/assets/blogImg/612_1.png" alt="enter description here"><br>Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。<br>MLflow 正是受现有的 ML 平台启发，主打开放性：</p><ul><li>开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。</li><li>开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。<br>MLflow 目前的 alpha 版本包含三个组件：<br><img src="/assets/blogImg/612_2.png" alt="enter description here"><br>其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow</span><br><span class="line"></span><br><span class="line"># Log parameters (key-value pairs)</span><br><span class="line">mlflow.log_param(&quot;num_dimensions&quot;, 8)</span><br><span class="line">mlflow.log_param(&quot;regularization&quot;, 0.1)</span><br><span class="line"></span><br><span class="line"># Log a metric; metrics can be updated throughout the run</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.1)</span><br><span class="line">...</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.45)</span><br><span class="line"></span><br><span class="line"># Log artifacts (output files)</span><br><span class="line">mlflow.log_artifact(&quot;roc.png&quot;)</span><br><span class="line">mlflow.log_artifact(&quot;model.pkl&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name: My Project</span><br><span class="line">conda_env: conda.yaml</span><br><span class="line">entry_points:</span><br><span class="line">  main:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">      regularization: &#123;type: float, default: 0.1&#125;</span><br><span class="line">    command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot;</span><br><span class="line">  validate:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">    command: &quot;python validate.py &#123;data_file&#125;&quot;</span><br></pre></td></tr></table></figure><p></p><p>MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_created: 2018-02-21T13:21:34.12</span><br><span class="line">flavors:</span><br><span class="line">  sklearn:</span><br><span class="line">    sklearn_version: 0.19.1</span><br><span class="line">    pickled_model: model.pkl</span><br><span class="line">  python_function:</span><br><span class="line">    loader_module: mlflow.sklearn</span><br><span class="line">    pickled_model: model.pkl</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。&lt;br&gt;
    
    </summary>
    
      <category term="Spark MLlib" scheme="http://yoursite.com/categories/Spark-MLlib/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming 状态管理函数的选择比较</title>
    <link href="http://yoursite.com/2018/06/06/SparkStreaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%AF%94%E8%BE%83/"/>
    <id>http://yoursite.com/2018/06/06/SparkStreaming 状态管理函数的选择比较/</id>
    <published>2018-06-05T16:00:00.000Z</published>
    <updated>2019-05-13T08:56:43.297Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</span><br></pre></td></tr></table></figure><p></p><p>也即是说它会统计全局的key的状态，就算没有数据输入，它也会在每一个批次的时候返回之前的key的状态。</p><p>缺点：若数据量太大的话，需要checkpoint的数据会占用较大的存储，效率低下。</p><p>程序示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：要使用updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start() </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;   </span><br><span class="line"> /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">def updateFunction(currentValues: Seq[Int], preValues: Option[Int]):                                Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="二、mapWithState"><a href="#二、mapWithState" class="headerlink" title="二、mapWithState"></a>二、mapWithState</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。效率更高，生产中建议使用</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount </span><br><span class="line">      &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;StatefulNetworkWordCount&quot;)</span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)   </span><br><span class="line">    val initialRDD = ssc.sparkContext</span><br><span class="line">      .parallelize(List((&quot;hello&quot;, 1),(&quot;world&quot;, 1)))  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], </span><br><span class="line">     state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">    StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="三、源码分析"><a href="#三、源码分析" class="headerlink" title="三、源码分析"></a>三、源码分析</h4><h5 id="upateStateByKey："><a href="#upateStateByKey：" class="headerlink" title="upateStateByKey："></a>upateStateByKey：</h5><ul><li>map返回的是MappedDStream，而MappedDStream并没有updateStateByKey方法，并且它的父类DStream中也没有该方法。但是DStream的伴生对象中有一个隐式转换函数：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">object DStream &#123;</span><br><span class="line">  implicit def toPairDStreamFunctions[K, V](stream: DStream[(K, V)])</span><br><span class="line">      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null):</span><br><span class="line">    PairDStreamFunctions[K, V] = &#123;</span><br><span class="line">    new PairDStreamFunctions[K, V](stream)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>跟进去 PairDStreamFunctions ，发现最终调用的是自己的updateStateByKey。<br>其中updateFunc就要传入的参数，他是一个函数，Seq[V]表示当前key对应的所有值，<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Option[S] 是当前key的历史状态，返回的是新的状态。</span><br><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Seq[V], Option[S]) =&gt; Option[S]</span><br><span class="line">  ): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>最终调用：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],</span><br><span class="line">    partitioner: Partitioner,</span><br><span class="line">    rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  val cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">  val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123;</span><br><span class="line">    cleanedFunc(it)</span><br><span class="line">  &#125;</span><br><span class="line">  new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>再跟进去 new StateDStream:<br>在这里面new出了一个StateDStream对象。在其compute方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中StateDStream的父类计算出的RDD（本次batch的单词计数）分别是prevStateRDD和parentRDD，然后在调用 computeUsingPreviousRDD 方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private [this] def computeUsingPreviousRDD(</span><br><span class="line">    batchTime: Time,</span><br><span class="line">    parentRDD: RDD[(K, V)],</span><br><span class="line">    prevStateRDD: RDD[(K, S)]) = &#123;</span><br><span class="line">  // Define the function for the mapPartition operation on cogrouped RDD;</span><br><span class="line">  // first map the cogrouped tuple to tuples of required type,</span><br><span class="line">  // and then apply the update function</span><br><span class="line">  val updateFuncLocal = updateFunc</span><br><span class="line">  val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) =&gt; &#123;</span><br><span class="line">    val i = iterator.map &#123; t =&gt;</span><br><span class="line">      val itr = t._2._2.iterator</span><br><span class="line">      val headOption = if (itr.hasNext) Some(itr.next()) else None</span><br><span class="line">      (t._1, t._2._1.toSeq, headOption)</span><br><span class="line">    &#125;</span><br><span class="line">    updateFuncLocal(batchTime, i)</span><br><span class="line">  &#125;</span><br><span class="line">  val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</span><br><span class="line">  val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</span><br><span class="line">  Some(stateRDD)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在这里两个RDD进行cogroup然后应用updateStateByKey传入的函数。我们知道cogroup的性能是比较低下，参考<a href="http://lxw1234.com/archives/2015/07/384.htm。" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/07/384.htm。</a></p><h5 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState:"></a>mapWithState:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Experimental</span><br><span class="line">def mapWithState[StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    spec: StateSpec[K, V, StateType, MappedType]</span><br><span class="line">  ): MapWithStateDStream[K, V, StateType, MappedType] = &#123;</span><br><span class="line">  new MapWithStateDStreamImpl[K, V, StateType, MappedType](</span><br><span class="line">    self,</span><br><span class="line">    spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>说明：StateSpec 封装了状态管理函数，并在该方法中创建了MapWithStateDStreamImpl对象。</p><p>MapWithStateDStreamImpl 中创建了一个InternalMapWithStateDStream类型对象internalStream，在MapWithStateDStreamImpl的compute方法中调用了internalStream的getOrCompute方法。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private[streaming] class MapWithStateDStreamImpl[</span><br><span class="line">    KeyType: ClassTag, ValueType: ClassTag, StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    dataStream: DStream[(KeyType, ValueType)],</span><br><span class="line">    spec: StateSpecImpl[KeyType, ValueType, StateType, MappedType])</span><br><span class="line">  extends MapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream.context) &#123;</span><br><span class="line"></span><br><span class="line">  private val internalStream =</span><br><span class="line">    new InternalMapWithStateDStream[KeyType, ValueType, StateType, MappedType](dataStream, spec)</span><br><span class="line"></span><br><span class="line">  override def slideDuration: Duration = internalStream.slideDuration</span><br><span class="line"></span><br><span class="line">  override def dependencies: List[DStream[_]] = List(internalStream)</span><br><span class="line"></span><br><span class="line">  override def compute(validTime: Time): Option[RDD[MappedType]] = &#123;</span><br><span class="line">    internalStream.getOrCompute(validTime).map &#123; _.flatMap[MappedType] &#123; _.mappedData &#125; &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>InternalMapWithStateDStream中没有getOrCompute方法，这里调用的是其父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream的Compute方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">/** Method that generates an RDD for the given time */</span><br><span class="line">override def compute(validTime: Time): Option[RDD[MapWithStateRDDRecord[K, S, E]]] = &#123;</span><br><span class="line">  // Get the previous state or create a new empty state RDD</span><br><span class="line">  val prevStateRDD = getOrCompute(validTime - slideDuration) match &#123;</span><br><span class="line">    case Some(rdd) =&gt;</span><br><span class="line">      if (rdd.partitioner != Some(partitioner)) &#123;</span><br><span class="line">        // If the RDD is not partitioned the right way, let us repartition it using the</span><br><span class="line">        // partition index as the key. This is to ensure that state RDD is always partitioned</span><br><span class="line">        // before creating another state RDD using it</span><br><span class="line">        MapWithStateRDD.createFromRDD[K, V, S, E](</span><br><span class="line">          rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rdd</span><br><span class="line">      &#125;</span><br><span class="line">    case None =&gt;</span><br><span class="line">      MapWithStateRDD.createFromPairRDD[K, V, S, E](</span><br><span class="line">        spec.getInitialStateRDD().getOrElse(new EmptyRDD[(K, S)](ssc.sparkContext)),</span><br><span class="line">        partitioner,</span><br><span class="line">        validTime</span><br><span class="line">      )</span><br><span class="line">  &#125;</span><br><span class="line">  // Compute the new state RDD with previous state RDD and partitioned data RDD</span><br><span class="line">  // Even if there is no data RDD, use an empty one to create a new state RDD</span><br><span class="line">  val dataRDD = parent.getOrCompute(validTime).getOrElse &#123;</span><br><span class="line">    context.sparkContext.emptyRDD[(K, V)]</span><br><span class="line">  &#125;</span><br><span class="line">  val partitionedDataRDD = dataRDD.partitionBy(partitioner)</span><br><span class="line">  val timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt;</span><br><span class="line">    (validTime - interval).milliseconds</span><br><span class="line">  &#125;</span><br><span class="line">  Some(new MapWithStateRDD(</span><br><span class="line">    prevStateRDD, partitionedDataRDD, mappingFunction, </span><br><span class="line">    validTime, timeoutThresholdTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>根据给定的时间生成一个MapWithStateRDD，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD:dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取partitionedDataRDD。最后将preStateRDD，partitionedDataRDD和用户定义的函数mappingFunction传给新生成的MapWithStateRDD对象返回。</p><p>后续若有兴趣可以继续跟进MapWithStateRDD的compute方法，限于篇幅不再展示。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL 之外部数据源如何成为在企业开发中的一把利器</title>
    <link href="http://yoursite.com/2018/06/06/Spark%20SQL%20%E4%B9%8B%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%9C%A8%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%8A%8A%E5%88%A9%E5%99%A8/"/>
    <id>http://yoursite.com/2018/06/06/Spark SQL 之外部数据源如何成为在企业开发中的一把利器/</id>
    <published>2018-06-05T16:00:00.000Z</published>
    <updated>2019-05-13T08:59:22.158Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h4><p>1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。<br>2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。<br>3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。<br>4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。<br><a id="more"></a></p><h4 id="2-读取json文件"><a href="#2-读取json文件" class="headerlink" title="2.读取json文件"></a>2.读取json文件</h4><p>启动shell进行测试<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//标准写法</span><br><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;path&quot;)</span><br><span class="line">//另外一种写法</span><br><span class="line">spark.read.json(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">看看源码这两者之间到底有啥不同呢？</span><br><span class="line">/**</span><br><span class="line">   * Loads a JSON file and returns the results as a `DataFrame`.</span><br><span class="line">   *</span><br><span class="line">   * See the documentation on the overloaded `json()` method with varargs for more details.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def json(path: String): DataFrame = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    json(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line">我们调用josn() 方法其实进行了 overloaded ，我们继续查看</span><br><span class="line"> def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*)</span><br><span class="line"> 这句话是不是很熟悉，其实就是我们的标准写法</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> scala&gt; val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"> df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h4 id="3-读取parquet数据"><a href="#3-读取parquet数据" class="headerlink" title="3 读取parquet数据"></a>3 读取parquet数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;parquet&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h4 id="4-读取hive中的数据"><a href="#4-读取hive中的数据" class="headerlink" title="4 读取hive中的数据"></a>4 读取hive中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line">| default|states_raw|      false|</span><br><span class="line">| default|states_seq|      false|</span><br><span class="line">| default|        t1|      false|</span><br><span class="line">+--------+----------+-----------+</span><br><span class="line"></span><br><span class="line">spark.table(&quot;states_raw&quot;).show</span><br><span class="line">+-----+------+</span><br><span class="line">| code|  name|</span><br><span class="line">+-----+------+</span><br><span class="line">|hello|  java|</span><br><span class="line">|hello|hadoop|</span><br><span class="line">|hello|  hive|</span><br><span class="line">|hello| sqoop|</span><br><span class="line">|hello|  hdfs|</span><br><span class="line">|hello| spark|</span><br><span class="line">+-----+------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select name from states_raw &quot;).show</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|  java|</span><br><span class="line">|hadoop|</span><br><span class="line">|  hive|</span><br><span class="line">| sqoop|</span><br><span class="line">|  hdfs|</span><br><span class="line">| spark|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure><h4 id="5-保存数据"><a href="#5-保存数据" class="headerlink" title="5 保存数据"></a>5 保存数据</h4><p>注意：</p><ol><li>保存的文件夹不能存在，否则报错(默认情况下，可以选择不同的模式)：org.apache.spark.sql.AnalysisException: path file:/home/hadoop/data already exists.;</li><li>保存成文本格式，只能保存一列，否则报错：org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;file:///opt/software/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line">//保存</span><br><span class="line">df.select(&quot;name&quot;).write.format(&quot;text&quot;).save(&quot;file:///home/hadoop/data/out&quot;)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[hadoop@hadoop out]$ pwd</span><br><span class="line">/home/hadoop/data/out</span><br><span class="line">[hadoop@hadoop out]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 20 Apr 24 00:34 part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:34 _SUCCESS</span><br><span class="line">[hadoop@hadoop out]$ cat part-00000-ed7705d2-3fdd-4f08-a743-5bc355471076-c000.txt </span><br><span class="line">Michael</span><br><span class="line">Andy</span><br><span class="line">Justin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//保存为json格式</span><br><span class="line">df.write.format(&quot;json&quot;).save(&quot;file:///home/hadoop/data/out1&quot;)</span><br><span class="line"></span><br><span class="line">结果</span><br><span class="line">[hadoop@hadoop data]$ cd out1</span><br><span class="line">[hadoop@hadoop out1]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 71 Apr 24 00:35 part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  0 Apr 24 00:35 _SUCCESS</span><br><span class="line">[hadoop@hadoop out1]$ cat part-00000-948b5b30-f104-4aa4-9ded-ddd70f1f5346-c000.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:30,&quot;name&quot;:&quot;Andy&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:19,&quot;name&quot;:&quot;Justin&quot;&#125;</span><br></pre></td></tr></table></figure></li></ol><p>上面说了在保存数据时如果目录已经存在，在默认模式下会报错，那我们下面讲解保存的几种模式：<br><img src="/assets/blogImg/606_1.png" alt="enter description here"></p><h4 id="6-读取mysql中的数据"><a href="#6-读取mysql中的数据" class="headerlink" title="6 读取mysql中的数据"></a>6 读取mysql中的数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">val jdbcDF = spark.read</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;basic01.tbls&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;123456&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- TBL_ID: long (nullable = false)</span><br><span class="line"> |-- CREATE_TIME: integer (nullable = false)</span><br><span class="line"> |-- DB_ID: long (nullable = true)</span><br><span class="line"> |-- LAST_ACCESS_TIME: integer (nullable = false)</span><br><span class="line"> |-- OWNER: string (nullable = true)</span><br><span class="line"> |-- RETENTION: integer (nullable = false)</span><br><span class="line"> |-- SD_ID: long (nullable = true)</span><br><span class="line"> |-- TBL_NAME: string (nullable = true)</span><br><span class="line"> |-- TBL_TYPE: string (nullable = true)</span><br><span class="line"> |-- VIEW_EXPANDED_TEXT: string (nullable = true)</span><br><span class="line"> |-- VIEW_ORIGINAL_TEXT: string (nullable = true)</span><br><span class="line"></span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="7-spark-SQL操作mysql表数据"><a href="#7-spark-SQL操作mysql表数据" class="headerlink" title="7 spark SQL操作mysql表数据"></a>7 spark SQL操作mysql表数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW jdbcTable</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost:3306&quot;,</span><br><span class="line">  dbtable &quot;basic01.tbls&quot;,</span><br><span class="line">  user &apos;root&apos;,</span><br><span class="line">  password &apos;123456&apos;,</span><br><span class="line">  driver &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">查看：</span><br><span class="line">show tables;</span><br><span class="line">default states_raw      false</span><br><span class="line">default states_seq      false</span><br><span class="line">default t1      false</span><br><span class="line">jdbctable       true</span><br><span class="line"></span><br><span class="line">select * from jdbctable;</span><br><span class="line">1       1519944170      6       0       hadoop  0       1       page_views      MANAGED_TABLE   NULL    NULL</span><br><span class="line">2       1519944313      6       0       hadoop  0       2       page_views_bzip2        MANAGED_TABLE   NULL    NULL</span><br><span class="line">3       1519944819      6       0       hadoop  0       3       page_views_snappy       MANAGED_TABLE   NULL    NULL</span><br><span class="line">21      1520067771      6       0       hadoop  0       21      tt      MANAGED_TABLE   NULL    NULL</span><br><span class="line">22      1520069148      6       0       hadoop  0       22      page_views_seq  MANAGED_TABLE   NULL    NULL</span><br><span class="line">23      1520071381      6       0       hadoop  0       23      page_views_rcfile       MANAGED_TABLE   NULL    NULL</span><br><span class="line">24      1520074675      6       0       hadoop  0       24      page_views_orc_zlib     MANAGED_TABLE   NULL    NULL</span><br><span class="line">27      1520078184      6       0       hadoop  0       27      page_views_lzo_index    MANAGED_TABLE   NULL    NULL</span><br><span class="line">30      1520083461      6       0       hadoop  0       30      page_views_lzo_index1   MANAGED_TABLE   NULL    NULL</span><br><span class="line">31      1524370014      1       0       hadoop  0       31      t1      EXTERNAL_TABLE  NULL    NULL</span><br><span class="line">37      1524468636      1       0       hadoop  0       37      states_raw      MANAGED_TABLE   NULL    NULL</span><br><span class="line">38      1524468678      1       0       hadoop  0       38      states_seq      MANAGED_TABLE   NULL    NULL</span><br><span class="line"></span><br><span class="line">mysql中的tbls的数据已经存在jdbctable表中了。</span><br><span class="line">jdbcDF.show</span><br></pre></td></tr></table></figure><h4 id="8-分区推测（Partition-Discovery）"><a href="#8-分区推测（Partition-Discovery）" class="headerlink" title="8 分区推测（Partition Discovery）"></a>8 分区推测（Partition Discovery）</h4><p>表分区是在像Hive这样的系统中使用的常见优化方法。 在分区表中，数据通常存储在不同的目录中，分区列值在每个分区目录的路径中编码。 所有内置的文件源（包括Text / CSV / JSON / ORC / Parquet）都能够自动发现和推断分区信息。 例如，我们创建如下的目录结构;<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse/gender=male/country=CN</span><br><span class="line"></span><br><span class="line">添加json文件：</span><br><span class="line">people.json </span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line"></span><br><span class="line"> hdfs dfs -put people.json /user/hive/warehouse/gender=male/country=CN</span><br></pre></td></tr></table></figure><p></p><p>我们使用spark sql读取外部数据源：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/country=CN/people.json&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p></p><p>我们改变读取的目录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.format(&quot;json&quot;).load(&quot;/user/hive/warehouse/gender=male/&quot;)</span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- country: string (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+-------+</span><br><span class="line">| age|   name|country|</span><br><span class="line">+----+-------+-------+</span><br><span class="line">|null|Michael|     CN|</span><br><span class="line">|  30|   Andy|     CN|</span><br><span class="line">|  19| Justin|     CN|</span><br><span class="line">+----+-------+-------+</span><br></pre></td></tr></table></figure><p></p><p>大家有没有发现什么呢？Spark SQL将自动从路径中提取分区信息。<br>注意，分区列的数据类型是自动推断的。目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以通过</p><p><font color="#FF4500">spark.sql.sources.partitionColumnTypeInference.enabled</font>进行配置，默认为true。当禁用类型推断时，字符串类型将用于分区列。<br>从Spark 1.6.0开始，默认情况下，分区发现仅在给定路径下找到分区。对于上面的示例，如果用户将路径/table/gender=male传递给</p><p><font color="#FF4500">SparkSession.read.parquet或SparkSession.read.load</font>，则不会将性别视为分区列。如果用户需要指定启动分区发现的基本路径，则可以basePath在数据源选项中进行设置。例如，当path/to/table/gender=male是数据路径并且用户将basePath设置为path/to/table/时，性别将是分区列。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1 概述&quot;&gt;&lt;/a&gt;1 概述&lt;/h4&gt;&lt;p&gt;1.Spark1.2中，Spark SQL开始正式支持外部数据源。Spark SQL开放了一系列接入外部数据源的接口，来让开发者可以实现。使得Spark SQL可以加载任何地方的数据，例如mysql，hive，hdfs，hbase等，而且支持很多种格式如json, parquet, avro, csv格式。我们可以开发出任意的外部数据源来连接到Spark SQL，然后我们就可以通过外部数据源API来进行操作。&lt;br&gt;2.我们通过外部数据源API读取各种格式的数据，会得到一个DataFrame，这是我们熟悉的方式啊，就可以使用DataFrame的API或者SQL的API进行操作哈。&lt;br&gt;3.外部数据源的API可以自动做一些列的裁剪，什么叫列的裁剪，假如一个user表有id,name,age,gender4个列，在做select的时候你只需要id,name这两列，那么其他列会通过底层的优化去给我们裁剪掉。&lt;br&gt;4.保存操作可以选择使用SaveMode，指定如何保存现有数据（如果存在）。&lt;br&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统重要参数调优，你知道吗</title>
    <link href="http://yoursite.com/2018/06/04/Linux%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97/"/>
    <id>http://yoursite.com/2018/06/04/Linux系统重要参数调优，你知道吗/</id>
    <published>2018-06-03T16:00:00.000Z</published>
    <updated>2019-05-13T08:59:29.283Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="当前会话生效"><a href="#当前会话生效" class="headerlink" title="当前会话生效"></a>当前会话生效</h4><p>ulimit -u -&gt; 查看当前最大进程数<br>ulimit -n -&gt;查看当前最大文件数<br>ulimit -u xxx -&gt; 修改当前最大进程数为xxx<br>ulimit -n xxx -&gt; 修改当前最大文件数为xxx</p><h4 id="永久生效"><a href="#永久生效" class="headerlink" title="永久生效"></a>永久生效</h4><p>1.vi /etc/security/limits.conf，添加如下的行</p><ul><li>soft noproc 11000</li><li>hard noproc 11000</li><li>soft nofile 4100</li><li>hard nofile 4100<a id="more"></a> 说明：</li><li>代表针对所有用户<br>noproc 是代表最大进程数<br>nofile 是代表最大文件打开数</li></ul><h4 id="2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制："><a href="#2-让-SSH-接受-Login-程式的登入，方便在-ssh-客户端查看-ulimit-a-资源限制：" class="headerlink" title="2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制："></a>2.让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制：</h4><ul><li>1)、vi /etc/ssh/sshd_config<br>把 UserLogin 的值改为 yes，并把 # 注释去掉</li><li>2)、重启 sshd 服务<br>/etc/init.d/sshd restart</li><li>3)、修改所有 linux 用户的环境变量文件：<br>vi /etc/profile<br>ulimit -u 10000<br>ulimit -n 4096<br>ulimit -d unlimited<br>ulimit -m unlimited<br>ulimit -s unlimited<br>ulimit -t unlimited<br>ulimit -v unlimited</li><li>4)、生效<br>source /etc/profile</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;当前会话生效&quot;&gt;&lt;a href=&quot;#当前会话生效&quot; class=&quot;headerlink&quot; title=&quot;当前会话生效&quot;&gt;&lt;/a&gt;当前会话生效&lt;/h4&gt;&lt;p&gt;ulimit -u -&amp;gt; 查看当前最大进程数&lt;br&gt;ulimit -n -&amp;gt;查看当前最大文件数&lt;br&gt;ulimit -u xxx -&amp;gt; 修改当前最大进程数为xxx&lt;br&gt;ulimit -n xxx -&amp;gt; 修改当前最大文件数为xxx&lt;/p&gt;&lt;h4 id=&quot;永久生效&quot;&gt;&lt;a href=&quot;#永久生效&quot; class=&quot;headerlink&quot; title=&quot;永久生效&quot;&gt;&lt;/a&gt;永久生效&lt;/h4&gt;&lt;p&gt;1.vi /etc/security/limits.conf，添加如下的行&lt;/p&gt;&lt;ul&gt;&lt;li&gt;soft noproc 11000&lt;/li&gt;&lt;li&gt;hard noproc 11000&lt;/li&gt;&lt;li&gt;soft nofile 4100&lt;/li&gt;&lt;li&gt;hard nofile 4100
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Spark动态内存管理源码解析！</title>
    <link href="http://yoursite.com/2018/06/03/Spark%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%81/"/>
    <id>http://yoursite.com/2018/06/03/Spark动态内存管理源码解析！/</id>
    <published>2018-06-02T16:00:00.000Z</published>
    <updated>2019-05-13T08:57:27.129Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="一、Spark内存管理模式"><a href="#一、Spark内存管理模式" class="headerlink" title="一、Spark内存管理模式"></a>一、Spark内存管理模式</h4><p>Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// SparkEnv.scala</span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="二、Spark动态内存管理空间分配"><a href="#二、Spark动态内存管理空间分配" class="headerlink" title="二、Spark动态内存管理空间分配"></a>二、Spark动态内存管理空间分配</h4><p><img src="/assets/blogImg/603_1.png" alt="enter description here"><br>相比于Static MemoryManager模式，Unified MemoryManager模型打破了存储内存和运行内存的界限，使每一个内存区能够动态伸缩，降低OOM的概率。由上图可知，executor JVM内存主要由以下几个区域组成：</p><ul><li>（1）Reserved Memory（预留内存）：这部分内存预留给系统使用，默认为300MB，可通过spark.testing.reservedMemory进行设置。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024</span><br></pre></td></tr></table></figure></li></ul><p>另外，JVM内存的最小值也与reserved Memory有关，即minSystemMemory = reserved Memory<em>1.5，即默认情况下JVM内存最小值为300MB</em>1.5=450MB。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong</span><br></pre></td></tr></table></figure><p></p><ul><li>（2）Spark Memeoy:分为execution Memory和storage Memory。去除掉reserved Memory，剩下usableMemory的一部分用于execution和storage这两类堆内存，默认是0.6，可通过spark.memory.fraction进行设置。例如：JVM内存是1G，那么用于execution和storage的默认内存为（1024-300）*0.6=434MB。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">    val usableMemory = systemMemory - reservedMemory</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    (usableMemory * memoryFraction).toLong</span><br></pre></td></tr></table></figure></li></ul><p>他们的边界由spark.memory.storageFraction设定，默认为0.5。即默认状态下storage Memory和execution Memory为1：1.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManager.scala</span><br><span class="line">     onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</span><br><span class="line">      numCores = numCores)</span><br></pre></td></tr></table></figure><p></p><ul><li>（3）user Memory:剩余内存，用户根据需要使用，默认占usableMemory的（1-0.6）=0.4.</li></ul><h5 id="三、内存控制详解"><a href="#三、内存控制详解" class="headerlink" title="三、内存控制详解"></a>三、内存控制详解</h5><p>首先我们先来了解一下Spark内存管理实现类之前的关系。<br><img src="/assets/blogImg/603_2.png" alt="enter description here"></p><h5 id="1-MemoryManager主要功能是："><a href="#1-MemoryManager主要功能是：" class="headerlink" title="1.MemoryManager主要功能是："></a>1.MemoryManager主要功能是：</h5><ul><li>（1）记录用了多少StorageMemory和ExecutionMemory；</li><li>（2）申请Storage、Execution和Unroll Memory；</li><li>（3）释放Stroage和Execution Memory。</li></ul><p>Execution内存用来执行shuffle、joins、sorts和aggegations操作，Storage内存用于缓存和广播数据，每一个JVM中都存在着一个MemoryManager。构造MemoryManager需要指定onHeapStorageMemory和onHeapExecutionMemory参数。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> // MemoryManager.scala</span><br><span class="line">private[spark] abstract class MemoryManager(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    numCores: Int,</span><br><span class="line">    onHeapStorageMemory: Long,</span><br><span class="line">    onHeapExecutionMemory: Long) extends Logging &#123;</span><br></pre></td></tr></table></figure><p></p><p>创建StorageMemoryPool和ExecutionMemoryPool对象，用来创建堆内或堆外的Storage和Execution内存池，管理Storage和Execution的内存分配。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP)</span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)</span><br></pre></td></tr></table></figure><p></p><p>默认情况下，不使用堆外内存，可通过saprk.memory.offHeap.enabled设置，默认堆外内存为0，可使用spark.memory.offHeap.size参数设置。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// All the code you will ever need</span><br><span class="line"> final val tungstenMemoryMode: MemoryMode = &#123;</span><br><span class="line">    if (conf.getBoolean(&quot;spark.memory.offHeap.enabled&quot;, false)) &#123;</span><br><span class="line">      require(conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0) &gt; 0,</span><br><span class="line">        &quot;spark.memory.offHeap.size must be &gt; 0 when spark.memory.offHeap.enabled == true&quot;)</span><br><span class="line">      require(Platform.unaligned(),</span><br><span class="line">        &quot;No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.&quot;)</span><br><span class="line">      MemoryMode.OFF_HEAP</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MemoryMode.ON_HEAP</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> protected[this] val maxOffHeapMemory = conf.getSizeAsBytes(&quot;spark.memory.offHeap.size&quot;, 0)</span><br></pre></td></tr></table></figure><p>释放numBytes字节的Execution内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放指定task的所有Execution内存并将该task标记为inactive。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line"> private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized &#123;</span><br><span class="line">    onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) +</span><br><span class="line">      offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放numBytes字节的Stoarge内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized &#123;</span><br><span class="line">    memoryMode match &#123;</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; onHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; offHeapStorageMemoryPool.releaseMemory(numBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>释放所有Storage内存方法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// MemoryManager.scala</span><br><span class="line">final def releaseAllStorageMemory(): Unit = synchronized &#123;</span><br><span class="line">    onHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">    offHeapStorageMemoryPool.releaseAllMemory()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"><a href="#2-接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？" class="headerlink" title="2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？"></a>2.接下来我们了解一下，UnifiedMemoryManager是如何对内存进行控制的？动态内存是如何实现的呢？</h5><p>UnifiedMemoryManage继承了MemoryManager<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">private[spark] class UnifiedMemoryManager private[memory] (</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    val maxHeapMemory: Long,</span><br><span class="line">    onHeapStorageRegionSize: Long,</span><br><span class="line">    numCores: Int)</span><br><span class="line">  extends MemoryManager(</span><br><span class="line">    conf,</span><br><span class="line">    numCores,</span><br><span class="line">    onHeapStorageRegionSize,</span><br><span class="line">    maxHeapMemory - onHeapStorageRegionSize) &#123;</span><br></pre></td></tr></table></figure><p></p><p>重写了maxOnHeapStorageMemory方法，最大Storage内存=最大内存-最大Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line"> override def maxOnHeapStorageMemory: Long = synchronized &#123;</span><br><span class="line">    maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireStorageMemory：申请Storage内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override def acquireStorageMemory(</span><br><span class="line">      blockId: BlockId,</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      memoryMode: MemoryMode): Boolean = synchronized &#123;</span><br><span class="line">    assertInvariants()</span><br><span class="line">    assert(numBytes &gt;= 0)</span><br><span class="line">    val (executionPool, storagePool, maxMemory) = memoryMode match &#123;</span><br><span class="line">      //根据不同的内存模式去创建StorageMemoryPool和ExecutionMemoryPool</span><br><span class="line">      case MemoryMode.ON_HEAP =&gt; (</span><br><span class="line">        onHeapExecutionMemoryPool,</span><br><span class="line">        onHeapStorageMemoryPool,</span><br><span class="line">        maxOnHeapStorageMemory)</span><br><span class="line">      case MemoryMode.OFF_HEAP =&gt; (</span><br><span class="line">        offHeapExecutionMemoryPool,</span><br><span class="line">        offHeapStorageMemoryPool,</span><br><span class="line">        maxOffHeapMemory)</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; maxMemory) &#123;</span><br><span class="line">      // 若申请内存大于最大内存，则申请失败</span><br><span class="line">      logInfo(s&quot;Will not store $blockId as the required space ($numBytes bytes) exceeds our &quot; +</span><br><span class="line">        s&quot;memory limit ($maxMemory bytes)&quot;)</span><br><span class="line">      return false</span><br><span class="line">    &#125;</span><br><span class="line">    if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">      // 如果Storage内存池没有足够的内存，则向Execution内存池借用</span><br><span class="line">      val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree, numBytes)//当Execution内存有空闲时，Storage才能借到内存</span><br><span class="line">      executionPool.decrementPoolSize(memoryBorrowedFromExecution)//缩小Execution内存</span><br><span class="line">      storagePool.incrementPoolSize(memoryBorrowedFromExecution)//增加Storage内存</span><br><span class="line">    &#125;</span><br><span class="line">    storagePool.acquireMemory(blockId, numBytes)</span><br></pre></td></tr></table></figure><p></p><p>核心方法acquireExecutionMemory：申请Execution内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">override private[memory] def acquireExecutionMemory(</span><br><span class="line">      numBytes: Long,</span><br><span class="line">      taskAttemptId: Long,</span><br><span class="line">      memoryMode: MemoryMode): Long = synchronized &#123;//使用了synchronized关键字，调用acquireExecutionMemory方法可能会阻塞，直到Execution内存池有足够的内存。</span><br><span class="line">   ...</span><br><span class="line">    executionPool.acquireMemory(</span><br><span class="line">      numBytes, taskAttemptId, maybeGrowExecutionPool, computeMaxExecutionPoolSize)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><p>方法最后调用了ExecutionMemoryPool的acquireMemory方法，该方法的参数需要两个函数：maybeGrowExecutionPool()和computeMaxExecutionPoolSize()。<br>每个Task能够使用的内存被限制在pooSize / (2 * numActiveTask) ~ maxPoolSize / numActiveTasks。其中maxPoolSize代表了execution pool的最大内存，poolSize表示当前这个pool的大小。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// ExecutionMemoryPool.scala</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      val maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">      val minMemoryPerTask = poolSize / (2 * numActiveTasks)</span><br></pre></td></tr></table></figure><p></p><p>maybeGrowExecutionPool()方法实现了如何动态增加Execution内存区的大小。<br>在每次申请execution内存的同时，execution内存池会进行多次尝试，每次尝试都可能会回收一些存储内存。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// UnifiedMemoryManage.scala</span><br><span class="line">     def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">      if (extraMemoryNeeded &gt; 0) &#123;//如果申请的内存大于0</span><br><span class="line">        //计算execution可借到的storage内存，是storage剩余内存和可借出内存的最大值</span><br><span class="line">        val memoryReclaimableFromStorage = math.max(</span><br><span class="line">          storagePool.memoryFree,</span><br><span class="line">          storagePool.poolSize - storageRegionSize)</span><br><span class="line">        if (memoryReclaimableFromStorage &gt; 0) &#123;//如果可以申请到内存</span><br><span class="line">          val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">            math.min(extraMemoryNeeded, memoryReclaimableFromStorage))//实际需要的内存，取实际需要的内存和storage内存区域全部可用内存大小的最小值</span><br><span class="line">          storagePool.decrementPoolSize(spaceToReclaim)//storage内存区域减少</span><br><span class="line">          executionPool.incrementPoolSize(spaceToReclaim)//execution内存区域增加</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;一、Spark内存管理模式&quot;&gt;&lt;a href=&quot;#一、Spark内存管理模式&quot; class=&quot;headerlink&quot; title=&quot;一、Spark内存管理模式&quot;&gt;&lt;/a&gt;一、Spark内存管理模式&lt;/h4&gt;&lt;p&gt;Spark有两种内存管理模式，静态内存管理(Static MemoryManager)和动态（统一）内存管理（Unified MemoryManager）。动态内存管理从Spark1.6开始引入，在SparkEnv.scala中的源码可以看到，Spark目前默认采用动态内存管理模式，若将spark.memory.useLegacyMode设置为true，则会改为采用静态内存管理。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;// SparkEnv.scala&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    val useLegacyMemoryManager = conf.getBoolean(&amp;quot;spark.memory.useLegacyMode&amp;quot;, false)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    val memoryManager: MemoryManager =&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      if (useLegacyMemoryManager) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        new StaticMemoryManager(conf, numUsableCores)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125; else &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        UnifiedMemoryManager(conf, numUsableCores)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="源码阅读" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>若泽大数据-零基础学员深圳某司高薪面试题</title>
    <link href="http://yoursite.com/2018/05/31/%E8%8B%A5%E6%B3%BD%E5%A4%A7%E6%95%B0%E6%8D%AE-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E5%91%98%E6%B7%B1%E5%9C%B3%E6%9F%90%E5%8F%B8%E9%AB%98%E8%96%AA%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <id>http://yoursite.com/2018/05/31/若泽大数据-零基础学员深圳某司高薪面试题/</id>
    <published>2018-05-30T16:00:00.000Z</published>
    <updated>2019-05-13T08:00:04.842Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>啥也不说！直接上题</p><p>面试时间：20180531</p><ul><li>简单说下hdfs读文件和写文件的流程</li><li>每天数据量有多大？生产集群规模有多大？</li><li>说几个spark开发中遇到的问题，和解决的方案</li><li>阐述一下最近开发的项目，以及担任的角色位置</li><li>kafka有做过哪些调优</li><li>我们项目中数据倾斜的场景和解决方案</li></ul><p>零基础➕四个月紧跟若泽大数据学习之后是这样</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>从Hive中的stored as file_foramt看hive调优</title>
    <link href="http://yoursite.com/2018/05/30/%E4%BB%8EHive%E4%B8%AD%E7%9A%84stored%20as%20file_foramt%E7%9C%8Bhive%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2018/05/30/从Hive中的stored as file_foramt看hive调优/</id>
    <published>2018-05-29T16:00:00.000Z</published>
    <updated>2019-05-13T08:58:00.192Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="一、行式数据库和列式数据库的对比"><a href="#一、行式数据库和列式数据库的对比" class="headerlink" title="一、行式数据库和列式数据库的对比"></a>一、行式数据库和列式数据库的对比</h4><h5 id="1、存储比较"><a href="#1、存储比较" class="headerlink" title="1、存储比较"></a>1、存储比较</h5><p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p><h5 id="2、压缩比较"><a href="#2、压缩比较" class="headerlink" title="2、压缩比较"></a>2、压缩比较</h5><a id="more"></a><p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p><p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p><h5 id="3、查询比较"><a href="#3、查询比较" class="headerlink" title="3、查询比较"></a>3、查询比较</h5><p>假设执行的查询操作是：select id,name from table_emp;</p><p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p><p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p><p>假设执行的查询操作是：select * from table_emp;</p><p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p><p><strong><font color="#FF4500">但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</font></strong></p><h4 id="二、stored-as-file-format-详解"><a href="#二、stored-as-file-format-详解" class="headerlink" title="二、stored as file_format 详解"></a>二、stored as file_format 详解</h4><h5 id="1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"><a href="#1、建一张表时，可以使用“stored-as-file-format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。" class="headerlink" title="1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。"></a>1、建一张表时，可以使用“stored as file_format”来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tt (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CREATE TABLE tt2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS TEXTFILE;</span><br><span class="line"></span><br><span class="line">CREATE TABLE tt3 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS </span><br><span class="line">INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;</span><br><span class="line">OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;;</span><br><span class="line"></span><br><span class="line">#以上三种方式存储的格式都是TEXTFILE。</span><br></pre></td></tr></table></figure><h5 id="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"><a href="#2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较" class="headerlink" title="2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较"></a>2、TEXTFILE、SEQUENCEFILE、RCFILE、ORC等四种储存格式及它们对于hive在存储数据和查询数据时性能的优劣比较</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">file_format:</span><br><span class="line">  | SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><p><strong>TEXTFILE:</strong> 只是hive中表数据默认的存储格式，它将所有类型的数据都存储为String类型，不便于数据的解析，但它却比较通用。不具备随机读写的能力。支持压缩。</p><p><strong>SEQUENCEFILE:</strong> 这种储存格式比TEXTFILE格式多了头部、标识、信息长度等信息，这些信息使得其具备随机读写的能力。支持压缩，但压缩的是value。（存储相同的数据，SEQUENCEFILE比TEXTFILE略大）</p><p><strong>RCFILE（Record Columnar File）:</strong> 现在水平上划分为很多个Row Group,每个Row Group默认大小4MB，Row Group内部再按列存储信息。由facebook开源，比标准行式存储节约10%的空间。</p><p><strong>ORC:</strong> 优化过后的RCFile,现在水平上划分为多个Stripes,再在Stripe中按列存储。每个Stripe由一个Index Data、一个Row Data、一个Stripe Footer组成。每个Stripes的大小为250MB，每个Index Data记录的是整型数据最大值最小值、字符串数据前后缀信息，每个列的位置等等诸如此类的信息。这就使得查询十分得高效，默认每一万行数据建立一个Index Data。ORC存储大小为TEXTFILE的40%左右，使用压缩则可以进一步将这个数字降到10%~20%。</p><p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure><p></p><p>The parameters are all placed in the TBLPROPERTIES (see Create Table). They are:</p><p>Key|Default|Notes<br>|-|-|-|<br>orc.compress|ZLIB|high level compression (one of NONE, ZLIB, SNAPPY)<br>|orc.compress.size|262,144|number of bytes in each compression chunk<br>|orc.stripe.size|67,108,864|number of bytes in each stripe<br>|orc.row.index.stride|10,000|number of rows between index entries (must be &gt;= 1000)<br>|orc.create.index|true|whether to create row indexes<br>|orc.bloom.filter.columns |””| comma separated list of column names for which bloom filter should be created<br>|orc.bloom.filter.fpp| 0.05| false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</p><p>示例：创建带压缩的ORC存储表<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table Addresses (</span><br><span class="line">  name string,</span><br><span class="line">  street string,</span><br><span class="line">  city string,</span><br><span class="line">  state string,</span><br><span class="line">  zip int</span><br><span class="line">) stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure><p></p><p>PARQUET: 存储大小为TEXTFILE的60%~70%，压缩后在20%~30%之间。</p><hr><p>注意：</p><ol><li><p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p></li><li><p>如果要使用TEXTFILE作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“insert into table table_stored_file_ORC select <em>from table_t0;”创建。或者使用”create table as select </em>from table_t0;”创建。</p></li></ol><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;一、行式数据库和列式数据库的对比&quot;&gt;&lt;a href=&quot;#一、行式数据库和列式数据库的对比&quot; class=&quot;headerlink&quot; title=&quot;一、行式数据库和列式数据库的对比&quot;&gt;&lt;/a&gt;一、行式数据库和列式数据库的对比&lt;/h4&gt;&lt;h5 id=&quot;1、存储比较&quot;&gt;&lt;a href=&quot;#1、存储比较&quot; class=&quot;headerlink&quot; title=&quot;1、存储比较&quot;&gt;&lt;/a&gt;1、存储比较&lt;/h5&gt;&lt;p&gt;行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。&lt;/p&gt;&lt;h5 id=&quot;2、压缩比较&quot;&gt;&lt;a href=&quot;#2、压缩比较&quot; class=&quot;headerlink&quot; title=&quot;2、压缩比较&quot;&gt;&lt;/a&gt;2、压缩比较&lt;/h5&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark之序列化在生产中的应用</title>
    <link href="http://yoursite.com/2018/05/29/Spark%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%9C%A8%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2018/05/29/Spark之序列化在生产中的应用/</id>
    <published>2018-05-28T16:00:00.000Z</published>
    <updated>2019-05-13T08:57:54.261Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h4 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h4><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_1.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_2.png" alt="enter description here"></p><h4 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h4><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。<br><strong>测试代码：</strong><br><img src="/assets/blogImg/529_3.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_4.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次。<br><img src="/assets/blogImg/529_5.png" alt="enter description here"><br><strong>测试结果：</strong><br><img src="/assets/blogImg/529_6.png" alt="enter description here"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br><img src="/assets/blogImg/529_7.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。在生产中，我们通常会创建大量的自定义实体对象，这些对象在网络传输时需要序列化，而一种好的序列化方式可以让数据有更好的压缩比，从而提升网络传输速率，提高spark作业的运行速度。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。&lt;br&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>若泽数据带你随时了解业界面试题，随时跳高薪</title>
    <link href="http://yoursite.com/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/"/>
    <id>http://yoursite.com/2018/05/25/若泽数据带你随时了解业界面试题，随时跳高薪/</id>
    <published>2018-05-24T16:00:00.000Z</published>
    <updated>2019-05-13T08:04:04.163Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="链家-一面，二面"><a href="#链家-一面，二面" class="headerlink" title="链家(一面，二面)"></a>链家(一面，二面)</h4><p>0.自我介绍</p><p>1.封装继承多态概念</p><p>2.mvc设计思想</p><p>3.线程池,看过源码吗<br><a id="more"></a><br>4.ssh框架中分别对应mvc中那一层</p><p>5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）</p><p>6.spring ioc aop 原理</p><p>7.单利模式</p><p>8.SQL题，想不起来了。。</p><p>9.jvm 运行时数据区域</p><p>10.spring mvc知道吗。。</p><p>11.工厂模式</p><p>12.mr 计算流程</p><p>13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费 查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）</p><p>14.git的使用</p><p>15.hadoop的理解</p><p>16.hive内部表和外部表的区别</p><p>17.hive存储格式和压缩格式</p><p>18.对spark了解吗？ 当时高级班还没学。。</p><p>19.hive于关系型数据库的区别</p><p>20.各种排序 手写堆排序,说说原理</p><p>21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。</p><p>22中间也穿插了项目。</p><p>无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。</p><p>若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;链家-一面，二面&quot;&gt;&lt;a href=&quot;#链家-一面，二面&quot; class=&quot;headerlink&quot; title=&quot;链家(一面，二面)&quot;&gt;&lt;/a&gt;链家(一面，二面)&lt;/h4&gt;&lt;p&gt;0.自我介绍&lt;/p&gt;&lt;p&gt;1.封装继承多态概念&lt;/p&gt;&lt;p&gt;2.mvc设计思想&lt;/p&gt;&lt;p&gt;3.线程池,看过源码吗&lt;br&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>一次跳槽经历（阿里/美团/头条/网易/有赞...)</title>
    <link href="http://yoursite.com/2018/05/24/%E6%9C%89%E8%B5%9E...)/"/>
    <id>http://yoursite.com/2018/05/24/有赞...)/</id>
    <published>2018-05-23T16:00:00.000Z</published>
    <updated>2019-04-26T13:21:04.634Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h6 id="为啥跳槽"><a href="#为啥跳槽" class="headerlink" title="为啥跳槽"></a>为啥跳槽</h6><p>每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。<br><img src="./assets/blogImg/tiaocao524.png" alt="enter description here"><br><a id="more"></a></p><h6 id="面试过程"><a href="#面试过程" class="headerlink" title="面试过程"></a>面试过程</h6><p>（先打个广告，有兴趣加入阿里的欢迎发简历至 <a href="mailto:zhangzb2007@gmail.com" target="_blank" rel="noopener">zhangzb2007@gmail.com</a>，或简书上给我发信息）<br>面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。</p><h6 id="上半场"><a href="#上半场" class="headerlink" title="上半场"></a>上半场</h6><ul><li><p>曹操专车<br>这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。</p></li><li><p>美亚柏科<br>估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。</p></li><li><p>有赞<br>绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。<br>这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。</p></li><li><p>字节跳动(今日头条)<br>HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。<br>一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。<br>二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。</p></li></ul><h6 id="下半场"><a href="#下半场" class="headerlink" title="下半场"></a>下半场</h6><p>一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。</p><ul><li><p>美团<br>这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。<br>两点半进去。<br>一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。<br>二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。<br>三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。<br>四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。<br>出来的时候已经是六点半。</p></li><li><p>网易<br>面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。<br>一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。<br>然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。</p></li><li><p>阿里<br>这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。<br>一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。<br>二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。<br>三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。<br>不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。</p></li></ul><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。<strong>整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。</strong> 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h6 id=&quot;为啥跳槽&quot;&gt;&lt;a href=&quot;#为啥跳槽&quot; class=&quot;headerlink&quot; title=&quot;为啥跳槽&quot;&gt;&lt;/a&gt;为啥跳槽&lt;/h6&gt;&lt;p&gt;每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。&lt;br&gt;&lt;img src=&quot;./assets/blogImg/tiaocao524.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Hive中自定义UDAF函数生产小案例</title>
    <link href="http://yoursite.com/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2018/05/23/Hive中自定义UDAF函数生产小案例/</id>
    <published>2018-05-22T16:00:00.000Z</published>
    <updated>2019-04-26T13:11:34.049Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h4 id="一、UDAF-回顾"><a href="#一、UDAF-回顾" class="headerlink" title="一、UDAF 回顾"></a>一、UDAF 回顾</h4><ul><li>1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。</li><li>2.Hive有两种UDAF：简单和通用<br>简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。<br>通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。</li><li>3.一个计算函数必须实现的5个方法的具体含义如下：<br>init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。<br>iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。<br>terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。<br>merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。<br>terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。<h4 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h4>使用UDAF简单方式实现统计区域产品用户访问排名<a id="more"></a><h4 id="三、自定义UDAF函数代码实现"><a href="#三、自定义UDAF函数代码实现" class="headerlink" title="三、自定义UDAF函数代码实现"></a>三、自定义UDAF函数代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">package hive.org.ruozedata;</span><br><span class="line">import java.util.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">public class UserClickUDAF extends UDAF &#123;</span><br><span class="line">    // 日志对象初始化</span><br><span class="line">    public static Logger logger = Logger.getLogger(UserClickUDAF.class);</span><br><span class="line">    // 静态类实现UDAFEvaluator</span><br><span class="line">    public static class Evaluator implements UDAFEvaluator &#123;</span><br><span class="line">        // 设置成员变量，存储每个统计范围内的总记录数</span><br><span class="line">        private static Map&lt;String, String&gt; courseScoreMap;</span><br><span class="line">        private static Map&lt;String, String&gt; city_info;</span><br><span class="line">        private static Map&lt;String, String&gt; product_info;</span><br><span class="line">        private static Map&lt;String, String&gt; user_click;</span><br><span class="line">        //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span><br><span class="line">        public Evaluator() &#123;</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line">        // 初始化函数间传递的中间变量</span><br><span class="line">        public void init() &#123;</span><br><span class="line">            courseScoreMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">            city_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">            product_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">        //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出</span><br><span class="line">        public boolean iterate(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (pcid == null || pcname == null || pccount == null) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (pccount.equals(&quot;-1&quot;)) &#123;</span><br><span class="line">                // 城市表</span><br><span class="line">                city_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (pccount.equals(&quot;-2&quot;)) &#123;</span><br><span class="line">                // 产品表</span><br><span class="line">                product_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // 处理用户点击关联</span><br><span class="line">                unionCity_Prod_UserClic1(pcid, pcname, pccount);</span><br><span class="line">           &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 处理用户点击关联</span><br><span class="line">        private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (product_info.containsKey(pcid)) &#123;</span><br><span class="line">                if (city_info.containsKey(pcname)) &#123;</span><br><span class="line">                    String city_name = city_info.get(pcname);</span><br><span class="line">                    String prod_name = product_info.get(pcid);</span><br><span class="line">                    String cp_name = city_name + prod_name;</span><br><span class="line">                    // 如果之前已经Put过Key值为区域信息，则把记录相加处理</span><br><span class="line">                    if (courseScoreMap.containsKey(cp_name)) &#123;</span><br><span class="line">                        int pcrn = 0;</span><br><span class="line">                        String strTemp = courseScoreMap.get(cp_name);</span><br><span class="line">                        String courseScoreMap_pn </span><br><span class="line">                         = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim();</span><br><span class="line">                        pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn);</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn));</span><br><span class="line">                    &#125;</span><br><span class="line">                    else &#123;</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput</span><br><span class="line">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span><br><span class="line">         */</span><br><span class="line">        public Map&lt;String, String&gt; terminatePartial() &#123;</span><br><span class="line">            return courseScoreMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span><br><span class="line">        public boolean merge(Map&lt;String, String&gt; mapOutput) &#123;</span><br><span class="line">            this.courseScoreMap.putAll(mapOutput);</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span><br><span class="line">        public String terminate() &#123;</span><br><span class="line">            return courseScoreMap.toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="四、创建hive中的临时函数"><a href="#四、创建hive中的临时函数" class="headerlink" title="四、创建hive中的临时函数"></a>四、创建hive中的临时函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION user_click;</span><br><span class="line">add jar /data/hive_udf-1.0.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;</span><br></pre></td></tr></table></figure><h4 id="五、调用自定义UDAF函数处理数据"><a href="#五、调用自定义UDAF函数处理数据" class="headerlink" title="五、调用自定义UDAF函数处理数据"></a>五、调用自定义UDAF函数处理数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from (</span><br><span class="line">  select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from (</span><br><span class="line">    select * from (</span><br><span class="line">      select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info</span><br><span class="line">      union all</span><br><span class="line">      select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info</span><br><span class="line">      union all</span><br><span class="line">      select count(1) as type,</span><br><span class="line">             product_id as pcid,</span><br><span class="line">             city_id as pcname</span><br><span class="line">        from user_click</span><br><span class="line">       where action_time=&apos;2016-05-05&apos;</span><br><span class="line">      group by product_id,city_id</span><br><span class="line">    ) a</span><br><span class="line">  order by type) b</span><br><span class="line">) c ;</span><br></pre></td></tr></table></figure><h4 id="六、创建Hive临时外部表"><a href="#六、创建Hive临时外部表" class="headerlink" title="六、创建Hive临时外部表"></a>六、创建Hive临时外部表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table tmp1(</span><br><span class="line">city_name string,</span><br><span class="line">product_name string,</span><br><span class="line">rn string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">location &apos;/works/tmp1&apos;;</span><br></pre></td></tr></table></figure><h4 id="七、统计最终区域前3产品排名"><a href="#七、统计最终区域前3产品排名" class="headerlink" title="七、统计最终区域前3产品排名"></a>七、统计最终区域前3产品排名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">select city_name,</span><br><span class="line">       product_name,</span><br><span class="line">       floor(sum(rn)) visit_num,</span><br><span class="line">       row_number()over(partition by city_name order by sum(rn) desc) rn,</span><br><span class="line">       &apos;2016-05-05&apos; action_time</span><br><span class="line">  from tmp1 </span><br><span class="line"> group by city_name,product_name</span><br><span class="line">) a where rn &lt;=3 ;</span><br></pre></td></tr></table></figure><h4 id="八、最终结果"><a href="#八、最终结果" class="headerlink" title="八、最终结果"></a>八、最终结果</h4><p><img src="/assets/blogImg/hive523.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;一、UDAF-回顾&quot;&gt;&lt;a href=&quot;#一、UDAF-回顾&quot; class=&quot;headerlink&quot; title=&quot;一、UDAF 回顾&quot;&gt;&lt;/a&gt;一、UDAF 回顾&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。&lt;/li&gt;&lt;li&gt;2.Hive有两种UDAF：简单和通用&lt;br&gt;简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。&lt;br&gt;通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。&lt;/li&gt;&lt;li&gt;3.一个计算函数必须实现的5个方法的具体含义如下：&lt;br&gt;init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。&lt;br&gt;iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。&lt;br&gt;terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。&lt;br&gt;merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。&lt;br&gt;terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。&lt;h4 id=&quot;二、需求&quot;&gt;&lt;a href=&quot;#二、需求&quot; class=&quot;headerlink&quot; title=&quot;二、需求&quot;&gt;&lt;/a&gt;二、需求&lt;/h4&gt;使用UDAF简单方式实现统计区域产品用户访问排名
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark History Server Web UI配置</title>
    <link href="http://yoursite.com/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2018/05/21/Spark History Server Web UI配置/</id>
    <published>2018-05-20T16:00:00.000Z</published>
    <updated>2019-05-13T09:00:09.786Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd /opt/app/spark/conf</span><br><span class="line">[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /Found 3 items</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line">drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 items</span><br><span class="line">drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog</span><br></pre></td></tr></table></figure><p>在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息</p><h5 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir             hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br></pre></td></tr></table></figure><p>spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建<br>spark.yarn.historyServer.address : Spark history server的地址(不加http://).<br>这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</p><h5 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 conf]# vi spark-env.sh</span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h5 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01  ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer</span><br><span class="line">[root@hadoop01  ~]# ps -ef|grep sparkroot     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop01  ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java</span><br></pre></td></tr></table></figure><p>以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h5 id=&quot;1-进入spark目录和配置文件&quot;&gt;&lt;a href=&quot;#1-进入spark目录和配置文件&quot; class=&quot;headerlink&quot; title=&quot;1.进入spark目录和配置文件&quot;&gt;&lt;/a&gt;1.进入spark目录和配置文件&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@hadoop000 ~]# cd /opt/app/spark/conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 基本概念</title>
    <link href="http://yoursite.com/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/05/21/Spark 基本概念/</id>
    <published>2018-05-20T16:00:00.000Z</published>
    <updated>2019-05-13T09:00:15.344Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><p><strong>基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）</strong><br><a id="more"></a></p><h5 id="spark名词解释"><a href="#spark名词解释" class="headerlink" title="spark名词解释"></a>spark名词解释</h5><ul><li><p>Application jar：应用程序jar包<br>包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；</p></li><li><p>Driver Program：<br>这个进程运行应用程序的 main 方法并且新建 SparkContext ；</p></li><li><p>Cluster Manager：集群管理者<br>在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）</p></li><li><p>Deploy mode：部署模式<br>告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；</p></li><li><p>Worker Node：工作节点<br>集群中任何可以运行应用代码的节点；（yarn上就是node manager）</p></li><li><p>Executor：<br>在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；</p></li><li><p>Task：任务<br>被送到某个 executor 上执行的工作单元；</p></li><li><p>Job：<br>包含很多并行计算的task。一个 action 就会产生一个job；</p></li><li><p>Stage：<br>一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。</p></li></ul><h5 id="spark工作流程"><a href="#spark工作流程" class="headerlink" title="spark工作流程"></a>spark工作流程</h5><p>1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。</p><p>spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。<br>如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行</p><ul><li>1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。</li><li>2、spark并不关心底层的集群管理。</li><li>3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。</li><li>4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;p&gt;&lt;strong&gt;基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark不得不理解的重要概念——从源码角度看RDD</title>
    <link href="http://yoursite.com/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
    <id>http://yoursite.com/2018/05/20/Spark不得不理解的重要概念——从源码角度看RDD/</id>
    <published>2018-05-19T16:00:00.000Z</published>
    <updated>2019-05-13T09:00:03.422Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1.RDD是什么"></a>1.RDD是什么</h4><p>Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合</p><h4 id="2-RDD五大特性"><a href="#2-RDD五大特性" class="headerlink" title="2.RDD五大特性"></a>2.RDD五大特性</h4><a id="more"></a><ol><li><p>A list of partitions<br>每个rdd有多个分区<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>计算作用到每个分区<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rdd之间存在依赖（RDD的血缘关系）如：<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选，默认哈希的分区<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>源码来自github。</p><h4 id="3-如何创建RDD"><a href="#3-如何创建RDD" class="headerlink" title="3.如何创建RDD"></a>3.如何创建RDD</h4><p>创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfile（）</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><p><strong>源码总结：<br>1）.取_2是因为数据为（key（偏移量），value（数据））</strong></p><h4 id="4-常见的transformation和action"><a href="#4-常见的transformation和action" class="headerlink" title="4.常见的transformation和action"></a>4.常见的transformation和action</h4><p>由于比较简单，大概说一下常用的用处，不做代码测试</p><p>transformation</p><ul><li>Map：对数据集的每一个元素进行操作</li><li>FlatMap：先对数据集进行扁平化处理，然后再Map</li><li>Filter：对数据进行过滤，为true则通过</li><li>destinct：去重操作</li></ul><p>action</p><ul><li>reduce：对数据进行聚集</li><li>reduceBykey：对key值相同的进行操作</li><li>collect：没有效果的action，但是很有用</li><li>saveAstextFile：数据存入文件系统</li><li>foreach：对每个元素进行func的操作</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;h4 id=&quot;1-RDD是什么&quot;&gt;&lt;a href=&quot;#1-RDD是什么&quot; class=&quot;headerlink&quot; title=&quot;1.RDD是什么&quot;&gt;&lt;/a&gt;1.RDD是什么&lt;/h4&gt;&lt;p&gt;Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合&lt;/p&gt;&lt;h4 id=&quot;2-RDD五大特性&quot;&gt;&lt;a href=&quot;#2-RDD五大特性&quot; class=&quot;headerlink&quot; title=&quot;2.RDD五大特性&quot;&gt;&lt;/a&gt;2.RDD五大特性&lt;/h4&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
</feed>
