<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-13T12:17:53.768Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</title>
    <link href="http://yoursite.com/2019/05/13/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE-CDH5.16.1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2(%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%86%EF%BC%8C%E9%85%8D%E5%A5%97%E8%A7%86%E9%A2%91%EF%BC%8C%E7%94%9F%E4%BA%A7%E5%8F%AF%E5%AE%9E%E8%B7%B5)/"/>
    <id>http://yoursite.com/2019/05/13/若泽数据-CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)/</id>
    <published>2019-05-12T16:00:00.000Z</published>
    <updated>2019-05-13T12:17:53.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><h2 id="若泽数据"><a href="#若泽数据" class="headerlink" title="若泽数据"></a><a href="www.ruozedata.com">若泽数据</a></h2><h2 id="CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践"><a href="#CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践" class="headerlink" title="CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)"></a>CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)</h2><p>视频:<a href="https://www.bilibili.com/video/av52167219" target="_blank" rel="noopener">https://www.bilibili.com/video/av52167219</a><br>PS:建议先看课程视频1-2篇，再根据视频或文档部署，<br>如有问题，及时与@若泽数据J哥联系。</p><a id="more"></a><hr><h2 id="一-准备工作"><a href="#一-准备工作" class="headerlink" title="一.准备工作"></a>一.准备工作</h2><h4 id="1-离线部署主要分为三块"><a href="#1-离线部署主要分为三块" class="headerlink" title="1.离线部署主要分为三块:"></a>1.离线部署主要分为三块:</h4><p>a.MySQL离线部署<br>b.CM离线部署<br>c.Parcel文件离线源部署</p><h4 id="2-规划"><a href="#2-规划" class="headerlink" title="2.规划:"></a>2.规划:</h4><table><thead><tr><th>节点</th><th>MySQL部署组件</th><th>Parcel文件离线源</th><th>CM服务进程</th><th>大数据组件</th></tr></thead><tbody><tr><td>hadoop001</td><td>MySQL</td><td>Parcel</td><td>Activity Monitor<br></td><td>NN RM DN NM</td></tr><tr><td>hadoop002</td><td></td><td></td><td>Alert Publisher<br>Event Server</td><td>DN NM</td></tr><tr><td>hadoop003</td><td></td><td></td><td>Host Monitor<br>Service Monitor</td><td>DN NM</td></tr></tbody></table><h3 id="3-下载源"><a href="#3-下载源" class="headerlink" title="3.下载源:"></a>3.下载源:</h3><ul><li>CM<br><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz" target="_blank" rel="noopener">cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</a></li><li>Parcel<br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1" target="_blank" rel="noopener">CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</a><br><a href="http://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json" target="_blank" rel="noopener">manifest.json</a></li><li><p>JDK<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a><br>下载jdk-8u202-linux-x64.tar.gz</p></li><li><p>MySQL<br><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a><br>下载mysql-5.7.26-el7-x86_64.tar.gz</p></li><li><p>MySQL jdbc jar<br><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.47.jar</a><br>下载完成后要重命名去掉版本号，<br>mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar</p></li></ul><hr><p>###准备好百度云,下载安装包:<br>链接:<a href="https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA" target="_blank" rel="noopener">https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA</a> 密码:viqp</p><h2 id="二-集群节点初始化"><a href="#二-集群节点初始化" class="headerlink" title="二.集群节点初始化"></a>二.集群节点初始化</h2><h3 id="1-阿里云上海区购买3台，按量付费虚拟机"><a href="#1-阿里云上海区购买3台，按量付费虚拟机" class="headerlink" title="1.阿里云上海区购买3台，按量付费虚拟机"></a>1.阿里云上海区购买3台，按量付费虚拟机</h3><p>CentOS7.2操作系统，2核8G最低配置</p><h3 id="2-当前笔记本或台式机配置hosts文件"><a href="#2-当前笔记本或台式机配置hosts文件" class="headerlink" title="2.当前笔记本或台式机配置hosts文件"></a>2.当前笔记本或台式机配置hosts文件</h3><ul><li>MAC: /etc/hosts</li><li>Window: C:\windows\system32\drivers\etc\hosts</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">公网地址: </span><br><span class="line">106.15.234.222 hadoop001  </span><br><span class="line">106.15.235.200 hadoop002  </span><br><span class="line">106.15.234.239 hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-设置所有节点的hosts文件"><a href="#3-设置所有节点的hosts文件" class="headerlink" title="3.设置所有节点的hosts文件"></a>3.设置所有节点的hosts文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">私有地铁、内网地址:</span><br><span class="line">echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hosts</span><br><span class="line">echo &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="4-关闭所有节点的防火墙及清空规则"><a href="#4-关闭所有节点的防火墙及清空规则" class="headerlink" title="4.关闭所有节点的防火墙及清空规则"></a>4.关闭所有节点的防火墙及清空规则</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure><h3 id="5-关闭所有节点的selinux"><a href="#5-关闭所有节点的selinux" class="headerlink" title="5.关闭所有节点的selinux"></a>5.关闭所有节点的selinux</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">将SELINUX=enforcing改为SELINUX=disabled </span><br><span class="line">设置后需要重启才能生效</span><br></pre></td></tr></table></figure><h3 id="6-设置所有节点的时区一致及时钟同步"><a href="#6-设置所有节点的时区一致及时钟同步" class="headerlink" title="6.设置所有节点的时区一致及时钟同步"></a>6.设置所有节点的时区一致及时钟同步</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">6.1.时区</span><br><span class="line">[root@hadoop001 ~]# date</span><br><span class="line">Sat May 11 10:07:53 CST 2019</span><br><span class="line">[root@hadoop001 ~]# timedatectl</span><br><span class="line">      Local time: Sat 2019-05-11 10:10:31 CST</span><br><span class="line">  Universal time: Sat 2019-05-11 02:10:31 UTC</span><br><span class="line">        RTC time: Sat 2019-05-11 10:10:29</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br><span class="line">     NTP enabled: yes</span><br><span class="line">NTP synchronized: yes</span><br><span class="line"> RTC in local TZ: yes</span><br><span class="line">      DST active: n/a</span><br><span class="line"></span><br><span class="line">#查看命令帮助，学习至关重要，无需百度，太👎</span><br><span class="line">[root@hadoop001 ~]# timedatectl --help</span><br><span class="line">timedatectl [OPTIONS...] COMMAND ...</span><br><span class="line"></span><br><span class="line">Query or change system time and date settings.</span><br><span class="line"></span><br><span class="line">  -h --help                Show this help message</span><br><span class="line">     --version             Show package version</span><br><span class="line">     --no-pager            Do not pipe output into a pager</span><br><span class="line">     --no-ask-password     Do not prompt for password</span><br><span class="line">  -H --host=[USER@]HOST    Operate on remote host</span><br><span class="line">  -M --machine=CONTAINER   Operate on local container</span><br><span class="line">     --adjust-system-clock Adjust system clock when changing local RTC mode</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  status                   Show current time settings</span><br><span class="line">  set-time TIME            Set system time</span><br><span class="line">  set-timezone ZONE        Set system time zone</span><br><span class="line">  list-timezones           Show known time zones</span><br><span class="line">  set-local-rtc BOOL       Control whether RTC is in local time</span><br><span class="line">  set-ntp BOOL             Control whether NTP is enabled</span><br><span class="line"></span><br><span class="line">#查看哪些时区</span><br><span class="line">[root@hadoop001 ~]# timedatectl list-timezones</span><br><span class="line">Africa/Abidjan</span><br><span class="line">Africa/Accra</span><br><span class="line">Africa/Addis_Ababa</span><br><span class="line">Africa/Algiers</span><br><span class="line">Africa/Asmara</span><br><span class="line">Africa/Bamako</span><br><span class="line"></span><br><span class="line">#所有节点设置亚洲上海时区 </span><br><span class="line">[root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line">[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">6.2.时间</span><br><span class="line">#所有节点安装ntp</span><br><span class="line">[root@hadoop001 ~]# yum install -y ntp</span><br><span class="line"></span><br><span class="line">#选取hadoop001为ntp的主节点</span><br><span class="line">[root@hadoop001 ~]# vi /etc/ntp.conf </span><br><span class="line"></span><br><span class="line">#time</span><br><span class="line">server 0.asia.pool.ntp.org</span><br><span class="line">server 1.asia.pool.ntp.org</span><br><span class="line">server 2.asia.pool.ntp.org</span><br><span class="line">server 3.asia.pool.ntp.org</span><br><span class="line">#当外部时间不可用时，可使用本地硬件时间</span><br><span class="line">server 127.127.1.0 iburst local clock </span><br><span class="line">#允许哪些网段的机器来同步时间</span><br><span class="line">restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#开启ntpd及查看状态</span><br><span class="line">[root@hadoop001 ~]# systemctl start ntpd</span><br><span class="line">[root@hadoop001 ~]# systemctl status ntpd</span><br><span class="line"> ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago</span><br><span class="line"> Main PID: 18518 (ntpd)</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           └─18518 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line"></span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usec</span><br><span class="line">May 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabled</span><br><span class="line">May 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service.</span><br><span class="line"></span><br><span class="line">#验证</span><br><span class="line">[root@hadoop001 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> LOCAL(0)        .LOCL.          10 l  726   64    0    0.000    0.000   0.000</span><br><span class="line"></span><br><span class="line">#其他从节点停止禁用ntpd服务 </span><br><span class="line">[root@hadoop002 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop002 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop002 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001  </span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]# systemctl stop ntpd</span><br><span class="line">[root@hadoop004 ~]# systemctl disable ntpd</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.</span><br><span class="line">[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop001</span><br><span class="line">11 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec</span><br><span class="line">#每天凌晨同步hadoop001节点时间</span><br><span class="line">[root@hadoop003 ~]# crontab -e</span><br><span class="line">00 00 * * * /usr/sbin/ntpdate hadoop001</span><br></pre></td></tr></table></figure><h3 id="7-部署集群的JDK"><a href="#7-部署集群的JDK" class="headerlink" title="7.部署集群的JDK"></a>7.部署集群的JDK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/</span><br><span class="line">#切记必须修正所属用户及用户组</span><br><span class="line">chown -R root:root /usr/java/jdk1.8.0_45</span><br><span class="line"></span><br><span class="line">echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure><h3 id="8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档"><a href="#8-hadoop001节点离线部署MySQL5-7-假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档" class="headerlink" title="8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)"></a>8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档)</h3><ul><li>文档链接:<a href="https://github.com/Hackeruncle/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL</a></li><li>视频链接:<a href="https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ</a> 密码:whic</li></ul><h3 id="9-创建CDH的元数据库和用户、amon服务的数据库及用户"><a href="#9-创建CDH的元数据库和用户、amon服务的数据库及用户" class="headerlink" title="9.创建CDH的元数据库和用户、amon服务的数据库及用户"></a>9.创建CDH的元数据库和用户、amon服务的数据库及用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create database cmf DEFAULT CHARACTER SET utf8;</span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="10-hadoop001节点部署mysql-jdbc-jar"><a href="#10-hadoop001节点部署mysql-jdbc-jar" class="headerlink" title="10.hadoop001节点部署mysql jdbc jar"></a>10.hadoop001节点部署mysql jdbc jar</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java.jar /usr/share/java/</span><br></pre></td></tr></table></figure><h2 id="三-CDH部署"><a href="#三-CDH部署" class="headerlink" title="三.CDH部署"></a>三.CDH部署</h2><h3 id="1-离线部署cm-server及agent"><a href="#1-离线部署cm-server及agent" class="headerlink" title="1.离线部署cm server及agent"></a>1.离线部署cm server及agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1.1.所有节点创建目录及解压</span><br><span class="line">mkdir /opt/cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/</span><br><span class="line"></span><br><span class="line">1.2.所有节点修改agent的配置，指向server的节点hadoop001</span><br><span class="line">sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">1.3.主节点修改server的配置:</span><br><span class="line">vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties </span><br><span class="line">com.cloudera.cmf.db.type=mysql</span><br><span class="line">com.cloudera.cmf.db.host=hadoop001</span><br><span class="line">com.cloudera.cmf.db.name=cmf</span><br><span class="line">com.cloudera.cmf.db.user=cmf</span><br><span class="line">com.cloudera.cmf.db.password=Ruozedata123456!</span><br><span class="line">com.cloudera.cmf.db.setupType=EXTERNAL</span><br><span class="line"></span><br><span class="line">1.4.所有节点创建用户</span><br><span class="line">useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line"></span><br><span class="line">1.5.目录修改用户及用户组</span><br><span class="line">chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager</span><br></pre></td></tr></table></figure><h3 id="2-hadoop001节点部署离线parcel源"><a href="#2-hadoop001节点部署离线parcel源" class="headerlink" title="2.hadoop001节点部署离线parcel源"></a>2.hadoop001节点部署离线parcel源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2.1.部署离线parcel源</span><br><span class="line">$ mkdir -p /opt/cloudera/parcel-repo</span><br><span class="line">$ ll</span><br><span class="line">total 3081664</span><br><span class="line">-rw-r--r-- 1 root root 2127506677 May  9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</span><br><span class="line">-rw-r--r-- 1 root root         41 May  9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</span><br><span class="line">-rw-r--r-- 1 root root  841524318 May  9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  185515842 Aug 10  2017 jdk-8u144-linux-x64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root      66538 May  9 18:03 manifest.json</span><br><span class="line">-rw-r--r-- 1 root root     989495 May 25  2017 mysql-connector-java.jar</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">#切记cp时，重命名去掉1，不然在部署过程CM认为如上文件下载未完整，会持续下载</span><br><span class="line">$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha</span><br><span class="line">$ cp manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">2.2.目录修改用户及用户组</span><br><span class="line">$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/</span><br></pre></td></tr></table></figure><h3 id="3-所有节点创建软件安装目录、用户及用户组权限"><a href="#3-所有节点创建软件安装目录、用户及用户组权限" class="headerlink" title="3.所有节点创建软件安装目录、用户及用户组权限"></a>3.所有节点创建软件安装目录、用户及用户组权限</h3><p>mkdir -p /opt/cloudera/parcels<br>chown -R cloudera-scm:cloudera-scm /opt/cloudera/</p><h3 id="4-hadoop001节点启动Server"><a href="#4-hadoop001节点启动Server" class="headerlink" title="4.hadoop001节点启动Server"></a>4.hadoop001节点启动Server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.1.启动server</span><br><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start</span><br><span class="line"></span><br><span class="line">4.2.阿里云web界面，设置该hadoop001节点防火墙放开7180端口</span><br><span class="line">4.3.等待1min，打开 http://hadoop001:7180 账号密码:admin/admin</span><br><span class="line">4.4.假如打不开，去看server的log，根据错误仔细排查错误</span><br></pre></td></tr></table></figure><h3 id="5-所有节点启动Agent"><a href="#5-所有节点启动Agent" class="headerlink" title="5.所有节点启动Agent"></a>5.所有节点启动Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h3 id="6-接下来，全部Web界面操作"><a href="#6-接下来，全部Web界面操作" class="headerlink" title="6.接下来，全部Web界面操作"></a>6.接下来，全部Web界面操作</h3><p><a href="http://hadoop001:7180/" target="_blank" rel="noopener">http://hadoop001:7180/</a><br>账号密码:admin/admin</p><h3 id="7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选"><a href="#7-欢迎使用Cloudera-Manager–最终用户许可条款与条件。勾选" class="headerlink" title="7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选"></a>7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选</h3><p><img src="install pictures/1.png" alt="avatar"></p><h3 id="8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本"><a href="#8-欢迎使用Cloudera-Manager–您想要部署哪个版本？选择Cloudera-Express免费版本" class="headerlink" title="8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本"></a>8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本</h3><p><img src="install pictures/2.png" alt="avatar"></p><h3 id="9-感谢您选择Cloudera-Manager和CDH"><a href="#9-感谢您选择Cloudera-Manager和CDH" class="headerlink" title="9.感谢您选择Cloudera Manager和CDH"></a>9.感谢您选择Cloudera Manager和CDH</h3><p><img src="install pictures/3.png" alt="avatar"></p><h3 id="10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选"><a href="#10-为CDH集群安装指导主机。选择-当前管理的主机-，全部勾选" class="headerlink" title="10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选"></a>10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选</h3><p><img src="install pictures/4.png" alt="avatar"></p><h3 id="11-选择存储库"><a href="#11-选择存储库" class="headerlink" title="11.选择存储库"></a>11.选择存储库</h3><p><img src="install pictures/5.png" alt="avatar"></p><h3 id="12-集群安装–正在安装选定Parcel假如"><a href="#12-集群安装–正在安装选定Parcel假如" class="headerlink" title="12.集群安装–正在安装选定Parcel假如"></a>12.集群安装–正在安装选定Parcel假如</h3><p>本地parcel离线源配置正确，则”下载”阶段瞬间完成，其余阶段视节点数与内部网络情况决定。<br><img src="install pictures/6.png" alt="avatar"></p><h3 id="13-检查主机正确性"><a href="#13-检查主机正确性" class="headerlink" title="13.检查主机正确性"></a>13.检查主机正确性</h3><p><img src="install pictures/7.png" alt="avatar"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">13.1.建议将/proc/sys/vm/swappiness设置为最大值10。</span><br><span class="line">swappiness值控制操作系统尝试交换内存的积极；</span><br><span class="line">swappiness=0：表示最大限度使用物理内存，之后才是swap空间；</span><br><span class="line">swappiness=100：表示积极使用swap分区，并且把内存上的数据及时搬迁到swap空间；</span><br><span class="line">如果是混合服务器，不建议完全禁用swap，可以尝试降低swappiness。</span><br><span class="line"></span><br><span class="line">临时调整：</span><br><span class="line">sysctl vm.swappiness=10</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># Adjust swappiness value</span><br><span class="line">vm.swappiness=10</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">13.2.已启用透明大页面压缩，可能会导致重大性能问题，建议禁用此设置。</span><br><span class="line">临时调整：</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"></span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local</span><br><span class="line"># Disable transparent_hugepage</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># centos7.x系统，需要为&quot;/etc/rc.d/rc.local&quot;文件赋予执行权限</span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务"><a href="#14-自定义服务，选择部署Zookeeper、HDFS、Yarn服务" class="headerlink" title="14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务"></a>14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务</h3><p><img src="install pictures/8.png" alt="avatar"></p><h3 id="15-自定义角色分配"><a href="#15-自定义角色分配" class="headerlink" title="15.自定义角色分配"></a>15.自定义角色分配</h3><p><img src="install pictures/9.png" alt="avatar"></p><h3 id="16-数据库设置"><a href="#16-数据库设置" class="headerlink" title="16.数据库设置"></a>16.数据库设置</h3><p><img src="install pictures/10.png" alt="avatar"></p><h3 id="17-审改设置，默认即可"><a href="#17-审改设置，默认即可" class="headerlink" title="17.审改设置，默认即可"></a>17.审改设置，默认即可</h3><p><img src="install pictures/11.png" alt="avatar"></p><h3 id="18-首次运行"><a href="#18-首次运行" class="headerlink" title="18.首次运行"></a>18.首次运行</h3><p><img src="install pictures/12.png" alt="avatar"></p><h3 id="19-恭喜您"><a href="#19-恭喜您" class="headerlink" title="19.恭喜您!"></a>19.恭喜您!</h3><p><img src="install pictures/13.png" alt="avatar"></p><h3 id="20-主页"><a href="#20-主页" class="headerlink" title="20.主页"></a>20.主页</h3><p><img src="install pictures/14.png" alt="avatar"></p><hr><h3 id="CDH全套课程目录，如有buy，加微信-ruoze-star"><a href="#CDH全套课程目录，如有buy，加微信-ruoze-star" class="headerlink" title="CDH全套课程目录，如有buy，加微信(ruoze_star)"></a>CDH全套课程目录，如有buy，加微信(ruoze_star)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">0.青云环境介绍和使用 </span><br><span class="line">1.Preparation        </span><br><span class="line">谈谈怎样入门大数据 </span><br><span class="line">谈谈怎样做好一个大数据平台的运营工作 </span><br><span class="line">Linux机器,各软件版本介绍及安装(录播) </span><br><span class="line">2.Introduction      </span><br><span class="line">Cloudera、CM及CDH介绍 </span><br><span class="line">CDH版本选择 </span><br><span class="line">CDH安装几种方式解读 </span><br><span class="line">3.Install&amp;UnInstall  </span><br><span class="line">集群节点规划,环境准备(NTP,Jdk and etc) </span><br><span class="line">MySQL编译安装及常用命令 </span><br><span class="line">推荐:CDH离线安装(踩坑心得,全面剖析) </span><br><span class="line">解读暴力卸载脚本 </span><br><span class="line"></span><br><span class="line">4.CDH Management      </span><br><span class="line">CDH体系架构剖析 </span><br><span class="line">CDH配置文件深度解析 </span><br><span class="line">CM的常用命令 </span><br><span class="line">CDH集群正确启动和停止顺序 </span><br><span class="line">CDH Tsquery Language </span><br><span class="line">CDH常规管理(监控/预警/配置/资源/日志/安全) </span><br><span class="line"></span><br><span class="line">5.Maintenance Experiment  </span><br><span class="line">HDFS HA 配置 及hadoop/hdfs常规命令 </span><br><span class="line">Yarn HA 配置 及yarn常规命令 </span><br><span class="line">Other CDH Components HA 配置 </span><br><span class="line">CDH动态添加删除服务(hive/spark/hbase) </span><br><span class="line">CDH动态添加删除机器 </span><br><span class="line">CDH动态添加删除及迁移DataNode进程等 </span><br><span class="line">CDH升级(5.10.0--&gt;5.12.0) </span><br><span class="line"></span><br><span class="line">6.Resource Management    </span><br><span class="line">Linux Cgroups </span><br><span class="line">静态资源池 </span><br><span class="line">动态资源池 </span><br><span class="line">多租户案例 </span><br><span class="line"></span><br><span class="line">7.Performance Tunning    </span><br><span class="line">Memory/CPU/Network/Disk及集群规划 </span><br><span class="line">Linux参数 </span><br><span class="line">HDFS参数 </span><br><span class="line">MapReduce及Yarn参数 </span><br><span class="line">其他服务参数 </span><br><span class="line"></span><br><span class="line">8.Cases Share </span><br><span class="line">CDH4&amp;5之Alternatives命令 的研究 </span><br><span class="line">CDH5.8.2安装之Hash verification failed </span><br><span class="line">记录一次CDH4.8.6 配置HDFS HA 坑 </span><br><span class="line">CDH5.0集群IP更改 </span><br><span class="line">CDH的active namenode exit(GC)和彩蛋分享 </span><br><span class="line"></span><br><span class="line">9. Kerberos</span><br><span class="line">Kerberos简介</span><br><span class="line">Kerberos体系结构</span><br><span class="line">Kerberos工作机制</span><br><span class="line">Kerberos安装部署</span><br><span class="line">CDH启用kerberos</span><br><span class="line">Kerberos开发使用(真实代码)</span><br><span class="line"></span><br><span class="line">10.Summary         </span><br><span class="line">总结</span><br></pre></td></tr></table></figure><hr><h4 id="Join-us-if-you-have-a-dream"><a href="#Join-us-if-you-have-a-dream" class="headerlink" title="Join us if you have a dream."></a>Join us if you have a dream.</h4><h5 id="若泽数据官网-http-ruozedata-com"><a href="#若泽数据官网-http-ruozedata-com" class="headerlink" title="若泽数据官网: http://ruozedata.com"></a>若泽数据官网: <a href="http://ruozedata.com" target="_blank" rel="noopener">http://ruozedata.com</a></h5><h5 id="腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com"><a href="#腾讯课堂，搜若泽数据-http-ruoze-ke-qq-com" class="headerlink" title="腾讯课堂，搜若泽数据: http://ruoze.ke.qq.com"></a>腾讯课堂，搜若泽数据: <a href="http://ruoze.ke.qq.com" target="_blank" rel="noopener">http://ruoze.ke.qq.com</a></h5><h5 id="Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323"><a href="#Bilibili网站-搜若泽数据-https-space-bilibili-com-356836323" class="headerlink" title="Bilibili网站,搜若泽数据: https://space.bilibili.com/356836323"></a>Bilibili网站,搜若泽数据: <a href="https://space.bilibili.com/356836323" target="_blank" rel="noopener">https://space.bilibili.com/356836323</a></h5><h5 id="若泽大数据–官方博客"><a href="#若泽大数据–官方博客" class="headerlink" title="若泽大数据–官方博客"></a><a href="https://ruozedata.github.io" target="_blank" rel="noopener">若泽大数据–官方博客</a></h5><h5 id="若泽大数据–博客一览"><a href="#若泽大数据–博客一览" class="headerlink" title="若泽大数据–博客一览"></a><a href="https://github.com/ruozedata/BigData/blob/master/blog/BigDataBlogOverview.md" target="_blank" rel="noopener">若泽大数据–博客一览</a></h5><h5 id="若泽大数据–内部学员面试题"><a href="#若泽大数据–内部学员面试题" class="headerlink" title="若泽大数据–内部学员面试题"></a><a href="https://github.com/ruozedata/BigData/blob/master/interview/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.md" target="_blank" rel="noopener">若泽大数据–内部学员面试题</a></h5><h5 id="扫一扫，学一学"><a href="#扫一扫，学一学" class="headerlink" title="扫一扫，学一学:"></a>扫一扫，学一学:</h5><p><img src="install pictures/若泽数据--扫描入口.png" alt="avatar"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;若泽数据&quot;&gt;&lt;a href=&quot;#若泽数据&quot; class=&quot;headerlink&quot; title=&quot;若泽数据&quot;&gt;&lt;/a&gt;&lt;a href=&quot;www.ruozedata.com&quot;&gt;若泽数据&lt;/a&gt;&lt;/h2&gt;&lt;h2 id=&quot;CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践&quot;&gt;&lt;a href=&quot;#CDH5-16-1集群企业真正离线部署-全网最细，配套视频，生产可实践&quot; class=&quot;headerlink&quot; title=&quot;CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)&quot;&gt;&lt;/a&gt;CDH5.16.1集群企业真正离线部署(全网最细，配套视频，生产可实践)&lt;/h2&gt;&lt;p&gt;视频:&lt;a href=&quot;https://www.bilibili.com/video/av52167219&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.bilibili.com/video/av52167219&lt;/a&gt;&lt;br&gt;PS:建议先看课程视频1-2篇，再根据视频或文档部署，&lt;br&gt;如有问题，及时与@若泽数据J哥联系。&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="cdh" scheme="http://yoursite.com/tags/cdh/"/>
    
  </entry>
  
  <entry>
    <title>若泽数据课程一览</title>
    <link href="http://yoursite.com/2019/05/08/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E8%AF%BE%E7%A8%8B%E4%B8%80%E8%A7%88/"/>
    <id>http://yoursite.com/2019/05/08/若泽数据课程一览/</id>
    <published>2019-05-07T16:00:00.000Z</published>
    <updated>2019-05-13T08:00:08.535Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h1 id="若泽数据课程系列"><a href="#若泽数据课程系列" class="headerlink" title="若泽数据课程系列"></a>若泽数据课程系列</h1><h2 id="基础班"><a href="#基础班" class="headerlink" title="基础班"></a>基础班</h2><h3 id="Liunx"><a href="#Liunx" class="headerlink" title="Liunx"></a>Liunx</h3><ul><li>VM虚拟机安装</li><li>Liunx常用命令（重点）</li><li>开发环境搭</li></ul><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ul><li>源码安装&amp;yum安装</li><li>CRUD编写</li><li>权限控制</li></ul><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>架构介绍&amp;&amp;源码编译</li><li>伪分布式安装&amp;&amp;企业应用</li><li><p>HDFS（重点）</p><ul><li>架构设计</li><li>副本放置策略</li><li>读写流程</li></ul></li><li><p>YARN（重点）</p><ul><li>架构设计</li><li>工作流程</li><li>调度管理&amp;&amp;常见参数配置（调优）</li></ul></li><li><p>MapReduce</p><ul><li>架构设计</li><li>wordcount原理&amp;&amp;join原理和案例<a id="more"></a><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3></li></ul></li><li><p>架构设计</p></li><li>Hive DDL&amp;DML</li><li>join在大数据中的使用</li><li>使用自带UDF和开发自定义UDF</li></ul><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><ul><li>架构设计</li><li>RDBMS导入导出</li></ul><h3 id="整合项目将所有组件合作使用。"><a href="#整合项目将所有组件合作使用。" class="headerlink" title="整合项目将所有组件合作使用。"></a>整合项目将所有组件合作使用。</h3><h3 id="人工智能基础"><a href="#人工智能基础" class="headerlink" title="人工智能基础"></a>人工智能基础</h3><ul><li>python基础</li><li>常用库——pandas、numpy、sklearn、keras</li></ul><h2 id="高级班"><a href="#高级班" class="headerlink" title="高级班"></a>高级班</h2><h3 id="scala编程（重点）"><a href="#scala编程（重点）" class="headerlink" title="scala编程（重点）"></a>scala编程（重点）</h3><h3 id="Spark（五星重点）"><a href="#Spark（五星重点）" class="headerlink" title="Spark（五星重点）"></a>Spark（五星重点）</h3><h3 id="Hadoop高级"><a href="#Hadoop高级" class="headerlink" title="Hadoop高级"></a>Hadoop高级</h3><h3 id="Hive高级"><a href="#Hive高级" class="headerlink" title="Hive高级"></a>Hive高级</h3><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h3 id="调度平台"><a href="#调度平台" class="headerlink" title="调度平台"></a>调度平台</h3><h2 id="线下班"><a href="#线下班" class="headerlink" title="线下班"></a>线下班</h2><p><img src="/assets/blogImg/若泽数据.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;若泽数据课程系列&quot;&gt;&lt;a href=&quot;#若泽数据课程系列&quot; class=&quot;headerlink&quot; title=&quot;若泽数据课程系列&quot;&gt;&lt;/a&gt;若泽数据课程系列&lt;/h1&gt;&lt;h2 id=&quot;基础班&quot;&gt;&lt;a href=&quot;#基础班&quot; class=&quot;headerlink&quot; title=&quot;基础班&quot;&gt;&lt;/a&gt;基础班&lt;/h2&gt;&lt;h3 id=&quot;Liunx&quot;&gt;&lt;a href=&quot;#Liunx&quot; class=&quot;headerlink&quot; title=&quot;Liunx&quot;&gt;&lt;/a&gt;Liunx&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;VM虚拟机安装&lt;/li&gt;&lt;li&gt;Liunx常用命令（重点）&lt;/li&gt;&lt;li&gt;开发环境搭&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;MySQL&quot;&gt;&lt;a href=&quot;#MySQL&quot; class=&quot;headerlink&quot; title=&quot;MySQL&quot;&gt;&lt;/a&gt;MySQL&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;源码安装&amp;amp;yum安装&lt;/li&gt;&lt;li&gt;CRUD编写&lt;/li&gt;&lt;li&gt;权限控制&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;Hadoop&quot;&gt;&lt;a href=&quot;#Hadoop&quot; class=&quot;headerlink&quot; title=&quot;Hadoop&quot;&gt;&lt;/a&gt;Hadoop&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;架构介绍&amp;amp;&amp;amp;源码编译&lt;/li&gt;&lt;li&gt;伪分布式安装&amp;amp;&amp;amp;企业应用&lt;/li&gt;&lt;li&gt;&lt;p&gt;HDFS（重点）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;副本放置策略&lt;/li&gt;&lt;li&gt;读写流程&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;YARN（重点）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;工作流程&lt;/li&gt;&lt;li&gt;调度管理&amp;amp;&amp;amp;常见参数配置（调优）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;MapReduce&lt;/p&gt;&lt;ul&gt;&lt;li&gt;架构设计&lt;/li&gt;&lt;li&gt;wordcount原理&amp;amp;&amp;amp;join原理和案例
    
    </summary>
    
      <category term="课程" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B/"/>
    
    
      <category term="课程" scheme="http://yoursite.com/tags/%E8%AF%BE%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>docker常用命令以及安装mysql</title>
    <link href="http://yoursite.com/2019/05/08/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85mysql/"/>
    <id>http://yoursite.com/2019/05/08/docker常用命令以及安装mysql/</id>
    <published>2019-05-07T16:00:00.000Z</published>
    <updated>2019-05-13T08:00:04.117Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>Docker是一个开源的应用容器引擎；是一个轻量级容器技术；</p><p>Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；</p><p>运行中的这个镜像称为容器，容器启动是非常快速的。<br><a id="more"></a></p><h3 id="2-核心概念"><a href="#2-核心概念" class="headerlink" title="2.核心概念"></a>2.核心概念</h3><p>docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）；</p><p>docker客户端(Client)：连接docker主机进行操作；</p><p>docker仓库(Registry)：用来保存各种打包好的软件镜像；</p><p>docker镜像(Images)：软件打包好的镜像；放在docker仓库中；</p><p>docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用</p><h3 id="3-安装环境"><a href="#3-安装环境" class="headerlink" title="3.安装环境"></a>3.安装环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VM ware Workstation10</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">uname -r</span><br><span class="line">3.10.0-862.el7.x86_64</span><br></pre></td></tr></table></figure><p><strong>检查内核版本，必须是3.10及以上</strong> 查看命令：uname -r</p><h3 id="4-在linux虚拟机上安装docker"><a href="#4-在linux虚拟机上安装docker" class="headerlink" title="4.在linux虚拟机上安装docker"></a>4.在linux虚拟机上安装docker</h3><p>步骤：</p><p>1、检查内核版本，必须是3.10及以上<br>uname -r</p><p>2、安装docker<br>yum install docker</p><p>3、输入y确认安装<br>Dependency Updated:<br>audit.x86_64 0:2.8.1-3.el7_5.1 audit-libs.x86_64 0:2.8.1-3.el7_5.1</p><p>Complete!<br>(成功标志)</p><p>4、启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl start docker</span><br><span class="line">[root@hadoop000 ~]# docker -v</span><br><span class="line">Docker version 1.13.1, build 8633870/1.13.1</span><br></pre></td></tr></table></figure><p></p><p>5、开机启动docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><p></p><p>6、停止docker<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# systemctl stop docker</span><br><span class="line">``` </span><br><span class="line">### 5.常用命令</span><br><span class="line"></span><br><span class="line">镜像操作</span><br><span class="line">|操作|命令|说明|</span><br><span class="line">|---|---|---|</span><br><span class="line">检索|docker search 关键字 eg：docker search redis|我们经常去docker hub上检索镜像的详细信息，如镜像的TAG。|</span><br><span class="line">拉取|docker pull 镜像名:tag|:tag是可选的，tag表示标签，多为软件的版本，默认是latest</span><br><span class="line">列表|docker images|查看所有本地镜像</span><br><span class="line">删除|docker rmi image-id|删除指定的本地镜像</span><br><span class="line"></span><br><span class="line">当然大家也可以在官网查找：https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">容器操作</span><br><span class="line">软件镜像（QQ安装程序）----运行镜像----产生一个容器（正在运行的软件，运行的QQ）；</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line"></span><br><span class="line">- 1、搜索镜像</span><br><span class="line">[root@localhost ~]# docker search tomcat</span><br><span class="line">- 2、拉取镜像</span><br><span class="line">[root@localhost ~]# docker pull tomcat</span><br><span class="line">- 3、根据镜像启动容器</span><br><span class="line">docker run --name mytomcat -d tomcat:latest</span><br><span class="line">- 4、docker ps  </span><br><span class="line">查看运行中的容器</span><br><span class="line">- 5、 停止运行中的容器</span><br><span class="line">docker stop  容器的id</span><br><span class="line">- 6、查看所有的容器</span><br><span class="line">docker ps -a</span><br><span class="line">- 7、启动容器</span><br><span class="line">docker start 容器id</span><br><span class="line">- 8、删除一个容器</span><br><span class="line"> docker rm 容器id</span><br><span class="line">- 9、启动一个做了端口映射的tomcat</span><br><span class="line">[root@localhost ~]# docker run -d -p 8888:8080 tomcat</span><br><span class="line">-d：后台运行</span><br><span class="line">-p: 将主机的端口映射到容器的一个端口    主机端口:容器内部的端口</span><br><span class="line"></span><br><span class="line">- 10、为了演示简单关闭了linux的防火墙</span><br><span class="line">service firewalld status ；查看防火墙状态</span><br><span class="line">service firewalld stop：关闭防火墙</span><br><span class="line">systemctl disable firewalld.service #禁止firewall开机启动</span><br><span class="line">- 11、查看容器的日志</span><br><span class="line">docker logs container-name/container-id</span><br><span class="line"></span><br><span class="line">更多命令参看</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/docker/</span><br><span class="line">可以参考镜像文档</span><br><span class="line"></span><br><span class="line">### 6.使用docker安装mysql</span><br><span class="line"></span><br><span class="line">- docker pull mysql</span><br></pre></td></tr></table></figure><p></p><p>docker pull mysql<br>Using default tag: latest<br>Trying to pull repository docker.io/library/mysql …<br>latest: Pulling from docker.io/library/mysql<br>a5a6f2f73cd8: Pull complete<br>936836019e67: Pull complete<br>283fa4c95fb4: Pull complete<br>1f212fb371f9: Pull complete<br>e2ae0d063e89: Pull complete<br>5ed0ae805b65: Pull complete<br>0283dc49ef4e: Pull complete<br>a7e1170b4fdb: Pull complete<br>88918a9e4742: Pull complete<br>241282fa67c2: Pull complete<br>b0fecf619210: Pull complete<br>bebf9f901dcc: Pull complete<br>Digest: sha256:b7f7479f0a2e7a3f4ce008329572f3497075dc000d8b89bac3134b0fb0288de8<br>Status: Downloaded newer image for docker.io/mysql:latest<br>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 启动</span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker images<br>REPOSITORY TAG IMAGE ID CREATED SIZE<br>docker.io/mysql latest f991c20cb508 10 days ago 486 MB<br>[root@hadoop000 ~]# docker run –name mysql01 -d mysql<br>756620c8e5832f4f7ef3e82117c31760d18ec169d45b8d48c0a10ff2536dcc4a<br>[root@hadoop000 ~]# docker ps -a<br>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES<br>756620c8e583 mysql “docker-entrypoint…” 9 seconds ago Exited (1) 7 seconds ago mysql01<br>[root@hadoop000 ~]# docker logs 756620c8e583<br>error: database is uninitialized and password option is not specified<br>You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以看到上面启动的方式是错误的，提示我们要带上具体的密码</span><br></pre></td></tr></table></figure><p></p><p>[root@hadoop000 ~]# docker run -p 3306:3306 –name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysql<br>eae86796e132027df994e5f29775eb04c6a1039a92905c247f1d149714fedc06<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">–name：给新创建的容器命名，此处命名为pwc-mysql</span><br><span class="line">-e：配置信息，此处配置mysql的root用户的登陆密码</span><br><span class="line">-p：端口映射，此处映射主机3306端口到容器pwc-mysql的3306端口</span><br><span class="line">-d：成功启动容器后输出容器的完整ID，例如上图 73f8811f669ee...</span><br></pre></td></tr></table></figure><p></p><ul><li><p>查看是否启动成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES</span><br><span class="line">eae86796e132        mysql               &quot;docker-entrypoint...&quot;   8 minutes ago       Up 8 minutes        0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql02</span><br></pre></td></tr></table></figure></li><li><p>登陆MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql04 /bin/bash</span><br><span class="line">root@e34aba02c0c3:/# mysql -uroot -p123456 </span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 80</span><br><span class="line">Server version: 8.0.13 MySQL Community Server - GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></li><li><p>其他的高级操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br><span class="line">把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面</span><br><span class="line">改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）</span><br><span class="line"></span><br><span class="line">docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br><span class="line">指定mysql的一些配置参数</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h3&gt;&lt;p&gt;Docker是一个开源的应用容器引擎；是一个轻量级容器技术；&lt;/p&gt;&lt;p&gt;Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像；&lt;/p&gt;&lt;p&gt;运行中的这个镜像称为容器，容器启动是非常快速的。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>spark2.4.2详细介绍</title>
    <link href="http://yoursite.com/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/04/23/spark2.4.2详细介绍/</id>
    <published>2019-04-22T16:00:00.000Z</published>
    <updated>2019-05-13T08:58:29.254Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>Spark发布了最新的版本spark-2.4.2<br>根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的</p><h4 id="版本介绍"><a href="#版本介绍" class="headerlink" title="版本介绍"></a>版本介绍</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。<font color="#FF4500"> <strong>我们强烈建议所有2.4用户升级到此稳定版本。</strong></font><br><a id="more"></a></p><h4 id="显著的变化"><a href="#显著的变化" class="headerlink" title="显著的变化"></a>显著的变化</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。</li><li>还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。</li></ul><h4 id="详细更改"><a href="#详细更改" class="headerlink" title="详细更改"></a>详细更改</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>在Spark Driver中发现Java死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>使用选项logConf = true时密码将以conf的明文形式记录</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>用Snappy 1.1.7.1解压、压缩空序列化数据时失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferTo中的潜在损坏</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointData因文件系统已缓存而无法清理</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>在仅使用空值列的AggregateEstimation之后的错误outputRows估计</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>修复包名称不匹配</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>修复updateTableStats以使用新统计信息或无更新表统计信息</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1静默删除DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>松开在ExpressionInfo的’examples’字段中换行断言条件</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>将jquery更新为1.12.x以获取安全修复程序</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>隐藏“org.apache.spark.util.kvstore”的API文档</td></tr></tbody></table><h6 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Spark发布了最新的版本spark-2.4.2&lt;br&gt;根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的&lt;/p&gt;&lt;h4 id=&quot;版本介绍&quot;&gt;&lt;a href=&quot;#版本介绍&quot; class=&quot;headerlink&quot; title=&quot;版本介绍&quot;&gt;&lt;/a&gt;版本介绍&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/spark2.4.2_1.jpg&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。&lt;font color=&quot;#FF4500&quot;&gt; &lt;strong&gt;我们强烈建议所有2.4用户升级到此稳定版本。&lt;/strong&gt;&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我司Kafka+Flink+MySQL生产完整案例代码</title>
    <link href="http://yoursite.com/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2018/12/20/我司Kafka+Flink+MySQL生产完整案例代码/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-05-03T01:24:05.187Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><font color="#FF4500"><br></font><h6 id="1-版本信息："><a href="#1-版本信息：" class="headerlink" title="1.版本信息："></a>1.版本信息：</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flink实战</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="3-工程pom-xml"><a href="#3-工程pom-xml" class="headerlink" title="3.工程pom.xml"></a>3.工程pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConf类 定义与MySQL连接的JDBC的参数<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="5-MySQLSlink类"><a href="#5-MySQLSlink类" class="headerlink" title="5.MySQLSlink类"></a>5.MySQLSlink类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-数据清洗日期工具类"><a href="#6-数据清洗日期工具类" class="headerlink" title="6.数据清洗日期工具类"></a>6.数据清洗日期工具类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //测试一下</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQL建表"><a href="#7-MySQL建表" class="headerlink" title="7.MySQL建表"></a>7.MySQL建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-主程序："><a href="#8-主程序：" class="headerlink" title="8.主程序："></a>8.主程序：</h6><p>主要是将time的格式转成yyyyMMddHHmmss,</p><p>还有取URL中的课程ID，将不是/class开头的过滤掉。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h6 id="9-启动主程序，查看MySQL表数据在递增"><a href="#9-启动主程序，查看MySQL表数据在递增" class="headerlink" title="9.启动主程序，查看MySQL表数据在递增"></a>9.启动主程序，查看MySQL表数据在递增</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafka过来的消息是我模拟的，一分钟产生100条。</p><p>以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;h6 id=&quot;1-版本信息：&quot;&gt;&lt;a href=&quot;#1-版本信息：&quot; class=&quot;headerlink&quot; title=&quot;1.版本信息：&quot;&gt;&lt;/a&gt;1.版本信息：&lt;/h6&gt;&lt;p&gt;Flink Version:1.6.2&lt;br&gt;Kafka Version:0.9.0.0&lt;br&gt;MySQL Version:5.6.21&lt;/p&gt;&lt;h6 id=&quot;2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot;&gt;&lt;a href=&quot;#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot; class=&quot;headerlink&quot; title=&quot;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&quot;&gt;&lt;/a&gt;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&lt;/h6&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1.74.103.143    2018-12-20 18:12:00  &amp;quot;GET /class/130.html HTTP/1.1&amp;quot;     404 https://search.yahoo.com/search?p=Flink实战&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark在携程的实践（二）</title>
    <link href="http://yoursite.com/2018/12/16/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/12/16/Spark在携程的实践（二）/</id>
    <published>2018-12-15T16:00:00.000Z</published>
    <updated>2019-05-22T12:19:20.084Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed May 22 2019 20:21:18 GMT+0800 (GMT+08:00) --><p>以下内容来自第三届携程大数据沙龙</p><h3 id="七、遇到的问题"><a href="#七、遇到的问题" class="headerlink" title="七、遇到的问题"></a>七、遇到的问题</h3><h5 id="orc-split"><a href="#orc-split" class="headerlink" title="orc split"></a>orc split</h5><p>Spark读取Hive表用的各个文件格式的InuptFormat，计算读取表需要的task数量依赖于InputFormat#getSplits<br>由于大部分表的存储格式主要使用的是orc，当一个orc文件超过256MB，split算法并行去读取orc元数据，有时候Driver内存飙升，OOM crash，Full GC导致network timeout，spark context stop<br>Hive读这些大表为何没有问题？因为Hive默认使用的是CombineHiveInputFormat，split是基于文件大小的。<br>Spark也需要实现类似于Hive的CombineInputFormat，还能解决小文件过多导致提交task数量过多的问题。<br>Executor Container killed<br>Executor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead<br><a id="more"></a></p><h5 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h5><p>1.Shuffle Read时netty堆外内存的使用<br>2.Window function spill threshold过小，导致每4096条或者64MB为一个文件写到磁盘<br>外部排序同时打开每个文件，每个文件占用1MB的堆外内存，导致container使用的内存远超过申请的内存，遂被yarn kill。<br>解决：<br>Patch：<br>[SPARK-19659] Fetch big blocks to disk when shuffle-read<br>[SPARK-21369][CORE] Don’t use Scala Tuple2 in common/network-<em><br>参数：spark.reducer.maxReqSizeShuffleToMem=209715200<br>Patch：<br>[SPARK-21595]Separate thresholds for buffering and spilling in ExternalAppendOnlyUnsafeRowArray<br>参数：<br>spark.sql.windowExec.buffer.in.memory.threshold=4096<br>spark.sql.windowExec.buffer.spill.threshold= 1024 </em>1024 * 1024 / 2</p><h5 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h5><p>Spark写数据时生成很多小文件，对NameNode产生巨大的压力，在一开始Spark灰度上线的时候，文件数和Block数飙升，文件变小导致压缩率降低，容量也跟着上去。</p><h5 id="移植Hive-MergeFileTask的实现"><a href="#移植Hive-MergeFileTask的实现" class="headerlink" title="移植Hive MergeFileTask的实现"></a>移植Hive MergeFileTask的实现</h5><p>在Spark最后写目标表的阶段追加入了一个MergeFileTask，参考了Hive的实现<br>org.apache.hadoop.hive.ql.io.merge.MergeFileTask<br>org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator</p><h5 id="无数据的情况下不创建空文件"><a href="#无数据的情况下不创建空文件" class="headerlink" title="无数据的情况下不创建空文件"></a>无数据的情况下不创建空文件</h5><p>[SPARK-21435][SQL]<br>Empty files should be skipped while write to file</p><h3 id="八、优化"><a href="#八、优化" class="headerlink" title="八、优化"></a>八、优化</h3><p>1.查询分区表时支持broadcast join，加速查询<br>2.减少Broadcast join的内存压力 SPARK-22170<br>3.Fetch失败后能快速失败，以免作业卡几个小时 SPARK-19753<br>4.Spark Thrift Server稳定性<br>经常挂掉，日志里异常，more than one active taskSet for stage<br>Apply SPARK-23433仍有少数挂掉的情况，<br>提交SPARK-24677到社区，修复之<br>5.作业hang住 SPARK-21834 SPARK-19326 SPARK-11334</p><h3 id="九、未来计划"><a href="#九、未来计划" class="headerlink" title="九、未来计划"></a>九、未来计划</h3><h5 id="自动调优内存"><a href="#自动调优内存" class="headerlink" title="自动调优内存"></a>自动调优内存</h5><p>手机spark driver和executor内存使用情况<br>根据作业历史的内存使用情况，在调度系统端自动设置合适的内存<br><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener">https://github.com/uber-common/jvm-profiler</a></p><h5 id="spark-adaptive"><a href="#spark-adaptive" class="headerlink" title="spark adaptive"></a>spark adaptive</h5><p>动态调整执行计划 SortMergeJoin转化为BroadcastHashJoin<br>动态处理数据倾斜<br><a href="https://issues.apache.org/jira/browse/SPARK-23128" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-23128</a><br><a href="https://github.com/Intel-bigdata/spark-adaptive" target="_blank" rel="noopener">https://github.com/Intel-bigdata/spark-adaptive</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed May 22 2019 20:21:18 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;以下内容来自第三届携程大数据沙龙&lt;/p&gt;&lt;h3 id=&quot;七、遇到的问题&quot;&gt;&lt;a href=&quot;#七、遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;七、遇到的问题&quot;&gt;&lt;/a&gt;七、遇到的问题&lt;/h3&gt;&lt;h5 id=&quot;orc-split&quot;&gt;&lt;a href=&quot;#orc-split&quot; class=&quot;headerlink&quot; title=&quot;orc split&quot;&gt;&lt;/a&gt;orc split&lt;/h5&gt;&lt;p&gt;Spark读取Hive表用的各个文件格式的InuptFormat，计算读取表需要的task数量依赖于InputFormat#getSplits&lt;br&gt;由于大部分表的存储格式主要使用的是orc，当一个orc文件超过256MB，split算法并行去读取orc元数据，有时候Driver内存飙升，OOM crash，Full GC导致network timeout，spark context stop&lt;br&gt;Hive读这些大表为何没有问题？因为Hive默认使用的是CombineHiveInputFormat，split是基于文件大小的。&lt;br&gt;Spark也需要实现类似于Hive的CombineInputFormat，还能解决小文件过多导致提交task数量过多的问题。&lt;br&gt;Executor Container killed&lt;br&gt;Executor : Container killed by YARN for exceeding memory limits. 13.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead&lt;br&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Spark在携程的实践（一）</title>
    <link href="http://yoursite.com/2018/12/09/Spark%E5%9C%A8%E6%90%BA%E7%A8%8B%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2018/12/09/Spark在携程的实践（一）/</id>
    <published>2018-12-08T16:00:00.000Z</published>
    <updated>2019-05-21T12:44:42.154Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue May 21 2019 20:46:50 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="一、Spark在携程应用的现状"><a href="#一、Spark在携程应用的现状" class="headerlink" title="一、Spark在携程应用的现状"></a>一、Spark在携程应用的现状</h3><h6 id="集群规模："><a href="#集群规模：" class="headerlink" title="集群规模："></a>集群规模：</h6><p>平均每天MR任务数：30W+</p><h6 id="开发平台："><a href="#开发平台：" class="headerlink" title="开发平台："></a>开发平台：</h6><p>调度系统运行的任务数：10W+<br>每天运行任务实例数：23W+<br>ETL/计算任务：~58%</p><h6 id="查询平台"><a href="#查询平台" class="headerlink" title="查询平台:"></a>查询平台:</h6><p>adhoc查询：2W+<br>支持Spark/Hive/Presto<br><img src="/assets/blogImg/1209_1.png" alt="enter description here"></p><h3 id="二、Hive与Spark的区别"><a href="#二、Hive与Spark的区别" class="headerlink" title="二、Hive与Spark的区别"></a>二、Hive与Spark的区别</h3><h6 id="Hive："><a href="#Hive：" class="headerlink" title="Hive："></a>Hive：</h6><p>优点：运行稳定，客户端内存消耗小。<br>存在问题：生成多个MapReduce作业；中间结果落地，IO开销大；频繁申请和释放container，资源没有合理充分利用</p><h6 id="Spark："><a href="#Spark：" class="headerlink" title="Spark："></a>Spark：</h6><p>快：高效的DAG执行引擎，可以基于内存来高效的处理数据流，节省大量IO开销<br>通用性：SparkSQL能直接使用HiveQL语法，Hive Metastore，Serdes，UDFs<br><img src="/assets/blogImg/1209_2.png" alt="enter description here"></p><h3 id="三、迁移SparkSQL的挑战"><a href="#三、迁移SparkSQL的挑战" class="headerlink" title="三、迁移SparkSQL的挑战"></a>三、迁移SparkSQL的挑战</h3><h6 id="兼容性："><a href="#兼容性：" class="headerlink" title="兼容性："></a>兼容性：</h6><p>Hive原先的权限控制<br>SQL语法，UDF和Hive的兼容性</p><h6 id="稳定性："><a href="#稳定性：" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明，低优先级用户无感知<br>监控作业迁移后成功率及运行时长对比</p><h6 id="准确性："><a href="#准确性：" class="headerlink" title="准确性："></a>准确性：</h6><p>数据一致<br>功能增强：<br>用户体验，是否易用，报错信息是否可读<br>潜在Bug<br>周边系统配合改造<br>血缘收集</p><h3 id="四、兼容性改造"><a href="#四、兼容性改造" class="headerlink" title="四、兼容性改造"></a>四、兼容性改造</h3><h6 id="移植hive权限"><a href="#移植hive权限" class="headerlink" title="移植hive权限"></a>移植hive权限</h6><p>Spark没有权限认证模块，可对任意表进行查询，有安全隐患<br>需要与Hive共享同一套权限</p><h6 id="方案："><a href="#方案：" class="headerlink" title="方案："></a>方案：</h6><p>执行SQL时，对SQL解析得到LogicalPlan，对LogicalPlan进行遍历，提取读取的表及写入的表，调用Hvie的认证方法进行检查，如果有权限则继续执行，否则拒绝该用户的操作。<br>SQL语法和hive兼容<br>Spark创建的某些视图，在Hive查询时报错，Spark创建的视图不会对SQL进行展开，视图定义没有当前的DB信息，Hive不兼容读取这样的视图</p><h6 id="方案：、"><a href="#方案：、" class="headerlink" title="方案：、"></a>方案：、</h6><p>保持与Hive一致，在Spark创建和修改视图时，使用hive cli driver去执行create/alter view sql<br>UDF与hive兼容<br>UDF计算结果不一样，即使是正常数据，Spark返回null，Hive结果正确；异常数据，Spark抛exception导致作业失败，Hive返回的null。</p><h6 id="方案：-1"><a href="#方案：-1" class="headerlink" title="方案："></a>方案：</h6><p>Spark函数修复，比如round函数<br>将hive一些函数移植，并注册成永久函数<br>整理Spark和Hive语法和UDF差异<br>五、稳定性和准确性</p><h6 id="稳定性：-1"><a href="#稳定性：-1" class="headerlink" title="稳定性："></a>稳定性：</h6><p>迁移透明：调度系统对低优先级作业，按作业粒度切换成Spark执行，失败后再切换成hive<br>灰度变更，多种变更规则：支持多版本Spark，自动切换引擎，Spark v2 -&gt; Spark v1 -&gt; Hive；灰度推送参数，调优参数，某些功能<br>监控：每日统计spark和hive运行对比，每时收集作业粒度失败的Spark作业，分析失败原因<br>准确性：<br>数据质量系统：校验任务，检查数据准确性</p><h3 id="六、功能增强"><a href="#六、功能增强" class="headerlink" title="六、功能增强"></a>六、功能增强</h3><h6 id="Spark-Thrift-Server："><a href="#Spark-Thrift-Server：" class="headerlink" title="Spark Thrift Server："></a>Spark Thrift Server：</h6><ul><li>1.基于delegation token的impersontion<br>Driver：<br>为不同的用户拿delegation token，写到staging目录，记录User-&gt;SQL-&gt;Job映射关系，分发task带上对应的username<br>Executor：<br>根据task信息带的username找到staging目录下的token，加到当前proxy user的ugi，实现impersonate</li><li>2.基于zookeeper的服务发现，支持多台server<br>这一块主要移植了Hive zookeeper的实现</li><li>3.限制大查询作业，防止driver OOM<br>限制每个job产生的task最大数量<br>限制查询SQL的最大行数，客户端查询大批量数据，数据挤压在Thrift Server，堆内内存飙升，强制在只有查的SQL加上limit<br>限制查询SQL的结果集数据大小</li><li>4.监控<br>对每个server定时查询，检测是否可用<br>多运行时长较久的作业，主动kill<h6 id="用户体验"><a href="#用户体验" class="headerlink" title="用户体验"></a>用户体验</h6>用户看到的是类似Hive MR进度的日志，INFO级别日志收集到ES，可供日志的分析和排查问题<br>收集生成的表或者分区的numRows numFile totalSize，输出到日志<br>对简单的语句，如DDL语句，自动使用–master=local方式启动<h6 id="Combine-input-Format"><a href="#Combine-input-Format" class="headerlink" title="Combine input Format"></a>Combine input Format</h6>在HadoopTableReader#makeRDDForTable，拿到对应table的InputFormatClass，转换成对应格式的CombineInputFormat<br>通过开关来决定是否启用这个特性<br>set spark.sql.combine.input.splits.enable=true<br>通过参数来调整每个split的total input size<br>mapreduce.input.fileinputformat.split.maxsize=256MB <em>1024</em>1024<br>之前driver读大表高峰时段split需要30分钟不止，才把任务提交上，现在只要几分钟就算好split的数量并提交任务，也解决了一些表不大，小文件多，能合并到同一个task进行读取</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue May 21 2019 20:46:50 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>代码 | Spark读取mongoDB数据写入Hive普通表和分区表</title>
    <link href="http://yoursite.com/2018/11/20/Spark%E8%AF%BB%E5%8F%96mongoDB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5Hive%E6%99%AE%E9%80%9A%E8%A1%A8%E5%92%8C%E5%88%86%E5%8C%BA%E8%A1%A8/"/>
    <id>http://yoursite.com/2018/11/20/Spark读取mongoDB数据写入Hive普通表和分区表/</id>
    <published>2018-11-19T16:00:00.000Z</published>
    <updated>2019-05-20T14:34:59.451Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 20 2019 22:37:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="版本："><a href="#版本：" class="headerlink" title="版本："></a>版本：</h3><p>spark 2.2.0<br>hive 1.1.0<br>scala 2.11.8<br>hadoop-2.6.0-cdh5.7.0<br>jdk 1.8<br>MongoDB 3.6.4</p><h3 id="一-原始数据及Hive表"><a href="#一-原始数据及Hive表" class="headerlink" title="一 原始数据及Hive表"></a>一 原始数据及Hive表</h3><h5 id="MongoDB数据格式"><a href="#MongoDB数据格式" class="headerlink" title="MongoDB数据格式"></a>MongoDB数据格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5af65d86222b639e0c2212f3&quot;),</span><br><span class="line">    &quot;id&quot; : &quot;1&quot;,</span><br><span class="line">    &quot;name&quot; : &quot;lisi&quot;,</span><br><span class="line">    &quot;age&quot; : &quot;18&quot;,</span><br><span class="line">    &quot;deptno&quot; : &quot;01&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="Hive普通表"><a href="#Hive普通表" class="headerlink" title="Hive普通表"></a>Hive普通表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table mg_hive_test(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string,</span><br><span class="line">deptno string</span><br><span class="line">)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h5 id="Hive分区表"><a href="#Hive分区表" class="headerlink" title="Hive分区表"></a>Hive分区表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table  mg_hive_external(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">age string</span><br><span class="line">)</span><br><span class="line">partitioned by (deptno string)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure><h3 id="二-IDEA-Maven-Java"><a href="#二-IDEA-Maven-Java" class="headerlink" title="二 IDEA+Maven+Java"></a>二 IDEA+Maven+Java</h3><h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.6.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.2.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/2 21:00</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import com.mongodb.spark.MongoSpark;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import org.bson.Document;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class sparkreadmgtohive &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        //spark 2.x</span><br><span class="line">        String warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath();</span><br><span class="line">        SparkSession spark = SparkSession.builder()</span><br><span class="line">                .master(&quot;local[2]&quot;)</span><br><span class="line">                .appName(&quot;SparkReadMgToHive&quot;)</span><br><span class="line">                .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">                .config(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line"></span><br><span class="line">        //spark 1.x</span><br><span class="line">//        JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-spark-connector_2.11-2.2.2.jar&quot;);</span><br><span class="line">//        sc.addJar(&quot;/Users/mac/zhangchun/jar/mongo-java-driver-3.6.3.jar&quot;);</span><br><span class="line">//        SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkReadMgToHive&quot;);</span><br><span class="line">//        conf.set(&quot;spark.mongodb.input.uri&quot;, &quot;mongodb://127.0.0.1:27017/test.mgtest&quot;);</span><br><span class="line">//        conf.set(&quot;spark. serializer&quot;,&quot;org.apache.spark.serializer.KryoSerialzier&quot;);</span><br><span class="line">//        HiveContext sqlContext = new HiveContext(sc);</span><br><span class="line">//        //create df from mongo</span><br><span class="line">//        Dataset&lt;Row&gt; df = MongoSpark.read(sqlContext).load().toDF();</span><br><span class="line">//        df.select(&quot;id&quot;,&quot;name&quot;,&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">        String querysql= &quot;select id,name,age,deptno,DateTime,Job from mgtable b&quot;;</span><br><span class="line">        String opType =&quot;P&quot;;</span><br><span class="line"></span><br><span class="line">        SQLUtils sqlUtils = new SQLUtils();</span><br><span class="line">        List&lt;String&gt; column = sqlUtils.getColumns(querysql);</span><br><span class="line"></span><br><span class="line">        //create rdd from mongo</span><br><span class="line">        JavaRDD&lt;Document&gt; rdd = MongoSpark.load(sc);</span><br><span class="line">        //将Document转成Object</span><br><span class="line">        JavaRDD&lt;Object&gt; Ordd = rdd.map(new Function&lt;Document, Object&gt;() &#123;</span><br><span class="line">            public Object call(Document document)&#123;</span><br><span class="line">                List list = new ArrayList();</span><br><span class="line">                for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">                    list.add(String.valueOf(document.get(column.get(i))));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line"></span><br><span class="line">//                return list.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(Ordd.first());</span><br><span class="line">        //通过编程方式将RDD转成DF</span><br><span class="line">        List ls= new ArrayList();</span><br><span class="line">        for (int i = 0; i &lt; column.size(); i++) &#123;</span><br><span class="line">            ls.add(column.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">        String schemaString = ls.toString().replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;);</span><br><span class="line">        System.out.println(schemaString);</span><br><span class="line"></span><br><span class="line">        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();</span><br><span class="line">        for (String fieldName : schemaString.split(&quot;,&quot;)) &#123;</span><br><span class="line">            StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);</span><br><span class="line">            fields.add(field);</span><br><span class="line">        &#125;</span><br><span class="line">        StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = Ordd.map((Function&lt;Object, Row&gt;) record -&gt; &#123;</span><br><span class="line">            List fileds = (List) record;</span><br><span class="line">//            String[] attributes = record.toString().split(&quot;,&quot;);</span><br><span class="line">            return RowFactory.create(fileds.toArray());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD,schema);</span><br><span class="line"></span><br><span class="line">        //将DF写入到Hive中</span><br><span class="line">        //选择Hive数据库</span><br><span class="line">        spark.sql(&quot;use datalake&quot;);</span><br><span class="line">        //注册临时表</span><br><span class="line">        df.registerTempTable(&quot;mgtable&quot;);</span><br><span class="line"></span><br><span class="line">        if (&quot;O&quot;.equals(opType.trim())) &#123;</span><br><span class="line">            System.out.println(&quot;数据插入到Hive ordinary table&quot;);</span><br><span class="line">            Long t1 = System.currentTimeMillis();</span><br><span class="line">            spark.sql(&quot;insert into mgtohive_2 &quot; + querysql + &quot; &quot; + &quot;where b.id not in (select id from mgtohive_2)&quot;);</span><br><span class="line">            Long t2 = System.currentTimeMillis();</span><br><span class="line">            System.out.println(&quot;共耗时：&quot; + (t2 - t1) / 60000 + &quot;分钟&quot;);</span><br><span class="line">        &#125;else if (&quot;P&quot;.equals(opType.trim())) &#123;</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;数据插入到Hive  dynamic partition table&quot;);</span><br><span class="line">        Long t3 = System.currentTimeMillis();</span><br><span class="line">        //必须设置以下参数 否则报错</span><br><span class="line">        spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;);</span><br><span class="line">        //depton为分区字段   select语句最后一个字段必须是deptno</span><br><span class="line">        spark.sql(&quot;insert into mg_hive_external partition(deptno) select id,name,age,deptno from mgtable b where b.id not in (select id from mg_hive_external)&quot;);</span><br><span class="line">        Long t4 = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;共耗时：&quot;+(t4 -t3)/60000+ &quot;分钟&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.huawei.mongo;/*</span><br><span class="line"> * @Author: Create by Achun</span><br><span class="line"> *@Time: 2018/6/3 23:20</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class SQLUtils &#123;</span><br><span class="line"></span><br><span class="line">    public List&lt;String&gt; getColumns(String querysql)&#123;</span><br><span class="line">        List&lt;String&gt; column = new ArrayList&lt;String&gt;();</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;select&quot;) + 6,</span><br><span class="line">                querysql.indexOf(&quot;from&quot;)).trim();</span><br><span class="line">        if (tmp.indexOf(&quot;*&quot;) == -1)&#123;</span><br><span class="line">            String cols[] = tmp.split(&quot;,&quot;);</span><br><span class="line">            for (String c:cols)&#123;</span><br><span class="line">                column.add(c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return column;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTBname(String querysql)&#123;</span><br><span class="line">        String tmp = querysql.substring(querysql.indexOf(&quot;from&quot;)+4).trim();</span><br><span class="line">        int sx = tmp.indexOf(&quot; &quot;);</span><br><span class="line">        if(sx == -1)&#123;</span><br><span class="line">            return tmp;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return tmp.substring(0,sx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三-错误解决办法"><a href="#三-错误解决办法" class="headerlink" title="三 错误解决办法"></a>三 错误解决办法</h3><p>1 IDEA会获取不到Hive的数据库和表，将hive-site.xml放入resources文件中。并且将resources设置成配置文件(设置成功文件夹是蓝色否则是灰色)<br>file–&gt;Project Structure–&gt;Modules–&gt;Source<br><img src="/assets/blogImg/1120_1.png" alt="enter description here"><br>2 上面错误处理完后如果报JDO类型的错误，那么检查HIVE_HOME/lib下时候否mysql驱动，如果确定有，那么就是IDEA获取不到。解决方法如下：</p><p>将mysql驱动拷贝到jdk1.8.0_171.jdk/Contents/Home/jre/lib/ext路径下(jdk/jre/lib/ext)<br>在IDEA项目External Libraries下的&lt;1.8&gt;里面添加mysql驱动<br><img src="/assets/blogImg/1120_2.png" alt="enter description here"></p><h3 id="四-注意点"><a href="#四-注意点" class="headerlink" title="四 注意点"></a>四 注意点</h3><p>由于将MongoDB数据表注册成了临时表和Hive表进行了关联，所以要将MongoDB中的id字段设置成索引字段，否则性能会很慢。<br>MongoDB设置索引方法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).ensureIndex(&#123;&quot;id&quot; : &quot;1&quot;&#125;),&#123;&quot;background&quot;:true&#125;</span><br></pre></td></tr></table></figure><p></p><p>查看索引：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.getCollection(&apos;mgtest&apos;).getIndexes()</span><br><span class="line">MongoSpark网址：https://docs.mongodb.com/spark-connector/current/java-api/</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 20 2019 22:37:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)</title>
    <link href="http://yoursite.com/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
    <id>http://yoursite.com/2018/11/10/最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)/</id>
    <published>2018-11-09T16:00:00.000Z</published>
    <updated>2019-05-03T11:57:01.078Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><h5 id="1-下载Flink安装包"><a href="#1-下载Flink安装包" class="headerlink" title="1.下载Flink安装包"></a>1.下载Flink安装包</h5><p>flink下载地址</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可</p><p>上传至机器的/opt目录下<br><a id="more"></a></p><h5 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-配置master节点"><a href="#3-配置master节点" class="headerlink" title="3.配置master节点"></a>3.配置master节点</h5><p>选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。</p><p>jobmanager.rpc.address: node1</p><p>(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)</p><p>rest.port: 8088</p><p>本次安装 master节点为node1，因为单机，slave节点也为node1</p><h5 id="4-配置slaves"><a href="#4-配置slaves" class="headerlink" title="4.配置slaves"></a>4.配置slaves</h5><p>将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。</p><h5 id="5-启动flink集群"><a href="#5-启动flink集群" class="headerlink" title="5.启动flink集群"></a>5.启动flink集群</h5><p>bin/start-cluster.sh</p><p>打开 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 查看web页面<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managers代表当前的flink只有一个节点，每个task还有两个slots</p><h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6.测试"></a>6.测试</h5><p><strong>依赖</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p></p><h5 id="7-Socket测试代码"><a href="#7-Socket测试代码" class="headerlink" title="7.Socket测试代码"></a>7.Socket测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)</p><p>在命令行set MAVEN_OPTS= -Xms128m -Xmx512m</p><p>继续执行mvn clean install</p><p>生成FlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>找到打成的jar，并upload，开始上传<br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>运行参数介绍<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个</p><p>发送数据<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure><p></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>点开running的job，你可以看见接收的字节数等信息</p><p>到log目录下可以清楚的看见输出<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure><p></p><p>除了可以在界面提交，还可以将jar上传的linux中进行提交任务</p><p>运行flink上传的jar<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure><p></p><p>其他步骤一致。</p><h5 id="8-使用kafka作为source"><a href="#8-使用kafka作为source" class="headerlink" title="8.使用kafka作为source"></a>8.使用kafka作为source</h5><p>加上依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //可以通过正则表达式来匹配合适的topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //配置从最新的地方开始消费</span><br><span class="line">        kafkaSource.setStartFromLatest();        //使用addsource，将kafka的输入转变为datastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-使用mysql作为sink"><a href="#9-使用mysql作为sink" class="headerlink" title="9.使用mysql作为sink"></a>9.使用mysql作为sink</h5><p>flink本身并没有提供datastream输出到mysql，需要我们自己去实现</p><p>首先，导入依赖<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。</p><p>使用这个mysqlsink也非常简单<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure><p></p><h5 id="10-总结"><a href="#10-总结" class="headerlink" title="10.总结"></a>10.总结</h5><p>本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;h5 id=&quot;1-下载Flink安装包&quot;&gt;&lt;a href=&quot;#1-下载Flink安装包&quot; class=&quot;headerlink&quot; title=&quot;1.下载Flink安装包&quot;&gt;&lt;/a&gt;1.下载Flink安装包&lt;/h5&gt;&lt;p&gt;flink下载地址&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/flink/flink-1.5.0/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://archive.apache.org/dist/flink/flink-1.5.0/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可&lt;/p&gt;&lt;p&gt;上传至机器的/opt目录下&lt;br&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>这是一篇热腾腾的面经</title>
    <link href="http://yoursite.com/2018/08/27/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%83%AD%E8%85%BE%E8%85%BE%E7%9A%84%E9%9D%A2%E7%BB%8F/"/>
    <id>http://yoursite.com/2018/08/27/这是一篇热腾腾的面经/</id>
    <published>2018-08-26T16:00:00.000Z</published>
    <updated>2019-05-19T14:24:12.651Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:24:17 GMT+0800 (GMT+08:00) --><p>伟梦：<br>1.主要还是项目？<br>基本上没问什么技术，我就说了一遍项目流程，<br>然后说几个优化点，比如上次讲的血案，我也顺带提了一下。<br>2.在大数据中，有没有什么是不足的，遇到过什么问题？<br><a id="more"></a></p><p>微盟：<br>1.SparkStreaming处理完一批次的数据，写偏移量之前挂了，数据怎么保证不重？<br>2.Maxwell的底层原理？<br>3.手写Spring？<br>4.遍历二叉树？<br>5.用过什么算法？<br>6.多线程方面，怎么实现一个主线程，等待其他子线程完成后再运行？<br>7.Maxwell和Cannal的比较？<br>8.direct比较receiver的优势？<br>9.原来是把数据传入到Hive，之后改了架构，怎么把Hive的数据导入到Hbase？<br>10.为什么用Kafka自己存储offset来替代checkpoint，怎么防止了数据双份落地，数据双份是指什么？<br>11.单例用过吗？</p><p>平安：<br>1.问项目，流程，业务？<br>2.数据量，增量？<br>3.几个人开发的，代码量多少？<br>4.你主要做什么的？<br>5.什么场景，用SparkSql分析什么东西？</p><p>总结：<br>基本上都是围绕项目来面，第一家问的比较少，而且都是关于项目；微盟的面试官做的项目，<br>跟简历上的项目，架构上基本一样，所以问的比较深，问我Maxwell的底层原理，对比Cannal有什么优势，<br>为什么选择它，这个我没回答上来，后来让手写Spring，算法，后来就让我走了；<br>平安也是基本围绕项目，业务，数据量，没问什么技术，而且我说了关于优化的点(面试官说不要说网上都有的东西)。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 22:24:17 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;伟梦：&lt;br&gt;1.主要还是项目？&lt;br&gt;基本上没问什么技术，我就说了一遍项目流程，&lt;br&gt;然后说几个优化点，比如上次讲的血案，我也顺带提了一下。&lt;br&gt;2.在大数据中，有没有什么是不足的，遇到过什么问题？&lt;br&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>spark中配置启用LZO压缩</title>
    <link href="http://yoursite.com/2018/08/20/spark%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%90%AF%E7%94%A8LZO%E5%8E%8B%E7%BC%A9/"/>
    <id>http://yoursite.com/2018/08/20/spark中配置启用LZO压缩/</id>
    <published>2018-08-19T16:00:00.000Z</published>
    <updated>2019-05-19T13:59:58.484Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --><p>Spark中配置启用LZO压缩，步骤如下：</p><h3 id="一、spark-env-sh配置"><a href="#一、spark-env-sh配置" class="headerlink" title="一、spark-env.sh配置"></a>一、spark-env.sh配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h3 id="二、spark-defaults-conf配置"><a href="#二、spark-defaults-conf配置" class="headerlink" title="二、spark-defaults.conf配置"></a>二、spark-defaults.conf配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br></pre></td></tr></table></figure><p><font color="#FF4500">注：指向编译生成lzo的jar包</font><br><a id="more"></a></p><h3 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h3><h4 id="1、读取Lzo文件"><a href="#1、读取Lzo文件" class="headerlink" title="1、读取Lzo文件"></a>1、读取Lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val page_views = sc.textFile(&quot;/user/hive/warehouse/page_views_lzo/page_views.dat.lzo&quot;)</span><br></pre></td></tr></table></figure><h4 id="2、写出lzo文件"><a href="#2、写出lzo文件" class="headerlink" title="2、写出lzo文件"></a>2、写出lzo文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br><span class="line">scala&gt; import com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">scala&gt; val lzoTest = sc.parallelize(1 to 10)</span><br><span class="line">scala&gt; lzoTest.saveAsTextFile(&quot;/input/test_lzo&quot;, classOf[LzopCodec])</span><br></pre></td></tr></table></figure><p>结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@spark220 common]$ hdfs dfs -ls /input/test_lzo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2018-03-16 23:24 /input/test_lzo/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         60 2018-03-16 23:24 /input/test_lzo/part-00000.lzo</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         61 2018-03-16 23:24 /input/test_lzo/part-00001.lzo</span><br></pre></td></tr></table></figure><p></p><p>至此配置与测试完成。</p><h3 id="四、配置与测试中存问题"><a href="#四、配置与测试中存问题" class="headerlink" title="四、配置与测试中存问题"></a>四、配置与测试中存问题</h3><h4 id="1、引用native，缺少LD-LIBRARY-PATH"><a href="#1、引用native，缺少LD-LIBRARY-PATH" class="headerlink" title="1、引用native，缺少LD_LIBRARY_PATH"></a>1、引用native，缺少LD_LIBRARY_PATH</h4><h5 id="1-1、错误提示："><a href="#1-1、错误提示：" class="headerlink" title="1.1、错误提示："></a>1.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.RuntimeException: native-lzo library not available</span><br><span class="line">  at com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:120)</span><br><span class="line">  at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178)</span><br><span class="line">  at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:111)</span><br><span class="line">  at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:246)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:245)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:203)</span><br><span class="line">  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br><span class="line">  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)</span><br><span class="line">  at org.apache.spark.scheduler.Task.run(Task.scala:108)</span><br><span class="line">  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><h5 id="1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容："><a href="#1-2、解决办法：在spark的conf中配置spark-evn-sh，增加以下内容：" class="headerlink" title="1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容："></a>1.2、解决办法：在spark的conf中配置spark-evn.sh，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*</span><br></pre></td></tr></table></figure><h4 id="2、无法找到LzopCodec类"><a href="#2、无法找到LzopCodec类" class="headerlink" title="2、无法找到LzopCodec类"></a>2、无法找到LzopCodec类</h4><h5 id="2-1、错误提示："><a href="#2-1、错误提示：" class="headerlink" title="2.1、错误提示："></a>2.1、错误提示：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzopCodec not found.</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)</span><br><span class="line">    at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzopCodec not found</span><br><span class="line">    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1980)</span><br><span class="line">    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)</span><br></pre></td></tr></table></figure><h5 id="2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容："><a href="#2-2、解决办法：在spark的conf中配置spark-defaults-conf，增加以下内容：" class="headerlink" title="2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容："></a>2.2、解决办法：在spark的conf中配置spark-defaults.conf，增加以下内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar</span><br><span class="line">spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.ja</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Spark中配置启用LZO压缩，步骤如下：&lt;/p&gt;&lt;h3 id=&quot;一、spark-env-sh配置&quot;&gt;&lt;a href=&quot;#一、spark-env-sh配置&quot; class=&quot;headerlink&quot; title=&quot;一、spark-env.sh配置&quot;&gt;&lt;/a&gt;一、spark-env.sh配置&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/app/hadoop-2.6.0-cdh5.7.0/lib/native&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export SPARK_CLASSPATH=$SPARK_CLASSPATH:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/tools/lib/*:/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/jars/*&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h3 id=&quot;二、spark-defaults-conf配置&quot;&gt;&lt;a href=&quot;#二、spark-defaults-conf配置&quot; class=&quot;headerlink&quot; title=&quot;二、spark-defaults.conf配置&quot;&gt;&lt;/a&gt;二、spark-defaults.conf配置&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;spark.driver.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark.executor.extraClassPath /app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.19.jar&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;font color=&quot;#FF4500&quot;&gt;注：指向编译生成lzo的jar包&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>HDFS之垃圾回收箱配置及使用</title>
    <link href="http://yoursite.com/2018/07/18/HDFS%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%B1%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/07/18/HDFS之垃圾回收箱配置及使用/</id>
    <published>2018-07-17T16:00:00.000Z</published>
    <updated>2019-05-19T13:53:51.770Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --><p>HDFS为每个用户创建一个回收站:<br>目录:/user/用户/.Trash/Current, 系统回收站都有一个周期,周期过后hdfs会彻底删除清空,周期内可以恢复。<br><a id="more"></a></p><h4 id="一、HDFS删除文件-无法恢复"><a href="#一、HDFS删除文件-无法恢复" class="headerlink" title="一、HDFS删除文件,无法恢复"></a>一、HDFS删除文件,无法恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">Deleted /123.log</span><br></pre></td></tr></table></figure><h4 id="二、-启用回收站功能"><a href="#二、-启用回收站功能" class="headerlink" title="二、 启用回收站功能"></a>二、 启用回收站功能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vim core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多长时间创建CheckPoint NameNode节点上运行的CheckPointer </span><br><span class="line">从Current文件夹创建CheckPoint; 默认: 0 由fs.trash.interval项指定 --&gt;</span><br><span class="line">&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;!--多少分钟.Trash下的CheckPoint目录会被删除,</span><br><span class="line">该配置服务器设置优先级大于客户端，默认:不启用 --&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;  -- 清除周期分钟(24小时)</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="1、重启hdfs服务"><a href="#1、重启hdfs服务" class="headerlink" title="1、重启hdfs服务"></a>1、重启hdfs服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sbin]$ ./stop-dfs.sh</span><br><span class="line">[hadoop@hadoop001 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure><h5 id="2、测试回收站功能"><a href="#2、测试回收站功能" class="headerlink" title="2、测试回收站功能"></a>2、测试回收站功能</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -put 123.log /</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /123.log</span><br></pre></td></tr></table></figure><h5 id="文件删除成功存放回收站路径下"><a href="#文件删除成功存放回收站路径下" class="headerlink" title="文件删除成功存放回收站路径下"></a>文件删除成功存放回收站路径下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -rm /123.log</span><br><span class="line">18/05/23 11:32:50 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://192.168.0.129:9000/123.log&apos; to trash at: hdfs://192.168.0.129:9000/user/hadoop/.Trash/Current/123.log</span><br><span class="line">[hadoop@hadoop001 opt]$ hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="恢复文件"><a href="#恢复文件" class="headerlink" title="恢复文件"></a>恢复文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mv /user/hadoop/.Trash/Current/123.log /456.log</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        162 2018-05-23 11:30 /456.log</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-05-23 11:32 /user</span><br></pre></td></tr></table></figure><h5 id="删除文件跳过回收站"><a href="#删除文件跳过回收站" class="headerlink" title="删除文件跳过回收站"></a>删除文件跳过回收站</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]$ hdfs dfs -rm -skipTrash /rz.log1</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -rm -skipTrash /456.log</span><br><span class="line">Deleted /456.log</span><br></pre></td></tr></table></figure><p>源码参考：<br><a href="https://blog.csdn.net/tracymkgld/article/details/17557655" target="_blank" rel="noopener">https://blog.csdn.net/tracymkgld/article/details/17557655</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 22:03:32 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;HDFS为每个用户创建一个回收站:&lt;br&gt;目录:/user/用户/.Trash/Current, 系统回收站都有一个周期,周期过后hdfs会彻底删除清空,周期内可以恢复。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark序列化，你了解吗</title>
    <link href="http://yoursite.com/2018/07/16/Spark%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
    <id>http://yoursite.com/2018/07/16/Spark序列化，你了解吗/</id>
    <published>2018-07-15T16:00:00.000Z</published>
    <updated>2019-05-17T14:24:36.362Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 17 2019 22:28:01 GMT+0800 (GMT+08:00) --><p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。<br><a id="more"></a></p><h3 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h3><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p><h4 id="测试代码："><a href="#测试代码：" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="测试结果："><a href="#测试结果：" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_1.png" alt="enter description here"></p><h3 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h3><p>Spark还可以使用Kryo库（版本2）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p><h4 id="测试代码：-1"><a href="#测试代码：-1" class="headerlink" title="测试代码："></a>测试代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/716_2.png" alt="enter description here"><br>测试结果中发现，使用 Kryo serialization 的序列化对象 比使用 Java serialization的序列化对象要大，与描述的不一样，这是为什么呢？<br>查找官网，发现这么一句话 Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.。<br>修改代码后在测试一次<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hihi.learn.sparkCore</span><br><span class="line"></span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">case class Student(id: String, name: String, age: Int, gender: String)</span><br><span class="line"></span><br><span class="line">object SerializationDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local[2]&quot;)</span><br><span class="line">      .setAppName(&quot;SerializationDemo&quot;)</span><br><span class="line">      .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">      .registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val stduentArr = new ArrayBuffer[Student]()</span><br><span class="line">    for (i &lt;- 1 to 1000000) &#123;</span><br><span class="line">      stduentArr += (Student(i + &quot;&quot;, i + &quot;a&quot;, 10, &quot;male&quot;))</span><br><span class="line">    &#125;</span><br><span class="line">    val JavaSerialization = sc.parallelize(stduentArr)</span><br><span class="line">    JavaSerialization.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line"></span><br><span class="line">    while(true) &#123;</span><br><span class="line">      Thread.sleep(10000)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="测试结果：-1"><a href="#测试结果：-1" class="headerlink" title="测试结果："></a>测试结果：</h4><p><img src="/assets/blogImg/716_3.png" alt="enter description here"></p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是使用Kryo需要将自定义的类先注册进去，使用起来比Java serialization麻烦。自从Spark 2.0.0以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。<br>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line"> * Component which configures serialization, compression and encryption for various Spark</span><br><span class="line"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span><br><span class="line"> */</span><br><span class="line">private[spark] class SerializerManager(</span><br><span class="line">    defaultSerializer: Serializer,</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    encryptionKey: Option[Array[Byte]]) &#123;</span><br><span class="line"></span><br><span class="line">  def this(defaultSerializer: Serializer, conf: SparkConf) = this(defaultSerializer, conf, None)</span><br><span class="line"></span><br><span class="line">  private[this] val kryoSerializer = new KryoSerializer(conf)</span><br><span class="line"></span><br><span class="line">  private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]]</span><br><span class="line">  private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = &#123;</span><br><span class="line">    val primitiveClassTags = Set[ClassTag[_]](</span><br><span class="line">      ClassTag.Boolean,</span><br><span class="line">      ClassTag.Byte,</span><br><span class="line">      ClassTag.Char,</span><br><span class="line">      ClassTag.Double,</span><br><span class="line">      ClassTag.Float,</span><br><span class="line">      ClassTag.Int,</span><br><span class="line">      ClassTag.Long,</span><br><span class="line">      ClassTag.Null,</span><br><span class="line">      ClassTag.Short</span><br><span class="line">    )</span><br><span class="line">    val arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">    primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri May 17 2019 22:28:01 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。&lt;br&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 状态管理函数，你了解吗</title>
    <link href="http://yoursite.com/2018/06/25/Spark%20Streaming%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%97/"/>
    <id>http://yoursite.com/2018/06/25/Spark Streaming 状态管理函数，你了解吗/</id>
    <published>2018-06-24T16:00:00.000Z</published>
    <updated>2019-05-16T11:42:29.333Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu May 16 2019 19:43:16 GMT+0800 (GMT+08:00) --><a id="more"></a><p>Spark Streaming 状态管理函数包括updateStateByKey和mapWithState</p><h4 id="一、updateStateByKey"><a href="#一、updateStateByKey" class="headerlink" title="一、updateStateByKey"></a>一、updateStateByKey</h4><p>官网原话：In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p><p>统计全局的key的状态，但是就算没有数据输入，他也会在每一个批次的时候返回之前的key的状态。</p><p>这样的缺点：如果数据量太大的话，我们需要checkpoint数据会占用较大的存储。而且效率也不高<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//[root@bda3 ~]# nc -lk 9999  </span><br><span class="line">object StatefulWordCountApp &#123;  </span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line">    val sparkConf = new SparkConf()  </span><br><span class="line">      .setAppName(&quot;StatefulWordCountApp&quot;)  </span><br><span class="line">      .setMaster(&quot;local[2]&quot;)  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(10))  </span><br><span class="line">    //注意：updateStateByKey必须设置checkpoint目录  </span><br><span class="line">    ssc.checkpoint(&quot;hdfs://bda2:8020/logs/realtime&quot;)  </span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bda3&quot;,9999)  </span><br><span class="line"></span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).map((_,1))  </span><br><span class="line">      .updateStateByKey(updateFunction).print()  </span><br><span class="line"></span><br><span class="line">    ssc.start()  // 一定要写  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">  /*状态更新函数  </span><br><span class="line">  * @param currentValues  key相同value形成的列表  </span><br><span class="line">  * @param preValues      key对应的value，前一状态  </span><br><span class="line">  * */  </span><br><span class="line">  def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;  </span><br><span class="line">    val curr = currentValues.sum   //seq列表中所有value求和  </span><br><span class="line">    val pre = preValues.getOrElse(0)  //获取上一状态值  </span><br><span class="line">    Some(curr + pre)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="二、mapWithState-效率更高，生产中建议使用"><a href="#二、mapWithState-效率更高，生产中建议使用" class="headerlink" title="二、mapWithState  (效率更高，生产中建议使用)"></a>二、mapWithState (效率更高，生产中建议使用)</h4><p>mapWithState：也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。</p><p>这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储。</p><p>官方代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/**  </span><br><span class="line"> * Counts words cumulatively in UTF8 encoded, &apos;\n&apos; delimited text received from the network every  </span><br><span class="line"> * second starting with initial value of word count.  </span><br><span class="line"> * Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;  </span><br><span class="line"> *   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive  </span><br><span class="line"> *   data.  </span><br><span class="line"> *  </span><br><span class="line"> * To run this on your local machine, you need to first run a Netcat server  </span><br><span class="line"> *    `$ nc -lk 9999`  </span><br><span class="line"> * and then run the example  </span><br><span class="line"> *    `$ bin/run-example  </span><br><span class="line"> *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`  </span><br><span class="line"> */  </span><br><span class="line">object StatefulNetworkWordCount &#123;  </span><br><span class="line">  def main(args: Array[String]) &#123;  </span><br><span class="line">    if (args.length &lt; 2) &#123;  </span><br><span class="line">      System.err.println(&quot;Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;)  </span><br><span class="line">      System.exit(1)  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    StreamingExamples.setStreamingLogLevels()  </span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;StatefulNetworkWordCount&quot;)  </span><br><span class="line">    // Create the context with a 1 second batch size  </span><br><span class="line">    val ssc = new StreamingContext(sparkConf, Seconds(1))  </span><br><span class="line">    ssc.checkpoint(&quot;.&quot;)  </span><br><span class="line"></span><br><span class="line">    // Initial state RDD for mapWithState operation  </span><br><span class="line">    val initialRDD = ssc.sparkContext.parallelize(List((&quot;hello&quot;, 1), (&quot;world&quot;, 1)))  </span><br><span class="line"></span><br><span class="line">    // Create a ReceiverInputDStream on target ip:port and count the  </span><br><span class="line">    // words in input stream of \n delimited test (eg. generated by &apos;nc&apos;)  </span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt)  </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))  </span><br><span class="line">    val wordDstream = words.map(x =&gt; (x, 1))  </span><br><span class="line"></span><br><span class="line">    // Update the cumulative count using mapWithState  </span><br><span class="line">    // This will give a DStream made of state (which is the cumulative count of the words)  </span><br><span class="line">    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123;  </span><br><span class="line">      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)  </span><br><span class="line">      val output = (word, sum)  </span><br><span class="line">      state.update(sum)  </span><br><span class="line">      output  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    val stateDstream = wordDstream.mapWithState(  </span><br><span class="line">      StateSpec.function(mappingFunc).initialState(initialRDD))  </span><br><span class="line">    stateDstream.print()  </span><br><span class="line">    ssc.start()  </span><br><span class="line">    ssc.awaitTermination()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu May 16 2019 19:43:16 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark和DL/AI结合，谁与争锋? 期待Spark3.0的到来！</title>
    <link href="http://yoursite.com/2018/06/22/AI%E7%BB%93%E5%90%88%EF%BC%8C%E8%B0%81%E4%B8%8E%E4%BA%89%E9%94%8B%20/"/>
    <id>http://yoursite.com/2018/06/22/AI结合，谁与争锋 /</id>
    <published>2018-06-21T16:00:00.000Z</published>
    <updated>2019-05-15T11:48:37.756Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:22 GMT+0800 (GMT+08:00) --><p><img src="/assets/blogImg/0622_1.png" alt="enter description here"><br><a id="more"></a></p><p>不知各位，是否关注社区的发展？关注Spark呢？</p><p>官网的Spark图标和解释语已经发生变化了。</p><p>然而在6-18号，社区提出Spark and DL/AI相结合，这无比再一次说明，Spark在大数据的地位是无法撼动的！期待Spark3.0的到来！</p><p>接下来对SPARK-24579的翻译:</p><p>在大数据和人工智能的十字路口，我们看到了Apache Spark作为一个统一的分析引擎以及AI框架如TensorFlow和Apache MXNet (正在孵化中)的兴起及这两大块的巨大成功 。</p><p>大数据和人工智能都是推动企业创新的不可或缺的组成部分， 两个社区的多次尝试，使他们结合在一起。</p><p>我们看到AI社区的努力，为AI框架实现数据解决方案，如TF.DATA和TF.Tror。然而，50+个数据源和内置SQL、数据流和流特征，Spark仍然是对于大数据社区选择。</p><p>这就是为什么我们看到许多努力,将DL/AI框架与Spark结合起来，以利用它的力量，例如，Spark数据源TFRecords、TensorFlowOnSpark, TensorFrames等。作为项目Hydrogen的一部分，这个SPIP将Spark+AI从不同的角度统一起来。</p><p>没有在Spark和外部DL/AI框架之间交换数据，这些集成都是不可能的,也有性能问题。然而，目前还没有一种标准的方式来交换数据，因此实现和性能优化就陷入了困境。例如，在Python中，TensorFlowOnSpark使用Hadoop InputFormat/OutputFormat作为TensorFlow的TFRecords，来加载和保存数据，并将RDD数据传递给TensorFlow。TensorFrames使用TensorFlow的Java API，转换为 Spark DataFrames Rows to/from TensorFlow Tensors 。我们怎样才能降低复杂性呢?</p><p>这里的建议是标准化Spark和DL/AI框架之间的数据交换接口(或格式)，并优化从/到这个接口的数据转换。因此，DL/AI框架可以利用Spark从任何地方加载数据，而无需花费额外的精力构建复杂的数据解决方案，比如从生产数据仓库读取特性或流模型推断。Spark用户可以使用DL/AI框架，而无需学习那里实现的特定数据api。而且双方的开发人员都可以独立地进行性能优化，因为接口本身不会带来很大的开销。</p><p>ISSUE: <a href="https://issues.apache.org/jira/browse/SPARK-24579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24579</a><br>若泽数据，星星本人水平有限，翻译多多包涵。</p><p>对了忘记说了，本ISSUE有个PDF文档，赶快去下载吧。<br><a href="https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf" target="_blank" rel="noopener">https://issues.apache.org/jira/secure/attachment/12928222/%5BSPARK-24579%5D%20SPIP_%20Standardize%20Optimized%20Data%20Exchange%20between%20Apache%20Spark%20and%20DL%252FAI%20Frameworks%20.pdf</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed May 15 2019 19:56:22 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/0622_1.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark MLlib" scheme="http://yoursite.com/categories/Spark-MLlib/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>最前沿！带你读Structured Streaming重量级论文！</title>
    <link href="http://yoursite.com/2018/06/14/%E6%9C%80%E5%89%8D%E6%B2%BF%EF%BC%81%E5%B8%A6%E4%BD%A0%E8%AF%BBStructured%20Streaming%E9%87%8D%E9%87%8F%E7%BA%A7%E8%AE%BA%E6%96%87%EF%BC%81/"/>
    <id>http://yoursite.com/2018/06/14/最前沿！带你读Structured Streaming重量级论文！/</id>
    <published>2018-06-13T16:00:00.000Z</published>
    <updated>2019-05-15T11:45:15.951Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="1-论文下载地址"><a href="#1-论文下载地址" class="headerlink" title="1.论文下载地址"></a>1.论文下载地址</h4><p><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf</a></p><h4 id="2-前言"><a href="#2-前言" class="headerlink" title="2.前言"></a>2.前言</h4><p>建议首先阅读Structured Streaming官网：<a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a><br>以及这两篇Databricks在2016年关于Structured Streaming的文章：</p><p><a href="https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html</a></p><p><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</a></p><p>言归正传<br>该论文收录自2018年ACM SIGMOD会议，是由美国计算机协会（ACM）发起的、在数据库领域具有最高学术地位的国际性学术会议。论文的作者为Databricks的工程师及Spark的开发者，其权威性、重要程度不言而喻。文章开头为该论文的下载地址，供读者阅读交流。本文对该论文进行简要的总结，希望大家能够下载原文细细品读，了解最前沿的大数据技术。</p><h4 id="3-论文简要总结"><a href="#3-论文简要总结" class="headerlink" title="3.论文简要总结"></a>3.论文简要总结</h4><p>题目：Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark</p><h5 id="3-1-摘要"><a href="#3-1-摘要" class="headerlink" title="3.1 摘要"></a>3.1 摘要</h5><p>摘要是一篇论文的精髓，这里给出摘要完整的翻译。<br>随着实时数据的普遍存在，我们需要可扩展的、易用的、易于集成的流式处理系统。结构化流是基于我们对Spark Streaming的经验开发出来的高级别的Spark流式API。结构化流与其他现有的流式API，如谷歌的Dataflow，主要有两点不同。第一，它是一个基于自动增量化的关系型查询API，无需用户自己构建DAG；第二，结构化流旨在于支持端到端的实时应用并整合流与批处理的交互分析。在实践中，我们发现这种整合是一个关键的挑战。结构化流通过Spark SQL的代码生成引擎实现了很高的性能，是Apache Flink的两倍以及Apache Kafka的90倍。它还提供了丰富的运行特性，如回滚、代码更新以及流/批混合执行。最后我们描述了系统的设计以及部署在Databricks几百个生产节点的一个用例。</p><h5 id="3-2-流式处理面临的挑战"><a href="#3-2-流式处理面临的挑战" class="headerlink" title="3.2 流式处理面临的挑战"></a>3.2 流式处理面临的挑战</h5><p>(1) 复杂、低级别的API<br>(2) 端到端应用的集成<br>(3) 运行时挑战：容灾，代码更新，监控等<br>(4) 成本和性能挑战</p><h5 id="3-3-结构化流基本概念"><a href="#3-3-结构化流基本概念" class="headerlink" title="3.3 结构化流基本概念"></a>3.3 结构化流基本概念</h5><p><img src="/assets/blogImg/614_1.png" alt="enter description here"><br>图1 结构化流的组成部分</p><p>(1) Input and Output<br>Input sources 必须是 replayable 的，支持节点宕机后从当前输入继续读取。例如：Apache Kinesis和Apache Kafka。<br>Output sinks 必须支持 idempotent （幂等），确保在节点宕机时可靠的恢复。<br>(2) APIs<br>编写结构化流程序时，可以使用Spark SQL的APIs：DataFrame和SQL来查询streams和tables，该查询定义了一个output table（输出表），用来接收来自steam的数据。engine决定如何计算并将输出表 incrementally（增量地）写入sink。不同的sinks支持不同的output modes（输出模式，后面会提到）。<br>为了处理流式数据，结构化流还增加了一些APIs与已有的Spark SQL API相配合：</p><ul><li>a. Triggers 控制engine多久执行一次计算</li><li>b. event time 是数据源的时间戳；watermark 策略，与event time 相差一段时间后不再接收数据。</li><li>c.Stateful operator（状态算子），类似于Spark Streaming 的updateStateByKey。</li></ul><p>(3) 执行<br>一旦接收到了查询，结构化流就会进行优化递增，并开始执行。结构化流使用两种持久化存储的方式实现容错：</p><ul><li>a.write-ahead log （WAL：预写日志）持续追踪哪些数据已被执行，确保数据的可靠写入。</li><li>b.系统采用大规模的 state store（状态存储）来保存长时间运行的聚合算子的算子状态快照。</li></ul><h5 id="3-4-编程模型"><a href="#3-4-编程模型" class="headerlink" title="3.4 编程模型"></a>3.4 编程模型</h5><p>结构化流将谷歌的Dataflow、增量查询和Spark Streaming 结合起来，以便在Spark SQL下实现流式处理。</p><ul><li>a. A Short Example<pre><code>首先从一个批处理作业开始，统计一个web应用在不同国家的点击数。假设输入数据是一个JSON文件，输出一个Parquet文件，该作业可以通过DataFrame来完成：</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read from static data</span><br><span class="line">2data = spark . read . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a static data sink</span><br><span class="line">6counts . write . format (&quot; parquet &quot;). save (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure></li></ul><p>把该作业变成使用结构化流仅仅需要改变输入和输出源，例如，如果新的JSON文件continually（持续地）上传，我们只需要改变第一行和最后一行。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1// Define a DataFrame to read streaming data</span><br><span class="line">2data = spark . readStream . format (&quot; json &quot;). load (&quot;/in&quot;)</span><br><span class="line">3// Transform it to compute a result</span><br><span class="line">4counts = data . groupBy ($&quot; country &quot;). count ()</span><br><span class="line">5// Write to a streaming data sink</span><br><span class="line">6counts . writeStream . format (&quot; parquet &quot;)</span><br><span class="line">7. outputMode (&quot; complete &quot;). start (&quot;/ counts &quot;)</span><br></pre></td></tr></table></figure><p></p><p>结构化流也支持 windowing（窗口）和通过Spark SQL已存在的聚合算子处理event time。例如：我们可以通过修改中间的代码，计算1小时的滑动窗口，每五分钟前进一次：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1// Count events by windows on the &quot; time &quot; field</span><br><span class="line">2data . groupBy ( window ($&quot; time &quot;,&quot;1h&quot;,&quot;5min&quot;)). count ()</span><br></pre></td></tr></table></figure><p></p><ul><li>b. 编程模型语义<br><img src="/assets/blogImg/614_2.png" alt="enter description here"></li></ul><p>图 2 两种输出模式</p><ul><li>i. 每一个输入源提供了一个基于时间的部分有序的记录集（set of records），例如，Kafka将流式数据分为各自有序的partitions。</li><li>ii. 用户提供跨输入数据执行的查询，该输入数据可以在任意给定的处理时间点输出一个 result table（结果表）。结构化流总会产生与所有输入源的数据的前缀上（prefix of the data in all input sources）查询相一致的结果。</li><li>iii. Triggers 告诉系统何时去运行一个新的增量计算，何时更新result table。例如，在微批处理模式，用户希望会每分钟触发一次增量计算。</li><li>iiii. engine支持三种output mode：<pre><code>  Complete：engine一次写所有result table。  Append：engine仅仅向sink增加记录。  Update：engine基于key更新每一个record，更新值改变的keys。该模型有两个特性：第一，结果表的内容独立于输出模式。第二，该模型具有很强的语义一致性，被称为prefix consistency。</code></pre>c.流式算子<pre><code>加入了两种类型的算子：watermarking算子告诉系统何时关闭event time window和输出结果；结构化流允许用户通过withWatermark算子来设置一个watermark，该算子给系统设置一个给定时间戳C的延迟阈值tC，在任意时间点，C的watermark是max（C）-tC。stateful operators允许用户编写自定义逻辑来实现复杂的功能。</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1// Define an update function that simply tracks the</span><br><span class="line"> 2// number of events for each key as its state , returns</span><br><span class="line"> 3// that as its result , and times out keys after 30 min.</span><br><span class="line"> 4def updateFunc (key: UserId , newValues : Iterator [ Event ],</span><br><span class="line"> 5state : GroupState [Int ]): Int = &#123;</span><br><span class="line"> 6val totalEvents = state .get () + newValues . size ()</span><br><span class="line"> 7state . update ( totalEvents )</span><br><span class="line"> 8state . setTimeoutDuration (&quot;30 min&quot;)</span><br><span class="line"> 9return totalEvents</span><br><span class="line">10&#125;</span><br><span class="line">11// Use this update function on a stream , returning a</span><br><span class="line">12// new table lens that contains the session lengths .</span><br><span class="line">13lens = events . groupByKey ( event =&gt; event . userId )</span><br><span class="line">14. mapGroupsWithState ( updateFunc )</span><br></pre></td></tr></table></figure></li></ul><p>用mapGroupWithState算子来追踪每个会话的事件数量，30分钟后关闭会话。</p><h5 id="3-5-运行特性"><a href="#3-5-运行特性" class="headerlink" title="3.5 运行特性"></a>3.5 运行特性</h5><p>(1) 代码更新（code update）<br>开发者能够在编程过程中更新UDF，并且可以简单的重启以使用新版本的代码。<br>(2) 手动回滚（manual rollback）<br>有时在用户发现之前，程序会输出错误的结果，因此回滚至关重要。结构化流很容易定位问题所在。同时手动回滚与前面提到的prefix consistency有很好的交互。<br>(3) 流式和批次混合处理<br>这是结构化流最显而易见的好处，用户能够共用流式处理和批处理作业的代码。<br>(4) 监控<br>结构化流使用Spark已有的API和结构化日志来报告信息，例如处理过的记录数量，跨网络的字节数等。这些接口被Spark开发者所熟知，并易于连接到不同的UI工具。</p><h5 id="3-6-生产用例与总结"><a href="#3-6-生产用例与总结" class="headerlink" title="3.6 生产用例与总结"></a>3.6 生产用例与总结</h5><p>给出简要架构图，篇幅原因不再赘述，希望详细了解的下载论文自行阅读。本文只挑选了部分关键点进行了浅层次的叙述，希望读者能够将论文下载下来认真品读，搞懂开发者的开发思路，跟上大数据的前沿步伐。<br><img src="/assets/blogImg/614_3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>生产开发必用-Spark RDD转DataFrame的两种方法</title>
    <link href="http://yoursite.com/2018/06/14/%E7%94%9F%E4%BA%A7%E5%BC%80%E5%8F%91%E5%BF%85%E7%94%A8-Spark%20RDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/06/14/生产开发必用-Spark RDD转DataFrame的两种方法/</id>
    <published>2018-06-13T16:00:00.000Z</published>
    <updated>2019-05-13T12:14:30.405Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><p>本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:<br><a href="https://blog.csdn.net/lemonzhaotao/article/details/80211231" target="_blank" rel="noopener">https://blog.csdn.net/lemonzhaotao/article/details/80211231</a></p><p>在本篇文章中，将介绍RDD转换为DataFrame的2种方式</p><p>官网之RDD转DF:<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds</a><br><a id="more"></a><br>DataFrame 与 RDD 的交互</p><p>Spark SQL它支持两种不同的方式转换已经存在的RDD到DataFrame</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>第一种方式是使用反射的方式，用反射去推倒出来RDD里面的schema。这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。<br>对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常非常多的，是不可能只有20来个字段的。<br>示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 1</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runInferSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  import spark.implicits._</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  val df = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">              .map(x =&gt; People(x(0), x(1).trim.toInt))  //将rdd的每一行都转换成了一个people</span><br><span class="line">              .toDF         //必须先导入import spark.implicits._  不然这个方法会报错</span><br><span class="line">  df.show()</span><br><span class="line">  df.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">  // 这个DF包含了两个字段name和age</span><br><span class="line">  val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">  // teenager(0)代表第一个字段</span><br><span class="line">  // 取值的第一种方式：index from zero</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line">  // 取值的第二种方式：byName</span><br><span class="line">  teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;) + &quot;,&quot; + teenager.getAs[Int](&quot;age&quot;)).show()</span><br><span class="line">&#125;</span><br><span class="line">// 注意：case class必须定义在main方法之外；否则会报错</span><br><span class="line">case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p></p><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>创建一个DataFrame，使用编程的方式 这个方式用的非常多。通过编程方式指定schema ，对于第一种方式的schema其实定义在了case class里面了。<br>官网解读：<br>当我们的case class不能提前定义(因为业务处理的过程当中，你的字段可能是在变化的),因此使用case class很难去提前定义。<br>使用该方式创建DF的三大步骤：</p><ul><li>Create an RDD of Rows from the original RDD;</li><li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li><li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * convert rdd to dataframe 2</span><br><span class="line">  * @param spark</span><br><span class="line">  */</span><br><span class="line">private def runProgrammaticSchemaExample(spark:SparkSession): Unit =&#123;</span><br><span class="line">  // 1.转成RDD</span><br><span class="line">  val rdd = spark.sparkContext.textFile(&quot;E:/大数据/data/people.txt&quot;)</span><br><span class="line">  // 2.定义schema，带有StructType的</span><br><span class="line">  // 定义schema信息</span><br><span class="line">  val schemaString = &quot;name age&quot;</span><br><span class="line">  // 对schema信息按空格进行分割</span><br><span class="line">  // 最终fileds里包含了2个StructField</span><br><span class="line">  val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">                            // 字段类型，字段名称判断是不是为空</span><br><span class="line">                           .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">  val schema = StructType(fields)</span><br><span class="line">  // 3.把我们的schema信息作用到RDD上</span><br><span class="line">  //   这个RDD里面包含了一些行</span><br><span class="line">  // 形成Row类型的RDD</span><br><span class="line">  val rowRDD = rdd.map(_.split(&quot;,&quot;))</span><br><span class="line">                  .map(x =&gt; Row(x(0), x(1).trim))</span><br><span class="line">  // 通过SparkSession创建一个DataFrame</span><br><span class="line">  // 传进来一个rowRDD和schema，将schema作用到rowRDD上</span><br><span class="line">  val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="扩展-生产上创建DataFrame的代码举例"><a href="#扩展-生产上创建DataFrame的代码举例" class="headerlink" title="[扩展]生产上创建DataFrame的代码举例"></a>[扩展]生产上创建DataFrame的代码举例</h4><p>在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，这里将展示部分代码：</p><h4 id="Schema的定义"><a href="#Schema的定义" class="headerlink" title="Schema的定义"></a>Schema的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">object AccessConvertUtil &#123;</span><br><span class="line">  val struct = StructType(</span><br><span class="line">    Array(</span><br><span class="line">      StructField(&quot;url&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsType&quot;,StringType),</span><br><span class="line">      StructField(&quot;cmsId&quot;,LongType),</span><br><span class="line">      StructField(&quot;traffic&quot;,LongType),</span><br><span class="line">      StructField(&quot;ip&quot;,StringType),</span><br><span class="line">      StructField(&quot;city&quot;,StringType),</span><br><span class="line">      StructField(&quot;time&quot;,StringType),</span><br><span class="line">      StructField(&quot;day&quot;,StringType)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  /**</span><br><span class="line">    * 根据输入的每一行信息转换成输出的样式</span><br><span class="line">    */</span><br><span class="line">  def parseLog(log:String) = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val splits = log.split(&quot;\t&quot;)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      val traffic = splits(2).toLong</span><br><span class="line">      val ip = splits(3)</span><br><span class="line">      val domain = &quot;http://www.imooc.com/&quot;</span><br><span class="line">      val cms = url.substring(url.indexOf(domain) + domain.length)</span><br><span class="line">      val cmsTypeId = cms.split(&quot;/&quot;)</span><br><span class="line">      var cmsType = &quot;&quot;</span><br><span class="line">      var cmsId = 0l</span><br><span class="line">      if (cmsTypeId.length &gt; 1) &#123;</span><br><span class="line">        cmsType = cmsTypeId(0)</span><br><span class="line">        cmsId = cmsTypeId(1).toLong</span><br><span class="line">      &#125;</span><br><span class="line">      val city = IpUtils.getCity(ip)</span><br><span class="line">      val time = splits(0)</span><br><span class="line">      val day = time.substring(0,10).replace(&quot;-&quot;,&quot;&quot;)</span><br><span class="line">      //这个Row里面的字段要和struct中的字段对应上</span><br><span class="line">      Row(url, cmsType, cmsId, traffic, ip, city, time, day)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt; Row(0)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object SparkStatCleanJob &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkStatCleanJob&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">    val accessRDD = spark.sparkContext.textFile(&quot;/Users/lemon/project/data/access.log&quot;)</span><br><span class="line">    //accessRDD.take(10).foreach(println)</span><br><span class="line">    //RDD ==&gt; DF，创建生成DataFrame</span><br><span class="line">    val accessDF = spark.createDataFrame(accessRDD.map(x =&gt; AccessConvertUtil.parseLog(x)),</span><br><span class="line">      AccessConvertUtil.struct)</span><br><span class="line">    accessDF.coalesce(1).write.format(&quot;parquet&quot;).mode(SaveMode.Overwrite)</span><br><span class="line">            .partitionBy(&quot;day&quot;).save(&quot;/Users/lemon/project/clean&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;本篇文章将介绍Spark SQL中的DataFrame，关于DataFrame的介绍可以参考:&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/lemonzhaotao/article/details/80211231&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/lemonzhaotao/article/details/80211231&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在本篇文章中，将介绍RDD转换为DataFrame的2种方式&lt;/p&gt;&lt;p&gt;官网之RDD转DF:&lt;br&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>你大爷永远是你大爷，RDD血缘关系源码详解！</title>
    <link href="http://yoursite.com/2018/06/13/%E4%BD%A0%E5%A4%A7%E7%88%B7%E6%B0%B8%E8%BF%9C%E6%98%AF%E4%BD%A0%E5%A4%A7%E7%88%B7%EF%BC%8CRDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%EF%BC%81/"/>
    <id>http://yoursite.com/2018/06/13/你大爷永远是你大爷，RDD血缘关系源码详解！/</id>
    <published>2018-06-12T16:00:00.000Z</published>
    <updated>2019-05-13T12:17:09.141Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --><h4 id="一、RDD的依赖关系"><a href="#一、RDD的依赖关系" class="headerlink" title="一、RDD的依赖关系"></a>一、RDD的依赖关系</h4><p>RDD的依赖关系分为两类：宽依赖和窄依赖。我们可以这样认为：</p><ul><li><p>（1）窄依赖：每个parent RDD 的 partition 最多被 child RDD 的一个partition 使用。</p></li><li><p>（2）宽依赖：每个parent RDD partition 被多个 child RDD 的partition 使用。</p></li></ul><p>窄依赖每个 child RDD 的 partition 的生成操作都是可以并行的，而宽依赖则需要所有的 parent RDD partition shuffle 结果得到后再进行。<br><a id="more"></a></p><h4 id="二、org-apache-spark-Dependency-scala-源码解析"><a href="#二、org-apache-spark-Dependency-scala-源码解析" class="headerlink" title="二、org.apache.spark.Dependency.scala 源码解析"></a>二、org.apache.spark.Dependency.scala 源码解析</h4><p>Dependency是一个抽象类：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">  def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。</p><h5 id="（1）NarrowDependency也是一个抽象类"><a href="#（1）NarrowDependency也是一个抽象类" class="headerlink" title="（1）NarrowDependency也是一个抽象类"></a>（1）NarrowDependency也是一个抽象类</h5><p>定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;  </span><br><span class="line">/**</span><br><span class="line">   * Get the parent partitions for a child partition.</span><br><span class="line">   * @param partitionId a partition of the child RDD</span><br><span class="line">   * @return the partitions of the parent RDD that the child partition depends upon</span><br><span class="line">   */</span><br><span class="line">  def getParents(partitionId: Int): Seq[Int]</span><br><span class="line"></span><br><span class="line">  override def rdd: RDD[T] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>窄依赖又有两个具体的实现：OneToOneDependency和RangeDependency。<br>（a）OneToOneDependency指child RDD的partition只依赖于parent RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = List(partitionId)</span><br><span class="line">&#125;</span><br><span class="line">        （b）RangeDependency指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。</span><br><span class="line"></span><br><span class="line">// Denpendency.scala</span><br><span class="line">class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)  </span><br><span class="line">  extends NarrowDependency[T](rdd) &#123;//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引</span><br><span class="line">  override def getParents(partitionId: Int): List[Int] = &#123;    </span><br><span class="line">    if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      List(partitionId - outStart + inStart)//表示于当前索引的相对位置</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      Nil</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h5 id="（2）ShuffleDependency指宽依赖"><a href="#（2）ShuffleDependency指宽依赖" class="headerlink" title="（2）ShuffleDependency指宽依赖"></a>（2）ShuffleDependency指宽依赖</h5><p>表示一个parent RDD的partition会被child RDD的partition使用多次。需要经过shuffle才能形成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Denpendency.scala</span><br><span class="line">class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</span><br><span class="line">    @transient private val _rdd: RDD[_ &lt;: Product2[K, V]],    </span><br><span class="line">    val partitioner: Partitioner,    </span><br><span class="line">    val serializer: Serializer = SparkEnv.get.serializer,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = false)</span><br><span class="line">  extends Dependency[Product2[K, V]] &#123;  //shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的</span><br><span class="line">  override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"></span><br><span class="line">  private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName</span><br><span class="line">  private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName</span><br><span class="line">  private[spark] val combinerClassName: Option[String] =</span><br><span class="line">    Option(reflect.classTag[C]).map(_.runtimeClass.getName)  //获取shuffleId</span><br><span class="line">  val shuffleId: Int = _rdd.context.newShuffleId()  //向shuffleManager注册shuffle信息</span><br><span class="line">  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, this)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。</p><h4 id="三、两种依赖的区分"><a href="#三、两种依赖的区分" class="headerlink" title="三、两种依赖的区分"></a>三、两种依赖的区分</h4><p>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 20:18:11 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;一、RDD的依赖关系&quot;&gt;&lt;a href=&quot;#一、RDD的依赖关系&quot; class=&quot;headerlink&quot; title=&quot;一、RDD的依赖关系&quot;&gt;&lt;/a&gt;一、RDD的依赖关系&lt;/h4&gt;&lt;p&gt;RDD的依赖关系分为两类：宽依赖和窄依赖。我们可以这样认为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;（1）窄依赖：每个parent RDD 的 partition 最多被 child RDD 的一个partition 使用。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;（2）宽依赖：每个parent RDD partition 被多个 child RDD 的partition 使用。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;窄依赖每个 child RDD 的 partition 的生成操作都是可以并行的，而宽依赖则需要所有的 parent RDD partition shuffle 结果得到后再进行。&lt;br&gt;
    
    </summary>
    
      <category term="Spark Core" scheme="http://yoursite.com/categories/Spark-Core/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Java可扩展线程池之ThreadPoolExecutor</title>
    <link href="http://yoursite.com/2018/06/13/Java%E5%8F%AF%E6%89%A9%E5%B1%95%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B9%8BThreadPoolExecutor/"/>
    <id>http://yoursite.com/2018/06/13/Java可扩展线程池之ThreadPoolExecutor/</id>
    <published>2018-06-12T16:00:00.000Z</published>
    <updated>2019-05-15T11:35:24.458Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --><h4 id="1、ThreadPoolExecutor"><a href="#1、ThreadPoolExecutor" class="headerlink" title="1、ThreadPoolExecutor"></a>1、ThreadPoolExecutor</h4><p>我们知道ThreadPoolExecutor是可扩展的,它提供了几个可以在子类中改写的空方法如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;</span><br><span class="line">protected void beforeExecute(Thread t, Runnable r) &#123; &#125;  </span><br><span class="line">protected void terminated() &#123; &#125;</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h4 id="2、为什么要进行扩展？"><a href="#2、为什么要进行扩展？" class="headerlink" title="2、为什么要进行扩展？"></a>2、为什么要进行扩展？</h4><p>因为在实际应用中，可以对线程池运行状态进行跟踪，输出一些有用的调试信息，以帮助故障诊断。</p><h4 id="3、ThreadPoolExecutor-Worker的run方法实现"><a href="#3、ThreadPoolExecutor-Worker的run方法实现" class="headerlink" title="3、ThreadPoolExecutor.Worker的run方法实现"></a>3、ThreadPoolExecutor.Worker的run方法实现</h4><p>通过看源码我们发现 ThreadPoolExecutor的工作线程其实就是Worker实例，Worker.runTask()会被线程池以多线程模式异步调用，则以上三个方法也将被多线程同时访问。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">1// 基于jdk1.8.0_161final void runWorker(Worker w) &#123;</span><br><span class="line"> 2         Thread wt = Thread.currentThread();</span><br><span class="line"> 3         Runnable task = w.firstTask;</span><br><span class="line"> 4         w.firstTask = null;</span><br><span class="line"> 5         w.unlock(); // allow interrupts</span><br><span class="line"> 6         boolean completedAbruptly = true;        </span><br><span class="line"> 7             try &#123;            </span><br><span class="line"> 8             while (task != null || (task = getTask()) != null) &#123;</span><br><span class="line"> 9                  w.lock();                </span><br><span class="line">10              if ((runStateAtLeast(ctl.get(), STOP) ||</span><br><span class="line">11                     (Thread.interrupted() &amp;&amp;</span><br><span class="line">12                      runStateAtLeast(ctl.get(), STOP))) &amp;&amp;</span><br><span class="line">13                    !wt.isInterrupted())</span><br><span class="line">14                    wt.interrupt();               </span><br><span class="line">15              try &#123;</span><br><span class="line">16                    beforeExecute(wt, task);</span><br><span class="line">17                    Throwable thrown = null;                   </span><br><span class="line">18              try &#123;</span><br><span class="line">19                        task.run();</span><br><span class="line">20                    &#125; catch (RuntimeException x) &#123;</span><br><span class="line">21                        thrown = x; throw x;</span><br><span class="line">22                    &#125; catch (Error x) &#123;</span><br><span class="line">23                        thrown = x; throw x;</span><br><span class="line">24                    &#125; catch (Throwable x) &#123;</span><br><span class="line">25                        thrown = x; throw new Error(x);</span><br><span class="line">26                    &#125; finally &#123;</span><br><span class="line">27                        afterExecute(task, thrown);</span><br><span class="line">28                    &#125;</span><br><span class="line">29                &#125; finally &#123;</span><br><span class="line">30                    task = null;</span><br><span class="line">31                    w.completedTasks++;</span><br><span class="line">32                    w.unlock();</span><br><span class="line">33                &#125;</span><br><span class="line">34            &#125;</span><br><span class="line">35            completedAbruptly = false;</span><br><span class="line">36        &#125; finally &#123;</span><br><span class="line">37            processWorkerExit(w, completedAbruptly);</span><br><span class="line">38        &#125;</span><br><span class="line">39    &#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="4、扩展线程池实现"><a href="#4、扩展线程池实现" class="headerlink" title="4、扩展线程池实现"></a>4、扩展线程池实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1public class ExtThreadPool &#123;</span><br><span class="line"> 2    public static class MyTask implements Runnable &#123;</span><br><span class="line"> 3        public String name;        </span><br><span class="line"> 4        public MyTask(String name) &#123;            </span><br><span class="line"> 5          this.name = name;</span><br><span class="line"> 6        &#125;       </span><br><span class="line"> 7        public void run() &#123;</span><br><span class="line"> 8            System.out.println(&quot;正在执行:Thread ID:&quot; + Thread.currentThread().getId() + &quot;,Task Name:&quot; + name);            try &#123;</span><br><span class="line"> 9                Thread.sleep(100);</span><br><span class="line">10            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">11                e.printStackTrace();</span><br><span class="line">12            &#125;</span><br><span class="line">13        &#125;</span><br><span class="line">14    &#125;    </span><br><span class="line">15public static void main(String args[]) throws InterruptedException &#123;</span><br><span class="line">16ExecutorService executorService = new ThreadPoolExecutor( 5,5,0L,</span><br><span class="line">17TimeUnit.MILLISECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()) &#123;            </span><br><span class="line">18protected void beforeExecute(Thread t, Runnable r) &#123;</span><br><span class="line">19 System.out.println(&quot;准备执行：&quot; + ((MyTask) r).name);</span><br><span class="line">20&#125;            </span><br><span class="line">21protected void afterExecute(Thread t, Runnable r) &#123;</span><br><span class="line">22  System.out.println(&quot;执行完成&quot; + ((MyTask) r).name);</span><br><span class="line">23&#125;            </span><br><span class="line">24protected void terminated() &#123;</span><br><span class="line">25  System.out.println(&quot;线程池退出！&quot;);</span><br><span class="line">26&#125;</span><br><span class="line">27&#125;;        </span><br><span class="line">28for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">29 MyTask task = new MyTask(&quot;TASK--&quot; + i);</span><br><span class="line">30            executorService.execute(task);</span><br><span class="line">31            Thread.sleep(10);</span><br><span class="line">32        &#125;</span><br><span class="line">33 executorService.shutdown();</span><br><span class="line">34    &#125;</span><br><span class="line">35&#125;</span><br></pre></td></tr></table></figure><p>输出结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">准备执行：TASK–0 </span><br><span class="line">正在执行:Thread ID:10,Task Name:TASK–0 </span><br><span class="line">准备执行：TASK–1 </span><br><span class="line">正在执行:Thread ID:11,Task Name:TASK–1 </span><br><span class="line">准备执行：TASK–2 </span><br><span class="line">正在执行:Thread ID:12,Task Name:TASK–2 </span><br><span class="line">准备执行：TASK–3 </span><br><span class="line">正在执行:Thread ID:13,Task Name:TASK–3 </span><br><span class="line">准备执行：TASK–4 </span><br><span class="line">正在执行:Thread ID:14,Task Name:TASK–4 </span><br><span class="line">线程池退出！</span><br></pre></td></tr></table></figure><p></p><p>这样就实现了在执行前后进行的一些控制，除此之外我们还可以输出每个线程的执行时间，或者一些其他增强操作。</p><h4 id="5、思考？"><a href="#5、思考？" class="headerlink" title="5、思考？"></a>5、思考？</h4><p>请读者思考shutdownNow和shutdown方法的区别？<br>如何优雅的关闭线程池？</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed May 15 2019 19:56:23 GMT+0800 (GMT+08:00) --&gt;&lt;h4 id=&quot;1、ThreadPoolExecutor&quot;&gt;&lt;a href=&quot;#1、ThreadPoolExecutor&quot; class=&quot;headerlink&quot; title=&quot;1、ThreadPoolExecutor&quot;&gt;&lt;/a&gt;1、ThreadPoolExecutor&lt;/h4&gt;&lt;p&gt;我们知道ThreadPoolExecutor是可扩展的,它提供了几个可以在子类中改写的空方法如下：&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;protected void beforeExecute(Thread t, Runnable r) &amp;#123; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;protected void beforeExecute(Thread t, Runnable r) &amp;#123; &amp;#125;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;protected void terminated() &amp;#123; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark 技术团队开源机器学习平台 MLflow</title>
    <link href="http://yoursite.com/2018/06/12/Apache%20Spark%20%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%BC%80%E6%BA%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%20MLflow/"/>
    <id>http://yoursite.com/2018/06/12/Apache Spark 技术团队开源机器学习平台 MLflow/</id>
    <published>2018-06-11T16:00:00.000Z</published>
    <updated>2019-05-13T08:56:44.124Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --><p>近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。<br><a id="more"></a><br><img src="/assets/blogImg/612_1.png" alt="enter description here"><br>Matei Zaharia 表示当前在使用机器学习的公司普遍存在工具过多、难以跟踪实验、难以重现结果、难以部署等问题。为让机器学习开发变得与传统软件开发一样强大、可预测和普及，许多企业已开始构建内部机器学习平台来管理 ML生命周期。像是 Facebook、Google 和 Uber 就已分别构建了 FBLearner Flow、TFX 和 Michelangelo 来管理数据、模型培训和部署。不过由于这些内部平台存在局限性和绑定性，无法很好地与社区共享成果，其他用户也无法轻易使用。<br>MLflow 正是受现有的 ML 平台启发，主打开放性：</p><ul><li>开放接口：可与任意 ML 库、算法、部署工具或编程语言一起使用。</li><li>开源：开发者可轻松地对其进行扩展，并跨组织共享工作流步骤和模型。<br>MLflow 目前的 alpha 版本包含三个组件：<br><img src="/assets/blogImg/612_2.png" alt="enter description here"><br>其中，MLflow Tracking（跟踪组件）提供了一组 API 和用户界面，用于在运行机器学习代码时记录和查询参数、代码版本、指标和输出文件，以便以后可视化它们。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow</span><br><span class="line"></span><br><span class="line"># Log parameters (key-value pairs)</span><br><span class="line">mlflow.log_param(&quot;num_dimensions&quot;, 8)</span><br><span class="line">mlflow.log_param(&quot;regularization&quot;, 0.1)</span><br><span class="line"></span><br><span class="line"># Log a metric; metrics can be updated throughout the run</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.1)</span><br><span class="line">...</span><br><span class="line">mlflow.log_metric(&quot;accuracy&quot;, 0.45)</span><br><span class="line"></span><br><span class="line"># Log artifacts (output files)</span><br><span class="line">mlflow.log_artifact(&quot;roc.png&quot;)</span><br><span class="line">mlflow.log_artifact(&quot;model.pkl&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>MLflow Projects（项目组件）提供了打包可重用数据科学代码的标准格式。每个项目都只是一个包含代码或 Git 存储库的目录，并使用一个描述符文件来指定它的依赖关系以及如何运行代码。每个 MLflow 项目都是由一个简单的名为 MLproject 的 YAML 文件进行自定义。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name: My Project</span><br><span class="line">conda_env: conda.yaml</span><br><span class="line">entry_points:</span><br><span class="line">  main:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">      regularization: &#123;type: float, default: 0.1&#125;</span><br><span class="line">    command: &quot;python train.py -r &#123;regularization&#125; &#123;data_file&#125;&quot;</span><br><span class="line">  validate:</span><br><span class="line">    parameters:</span><br><span class="line">      data_file: path</span><br><span class="line">    command: &quot;python validate.py &#123;data_file&#125;&quot;</span><br></pre></td></tr></table></figure><p></p><p>MLflow Models（模型组件）提供了一种用多种格式打包机器学习模型的规范，这些格式被称为 “flavor” 。MLflow 提供了多种工具来部署不同 flavor 的模型。每个 MLflow 模型被保存成一个目录，目录中包含了任意模型文件和一个 MLmodel 描述符文件，文件中列出了相应的 flavor 。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_created: 2018-02-21T13:21:34.12</span><br><span class="line">flavors:</span><br><span class="line">  sklearn:</span><br><span class="line">    sklearn_version: 0.19.1</span><br><span class="line">    pickled_model: model.pkl</span><br><span class="line">  python_function:</span><br><span class="line">    loader_module: mlflow.sklearn</span><br><span class="line">    pickled_model: model.pkl</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon May 13 2019 18:23:39 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;近日，来自 Databricks 的 Matei Zaharia 宣布推出开源机器学习平台 MLflow 。Matei Zaharia 是 Apache Spark 和 Apache Mesos 的核心作者，也是 Databrick 的首席技术专家。Databrick 是由 Apache Spark 技术团队所创立的商业化公司。MLflow 目前已处于早期测试阶段，开发者可下载源码体验。&lt;br&gt;
    
    </summary>
    
      <category term="Spark MLlib" scheme="http://yoursite.com/categories/Spark-MLlib/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
</feed>
