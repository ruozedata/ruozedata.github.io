<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-12T03:57:36.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop如何使用lzo压缩完整篇</title>
    <link href="http://yoursite.com/2019/10/12/Hadoop%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8lzo%E5%8E%8B%E7%BC%A9%E5%AE%8C%E6%95%B4%E7%AF%87/"/>
    <id>http://yoursite.com/2019/10/12/Hadoop如何使用lzo压缩完整篇/</id>
    <published>2019-10-11T16:00:00.000Z</published>
    <updated>2019-10-12T03:57:36.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="为啥使用了lzo仍然不能分片"><a href="#为啥使用了lzo仍然不能分片" class="headerlink" title="为啥使用了lzo仍然不能分片"></a>为啥使用了lzo仍然不能分片</h3><p>在hdfs.xml中，有这样的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.blocksize&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;134217728&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这个配置设置了块大小为128M，在mapreduce的过程中，inputformat执行完毕之后，默认就会根据该配置，对文件进行切块(split)，进而根据块的数量来决定map task的数量。</p><p>除了textFile之外，压缩格式中的lzo，bz2也可以进行文件的切块操作。</p><p>但是从一般情况，lzo本身是无法进行切块的——如果直接将大于128M的data.lzo文件作为map的输入时，默认blocksize为128M的情况下，number of splits的值仍然为1，即data.lzo仍然被当为一块直接输入map task。</p><p>所以为了实现lzo的切块，需要为lzo的压缩文件生成一个索引文件data.lzo.index。</p><h3 id="如何生成lzo文件"><a href="#如何生成lzo文件" class="headerlink" title="如何生成lzo文件"></a>如何生成lzo文件</h3><p><code>lzop -v data</code>，就会生成data.lzo文件</p><h3 id="给data-lzo配置索引文件"><a href="#给data-lzo配置索引文件" class="headerlink" title="给data.lzo配置索引文件"></a>给data.lzo配置索引文件</h3><p>需要准备hadoop-lzo-0.4.21-SNAPSHOT.jar，如果没有的话就需要编译生成一下。</p><p>1.安装编译所需文件</p><p><code>yum -y install lzo-devel zlib-devel gcc autoconf automake libtool</code></p><p>2.下载，解压</p><p><code>wget https://github.com/twitter/hadoop-lzo/archive/master.zip</code></p><p>3.修改pom.xml，将其中的hadoop.current.version改为自己的hadoop版本<br>4.编译</p><p>在hadoop-lzo-master/下执行<code>mvn clean package -Dmaven.test.skip=true</code>进行编译，编译好的jar包在hadoop-lzo-master/target/</p><p>5.修改hadoop的配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>6.重启hadoop集群，将data.lzo丢到hdfs里。</p><p>7.创建index文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 使用mapreduce创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/data.lzo</span><br><span class="line"></span><br><span class="line"># 使用本地程序创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /input/data.lzo</span><br></pre></td></tr></table></figure><p>8.执行自己的mapreduce程序的时候，输入路径为/input而非/input/data.lzo，这样就能实现lzo的分片操作了。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="压缩" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="lzo" scheme="http://yoursite.com/tags/lzo/"/>
    
  </entry>
  
  <entry>
    <title>Livy部署及提交Spark作业案例</title>
    <link href="http://yoursite.com/2019/09/06/Livy%E9%83%A8%E7%BD%B2%E5%8F%8A%E6%8F%90%E4%BA%A4Spark%E4%BD%9C%E4%B8%9A%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2019/09/06/Livy部署及提交Spark作业案例/</id>
    <published>2019-09-05T16:00:00.000Z</published>
    <updated>2019-10-14T07:27:29.337Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="Livy安装部署"><a href="#Livy安装部署" class="headerlink" title="Livy安装部署"></a>Livy安装部署</h2><p>官网:<a href="http://livy.incubator.apache.org/get-started/" target="_blank" rel="noopener">http://livy.incubator.apache.org/get-started/</a></p><h3 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 software]$ wget http://mirrors.hust.edu.cn/apache/incubator/livy/0.5.0-incubating/livy-0.5.0-incubating-bin.zip</span><br><span class="line">[hadoop@hadoop001 software]$ unzip livy-0.5.0-incubating-bin.zip</span><br><span class="line">[hadoop@hadoop001 software]$ mv livy-0.5.0-incubating-bin/ ../app/</span><br><span class="line">[hadoop@hadoop001 software]$ cd ../app/livy-0.5.0-incubating-bin/</span><br><span class="line">[hadoop@hadoop001 livy-0.5.0-incubating-bin]$ cd conf/</span><br><span class="line">[hadoop@hadoop001 conf]$ cp livy-env.sh.template livy-env.sh</span><br><span class="line">[hadoop@hadoop001 conf]$ vi livy-env.sh</span><br><span class="line">JAVA_HOME=/opt/app/jdk1.8.0_45</span><br><span class="line">HADOOP_CONF_DIR=/opt/app/hadoop-2.6.0-cdh5.7.0/conf</span><br><span class="line">SPARK_HOME=/opt/app/spark-2.2.0-bin-2.6.0-cdh5.7.0</span><br></pre></td></tr></table></figure><h3 id="修改日志，使其信息能打印在控制台上"><a href="#修改日志，使其信息能打印在控制台上" class="headerlink" title="修改日志，使其信息能打印在控制台上"></a>修改日志，使其信息能打印在控制台上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$vim log4j.properties</span><br><span class="line">log4j.rootCategory=INFO, console</span><br><span class="line">log4j.appender.console=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.target=System.err</span><br><span class="line">log4j.appender.console.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span><br><span class="line">log4j.logger.org.eclipse.jetty=WARN</span><br></pre></td></tr></table></figure><h3 id="启动Livy"><a href="#启动Livy" class="headerlink" title="启动Livy"></a>启动Livy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 livy-0.5.0-incubating-bin]$ ./bin/livy-server</span><br></pre></td></tr></table></figure><p>会报错，信息如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.IOException: Cannot write log directory /opt/app/livy-0.5.0-incubating-bin/logs</span><br><span class="line">                at org.eclipse.jetty.util.RolloverFileOutputStream.setFile(RolloverFileOutputStream.java:219)</span><br><span class="line">                at org.eclipse.jetty.util.RolloverFileOutputStream.&lt;init&gt;(RolloverFileOutputStream.java:166)</span><br><span class="line">                at org.eclipse.jetty.server.NCSARequestLog.doStart(NCSARequestLog.java:228)</span><br><span class="line">                at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)</span><br><span class="line">                at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)</span><br><span class="line">                at org.eclipse.jetty.server.handler.RequestLogHandler.doStart(RequestLogHandler.java:140)</span><br><span class="line">                at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)</span><br><span class="line">                at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)</span><br><span class="line">                at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)</span><br><span class="line">                at org.eclipse.jetty.server.Server.start(Server.java:387)</span><br><span class="line">                at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)</span><br><span class="line">                at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)</span><br><span class="line">                at org.eclipse.jetty.server.Server.doStart(Server.java:354)</span><br><span class="line">                at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)</span><br><span class="line">                at org.apache.livy.server.WebServer.start(WebServer.scala:92)</span><br><span class="line">                at org.apache.livy.server.LivyServer.start(LivyServer.scala:259)</span><br><span class="line">                at org.apache.livy.server.LivyServer$.main(LivyServer.scala:339)</span><br><span class="line">                at org.apache.livy.server.LivyServer.main(LivyServer.scala)</span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>权限问题，需要手动创建logs目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 livy-0.5.0-incubating-bin]$ mkdir logs</span><br></pre></td></tr></table></figure><h3 id="启动成功后进行Web访问："><a href="#启动成功后进行Web访问：" class="headerlink" title="启动成功后进行Web访问："></a>启动成功后进行Web访问：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Welcome to</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:       ____              __</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:      / __/__  ___ _____/ /__</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:     _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:    /___/ .__/\_,_/_/ /_/\_\   version 2.4.2</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:       /_/</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout:</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Using Scala version 2.11.12, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_201</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Branch</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Compiled by user hadoop on 2019-05-01T03:17:40Z</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Revision</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Url</span><br><span class="line">19/08/29 22:26:20 INFO LineBufferedStream: stdout: Type --help for more information.</span><br><span class="line">19/08/29 22:26:20 WARN LivySparkUtils$: Current Spark (2,4) is not verified in Livy, please use it carefully</span><br><span class="line">19/08/29 22:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/08/29 22:26:21 INFO StateStore$: Using BlackholeStateStore for recovery.</span><br><span class="line">19/08/29 22:26:21 INFO BatchSessionManager: Recovered 0 batch sessions. Next session id: 0</span><br><span class="line">19/08/29 22:26:21 INFO InteractiveSessionManager: Recovered 0 interactive sessions. Next session id: 0</span><br><span class="line">19/08/29 22:26:21 INFO InteractiveSessionManager: Heartbeat watchdog thread started.</span><br><span class="line">19/08/29 22:26:21 INFO WebServer: Starting server on http://hadoop000:8998</span><br><span class="line">---------------------------------------------------------------------------------------------</span><br><span class="line">#换成自己的IP地址</span><br><span class="line">http://hadoop000:8998</span><br></pre></td></tr></table></figure><h3 id="Livy配置文件解读"><a href="#Livy配置文件解读" class="headerlink" title="Livy配置文件解读"></a>Livy配置文件解读</h3><ul><li>livy.conf：配置了一些server的信息</li><li><p>spark-blacklist.conf</p><pre><code>会列出来一些spark配置中的一些东西，这些东西用户是不允许被修改掉的给用户的一些东西，有些是不能改的，比如：内存大小的设置、executor的设置这些给用户改，是不放心的；因此有些东西必然是不能够暴露的</code></pre></li><li><p>log4j.properties：日志信息</p></li></ul><p>livy.conf的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$ cp livy.conf.template livy.conf</span><br><span class="line">[hadoop@hadoop001 conf]$ vi livy.conf</span><br><span class="line">livy.server.host = 0.0.0.0</span><br><span class="line">livy.server.port = 8998</span><br><span class="line">livy.spark.master = local[2]</span><br></pre></td></tr></table></figure><h2 id="架构篇"><a href="#架构篇" class="headerlink" title="架构篇"></a>架构篇</h2><p><img src="/assets/pic/2019-09-06-1.png" alt="架构"></p><p>1、有个客户端client，中间有个livy server，后面有spark interactive session和spark batch session（在这2个里面的底层都是有一个SparkContext的）</p><p>2、client发请求过来(http或rest)到livy server，然后会去spark interactive session和spark batch session分别去创建2个session；与spark集群交互打交道，去创建session的方式有2种：http或rpc，现在用的比较多的方式是：rpc</p><p>3、livy server就是一个rest的服务，收到客户端的请求之后，与spark集群进行连接；客户端只需要把请求发到server上就可以了这样的话，就分为了3层：</p><ul><li>最左边：其实就是一个客户单，只需要向livy server发送请求</li><li>到livy server之后就会去spark集群创建我们的session</li><li>session创建好之后，客户端就可以把作业以代码片段的方式提交上来就OK了，其实就是以请求的方式发到server上就行</li></ul><p>这样能带来一个优点，对于原来提交作业机器的压力可以减少很多，我们只要保障Livy Server的HA就OK了<br>对于这个是可以保证的</p><p>此架构与spark-submit的对比：使用spark-submit(yarn-client模式)必须在客户端进行提交，如果客户端那台机器挂掉了(driver跑在客户端上，因此driver也就挂了)，那么作业全部都完成不了，这就存在一个单点问题</p><h3 id="架构概况："><a href="#架构概况：" class="headerlink" title="架构概况："></a>架构概况：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、客户端发一个请求到livy server</span><br><span class="line">2、Livy Server发一个请求到Spark集群，去创建session</span><br><span class="line">3、session创建完之后，会返回一个请求到Livy Server，这样Livy Server就知道session创建过程中的一个状态</span><br><span class="line">4、客户端的操作，如：如果客户端再发一个请求过来看一下，比如说看session信息啥的(可以通过GET API搞定)</span><br></pre></td></tr></table></figure><h3 id="多用户的特性："><a href="#多用户的特性：" class="headerlink" title="多用户的特性："></a>多用户的特性：</h3><p>上述是一个用户的操作，如果第二个、第三个用户来，可以这样操作：</p><ul><li>提交过去的时候，可以共享一个session</li><li>其实一个session就是一个SparkContext</li></ul><p>比如：蓝色的client共享一个session，黑色的client共享一个session，可以通过一定的标识，它们自己能够识别出来</p><h2 id="提交Spark作业案例"><a href="#提交Spark作业案例" class="headerlink" title="提交Spark作业案例"></a>提交Spark作业案例</h2><h3 id="创建交互式的session"><a href="#创建交互式的session" class="headerlink" title="创建交互式的session"></a>创建交互式的session</h3><p>使用交互式会话的前提是需要先创建会话。当前的Livy可在同一会话中支持spark，pyspark或是sparkr三种不同的解释器类型以满足不同语言的需求。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 livy-0.5.0-incubating-bin]$ curl -X POST --data &apos;&#123;&quot;kind&quot;:&quot;spark&quot;&#125;&apos; -H &quot;Content-Type:application/json&quot; hadoop000:8998/sessions</span><br><span class="line">------------------下面是创建Session返回的信息--------------------</span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: 1,</span><br><span class="line">    &quot;appId&quot;: null,</span><br><span class="line">    &quot;owner&quot;: null,</span><br><span class="line">    &quot;proxyUser&quot;: null,</span><br><span class="line">    &quot;state&quot;: &quot;starting&quot;,</span><br><span class="line">    &quot;kind&quot;: &quot;spark&quot;,</span><br><span class="line">    &quot;appInfo&quot;: &#123;</span><br><span class="line">        &quot;driverLogUrl&quot;: null,</span><br><span class="line">        &quot;sparkUiUrl&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;log&quot;: [&quot;stdout: &quot;, &quot;\nstderr: &quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中需要我们关注的是会话id，id代表了此会话，所有基于该会话的操作都需要指明其id</p><p><img src="/assets/pic/2019-09-06-2.png" alt="2"></p><h3 id="提交一个Spark的代码片段"><a href="#提交一个Spark的代码片段" class="headerlink" title="提交一个Spark的代码片段"></a>提交一个Spark的代码片段</h3><p><code>sc.parallelize(1 to 10).count()</code></p><p>Livy的REST提交方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl hadoop000:8998/sessions/1/statements -X POST -H &apos;Content-Type: application/json&apos; -d &apos;&#123;&quot;code&quot;:&quot;sc.parallelize(1 to 2).count()&quot;, &quot;kind&quot;: &quot;spark&quot;&#125;&apos;</span><br><span class="line">---------返回信息如下--------</span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: 1,</span><br><span class="line">    &quot;code&quot;: &quot;sc.parallelize(1 to 10).count()&quot;,</span><br><span class="line">    &quot;state&quot;: &quot;waiting&quot;,</span><br><span class="line">    &quot;output&quot;: null,</span><br><span class="line">    &quot;progress&quot;: 0.0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意此代码片段提交到session_id为1的session里面去了，所以Web点击1</p><p><img src="/assets/pic/2019-09-06-3.png" alt="3"></p><h3 id="以批处理会话-Batch-Session-提交打包的JAR"><a href="#以批处理会话-Batch-Session-提交打包的JAR" class="headerlink" title="以批处理会话(Batch Session)提交打包的JAR"></a>以批处理会话(Batch Session)提交打包的JAR</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.bigdata.spark.core01</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">object SparkWCApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">    .setAppName(&quot;SparkWCApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val lineRDD = sc.parallelize(Seq(&quot;hadoop&quot;,&quot;hadoop&quot;,&quot;Spark&quot;,&quot;Flink&quot;))</span><br><span class="line">    val rsRDD = lineRDD.flatMap(x =&gt; x.split(&quot;\t&quot;)).map(x =&gt; (x, 1)).reduceByKey(_ + _)</span><br><span class="line">    rsRDD.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码打包上传至</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 lib]$ pwd</span><br><span class="line">/home/hadoop/soul/lib</span><br><span class="line">[hadoop@hadoop000 lib]$ ll</span><br><span class="line">total 228</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 231035 Aug 29 23:09 spark-train-1.0.jar</span><br></pre></td></tr></table></figure><p>使用Livy提交</p><p>curl -H “Content-Type: application/json” -X POST -d ‘{ “file”:”/home/hadoop/soul/libspark-train-1.0.jar”, “className”:”com.soul.bigdata.spark.core01.SparkWCApp” }’ hadoop000:8998/batches<br>查看Livy的Web界面报错</p><p><img src="/assets/pic/2019-09-06-4.png" alt="4"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">19/08/29 23:19:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File hdfs://hadoop000:8020/home/hadoop/soul/lib/spark-train-1.0.jar does not exist.</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:705)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:106)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:763)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:759)</span><br><span class="line">    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:759)</span><br><span class="line">    at org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:755)</span><br><span class="line">    at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:723)</span><br><span class="line">    at org.apache.spark.deploy.DependencyUtils$.downloadFile(DependencyUtils.scala:137)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)</span><br><span class="line">    at scala.Option.map(Option.scala:146)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:366)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:143)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br></pre></td></tr></table></figure><p>所以File后面跟的Path需要是HDFS路径，而不是本地路径，将打包的JAR上传至HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 lib]$ hadoop fs -ls /lib</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup     231035 2019-08-29 23:20 /lib/spark-train-1.0.jar</span><br></pre></td></tr></table></figure><p>再次提交</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -X POST -d &apos;&#123; &quot;file&quot;:&quot;/lib/spark-train-1.0.jar&quot;, &quot;className&quot;:&quot;com.soul.bigdata.spark.core01.SparkWCApp&quot; &#125;&apos; hadoop000:8998/batches</span><br></pre></td></tr></table></figure><p>查看Web成功返回了我们需要的结果</p><p><img src="/assets/pic/2019-09-06-5.png" alt="5"></p><p><img src="/assets/pic/2019-09-06-6.png" alt="6"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="其他组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/"/>
    
    
      <category term="Livy" scheme="http://yoursite.com/tags/Livy/"/>
    
  </entry>
  
  <entry>
    <title>Spark之Accumulator陷阱&amp;解决</title>
    <link href="http://yoursite.com/2019/08/21/Spark%E4%B9%8BAccumulator%E9%99%B7%E9%98%B1&amp;%E8%A7%A3%E5%86%B3/"/>
    <id>http://yoursite.com/2019/08/21/Spark之Accumulator陷阱&amp;解决/</id>
    <published>2019-08-20T16:00:00.000Z</published>
    <updated>2019-10-12T04:04:54.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="Accumulator简介"><a href="#Accumulator简介" class="headerlink" title="Accumulator简介"></a>Accumulator简介</h3><p>Accumulator是Spark提供的累加器，顾名思义，该变量只能够增加。只有driver能获取到Accumulator的值（使用value方法），Task只能对其做增加操作（使用 +=）。<br>你也可以在为Accumulator命名，这样就会在Spark web ui中显示，可以帮助你了解程序运行的情况。</p><h3 id="举个最简单的accumulator的使用例子"><a href="#举个最简单的accumulator的使用例子" class="headerlink" title="举个最简单的accumulator的使用例子"></a>举个最简单的accumulator的使用例子</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;RuozedataAccumulator&quot;)</span><br><span class="line">  .master(&quot;local[2]&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">val sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">//在driver中定义</span><br><span class="line">val accum = sc.accumulator(0, &quot;Example Accumulator&quot;)</span><br><span class="line">//在task中进行累加</span><br><span class="line">sc.parallelize(1 to 10).foreach(x=&gt; accum += 1)</span><br><span class="line"></span><br><span class="line">//在driver中输出 结果将返回10</span><br></pre></td></tr></table></figure><h3 id="累加器的错误用法"><a href="#累加器的错误用法" class="headerlink" title="累加器的错误用法"></a>累加器的错误用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">val accum= sc.accumulator(0, &quot;Error Accumulator&quot;)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">//用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1</span><br><span class="line">val newData = data.map&#123;x =&gt; &#123;</span><br><span class="line">    if(x%2 == 0)&#123;</span><br><span class="line">    accum += 1</span><br><span class="line">    0</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">     1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&#125;</span><br><span class="line">//使用action操作触发执行</span><br><span class="line">newData.count</span><br><span class="line">//此时accum的值为5，是我们要的结果</span><br><span class="line">println(accum.value)</span><br><span class="line"></span><br><span class="line">//继续操作，查看刚才变动的数据,foreach也是action操作</span><br><span class="line">newData.foreach(println)</span><br><span class="line">//上个步骤没有进行累计器操作，可是累加器此时的结果已经是10了</span><br><span class="line">//这并不是我们想要的结果</span><br><span class="line">println(accum.value)</span><br></pre></td></tr></table></figure><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p>官方对这个问题的解释如下描述:</p><p>For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.</p><p>我们都知道，spark中的一系列transform操作会构成一串长的任务链，此时需要通过一个action操作来触发，accumulator也是一样。因此在一个action操作之前，你调用value方法查看其数值，肯定是没有任何变化的。</p><p>所以在第一次count(action操作)之后，我们发现累加器的数值变成了5，是我们要的答案。</p><p>之后又对新产生的的newData进行了一次foreach(action操作)，其实这个时候又执行了一次map(transform)操作，所以累加器又增加了5。最终获得的结果变成了10。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>看了上面的分析，大家都有这种印象了，那就是使用累加器的过程中只能使用一次action的操作才能保证结果的准确性。</p><p>事实上，还是有解决方案的，只要将任务之间的依赖关系切断就可以了。什么方法有这种功能呢？你们肯定都想到了，cache，persist。调用这个方法的时候会将之前的依赖切除，后续的累加器就不会再被之前的transfrom操作影响到了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">val accum= sc.accumulator(0, &quot;Correct Accumulator&quot;)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">//用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1</span><br><span class="line">val newData = data.map&#123;x =&gt; &#123;</span><br><span class="line">  if(x%2 == 0)&#123;</span><br><span class="line">    accum += 1</span><br><span class="line">    0</span><br><span class="line">  &#125;else&#123;</span><br><span class="line">    1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">//使用cache缓存数据，切断依赖。</span><br><span class="line">newData.cache</span><br><span class="line"></span><br><span class="line">//使用action操作触发执行</span><br><span class="line">newData.count</span><br><span class="line">//此时accum的值为5，是我们要的结果</span><br><span class="line">println(accum.value)</span><br><span class="line"></span><br><span class="line">//继续操作，查看刚才变动的数据,foreach也是action操作</span><br><span class="line">newData.foreach(println)</span><br><span class="line">//累加器此时的结果依旧是5了</span><br><span class="line">//这是我们想要的结果</span><br><span class="line">println(accum.value)</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>使用Accumulator时，为了保证准确性，只使用一次action操作。如果需要使用多次则使用cache或persist操作切断依赖。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>神器Apache Livy之初识篇</title>
    <link href="http://yoursite.com/2019/08/16/%E7%A5%9E%E5%99%A8Apache%20Livy%E4%B9%8B%E5%88%9D%E8%AF%86%E7%AF%87/"/>
    <id>http://yoursite.com/2019/08/16/神器Apache Livy之初识篇/</id>
    <published>2019-08-15T16:00:00.000Z</published>
    <updated>2019-08-16T07:59:40.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>对于Spark有这样一个应用场景：Spark应用程序都是通过spark-submit进行提交的；而在工作当中，肯定是要将我们的spark-submit封装到shell里面去的，比如：今天凌晨去处理昨天的数据，肯定是需要获取到date，然后做 date - 1 操作(当前天数减1就是昨天了)，之后配置crontab，把shell脚本配置上去，每天凌晨定时执行就行了，或者采用azkaban、oozie，都是这么做的、这么进行调度的</p><p>那么，现在有一个问题：spark-submit提交的时候，把相关的参数给指定上去就行，每次提交Spark作业的时候，都需要去申请资源(不管哪种模式，都是需要去申请资源)。特别是on Yarn的时候，申请资源是需要耗费一些时间的：如果是天级别的话，这样提交是没有问题的，我们每提交1次，就去申请资源，申请到资源之后再开始运行；但如果粒度更细一些，例如：1小时级别的，这样操作必然是有些问题存在的、会存在一些风险。比如说：我前面一个作业没有跑完，但后面的作业提交上来了，这样是很可能拿不到资源的，拿不到资源，那么在跑的时候就很可能导致作业的延迟，这样就会影响到SLA的要求；这就是目前对于大多数离线架构来说所存在的一个弊端</p><p>针对这个问题，如何解决呢？</p><p>社区上有一个框架Apache Livy。</p><p>官网：<a href="http://livy.incubator.apache.org/" target="_blank" rel="noopener">http://livy.incubator.apache.org/</a></p><p>该项目还在孵化阶段，还没有毕业<br>Livy专门为Spark所提供的REST服务</p><h3 id="Livy简介"><a href="#Livy简介" class="headerlink" title="Livy简介"></a>Livy简介</h3><ul><li><p><strong>Submit Jobs from Anywhere</strong></p><p>Livy能够以编程的、容错的、多租户的方式提交Spark，从web/mobile app处进行提交。也就是说一个服务跑起来之后，在页面上点一下就可以提交作业了，不需要Spark客户端了，也就意味着不需要再去客户端部署Spark了。比如在mobile端(手机端)去部署安装spark，也不需要使用 spark-submit 这种方式了，因此多个用户能够在Spark集群中以并发、可靠的方式进行交互。</p><p>这种方式，才是当今Spark使用的主流方式，可惜的是还是一个孵化版本，孵化版本在生产中使用还是需要慎重。</p></li><li><p><strong>Use Interactive Scala or Python</strong></p><p>能够交互的使用Scala和Python</p><p>怎么理解呢？</p><p>原来在Spark里面，我们可以使用spark-shell和pyspark来写scala代码片段和python的代码片段；相当于启动spark-shell或者pyspark之后，我们能在控制台里写代码。这就是我们所说的交互式：用户把代码片段(可以是scala、python)写进去，提交之后 后台去运行。livy也可以使用scala或是python，所以客户端可以使用这些语言远程地与spark集群进行通信。另外，批处理作业的提交也能使用scala、java、python去完成。既然是个REST Service，那么后台必然是有个服务的。</p></li><li><p><strong>What is Apache Livy?</strong></p><p>Livy是个service，能更加容易地去和Spark集群进行交互。通过REST接口进行交互，只需要将REST接口给暴露出来，这边通过REST API直接干过去就OK了，这样，就能使得我们非常容易的去提交Spark Job或者Spark代码片段，我们可以使用Spark代码片段来搞定。这一点在工作当中是非常非常重要的一个点，试想一个问题：如果非常简单的一个功能都需要写一堆代码，然后打个包再使用spark-submit的方式提交上去去运行，那么肯定是很麻烦的，在工作中肯定是不建议这么做的。</p><p>我们所做的数据平台必然是要提供一个功能，是需要能够支持代码片段的。一般来说，写Spark程序需要：</p><ul><li>写代码的时候要写个main方法</li><li>第一步需要创建SparkConf、SparkContext、SQLContext</li><li>然后再来写业务逻辑</li></ul></li></ul><p><strong>代码片段</strong>： 即公用的东西我们不用去写了，我们只需要将核心的代码给写到里面，然后提交上去跑就是了，这种方式才是主流的方式。</p><p>对于结果的获取支持同步或是异步的方式。</p><p>对于一个大数据作业来说，提交一个作业上去，可能会遇到运行的时间很长的情况，我们需要等嘛？</p><p>如果等的话，就等死了，因此对于结果的获取，支持同步或是异步的方式。</p><p>同时也有对SparkContext的管理</p><p>言下之意就是多个用户共享一个SparkContext，就不需要申请资源了，只要申请一次之后，大家就都能用了，之后通过一个REST接口或是RPC client library直接发过去就行了</p><p>Livy能够简化Spark和application servers之间的交互，使得Spark与web/mobile application之间的交互更加容易</p><p>有如下特点：</p><ul><li><p>运行Spark Context作为一个长服务</p><p>多个Spark jobs、多个client共用，达成Spark Context共享的目的</p></li><li><p>能够共享cache的RDD或者DataFrams在多个job之间、在多个client之间</p></li><li><p>多个Spark Context能够被管理</p><p>多个Spark Context在集群上 能够使用Livy Server来替代它，这拥有非常好的容错、并发</p></li><li><p>Jobs能够被提交通过以下方式：</p><p>预编译的jar包、代码片段 或 java/scala的客户端API</p></li><li><p>能做到安全性的验证，能够做到授权和验证</p><p>即：在作业运行的时候能有非常好的安全保障作用</p></li></ul><p><strong>关于安全性：</strong></p><p>如果使用spark-submit在提交的时候，我们的code中有以下一段代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem.delete(&quot;/&quot;)</span><br></pre></td></tr></table></figure><p>这段删除的代码写法不一定对，大致意思就是调用了FileSystem中的delete方法这样操作的后果是：咣当一下，整个集群就全挂了。不要以为这种代码不会写出来，在某些场景下，比如：移动数据的时候，是非常容易写错的。只要团队内的一个人写错了一层目录，就有可能误删除数据。<br>对此，有人会提出一个方案就是禁止delete，但是在我们的code中还可能有这种情况：</p><p>写了一段sql里有<code>overwrite</code>或<code>df.write.format().mode(&quot;overwrite&quot;).save()</code></p><p>overwrite也是会出现上述的这种情况的，如果我们的目录写错层级的话对于这种情况，我们的overwrite是不可能禁止的，因为有些时候是需要用到的</p><p>因此security务必是要在整个数据平台中做的很好的，而且在数据平台中肯定是要考虑的，十分重要。但是livy的权限是做的比较初级的，很多时候是控制不了的。</p><p>数据平台中安全性的要求</p><p>需要做到，什么人去访问什么表、甚至做到什么人能访问到哪些列：即对于一个表中不同的列，不同的人拥有不同的权限</p><p>权限如何设置？</p><p>肯定是要自己开发，HDP中有ranger，但是功能不是很完善，还是需要自己去开发</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="其他组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/"/>
    
    
      <category term="Livy" scheme="http://yoursite.com/tags/Livy/"/>
    
  </entry>
  
  <entry>
    <title>解决Resolving archive.cloudera.com... failed: Temporary failure</title>
    <link href="http://yoursite.com/2019/08/13/%E8%A7%A3%E5%86%B3Resolving%20archive.cloudera.com...%20failed-%20Temporary%20failure/"/>
    <id>http://yoursite.com/2019/08/13/解决Resolving archive.cloudera.com... failed- Temporary failure/</id>
    <published>2019-08-12T16:00:00.000Z</published>
    <updated>2019-09-23T06:10:42.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>阿里云ECS云主机，部署CDH，需要wget下载资源包。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5ODY0NzgxNA==&amp;mid=2247485164&amp;idx=1&amp;sn=d19b4cf1bd7d0d0fdd73f91019a70b3a&amp;chksm=908f2c85a7f8a593a03b10a8557e67e2088e5c6e820623040a693e0bd7da9fc3a41da0b5223b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">CDH5.16.1集群企业真正离线部署</a></p><h4 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ruozedata001 ~]# wget -c http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz</span><br><span class="line">--2019-07-23 16:59:08--  http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz</span><br><span class="line">Resolving archive.cloudera.com... failed: Temporary failure in name resolution.</span><br><span class="line">wget: unable to resolve host address “archive.cloudera.com”</span><br><span class="line"></span><br><span class="line">无法解析 archive.cloudera.com网址，下载资源包失败</span><br></pre></td></tr></table></figure><h4 id="修改网卡，添加DNS"><a href="#修改网卡，添加DNS" class="headerlink" title="修改网卡，添加DNS"></a>修改网卡，添加DNS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ruozedata001 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">DEVICE=eth0</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=172.31.236.240</span><br><span class="line">NETMASK=255.255.240.0</span><br><span class="line">#百度DNS服务器</span><br><span class="line">DNS1=180.76.76.76</span><br><span class="line">#阿里云DNS服务器</span><br><span class="line">DNS2=223.5.5.5</span><br></pre></td></tr></table></figure><h4 id="重启网络生效-切记"><a href="#重启网络生效-切记" class="headerlink" title="重启网络生效 切记"></a>重启网络生效 切记</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@ruozedata001 ~]# service network restart</span><br><span class="line">Shutting down interface eth0:                              [  OK  ]</span><br><span class="line">Shutting down loopback interface:                          [  OK  ]</span><br><span class="line">Bringing up loopback interface:                            [  OK  ]</span><br><span class="line">Bringing up interface eth0:  Determining if ip address 172.31.236.240 is already in use for device eth0...</span><br><span class="line">RTNETLINK answers: File exists</span><br><span class="line">                                                           [  OK  ]</span><br><span class="line">[root@ruozedata001 ~]# ping archive.cloudera.com</span><br><span class="line">PING prod.cloudera.map.fastly.net (151.101.76.167) 56(84) bytes of data.</span><br><span class="line">64 bytes from 151.101.76.167: icmp_seq=1 ttl=59 time=0.719 ms</span><br><span class="line">64 bytes from 151.101.76.167: icmp_seq=2 ttl=59 time=0.754 ms</span><br><span class="line">^Z</span><br><span class="line">[3]+  Stopped                 ping archive.cloudera.com</span><br></pre></td></tr></table></figure><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@ruozedata001 ~]# wget -c http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz</span><br><span class="line">--2019-07-23 17:00:29--  http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz</span><br><span class="line">Resolving archive.cloudera.com... 151.101.76.167</span><br><span class="line">Connecting to archive.cloudera.com|151.101.76.167|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 837950600 (799M) [binary/octet-stream]</span><br><span class="line">Saving to: “cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz”</span><br><span class="line">100%[=============================================================================&gt;] 837,950,600 12.3M/s   in 65s</span><br><span class="line">2019-07-23 17:01:34 (12.4 MB/s) - “cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz” saved [837950600/837950600]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ruozedata001 ~]# wget -c https://archive.cloudera.com/cdh5/parcels/latest/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel</span><br><span class="line"></span><br><span class="line">--2019-07-23 17:01:58--  https://archive.cloudera.com/cdh5/parcels/latest/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel</span><br><span class="line">Resolving archive.cloudera.com... 151.101.76.167</span><br><span class="line">Connecting to archive.cloudera.com|151.101.76.167|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 2132782197 (2.0G) [binary/octet-stream]</span><br><span class="line">Saving to: “CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel”</span><br><span class="line">100%[===========================================================================&gt;] 2,132,782,197 12.8M/s   in 2m 46s</span><br><span class="line">2019-07-23 17:04:44 (12.3 MB/s) - “CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel” saved [2132782197/2132782197]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ruozedata001 ~]# wget -c https://archive.cloudera.com/cdh5/parcels/latest/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha1</span><br><span class="line"></span><br><span class="line">--2019-07-23 17:04:56--  https://archive.cloudera.com/cdh5/parcels/latest/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha1</span><br><span class="line">Resolving archive.cloudera.com... 151.101.76.167</span><br><span class="line">Connecting to archive.cloudera.com|151.101.76.167|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 41 [text/plain]</span><br><span class="line">Saving to: “CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha1”</span><br><span class="line">100%[=============================================================================&gt;] 41          --.-K/s   in 0s</span><br><span class="line">2019-07-23 17:04:56 (3.57 MB/s) - “CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha1” saved [41/41]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ruozedata001 ~]# wget -c https://archive.cloudera.com/cdh5/parcels/latest/manifest.json</span><br><span class="line"></span><br><span class="line">--2019-07-23 17:05:09--  https://archive.cloudera.com/cdh5/parcels/latest/manifest.json</span><br><span class="line">Resolving archive.cloudera.com... 151.101.76.167</span><br><span class="line">Connecting to archive.cloudera.com|151.101.76.167|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 66804 (65K) [application/json]</span><br><span class="line">Saving to: “manifest.json”</span><br><span class="line">100%[=============================================================================&gt;] 66,804      --.-K/s   in 0.001s</span><br><span class="line"></span><br><span class="line">2019-07-23 17:05:09 (44.2 MB/s) - “manifest.json” saved [66804/66804]</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="故障案例" scheme="http://yoursite.com/categories/%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>如何避免生产Spark Shuffle的某场景</title>
    <link href="http://yoursite.com/2019/08/06/%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E7%94%9F%E4%BA%A7Spark%20Shuffle%E7%9A%84%E6%9F%90%E5%9C%BA%E6%99%AF/"/>
    <id>http://yoursite.com/2019/08/06/如何避免生产Spark Shuffle的某场景/</id>
    <published>2019-08-05T16:00:00.000Z</published>
    <updated>2019-10-14T07:26:52.873Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——<strong>数据倾斜</strong>，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h3 id="方案适用场景"><a href="#方案适用场景" class="headerlink" title="方案适用场景"></a>方案适用场景</h3><p>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><h3 id="方案实现思路"><a href="#方案实现思路" class="headerlink" title="方案实现思路"></a>方案实现思路</h3><p>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。</p><p>将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；</p><p>接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><h3 id="方案实现原理"><a href="#方案实现原理" class="headerlink" title="方案实现原理"></a>方案实现原理</h3><p>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示:</p><p><img src="/assets/pic/2019-08-06-1.png" alt="实现原理"></p><h3 id="方案优点"><a href="#方案优点" class="headerlink" title="方案优点"></a>方案优点</h3><p>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><h3 id="方案缺点"><a href="#方案缺点" class="headerlink" title="方案缺点"></a>方案缺点</h3><p>适用场景少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p>如果对于hive中的map join熟悉的同学看这幅图应该很好理解，所以建议大家看看hive中的map join原理，进行对比学习。</p><h3 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object RuozedataBroadCast &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf=new SparkConf().setAppName(&quot;Ruozedata_BroadCast&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc=new SparkContext(conf)</span><br><span class="line">    val smallRDD=sc.parallelize(Array(</span><br><span class="line">      (&quot;1&quot;,&quot;ruoze&quot;),</span><br><span class="line">      (&quot;2&quot;,&quot;jepson&quot;),</span><br><span class="line">      (&quot;3&quot;,&quot;xiaoyuzhou&quot;)</span><br><span class="line">    )).collectAsMap()</span><br><span class="line"></span><br><span class="line">    val smallBroadCast=sc.broadcast(smallRDD)</span><br><span class="line"></span><br><span class="line">    val bigRDD=sc.parallelize(Array(</span><br><span class="line">      (&quot;1&quot;,&quot;school1&quot;,&quot;male&quot;),</span><br><span class="line">      (&quot;2&quot;,&quot;school2&quot;,&quot;female&quot;),</span><br><span class="line">      (&quot;3&quot;,&quot;school3&quot;,&quot;male&quot;),</span><br><span class="line">      (&quot;4&quot;,&quot;school4&quot;,&quot;male&quot;),</span><br><span class="line">      (&quot;5&quot;,&quot;school5&quot;,&quot;female&quot;)</span><br><span class="line">    )).map(x=&gt;(x._1,x))</span><br><span class="line"></span><br><span class="line">    val broadCastValue=smallBroadCast.value</span><br><span class="line"></span><br><span class="line">     bigRDD.mapPartitions(partitions=&gt;&#123;</span><br><span class="line">    </span><br><span class="line">       for ((key,value)&lt;-partitions</span><br><span class="line">         if (broadCastValue.contains(key)))</span><br><span class="line">         yield(key,broadCastValue.getOrElse(key,&quot;&quot;),value._2,value._3)</span><br><span class="line">    </span><br><span class="line">     &#125;).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    /** 结果</span><br><span class="line">      * (1,ruoze,school1,male)</span><br><span class="line">      * (2,jepson,school2,female)</span><br><span class="line">      * (3,xiaoyuzhou,school3,male)</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>生产上Spark对MySQL加载并发提高的两种代码(彩蛋)</title>
    <link href="http://yoursite.com/2019/07/30/%E7%94%9F%E4%BA%A7%E4%B8%8ASpark%E5%AF%B9MySQL%E5%8A%A0%E8%BD%BD%E5%B9%B6%E5%8F%91%E6%8F%90%E9%AB%98%E7%9A%84%E4%B8%A4%E7%A7%8D%E4%BB%A3%E7%A0%81(%E5%BD%A9%E8%9B%8B)/"/>
    <id>http://yoursite.com/2019/07/30/生产上Spark对MySQL加载并发提高的两种代码(彩蛋)/</id>
    <published>2019-07-29T16:00:00.000Z</published>
    <updated>2019-10-14T07:27:45.783Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>1.通过使用表字段数字类型的最大值和最小值加上numPartitions组合做相应的分区设置</p><p>背景：tableA的主键ID为Int类型，且属于以1为自增步长的自增键，那么对全表做数据加载方式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">val sql = &quot;select * from tableA&quot; // 查询tableA数据</span><br><span class="line">val upperBound = jdbc(&quot;select count(1) from tableA&quot;) // 自定义jdbc方法，查询tableA的数据总量</span><br><span class="line">val readTablePartitionNum = upperBound / 10000 + 1 // 根据数据总量 / 10000 加上1的方式给此表自动计算分区数量</span><br><span class="line">val rowKeyCol = “ID” // 分区的字段(此种方式该值的数据类型必须为整形)</span><br><span class="line">mysqlDf = spark.read.format(&quot;jdbc&quot;)</span><br><span class="line">    .option(&quot;url&quot;, jdbcMap.get(&quot;url&quot;))</span><br><span class="line">    .option(&quot;driver&quot;, jdbcMap.get(&quot;drivername&quot;))</span><br><span class="line">    .option(&quot;dbtable&quot;, sql)</span><br><span class="line">    .option(&quot;user&quot;, jdbcMap.get(&quot;username&quot;))</span><br><span class="line">    .option(&quot;password&quot;, jdbcMap.get(&quot;password&quot;))</span><br><span class="line">    .option(&quot;numPartitions&quot;, readTablePartitionNum)</span><br><span class="line">    .option(&quot;partitionColumn&quot;, rowKeyCol)</span><br><span class="line">    .option(&quot;lowerBound&quot;, 0)</span><br><span class="line">    .option(&quot;upperBound&quot;, upperBound)</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure><p>其中此方法jdbc的源代码截图如下</p><p><img src="/assets/pic/2019-07-30-1.png" alt="1"></p><p>2.还有另外一种场景会出现主键ID并非数字类型，比如主键用的是UUID？上面的方式就不能使用了，但是又需要将数据相对均匀分布的放在N个partition内，此处可以使用第二种方式，按照不同条件做分区。</p><p>背景：tableA的表主键ID为UUID，有时间字段cretime，且此表数据属于均匀增长的业务表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/** 使用 创建时间字段 做分区</span><br><span class="line">* 1.MySQL主键为整形 且非全量校验</span><br><span class="line">* 2.MySQL主键为字符串</span><br><span class="line">*/</span><br><span class="line"> val prop = new java.util.Properties</span><br><span class="line"> prop.setProperty(&quot;user&quot;, jdbcMap.get(&quot;username&quot;))</span><br><span class="line"> prop.setProperty(&quot;password&quot;, jdbcMap.get(&quot;password&quot;))</span><br><span class="line"> prop.setProperty(&quot;Driver&quot;, jdbcMap.get(&quot;drivername&quot;))</span><br><span class="line"> </span><br><span class="line"> // 此处省略了Array里生成从开始时间结束时间，及粒度的方法(业务)，最后Array结果如下</span><br><span class="line"> // 这是J哥我留给你们的挑战作业，你会做吗？</span><br><span class="line"></span><br><span class="line"> val predicates = Array(&quot;cretime &gt;= &apos;2019-01-01&apos; and cretime &lt; &apos;2019-01-02&apos;&quot;</span><br><span class="line">                       ,&quot;cretime &gt;= &apos;2019-01-02&apos; and cretime &lt; &apos;2019-01-03&apos;&quot;</span><br><span class="line">                       ,&quot;cretime &gt;= &apos;2019-01-03&apos; and cretime &lt; &apos;2019-01-04&apos;&quot;)</span><br><span class="line"> </span><br><span class="line"> mysqlDf = spark.read.jdbc(jdbcMap.get(&quot;url&quot;), s&quot;test.tableA&quot;, predicates, prop)</span><br></pre></td></tr></table></figure><p>其中此方法jdbc的源代码截图如下</p><p><img src="/assets/pic/2019-07-30-2.png" alt="2"></p><p>总结两点：</p><p>1.其中截图都有一块明确说明，分区的数量不要创建太多，否则很容易将你拉取的mysql数据库拉垮掉(重要)。</p><p>此处有两种操作方式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A)给作业的总core不要设置太大</span><br><span class="line">B)在创建jdbc时，不要将numPartition计算过后的值设置太大(第一种方式)，或者不要将parts数组的长度设置太大(第二种方式)</span><br><span class="line">2.在操作中需要注意&quot;select * “的使用，此处我两种方式都未设置对列的筛选，得益于spark的优化引擎，我后续对此两个DF都只用到了每个DataFrame的两个列，另外，在拿取到dataframe的时候千万小心使用persist()</span><br></pre></td></tr></table></figure><p>方法，因为persist方法没有优化列选取，直接抓取的所有当前dataframe能拿到的列，这样对mysql是有压力的。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>人人都应该会的ZooKeeper实战操作</title>
    <link href="http://yoursite.com/2019/07/23/%E4%BA%BA%E4%BA%BA%E9%83%BD%E5%BA%94%E8%AF%A5%E4%BC%9A%E7%9A%84ZooKeeper%E5%AE%9E%E6%88%98%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/07/23/人人都应该会的ZooKeeper实战操作/</id>
    <published>2019-07-22T16:00:00.000Z</published>
    <updated>2019-07-24T02:21:20.245Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="ZooKeeper数据结构"><a href="#ZooKeeper数据结构" class="headerlink" title="ZooKeeper数据结构"></a>ZooKeeper数据结构</h3><p>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。</p><p>很显然zookeeper集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个znode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/kUEByXevaAuM.png?imageslim" alt="mark"></p><h3 id="Zookeeper节点类型"><a href="#Zookeeper节点类型" class="headerlink" title="Zookeeper节点类型"></a>Zookeeper节点类型</h3><ol><li><p>Znode有两种类型</p><ul><li>短暂(ephemeral):客户端和服务器端断开连接后，创建的节点自己删除</li><li>持久(persistent):客户端和服务器端断开连接后，创建的节点不删除</li></ul></li><li><p>ZNode有四种形式的目录节点(默认是persistent)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(1)持久化目录节点（PERSISTENT）</span><br><span class="line">   客户端与zookeeper断开连接后，该节点依旧存在</span><br><span class="line">(2)持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL）</span><br><span class="line">   客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号</span><br><span class="line">(3)临时目录节点（EPHEMERAL）</span><br><span class="line">客户端与zookeeper断开连接后，该节点被删除</span><br><span class="line">(4)临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）</span><br><span class="line">客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</p></li><li><p>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</p></li></ol><h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><ul><li>Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。</li><li>Leader负责进行投票的发起和决议，更新系统状态。</li><li>Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。</li><li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。</li><li>全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。</li><li>更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</li><li>数据更新原子性，一次数据更新要么成功，要么失败。</li><li>实时性，在一定时间范围内，client能读到最新数据。</li></ul><h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><table><thead><tr><th style="text-align:center">命令基本语法</th><th style="text-align:center">功能描述</th></tr></thead><tbody><tr><td style="text-align:center">help</td><td style="text-align:center">显示所有操作命令</td></tr><tr><td style="text-align:center">ls path [watch]</td><td style="text-align:center">使用 ls 命令来查看当前znode中所包含的内容</td></tr><tr><td style="text-align:center">ls2 path [watch]</td><td style="text-align:center">查看当前节点数据并能看到更新次数等数据</td></tr><tr><td style="text-align:center">create</td><td style="text-align:center">普通创建 -s 含有序列 -e 临时（重启或者超时消失）</td></tr><tr><td style="text-align:center">get path [watch]</td><td style="text-align:center">获得节点的值</td></tr><tr><td style="text-align:center">set</td><td style="text-align:center">设置节点的具体值</td></tr><tr><td style="text-align:center">stat</td><td style="text-align:center">查看节点状态</td></tr><tr><td style="text-align:center">delete</td><td style="text-align:center">删除节点</td></tr><tr><td style="text-align:center">rmr</td><td style="text-align:center">递归删除节点</td></tr></tbody></table><ol><li><p>启动客户端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop bin]$ ./zkCli.sh</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>查看当前znode中所包含的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, config]</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>查看当前节点数据并能看到更新次数等数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls2 /</span><br><span class="line">[controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, config]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Wed Dec 31 16:00:00 GMT-08:00 1969</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Wed Dec 31 16:00:00 GMT-08:00 1969</span><br><span class="line">pZxid = 0x658</span><br><span class="line">cversion = 45</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create /test test</span><br><span class="line">Created /test</span><br><span class="line">[zk: localhost:2181(CONNECTED) 10] create /test/t1 t1</span><br><span class="line">Created /test/t1</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>获取节点的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get /test</span><br><span class="line">test</span><br><span class="line">cZxid = 0x67b</span><br><span class="line">ctime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">mZxid = 0x67b</span><br><span class="line">mtime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67b</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 0</span><br><span class="line">[zk: localhost:2181(CONNECTED) 11] get /test/t1</span><br><span class="line">t1</span><br><span class="line">cZxid = 0x67c</span><br><span class="line">ctime = Sun Apr 29 21:12:47 GMT-08:00 2018</span><br><span class="line">mZxid = 0x67c</span><br><span class="line">mtime = Sun Apr 29 21:12:47 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67c</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 2</span><br></pre></td></tr></table></figure></li></ol><pre><code>这里获取节点信息可以看到一堆信息，那这些代表什么意思呢？下面我们来看看具体的含义：- czxid - 引起这个znode创建的zxid，创建节点的事务的zxid（ZooKeeper Transaction Id）。每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于    zxid2，那么zxid1在zxid2之前发生。- ctime - znode被创建的毫秒数(从1970年开始)- mzxid - znode最后更新的zxid- mtime - znode最后修改的毫秒数(从1970年开始)- pZxid-znode最后更新的子节点zxid- cversion - znode子节点变化号，znode子节点修改次数- dataversion - znode数据变化号- aclVersion - znode访问控制列表的变化号- ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。- dataLength- znode的数据长度- numChildren - znode子节点数量</code></pre><ol start="6"><li><p>创建临时节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] create -e /tmp_test tmp_test</span><br><span class="line">Created /tmp_test</span><br><span class="line">[zk: localhost:2181(CONNECTED) 13] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, test, tmp_test, admin, isr_change_notification, consumers, config]</span><br><span class="line">退出</span><br><span class="line">[zk: localhost:2181(CONNECTED) 14] quit</span><br><span class="line">Quitting...</span><br><span class="line">2018-04-29 21:17:58,754 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x1630235a6260042 closed</span><br><span class="line">2018-04-29 21:17:58,755 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@512] - EventThread shut down</span><br><span class="line">重启</span><br><span class="line">[hadoop@hadoop bin]$ ./zkCli.sh</span><br><span class="line">再次查看</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[controller_epoch, brokers, zookeeper, test, admin, isr_change_notification, consumers, config]</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>创建带序号的节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /num_test num_test</span><br><span class="line">Created /num_test0000000028</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /num_test1 num_test1</span><br><span class="line">Created /num_test10000000029</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /num_test2 num_test2</span><br><span class="line">Created /num_test20000000030</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>修改节点值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /test change_test</span><br><span class="line">cZxid = 0x67b</span><br><span class="line">ctime = Sun Apr 29 21:10:19 GMT-08:00 2018</span><br><span class="line">mZxid = 0x689</span><br><span class="line">mtime = Sun Apr 29 21:26:53 GMT-08:00 2018</span><br><span class="line">pZxid = 0x67c</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure></li><li><p>删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] delete /num_test0000000028</span><br></pre></td></tr></table></figure></li><li><p>递归删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] rmr /test</span><br></pre></td></tr></table></figure></li><li><p>查看节点状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] stat /consumers</span><br><span class="line">cZxid = 0x2</span><br><span class="line">ctime = Wed Apr 25 01:41:56 GMT-08:00 2018</span><br><span class="line">mZxid = 0x2</span><br><span class="line">mtime = Wed Apr 25 01:41:56 GMT-08:00 2018</span><br><span class="line">pZxid = 0x3f5</span><br><span class="line">cversion = 37</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="ZooKeeper" scheme="http://yoursite.com/categories/ZooKeeper/"/>
    
    
      <category term="ZooKeeper" scheme="http://yoursite.com/tags/ZooKeeper/"/>
    
      <category term="实战操作" scheme="http://yoursite.com/tags/%E5%AE%9E%E6%88%98%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Kafka之数据迁移</title>
    <link href="http://yoursite.com/2019/07/18/Kafka%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    <id>http://yoursite.com/2019/07/18/Kafka之数据迁移/</id>
    <published>2019-07-17T16:00:00.000Z</published>
    <updated>2019-07-24T02:19:13.265Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>当Kafka 减少Broker节点后，需要把数据分区迁移到其他节点上，以下将介绍我的一次迁移验证过程。</p><p>前3步为环境准备，实际数据操作看第4步即可</p><p>增加Broker节点，也可以采用步骤4相同的方法进行重新分区</p><p>方案思想：<code>使用kafka-reassign-partitions命令，把partition重新分配到指定的Broker上</code></p><h3 id="创建测试topic，具体为3个分区，2个副本"><a href="#创建测试topic，具体为3个分区，2个副本" class="headerlink" title="创建测试topic，具体为3个分区，2个副本"></a>创建测试topic，具体为3个分区，2个副本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics --create --topic test-topic \</span><br><span class="line">--zookeeper cdh-002/kafka \</span><br><span class="line">--replication-factor 2 --partitions 3</span><br></pre></td></tr></table></figure><h3 id="查看创建的topic"><a href="#查看创建的topic" class="headerlink" title="查看创建的topic"></a>查看创建的topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics --describe --zookeeper cdh-002/kafka --topic test-topic</span><br></pre></td></tr></table></figure><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/TJHSXayn2rpq.png?imageslim" alt="mark"></p><h3 id="产生若干条测试数据"><a href="#产生若干条测试数据" class="headerlink" title="产生若干条测试数据"></a>产生若干条测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer --topic test-topic \</span><br><span class="line">--broker-list cdh-004:9092</span><br></pre></td></tr></table></figure><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/XaBgiGo83Dhh.png?imageslim" alt="mark"></p><h3 id="使用命令进行重分区"><a href="#使用命令进行重分区" class="headerlink" title="使用命令进行重分区"></a>使用命令进行重分区</h3><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/UAX7PCNdJsRp.png?imageslim" alt="mark"></p><ol><li><p>新建文件topic-to-move.json ，比如加入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;:&quot;test-topic&quot;&#125;], &quot;version&quot;: 1&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用–generate生成迁移计划，broker-list根据自己环境设置，我的环境由于broker 75挂掉了，只剩下76和77</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--topics-to-move-json-file /opt/lb/topic-to-move.json \</span><br><span class="line">--broker-list &quot;76,77&quot; --generate</span><br></pre></td></tr></table></figure><p>输出日志：</p><p>（<font color="red">从日志可知各个分区副本所在的Broker节点，以及建议的副本分布</font>）</p><p>Current partition replica assignment (<font color="red">当前分区副本分布</font>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:0,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:2,</span><br><span class="line">&quot;replicas&quot;:[75,76]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:1,</span><br><span class="line">&quot;replicas&quot;:[77,75]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Proposed partition reassignment configuration (<font color="red">建议分区副本分布</font>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:0,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:2,</span><br><span class="line">&quot;replicas&quot;:[76,77]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">&quot;topic&quot;:&quot;test-topic&quot;,</span><br><span class="line">&quot;partition&quot;:1,</span><br><span class="line">&quot;replicas&quot;:[77,76]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>新建文件kafka-reassign-execute.json，并把建议的分区副本分布配置拷贝到新建文件中。(生产上一般会保留当前分区副本分布，仅更改下线的分区，这样数据移动更少)</p></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/5b90JllUjzyl.png?imageslim" alt="mark"></p><ol start="4"><li><p>使用–execute执行迁移计划 (有数据移动，broker 75上的数据会移到broker 76和77上，如果数据量大，执行的时间会比较久，耐心等待即可)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--reassignment-json-file /opt/lb/kafka-reassign-execute.json \</span><br><span class="line">--execute</span><br></pre></td></tr></table></figure></li><li><p>使用-verify查看迁移进度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions --zookeeper cdh-002/kafka \</span><br><span class="line">--reassignment-json-file /opt/lb/kafka-reassign-execute.json \</span><br><span class="line">--verify</span><br></pre></td></tr></table></figure></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/NRvEyHQV56rh.png?imageslim" alt="mark"></p><ol start="6"><li><p>通过消费者验证，可知，并未丢失数据。注意需要加–from-beginning。(此时broker 75和77同时宕机，也不会丢失数据，因为76上有了所有分区的副本)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer --topic test-topic --from-beginning --zookeeper cdh-002/kafka</span><br></pre></td></tr></table></figure></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/0B6vo0NKWXaj.png?imageslim" alt="mark"></p><pre><code>&lt;font color=&quot;red&quot; size=4&gt;另外一种验证方法是:（生产最佳实践）&lt;/font&gt;另外一种验证方法就是通过查看Kafka存储路径来确认，是否有迁移数据</code></pre><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/Bs9N0qEqie1Y.png?imageslim" alt="mark"></p><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cdh-003 ~]# cd /var/local/kafka/data/</span><br><span class="line">[root@cdh-003 data]# ll</span><br><span class="line">rwxr-xr-x 2 kafka kafka  110 Oct 23 14:21 test-topic-0</span><br><span class="line">drwxr-xr-x 2 kafka kafka  110 Oct 23 14:52 test-topic-1</span><br><span class="line">drwxr-xr-x 2 kafka kafka  110 Oct 23 14:21 test-topic-2</span><br></pre></td></tr></table></figure></code></pre><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://yoursite.com/tags/Kafka/"/>
    
      <category term="数据迁移" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>知遇若泽数据，成就自己未来</title>
    <link href="http://yoursite.com/2019/07/16/%E7%9F%A5%E9%81%87%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%88%90%E5%B0%B1%E8%87%AA%E5%B7%B1%E6%9C%AA%E6%9D%A5/"/>
    <id>http://yoursite.com/2019/07/16/知遇若泽数据，成就自己未来/</id>
    <published>2019-07-15T16:00:00.000Z</published>
    <updated>2019-07-24T02:24:47.066Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><center>不吹不擂，不自不黑，</center><br><center>你愿意花3min，阅读刚毕业的我吗？</center><br><center>以下所有内容真实，禁得住考验！</center><a id="more"></a><h2 id="应届的我"><a href="#应届的我" class="headerlink" title="应届的我"></a>应届的我</h2><p>我是一名应届毕业生，虽然是计算机专业，但是大学里过于理论与老旧的知识无法满足我的求知欲，听说大数据是热门，便通过各种方法学习。最后结缘若泽数据，也多亏若老和J哥，让我成功掌握大数据，端午节线下班第14期，一结束的第二天，我便拿到了自己满意的offer，坐标 长沙。(底部有截图)</p><h2 id="遇见你不容易"><a href="#遇见你不容易" class="headerlink" title="遇见你不容易"></a>遇见你不容易</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;哈哈哈，若泽数据唯一的缺点，就是广告不到位，对于一个啥也不懂的小白，甚至是有几年工作经验的码农，要找到它是真的不容易。相信很多小伙伴都有过类似的经历，<strong>百度一下大数据培训，那广告是琳琅满目，百花齐放</strong>，口号那是一个比一个响亮，简直是让人热血沸腾，看了都感觉自己好像已经达到了人生巅峰，可真正进去之后，就发现梦想很丰满，现实很骨感。别问我为什么，因为我就是。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;去年年底，因为我在某机构，实在学不懂spark，整整蒙蔽徘徊一个多星期后，决定咬牙另起炉灶自学，尝试过重新看机构录下来的视频，看网上五花八门的博客，可仍然是迷茫，一度怀疑自己的智商不适合学大数据了，直到偶然间在慕课上浏览并观看了若泽PK的Spark相关视频，我才找回了自己。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;就在看完慕课网的课程没多久，马上幸运女神又再次眷属我，当得知<font color="blue">若泽数据要办2019年上半年第六期高级班</font>，我就马上报名参加，在此也十分感谢我的家人，因为他们一直都支持我学习，提供第二次学习的经济支柱，为了不辜负他们的期望，便天天拿着小板凳盼着开班、开班、开班。</p><h2 id="高级班全力冲刺的我"><a href="#高级班全力冲刺的我" class="headerlink" title="高级班全力冲刺的我"></a>高级班全力冲刺的我</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;高级班的学习氛围非常浓厚，在这里也认识了许许多多有经验有梦想的大佬和小伙伴们，无论是班群还是小组群，都是<font color="blue">激情澎湃</font>的，我虽然大多数时间潜水，但围观大家的讨论，也学到了很多解决问题的办法，也避免了自己学习上可能会遇到的很多坑。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;老师们讲的内容都是直接跟<font color="red">企业生产</font>上挂钩的，就拿Spark来说，从零到有，深入浅出，结合具体案例和相关性能调优的方方面面细火慢炖，结合官网讲解，完美地将大数据的原滋原味呈现出来。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;除了课堂内容无可挑剔以外，老师为了保证我们的吸收效果，课下严格要求我们完成相应布置的作业，每周上课都会随机抽取2名同学讲解作业，再结合作业去理解老师的课堂内容就<font color="blue">如鱼得水</font>了。</p><h2 id="线下班使出吃奶劲的我"><a href="#线下班使出吃奶劲的我" class="headerlink" title="线下班使出吃奶劲的我"></a>线下班使出吃奶劲的我</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;线下班是含金量十足而且十分紧凑的，一群小伙伴早上九点学到晚上12点，<font color="blue">高端的食材，往往只需要最朴素的烹饪</font>，线下班所有的知识点都是平时高级班一点一点做好铺垫和前置的，我们只管做好笔记，就每一个生产可能会遇到的问题和具体的解决办法，细嚼慢咽，理解到骨髓里，那种成就感和迫不及待迎接offer的自信便油然而生了。</p><h2 id="希望有缘人，能够成为我的学弟"><a href="#希望有缘人，能够成为我的学弟" class="headerlink" title="希望有缘人，能够成为我的学弟"></a>希望有缘人，能够成为我的学弟</h2><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;若泽数据不仅仅传授的是知识，更多的是获取知识的方法，及时写博客记录，结合官网学习，遇见错误如何定位解决等等，都是我现在入职后受益很大的财富，希望越来越多的小伙伴能够认识若泽数据，找到自己的方向，走向人生巅峰。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;也祝愿若泽数据能够越办越好，力争中国第一大数据培训机构，为我国大数据的繁荣昌盛，输送更多的人才。</p><p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<font color="blue">学弟们，我是2019届刚毕业的高级班第6期某学员，也是你们的学长哟，长沙等你哟！</font></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/DwjTCTeHvKhr.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/0hUTrfwVQxjm.png?imageslim" alt="mark"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;&lt;center&gt;不吹不擂，不自不黑，&lt;/center&gt;&lt;br&gt;&lt;center&gt;你愿意花3min，阅读刚毕业的我吗？&lt;/center&gt;&lt;br&gt;&lt;center&gt;以下所有内容真实，禁得住考验！&lt;/center&gt;
    
    </summary>
    
      <category term="有缘大数据" scheme="http://yoursite.com/categories/%E6%9C%89%E7%BC%98%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://yoursite.com/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="企业在职" scheme="http://yoursite.com/tags/%E4%BC%81%E4%B8%9A%E5%9C%A8%E8%81%8C/"/>
    
      <category term="感悟" scheme="http://yoursite.com/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>Kudu与Spark集成</title>
    <link href="http://yoursite.com/2019/07/12/Kudu%E4%B8%8ESpark%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2019/07/12/Kudu与Spark集成/</id>
    <published>2019-07-11T16:00:00.000Z</published>
    <updated>2019-07-23T08:59:20.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">    &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;</span><br><span class="line">    &lt;kudu.version&gt;1.5.0&lt;/kudu.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line">import org.apache.kudu.client._</span><br><span class="line">import collection.JavaConverters._</span><br><span class="line">object KuduApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;KuduApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line">     //Read a table from Kudu</span><br><span class="line">    val df = spark.read</span><br><span class="line">          .options(Map(&quot;kudu.master&quot; -&gt; &quot;10.19.120.70:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;test_table&quot;))</span><br><span class="line">          .format(&quot;kudu&quot;).load</span><br><span class="line">        df.schema.printTreeString()</span><br><span class="line">//    // Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">//    val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    // The schema is encoded in a string</span><br><span class="line">//    val schemalString=&quot;id,age,name&quot;</span><br><span class="line">//</span><br><span class="line">//    // Generate the schema based on the string of schema</span><br><span class="line">//    val fields=schemalString.split(&quot;,&quot;).map(filedName=&gt;StructField(filedName,StringType,nullable =true ))</span><br><span class="line">//    val schema=StructType(fields)</span><br><span class="line">//</span><br><span class="line">//</span><br><span class="line">//    val KuduTable = kuduContext.createTable(</span><br><span class="line">//     &quot;test_table&quot;, schema, Seq(&quot;id&quot;),</span><br><span class="line">//     new CreateTableOptions()</span><br><span class="line">//       .setNumReplicas(1)</span><br><span class="line">//       .addHashPartitions(List(&quot;id&quot;).asJava, 3)).getSchema</span><br><span class="line">//</span><br><span class="line">//    val  id  = KuduTable.getColumn(&quot;id&quot;)</span><br><span class="line">//    print(id)</span><br><span class="line">//</span><br><span class="line">//    kuduContext.tableExists(&quot;test_table&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font size="3"><b>现象:通过spark sql 操作报如下错误:</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: Failed to find data source: kudu. Please find packages at http://spark.apache.org/third-party-projects.html</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:549)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)</span><br><span class="line">    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)</span><br><span class="line">    at cn.zhangyu.KuduApp$.main(KuduApp.scala:18)</span><br><span class="line">    at cn.zhangyu.KuduApp.main(KuduApp.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: kudu.DefaultSource</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21$$anonfun$apply$12.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$21.apply(DataSource.scala:533)</span><br><span class="line">    at scala.util.Try.orElse(Try.scala:84)</span><br><span class="line">    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:533)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure><font size="3"><b>而通过KuduContext是可以操作的没有报错,代码为上面注解部分</b></font><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>查询<a href="https://kudu.apache.org/" target="_blank" rel="noopener">kudu官网</a>:</p><p>官网中说出了版本的问题<br><br>如果将Spark 2与Scala 2.11一起使用，请使用kudu-spark2_2.11控件。<br><br>kudu-spark版本1.8.0及更低版本的语法略有不同。有关有效示例，请参阅您的版本的文档。可以在发布页面上找到版本化文档。</p><ul><li>spark-shell –packages org.apache.kudu:kudu-spark2_2.11:1.9.0 看到了 官网使用的是1.9.0的版本.</li></ul><font size="3"><b>但是但是但是</b></font><p>官网下面说到了下面几个集成问题</p><ul><li>Spark 2.2+在运行时需要Java 8，即使Kudu Spark 2.x集成与Java 7兼容。Spark 2.2是Kudu 1.5.0的默认依赖版本。</li><li>当注册为临时表时，必须为名称包含大写或非ascii字符的Kudu表分配备用名称。</li><li>包含大写或非ascii字符的列名的Kudu表不能与SparkSQL一起使用。可以在Kudu中重命名列以解决此问题。</li><li>并且OR谓词不会被推送到Kudu，而是由Spark任务进行评估。只有LIKE带有后缀通配符的谓词才会被推送到Kudu，这意味着它LIKE “FOO%”被推下但LIKE “FOO%BAR”不是。</li><li>Kudu不支持Spark SQL支持的每种类型。例如， Date不支持复杂类型。</li><li>Kudu表只能在SparkSQL中注册为临时表。使用HiveContext可能无法查询Kudu表。</li></ul><font size="3"><b>那就很奇怪了我用的1.5.0版本报错为:找不到类,数据源有问题</b></font><p>但是把Kudu改成<code>1.9.0</code> 问题解决</p><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = false)</span><br><span class="line"> |-- age: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Spark集成最佳实践"><a href="#Spark集成最佳实践" class="headerlink" title="Spark集成最佳实践"></a>Spark集成最佳实践</h3><ul><li><p>每个群集避免多个Kudu客户端。</p><p>一个常见的Kudu-Spark编码错误是实例化额外的KuduClient对象。在kudu-spark中，a KuduClient属于KuduContext。Spark应用程序代码不应创建另一个KuduClient连接到同一群集。相反，应用程序代码应使用KuduContext访问KuduClient使用 KuduContext#syncClient。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Use KuduContext to create, delete, or write to Kudu tables</span><br><span class="line">   val kuduContext = new KuduContext(&quot;10.19.120.70:7051&quot;, spark.sparkContext)</span><br><span class="line">   val list = kuduContext.syncClient.getTablesList.getTablesList</span><br><span class="line">   if (list.iterator().hasNext)&#123;</span><br><span class="line">     print(list.iterator().next())</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li>要诊断KuduClientSpark作业中的多个实例，请查看主服务器的日志中的符号，这些符号会被来自不同客户端的许多GetTableLocations或 GetTabletLocations请求过载，通常大约在同一时间。这种症状特别适用于Spark Streaming代码，其中创建KuduClient每个任务将导致来自新客户端的主请求的周期性波。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
      <category term="集成" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>生产上Flume如何源码编译and远程Debug</title>
    <link href="http://yoursite.com/2019/07/11/%E7%94%9F%E4%BA%A7%E4%B8%8AFlume%E5%A6%82%E4%BD%95%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91and%E8%BF%9C%E7%A8%8BDebug/"/>
    <id>http://yoursite.com/2019/07/11/生产上Flume如何源码编译and远程Debug/</id>
    <published>2019-07-10T16:00:00.000Z</published>
    <updated>2019-07-24T02:22:50.338Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h3><ol><li>apache-flume-1.8.0-src （官网下载源码，或者git下载）</li><li>jdk1.8</li></ol><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><ol><li>用Inteallij IDEA 导入已下载的flume工程</li><li><p>修改<code>flume-parent</code>下的 pom.xml 添加 aliyun的仓库（加快下载，有些包直接从maven repository上下载很慢 ）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;&lt;!-- 代码库 --&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;maven-ali&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">        &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">        &lt;/releases&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">            &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;</span><br><span class="line">            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;</span><br><span class="line">        &lt;/snapshots&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure></li><li><p>开始漫长的编译过程</p><p>如果是第一次的话，可能下载包要花2个多小时，中间可能会报错（报错主要是某些包没下载成功，此时可以手动从仓库中手动下载到本地，然后放在本地 的maven 包路径下，默认的本地的包路径是 C:\Users\你的用户名.m2\repository 下面）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn clean</span><br><span class="line">mvn install -DskipTests -U -Dtar</span><br></pre></td></tr></table></figure></li><li><p>由于整个项目是用pom管理包和模块，十分方便，如果在整个编译过程中，某些模块你需要编译，或者编译耗时，或者编译失败，并且你暂时用不到整个模块，可以从pom中注释掉这个模块，不做编译，具体做法如下图所示（具体的根据你的需求操作即可）<br><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/9xjUKMaIiAlm.png?imageslim" alt="mark"></p></li></ol><h3 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h3><ol><li><p>修改服务器上的 bin/flume-ng 中的JAVA_OPTS变量，支持远程调试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS=&quot;-Xmx20m -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y&quot;</span><br></pre></td></tr></table></figure></li></ol><pre><code>具体如下：</code></pre><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/3LFwnuvEtCmR.png?imageslim" alt="mark"></p><ol start="2"><li>Inteallij IDEA配置 ，远程调试</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/kIM6h3SrtMSb.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/xq1oTGBX3gIa.png?imageslim" alt="mark"></p><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/IjVaRSdXrHWJ.png?imageslim" alt="mark"></p><ol start="3"><li>在任意代码出打上断点</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/zyyRQQtEuqLu.png?imageslim" alt="mark"></p><ol start="4"><li><p>启动flume-ng（按实际情况修改下面命令）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file ./conf/flume-custom.properties --name hd1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>启动后日志如下：</p></li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/vrL4ekeR7sdi.png?imageslim" alt="mark"></p><ol start="5"><li>Inteallij IDEA 开始debug，可以发现在断点处停止，debug流程成功了</li></ol><p><img src="http://pucwi7op1.bkt.clouddn.com/blog/20190724/hSlp85lEX5Dy.png?imageslim" alt="mark"></p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul><li>源码编译耗时费力，需要耐心，熬过去了，会有很大收获，同样也是更好理解源码的开始，万事开头难</li><li>remote debug可以更方便的了解执行流程，学习源码的捷径</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://yoursite.com/categories/Flume/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
      <category term="源码编译" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Docker实践之常用命令及自定义Web首页</title>
    <link href="http://yoursite.com/2019/06/28/Docker%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89Web%E9%A6%96%E9%A1%B5/"/>
    <id>http://yoursite.com/2019/06/28/Docker实践之常用命令及自定义Web首页/</id>
    <published>2019-06-27T16:00:00.000Z</published>
    <updated>2019-07-09T03:05:32.043Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker --help</span><br><span class="line">//常用命令：</span><br><span class="line">--------------------------------------------</span><br><span class="line">  exec        Run a command in a running container</span><br><span class="line">  history     Show the history of an image</span><br><span class="line">  images      List images</span><br><span class="line">  kill        Kill one or more running containers</span><br><span class="line">  logs        Fetch the logs of a container</span><br><span class="line">  ps          List containers</span><br><span class="line">  pull        Pull an image or a repository from a registry</span><br><span class="line">  push        Push an image or a repository to a registry</span><br><span class="line">  rename      Rename a container</span><br><span class="line">  restart     Restart one or more containers</span><br><span class="line">  rm          Remove one or more containers</span><br><span class="line">  rmi         Remove one or more images</span><br><span class="line">  run         Run a command in a new container</span><br><span class="line">  search      Search the Docker Hub for images</span><br><span class="line">  start       Start one or more stopped containers</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics</span><br><span class="line">  stop        Stop one or more running containers</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span><br><span class="line">  top         Display the running processes of a container</span><br><span class="line">  version     Show the Docker version information</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker search nginx</span><br><span class="line">NAME                                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">nginx                                                  Official build of Nginx.                        10179               [OK]</span><br><span class="line">jwilder/nginx-proxy                                    Automated Nginx reverse proxy for docker con…   1454                                    [OK]</span><br><span class="line">richarvey/nginx-php-fpm                                Container running Nginx + PHP-FPM capable of…   645                                     [OK]</span><br><span class="line">jrcs/letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as p…   436                                     [OK]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker pull nginx   //拉取官方版本的nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">f17d81b4b692: Pull complete</span><br><span class="line">82dca86e04c3: Downloading  11.24MB/22.2MB</span><br><span class="line">82dca86e04c3: Pull complete</span><br><span class="line">046ccb106982: Pull complete</span><br><span class="line">Digest: sha256:d59a1aa7866258751a261bae525a1842c7ff0662d4f34a355d5f36826abc0341</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>docker相当于一个小型的linux系统，但是它又只是一个单一的进程，可以不对外暴露端口号，如果对外暴露端口号，那也只能有一个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \     //运行一个实例</span><br><span class="line">--name huluwa-niginx-v1 \            //自定义一个名字</span><br><span class="line">-d \                                 //后台运行</span><br><span class="line">-p 8080:80 \                         //对外暴露的端口号，对应linux的8080端口号</span><br><span class="line">nginx:latest                         //运行的镜像名及版本</span><br><span class="line">d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps     //查看当前活动的实例</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   2 minutes ago</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# ps -ef | grep docker</span><br><span class="line">root     23182     1  0 22:00 ?        00:00:07 /usr/bin/dockerd</span><br><span class="line">root     23189 23182  0 22:00 ?        00:00:03 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root     25014 23182  0 22:27 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8080 -container-ip 172.17.0.2 -container-port 80</span><br><span class="line">root     25021 23189  0 22:27 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root     25492 10525  0 22:35 pts/0    00:00:00 grep --color=auto docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker-proxy -proto tcp&lt;br&gt;</span><br><span class="line">-host-ip 0.0.0.0   //&lt;br&gt;</span><br><span class="line">-host-port 8080   //linux系统的端口号&lt;br&gt;</span><br><span class="line">-container-ip 172.17.0.2  //docker相当于一个小型的linux系统，这就是小型系统的IP地址&lt;br&gt;</span><br><span class="line">-container-port 80  //docker内部的一个端口号&lt;br&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# netstat -nlp |grep 8080</span><br><span class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      25014/docker-proxy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker images    //查看所有的镜像</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              62f816a209e6        7 days ago          109MB</span><br><span class="line">mysql               5.6                 a46c2a2722b9        2 weeks ago         256MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        2 months ago        1.84kB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps -a   //查看所有实例，不论什么状态</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                    NAMES</span><br><span class="line">9883abaaad85        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour               0.0.0.0:3308-&gt;3306/tcp   huluwa-mysql-v5</span><br><span class="line">34fb53521694        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Created                                                 huluwa-mysql-v4</span><br><span class="line">2a5c95f3c043        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v3</span><br><span class="line">84e65fd24271        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v2</span><br><span class="line">b3c12bcb28eb        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysqlv1</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   12 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   13 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 13 hours ago        Exited (0) 13 hours ago                                 elastic_ritchie</span><br></pre></td></tr></table></figure><p>正在运行的status就是Up，已经关闭的status就是Exited</p><h3 id="自定义首页"><a href="#自定义首页" class="headerlink" title="自定义首页"></a>自定义首页</h3><ol><li>登录初始的nginx Web页面</li></ol><p><img src="/assets/blogImg/2019-06-28-1.png" alt="enter description here"></p><ol start="2"><li><p>通过index.html配置一个自定义的首页</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 html]# pwd</span><br><span class="line">/root/docker/nginx/html</span><br><span class="line">[root@hadoop004 html]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 92 Nov 13 23:09 index.html</span><br></pre></td></tr></table></figure><p>在windows中打开index.html页面是这样的：<br><img src="/assets/blogImg/2019-06-28-2.png" alt="enter description here"></p></li><li><p>将本地的html文件挂载到container中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \</span><br><span class="line">--name huluwa-niginx-v2 \</span><br><span class="line">-v /root/docker/nginx/html:/usr/share/nginx/html:ro \ //本地的/root/docker/nginx/html和容器里的/usr/share/nginx/html建立一个映射，将本地的文件夹挂载到容器里</span><br><span class="line">-d \</span><br><span class="line">-p 8082:80 \</span><br><span class="line">nginx:latest</span><br><span class="line">c7937fd855963c7cca831d495436881a16e7e9befa61288cb28e2ab8b986decf</span><br><span class="line">[root@hadoop004 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                         PORTS                  NAMES</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About a minute ago   Up About a minute              0.0.0.0:8082-&gt;80/tcp   huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About an hour ago    Up About an hour               0.0.0.0:8080-&gt;80/tcp   huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 About an hour ago    Exited (0) About an hour ago                          elastic_ritchie</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>打开ip:8082页面查看</li></ol><p><img src="/assets/blogImg/2019-06-28-3.png" alt="enter description here"></p><p>发现首页已经被置换为本地文件中的index.html文件<br><br>-v 把本地文件或文件夹挂载到容器中<br><br>挂载的目的，就是把容器中的数据保存在本地，容器进程移除后之后，数据不会丢失，如果不挂载的话，容器进程挂掉之后，数据就全没有了<br><br>ro：可读<br><br>rw：可读写<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch部署</title>
    <link href="http://yoursite.com/2019/06/21/Elasticsearch%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/06/21/Elasticsearch部署/</id>
    <published>2019-06-20T16:00:00.000Z</published>
    <updated>2019-07-15T08:23:48.910Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>上传解压elasticsearch的tar包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 elasticsearch]# ll</span><br><span class="line">total 236</span><br><span class="line">drwxr-xr-x  2 root root   4096 Apr 22 11:28 bin</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 config</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 lib</span><br><span class="line">-rw-r--r--  1 root root  11358 Aug 14  2017 LICENSE.txt</span><br><span class="line">drwxr-xr-x 13 root root   4096 Aug 14  2017 modules</span><br><span class="line">-rw-r--r--  1 root root 194187 Aug 14  2017 NOTICE.txt</span><br><span class="line">drwxr-xr-x  2 root root   4096 Aug 14  2017 plugins</span><br><span class="line">-rw-r--r--  1 root root   9549 Aug 14  2017 README.textile</span><br><span class="line">[root@hadoop001 elasticsearch]# cd config/</span><br><span class="line">[root@hadoop001 config]# ll</span><br><span class="line">total 16</span><br><span class="line">-rw-rw---- 1 root root 2854 Aug 14  2017 elasticsearch.yml</span><br><span class="line">-rw-rw---- 1 root root 3064 Aug 14  2017 jvm.options</span><br><span class="line">-rw-rw---- 1 root root 4456 Aug 14  2017 log4j2.properties</span><br><span class="line">[root@hadoop001 config]# vi elasticsearch.yml</span><br><span class="line">cluster.name: HLWCluster</span><br><span class="line">node.name: hadoop001</span><br><span class="line">path.data: /root/app/elasticsearch/data</span><br><span class="line">path.logs: /root/app/elasticsearch/logs</span><br><span class="line">network.host: 172.26.183.103</span><br><span class="line">[root@hadoop001 config]# cd ../</span><br><span class="line">[root@hadoop001 elasticsearch]# cd bin</span><br><span class="line">[root@hadoop001 bin]# ll</span><br><span class="line">total 348</span><br><span class="line">-rwxr-xr-x 1 root root   8075 Aug 14  2017 elasticsearch</span><br><span class="line">-rw-r--r-- 1 root root   3343 Aug 14  2017 elasticsearch.bat</span><br><span class="line">-rw-r--r-- 1 root root   1023 Aug 14  2017 elasticsearch.in.bat</span><br><span class="line">-rwxr-xr-x 1 root root    367 Aug 14  2017 elasticsearch.in.sh</span><br><span class="line">-rwxr-xr-x 1 root root   2550 Aug 14  2017 elasticsearch-keystore</span><br><span class="line">-rw-r--r-- 1 root root    743 Aug 14  2017 elasticsearch-keystore.bat</span><br><span class="line">-rwxr-xr-x 1 root root   2540 Aug 14  2017 elasticsearch-plugin</span><br><span class="line">-rw-r--r-- 1 root root    731 Aug 14  2017 elasticsearch-plugin.bat</span><br><span class="line">-rw-r--r-- 1 root root  11239 Aug 14  2017 elasticsearch-service.bat</span><br><span class="line">-rw-r--r-- 1 root root 104448 Aug 14  2017 elasticsearch-service-mgr.exe</span><br><span class="line">-rw-r--r-- 1 root root 103936 Aug 14  2017 elasticsearch-service-x64.exe</span><br><span class="line">-rw-r--r-- 1 root root  80896 Aug 14  2017 elasticsearch-service-x86.exe</span><br><span class="line">-rwxr-xr-x 1 root root    223 Aug 14  2017 elasticsearch-systemd-pre-exec</span><br><span class="line">-rwxr-xr-x 1 root root   2514 Aug 14  2017 elasticsearch-translog</span><br><span class="line">-rw-r--r-- 1 root root   1435 Aug 14  2017 elasticsearch-translog.bat</span><br><span class="line">[root@hadoop001 bin]# ./elasticsearch</span><br><span class="line">[2019-04-22T11:47:17,068][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [hadoop001] uncaught exception in thread [main]</span><br><span class="line">org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:127) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">Caused by: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        ... 6 more</span><br><span class="line">[root@hadoop001 bin]# cd ../logs</span><br><span class="line">[root@hadoop001 logs]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_deprecation.log</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_index_indexing_slowlog.log</span><br><span class="line">-rw-r--r-- 1 root root    0 Apr 22 11:47 HLWCluster_index_search_slowlog.log</span><br><span class="line">-rw-r--r-- 1 root root 2691 Apr 22 11:47 HLWCluster.log</span><br><span class="line">[root@hadoop001 logs]# cat HLWCluster.log</span><br><span class="line">[2019-04-22T11:47:17,058][ERROR][o.e.b.Bootstrap          ] Exception</span><br><span class="line">java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">[2019-04-22T11:47:17,068][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [hadoop001] uncaught exception in thread [main]</span><br><span class="line">org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:127) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.cli.Command.main(Command.java:88) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">Caused by: java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:106) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:351) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) ~[elasticsearch-5.5.2.jar:5.5.2]</span><br><span class="line">        ... 6 more</span><br></pre></td></tr></table></figure><p>第一个报错：不能用root用户去运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 app]# useradd esuser</span><br><span class="line">[root@hadoop001 app]# chown -R esuser:esuser elasticsearch-5.5.2</span><br><span class="line">[root@hadoop001 app]# mv ./elasticsearch-5.5.2 /home/esuser/elasticsearch-5.5.2</span><br><span class="line">[root@hadoop001 app]# su - esuser</span><br><span class="line">[esuser@hadoop001 ~]$ cd elasticsearch</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ pwd</span><br><span class="line">/home/esuser/elasticsearch</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ ll</span><br><span class="line">total 244</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:28 bin</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:46 config</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:42 data</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Aug 14  2017 lib</span><br><span class="line">-rw-r--r--  1 esuser esuser  11358 Aug 14  2017 LICENSE.txt</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Apr 22 11:47 logs</span><br><span class="line">drwxr-xr-x 13 esuser esuser   4096 Aug 14  2017 modules</span><br><span class="line">-rw-r--r--  1 esuser esuser 194187 Aug 14  2017 NOTICE.txt</span><br><span class="line">drwxr-xr-x  2 esuser esuser   4096 Aug 14  2017 plugins</span><br><span class="line">-rw-r--r--  1 esuser esuser   9549 Aug 14  2017 README.textile</span><br><span class="line">[esuser@hadoop001 elasticsearch]$ cd config</span><br><span class="line">[esuser@hadoop001 config]$ vi elasticsearch.yml</span><br><span class="line">path.data: /home/esuser/elasticsearch/data</span><br><span class="line">path.logs: /home/esuser/elasticsearch/logs</span><br><span class="line">[esuser@hadoop001 config]$ cd ../bin</span><br><span class="line">[esuser@hadoop001 bin]$ ./elasticsearch</span><br><span class="line">[2019-04-22T12:16:04,747][INFO ][o.e.n.Node               ] [hadoop001] initializing ...</span><br><span class="line">[2019-04-22T12:16:04,822][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [17.8gb], net total_space [39.2gb], spins? [unknown], types [rootfs]</span><br><span class="line">[2019-04-22T12:16:04,823][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] heap size [1.9gb], compressed ordinary object pointers [true]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] node name [hadoop001], node ID [oKhZUG8wTl2bz38DZ_5rHA]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] version[5.5.2], pid[4894], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_45/25.45-b02]</span><br><span class="line">[2019-04-22T12:16:04,824][INFO ][o.e.n.Node               ] [hadoop001] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/home/esuser/elasticsearch]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [aggs-matrix-stats]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [ingest-common]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-expression]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-groovy]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-mustache]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-painless]</span><br><span class="line">[2019-04-22T12:16:06,014][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [parent-join]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [percolator]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [reindex]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty3]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty4]</span><br><span class="line">[2019-04-22T12:16:06,015][INFO ][o.e.p.PluginsService     ] [hadoop001] no plugins loaded</span><br><span class="line">[2019-04-22T12:16:08,200][INFO ][o.e.d.DiscoveryModule    ] [hadoop001] using discovery type [zen]</span><br><span class="line">[2019-04-22T12:16:08,882][INFO ][o.e.n.Node               ] [hadoop001] initialized</span><br><span class="line">[2019-04-22T12:16:08,883][INFO ][o.e.n.Node               ] [hadoop001] starting ...</span><br><span class="line">[2019-04-22T12:16:09,051][INFO ][o.e.t.TransportService   ] [hadoop001] publish_address &#123;172.26.183.103:9300&#125;, bound_addresses &#123;172.26.183.103:9300&#125;</span><br><span class="line">[2019-04-22T12:16:09,063][INFO ][o.e.b.BootstrapChecks    ] [hadoop001] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</span><br><span class="line">ERROR: [2] bootstrap checks failed</span><br><span class="line">[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]</span><br><span class="line">[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span><br><span class="line">[2019-04-22T12:16:09,080][INFO ][o.e.n.Node               ] [hadoop001] stopping ...</span><br><span class="line">[2019-04-22T12:16:09,110][INFO ][o.e.n.Node               ] [hadoop001] stopped</span><br><span class="line">[2019-04-22T12:16:09,110][INFO ][o.e.n.Node               ] [hadoop001] closing ...</span><br><span class="line">[2019-04-22T12:16:09,128][INFO ][o.e.n.Node               ] [hadoop001] closed</span><br></pre></td></tr></table></figure><p>第二次报错：<br><br>ERROR: [2] bootstrap checks failed<br><br>[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]<br><br>[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072 </span><br><span class="line">[root@hadoop001 ~]# vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">[root@hadoop001 etc]# sysctl -p</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time = 120</span><br><span class="line">net.ipv4.conf.all.rp_filter = 0</span><br><span class="line">net.ipv4.conf.default.rp_filter = 0</span><br><span class="line">net.ipv4.conf.default.arp_announce = 2</span><br><span class="line">net.ipv4.conf.lo.arp_announce = 2</span><br><span class="line">net.ipv4.conf.all.arp_announce = 2</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 1024</span><br><span class="line">net.ipv4.tcp_synack_retries = 2</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br><span class="line">kernel.sysrq = 1</span><br><span class="line">vm.max_map_count = 262144</span><br></pre></td></tr></table></figure><p>以上设置永久生效需要重启reboot</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 etc]# reboot</span><br></pre></td></tr></table></figure><p>查看配置有没有生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# su - esuser</span><br><span class="line">[esuser@hadoop001 ~]$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 63461</span><br><span class="line">max locked memory       (kbytes, -l) 64</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 65536</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 4096</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br><span class="line">[root@hadoop001 config]# vi elasticsearch.yml</span><br><span class="line">bootstrap.memory_lock: false</span><br><span class="line">bootstrap.system_call_filter: false</span><br><span class="line">[esuser@hadoop001 bin]$ ./elasticsearch</span><br><span class="line">[2019-04-22T12:58:13,817][INFO ][o.e.n.Node               ] [hadoop001] initializing ...</span><br><span class="line">[2019-04-22T12:58:13,896][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [17.8gb], net total_space [39.2gb], spins? [unknown], types [rootfs]</span><br><span class="line">[2019-04-22T12:58:13,897][INFO ][o.e.e.NodeEnvironment    ] [hadoop001] heap size [1.9gb], compressed ordinary object pointers [true]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] node name [hadoop001], node ID [oKhZUG8wTl2bz38DZ_5rHA]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] version[5.5.2], pid[3085], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_45/25.45-b02]</span><br><span class="line">[2019-04-22T12:58:13,898][INFO ][o.e.n.Node               ] [hadoop001] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/home/esuser/elasticsearch]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [aggs-matrix-stats]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [ingest-common]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-expression]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-groovy]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-mustache]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [lang-painless]</span><br><span class="line">[2019-04-22T12:58:15,302][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [parent-join]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [percolator]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [reindex]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty3]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] loaded module [transport-netty4]</span><br><span class="line">[2019-04-22T12:58:15,303][INFO ][o.e.p.PluginsService     ] [hadoop001] no plugins loaded</span><br><span class="line">[2019-04-22T12:58:17,555][INFO ][o.e.d.DiscoveryModule    ] [hadoop001] using discovery type [zen]</span><br><span class="line">[2019-04-22T12:58:18,182][INFO ][o.e.n.Node               ] [hadoop001] initialized</span><br><span class="line">[2019-04-22T12:58:18,183][INFO ][o.e.n.Node               ] [hadoop001] starting ...</span><br><span class="line">[2019-04-22T12:58:18,357][INFO ][o.e.t.TransportService   ] [hadoop001] publish_address &#123;172.26.183.103:9300&#125;, bound_addresses &#123;172.26.183.103:9300&#125;</span><br><span class="line">[2019-04-22T12:58:18,368][INFO ][o.e.b.BootstrapChecks    ] [hadoop001] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</span><br><span class="line">[2019-04-22T12:58:21,428][INFO ][o.e.c.s.ClusterService   ] [hadoop001] new_master &#123;hadoop001&#125;&#123;oKhZUG8wTl2bz38DZ_5rHA&#125;&#123;aY3CSi1XTOCq29GPw3cFqA&#125;&#123;172.26.183.103&#125;&#123;172.26.183.103:9300&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)</span><br><span class="line">[2019-04-22T12:58:21,471][INFO ][o.e.h.n.Netty4HttpServerTransport] [hadoop001] publish_address &#123;172.26.183.103:9200&#125;, bound_addresses &#123;172.26.183.103:9200&#125;</span><br><span class="line">[2019-04-22T12:58:21,471][INFO ][o.e.n.Node               ] [hadoop001] started</span><br><span class="line">[2019-04-22T12:58:21,473][INFO ][o.e.g.GatewayService     ] [hadoop001] recovered [0] indices into cluster_state</span><br></pre></td></tr></table></figure><p>需要后台运行的话加个 -d 参数：./elasticsearch -d</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[esuser@hadoop001 ~]$ curl -XGET &apos;172.26.183.103:9200/?pretty&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;hadoop001&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;HLWCluster&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;jb0pPZNBTwmQj6iNBWtvzg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;5.5.2&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;b2f0c09&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2017-08-14T12:33:14.154Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;6.6.0&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://yoursite.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://yoursite.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>不得不会的Spark SQL常见4种数据源</title>
    <link href="http://yoursite.com/2019/06/20/%E4%B8%8D%E5%BE%97%E4%B8%8D%E4%BC%9A%E7%9A%84Spark%20SQL%E5%B8%B8%E8%A7%814%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/</id>
    <published>2019-06-19T16:00:00.000Z</published>
    <updated>2019-06-20T02:02:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h3><h4 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h4><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。</p><p>修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          </span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><h4 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h3><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Encoders for most common types are automatically provided by importing spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// DataFrames can be saved as Parquet files, maintaining the schema information</span><br><span class="line">peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Read in the parquet file created above</span><br><span class="line">// Parquet files are self-describing so the schema is preserved</span><br><span class="line">// The result of loading a Parquet file is also a DataFrame</span><br><span class="line">val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet files can also be used to create a temporary view and then used in SQL statements</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">// +------------+</span><br><span class="line">// |       value|</span><br><span class="line">// +------------+</span><br><span class="line">// |Name: Justin|</span><br><span class="line">// +------------+</span><br></pre></td></tr></table></figure><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet</p><p>或SQLContext.read.load，Spark SQL将自动解析分区信息。</p><p>返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项：</p><p><code>spark.sql.parquet.mergeSchema</code>为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sqlContext from the previous example is used in this example.</span><br><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, stored into a partition directory</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths.</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">.config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><h4 id="内嵌Hive应用"><a href="#内嵌Hive应用" class="headerlink" title="内嵌Hive应用"></a>内嵌Hive应用</h4><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.warehouse.dir=</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive应用"><a href="#外部Hive应用" class="headerlink" title="外部Hive应用"></a>外部Hive应用</h4><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;rddtable2&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之六: JVM参数设置及分析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%85%AD_JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之六_JVM参数设置及分析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:28.434Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --><a id="more"></a><p>不管是YGC还是Full GC,GC过程中都会对导致程序运行中中断,正确的选择不同的GC策略,调整JVM、GC的参数，可以极大的减少由于GC工作，而导致的程序运行中断方面的问题，进而适当的提高Java程序的工作效率。但是调整GC是以个极为复杂的过程，由于各个程序具备不同的特点，如：web和GUI程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要cup个数，内存不同），所以使用的GC种类也会不同(如何选择见GC种类及如何选择)。本文将注重介绍JVM、GC的一些重要参数的设置来提高系统的性能。</p><p>JVM内存组成及GC相关内容请见之前的文章:JVM内存组成 GC策略&amp;内存申请</p><font size="3"><strong>JVM参数的含义</strong></font>实例见实例分析<br><br>参数名称|含义|默认值| |<br>——|—–|—–|—<br>-Xms|初始堆大小|物理内存的1/64(&lt;1GB)|默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.<br>-Xmx|最大堆大小|物理内存的1/4(&lt;1GB)|默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制<br>-Xmn|年轻代大小(1.4or lator)||注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。<br>整个堆大小=年轻代大小 + 年老代大小 + 持久代大小.<br>增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8<br>-XX:NewSize|设置年轻代大小(for 1.3/1.4)|<br>-XX:MaxNewSize|年轻代最大值(for 1.3/1.4)|<br>-XX:PermSize|设置持久代(perm gen)初始值|物理内存的1/64<br>-XX:MaxPermSize|设置持久代最大值|物理内存的1/4<br>-Xss|每个线程的堆栈大小||JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右。<br>一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。<br>-XX:ThreadStackSize|Thread Stack Size||(0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.]<br>-XX:NewRatio|年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)||-XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5<br>Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。<br>-XX:SurvivorRatio|Eden区与Survivor区的大小比值||设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10<br>-XX:LargePageSizeInBytes|内存页的大小不可设置过大， 会影响Perm的大小||=128m<br>-XX:+UseFastAccessorMethods|原始类型的快速优化<br>-XX:+DisableExplicitGC|关闭System.gc()||这个参数需要严格的测试<br>-XX:MaxTenuringThreshold|垃圾最大年龄||如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率<br>该参数只有在串行GC时才有效.<br>-XX:+AggressiveOpts|加快编译<br>-XX:+UseBiasedLocking|锁机制的性能改善<br>-Xnoclassgc|禁用垃圾回收<br>-XX:SoftRefLRUPolicyMSPerMB|每兆堆空闲空间中SoftReference的存活时间|1s|softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap<br>-XX:PretenureSizeThreshold|对象超过多大是直接在旧生代分配|0|单位字节 新生代采用Parallel Scavenge GC时无效<br>另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.<br>-XX:TLABWasteTargetPercent|TLAB占eden区的百分比|1%<br>-XX:+CollectGen0First|FullGC时是否先YGC|false<br><br><font size="3"><strong>并行收集器相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseParallelGC</td><td>Full GC采用parallel MSC<br>(此项待验证)</td><td></td><td>选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证)</td></tr><tr><td>-XX:+UseParNewGC</td><td>设置年轻代为并行收集</td><td></td><td>可与CMS收集同时使用<br>JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值</td></tr><tr><td>-XX:ParallelGCThreads</td><td>并行收集器的线程数</td><td></td><td>此值最好配置与处理器数目相等 同样适用于CMS</td></tr><tr><td>-XX:+UseParallelOldGC</td><td>年老代垃圾收集方式为并行收集(Parallel Compacting)</td><td></td><td>这个是JAVA 6出现的参数选项</td></tr><tr><td>-XX:MaxGCPauseMillis</td><td>每次年轻代垃圾回收的最长时间(最大暂停时间)</td><td></td><td>如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.</td></tr></tbody></table><p>-XX:+UseAdaptiveSizePolicy 自动选择年轻代区大小和相应的Survivor区比例<br>设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开.<br>-XX:GCTimeRatio|设置垃圾回收时间占程序运行时间的百分比||公式为1/(1+n)<br>-XX:+ScavengeBeforeFullGC|Full GC前调用YGC|true|Do young generation GC prior to a full GC. (Introduced in 1.4.1.)</p><font size="3"><strong>CMS相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseConcMarkSweepGC</td><td>使用CMS内存收集</td><td></td><td>测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.???</td></tr><tr><td>-XX:+AggressiveHeap</td><td></td><td></td><td>试图是使用大量的物理内存，长时间大内存使用的优化，能检查计算资源（内存， 处理器数量），至少需要256MB内存，大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升）</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>多少次后进行内存压缩</td><td>由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生”碎片”,使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理.</td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>降低标记停顿</td></tr><tr><td>-XX+UseCMSCompactAtFullCollection</td><td>在FULL GC的时候， 对年老代的压缩</td><td></td><td>CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。可能会影响性能,但是可以消除碎片</td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td><td>禁止hostspot自行触发CMS GC</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction=70</td><td>使用cms作为垃圾回收，使用70％后开始CMS收集</td><td>92</td><td>为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式<strong>CMSInitiatingOccupancyFraction</strong>计算公式</td></tr><tr><td>-XX:CMSInitiatingPermOccupancyFraction</td><td>设置Perm Gen使用到达多少比率时触发</td><td>92</td></tr><tr><td>-XX:+CMSIncrementalMode</td><td>设置为增量模式</td><td></td><td>用于单CPU情况</td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr></tbody></table><font size="3"><strong>辅助信息</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+PrintGC</td><td></td><td></td><td>输出形式:<br>[GC 118250K-&gt;113543K(130112K), 0.0094143 secs]<br>[Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs]</td></tr><tr><td>-XX:+PrintGCDetails</td><td></td><td></td><td>输出形式:<br>[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs]<br>[GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs]</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td></td></tr><tr><td>-XX:+PrintGC:PrintGCTimeStamps</td><td></td><td></td><td>可与-XX:+PrintGC -XX:+PrintGCDetails混合使用<br>输出形式:11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs]</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td><td></td><td>输出形式:Total time for which application threads were stopped: 0.0468229 seconds</td></tr><tr><td>-XX:+PrintGCApplicationConcurrentTime</td><td>打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用</td><td></td><td>输出形式:Application time: 0.5291524 seconds</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td><td></td></tr><tr><td>-Xloggc:filename</td><td>把相关日志信息记录到文件以便分析.<br>与上面几个配合使用</td><td></td></tr><tr><td>-XX:+PrintClassHistogram</td><td>garbage collects before printing the histogram.</td><td></td></tr><tr><td>-XX:+PrintTLAB</td><td>查看TLAB空间的使用情况</td><td></td></tr><tr><td>XX:+PrintTenuringDistribution</td><td>查看每次minor GC后新的存活周期的阈值</td><td></td><td>Desired survivor size 1048576 bytes, new threshold 7 (max 15)</td></tr></tbody></table><p>new threshold 7即标识新的存活周期的阈值为7。</p><font size="3" color="#FF4500"><strong>GC性能方面的考虑</strong></font><p>对于GC的性能主要有2个方面的指标：吞吐量throughput（工作时间不算gc的时间占总的时间比）和暂停pause（gc发生时app对外显示的无法响应）</p><ol><li><p>Total Heap</p><p>默认情况下，vm会增加/减少heap大小以维持free space在整个vm中占的比例，这个比例由MinHeapFreeRatio和MaxHeapFreeRatio指定。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>对vm分配尽可能多的memory；</li><li>将Xms和Xmx设为一样的值。如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。</li><li>处理器核数增加，内存也跟着增大。</li></ul></li><li><p>The Young Generation</p><p>另外一个对于app流畅性运行影响的因素是young generation的大小。young generation越大，minor collection越少；但是在固定heap size情况下，更大的young generation就意味着小的tenured generation，就意味着更多的major collection(major collection会引发minor collection)。<br><br>NewRatio反映的是young和tenured generation的大小比例。NewSize和MaxNewSize反映的是young generation大小的下限和上限，将这两个值设为一样就固定了young generation的大小（同Xms和Xmx设为一样）。<br><br>如果希望，SurvivorRatio也可以优化survivor的大小，不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>首先决定能分配给vm的最大的heap size，然后设定最佳的young generation的大小；</li><li>如果heap size固定后，增加young generation的大小意味着减小tenured generation大小。让tenured generation在任何时候够大，能够容纳所有live的data（留10%-20%的空余）。</li></ul></li></ol><font size="3" color="#FF4500"><strong>经验&amp;&amp;规则</strong></font><ul><li><p>年轻代大小选择</p><ul><li>响应时间优先的应用:尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象.</li><li>吞吐量优先的应用:尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用.</li><li>避免设置过小.当新生代设置过小时会导致:1.YGC次数更加频繁 2.可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC.</li></ul></li><li><p>年老代大小选择</p><ul><li>响应时间优先的应用:年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎 片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得:<br>并发垃圾收集信息、持久代并发收集次数、传统GC信息、花在年轻代和年老代回收上的时间比例。</li><li>吞吐量优先的应用:一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.原因是,这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象.</li></ul></li><li><p>较小堆引起的碎片问题</p><p>因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置:<br><br>-XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩.<br><br>-XX:CMSFullGCsBeforeCompaction=0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩</p></li><li><p>用64位操作系统，Linux下64位的jdk比32位jdk要慢一些，但是吃得内存更多，吞吐量更大</p></li><li>XMX和XMS设置一样大，MaxPermSize和MinPermSize设置一样大，这样可以减轻伸缩堆大小带来的压力</li><li>使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 然后老生代利用CMS并行收集， 这样能保证系统低延迟的吞吐效率。 实际上cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间</li><li>系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。(相关工具的使用方法将在后面的blog中介绍)</li><li>仔细了解自己的应用，如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。</li><li>采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿</li><li>JVM参数的设置(特别是 –Xmx –Xms –Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。</li></ul><p><strong>promotion failed</strong></p><p>垃圾回收时promotion failed是个很头痛的问题，一般可能是两种原因产生，第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间；第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。</p><p><strong>解决方方案一</strong></p><p>第一个原因我的最终解决办法是去掉救助空间，设置-XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0即可，第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。</p><p><strong>解决方案一的改进方案</strong></p><p>又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。</p><p>-Xmx4000M -Xms4000M -Xmn600M -XX:PermSize=500M -XX:MaxPermSize=500M -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log</p><font size="3" color="#FF4500">CMSInitiatingOccupancyFraction值与Xmn的关系公式</font><p>上面介绍了promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即：<br>(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2)) 进而推断出：</p><pre><code>CMSInitiatingOccupancyFraction &lt;=((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100 </code></pre><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当xmx=128 xmn=36 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-36)-(36-36/(1+2)))/(128-36)*100 =73.913 </span><br><span class="line"></span><br><span class="line">当xmx=128 xmn=24 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-24)-(24-24/(1+2)))/(128-24)*100=84.615… </span><br><span class="line"></span><br><span class="line">当xmx=3000 xmn=600 SurvivorRatior=1时  CMSInitiatingOccupancyFraction&lt;=((3000.0-600)-(600-600/(1+2)))/(3000-600)*100=83.33</span><br></pre></td></tr></table></figure><p>CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。</p><p>令：</p><p>网上一童鞋推断出的公式是：:(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn 这个公式个人认为不是很严谨，在内存小的时候会影响xmn的计算。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:39 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之四: 堆内存分配的CMS公式解析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%9B%9B_%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之四_堆内存分配的CMS公式解析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:36.520Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="JVM-堆内存组成"><a href="#JVM-堆内存组成" class="headerlink" title="JVM 堆内存组成"></a>JVM 堆内存组成</h3><p>Java堆由Perm区和Heap区组成，Heap区由Old区和New区（也叫Young区）组成，New区由Eden区、From区和To区（Survivor）组成。</p><p><img src="/assets/pic/2019-06-19-4-1.png" alt="JVM 堆内存组成"></p><p>Eden区用于存放新生成的对象。Eden中的对象生命不会超过一次Minor GC。Survivor Space 有两个，存放每次垃圾回收后存活的对象，即图的S0和S1。Old Generation Old区，也称老生代，主要存放应用程序中生命周期长的存活对象</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：</p><p>eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况。</p><p>即：</p><font color="blue">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2))</font><p>进而推断出：</p><font color="blue">CMSInitiatingOccupancyFraction &lt;= ((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100</font><table><thead><tr><th style="text-align:left">参数</th><th>含义</th></tr></thead><tbody><tr><td style="text-align:left">Xmx-Xmn</td><td>Old区大小</td></tr><tr><td style="text-align:left">CMSInitiatingOccupancyFraction/100</td><td>Old区百分之多少时,cms开始gc</td></tr><tr><td style="text-align:left">1-CMSInitiatingOccupancyFraction/100</td><td>Old区开始gc回收时剩余空间百分比</td></tr><tr><td style="text-align:left">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)</td><td>Old区开始gc回收时剩余空间大小</td></tr><tr><td style="text-align:left">(Xmn-Xmn/(SurvivorRatior+2))</td><td>eden+from survivor区的大小</td></tr></tbody></table><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存</td></tr><tr><td>-Xmn</td><td>java heap最小值，一般设置为Xmx的3、4分之一,等同于-XX:NewSize 和 -XX:MaxNewSize ,其实为<font color="blue">young区大小</font></td></tr><tr><td>-XX</td><td>CMSInitiatingOccupancyFraction=70 :使用cms作为垃圾回收使用70％后开始CMS收集</td></tr><tr><td>-XX</td><td>SurvivorRatio=2: 生还者池的大小，默认是2</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="堆内存分配的CMS公式解析" scheme="http://yoursite.com/tags/%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之七: Java程序性能分析工具Java VisualVM(Visual GC)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%83_Java%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7Java%20VisualVM(Visual%20GC)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之七_Java程序性能分析工具Java VisualVM(Visual GC)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:24.927Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><p>VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。</p><p>在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。</p><a id="more"></a><h3 id="Visual-GC-监控垃圾回收器"><a href="#Visual-GC-监控垃圾回收器" class="headerlink" title="Visual GC(监控垃圾回收器)"></a>Visual GC(监控垃圾回收器)</h3><p>Java VisualVM默认没有安装Visual GC插件，需要手动安装，JDK的安装目录的bin目露下双击jvisualvm.exe，即可打开Java VisualVM，点击菜单栏 工具-&gt;插件 安装Visual GC</p><p><img src="/assets/pic/2019-06-19-7-1.png" alt="Visual GC(监控垃圾回收器)1"></p><p>安装完成后重启Java VisualVM，Visual GC界面自动打开，即可看到JVM中堆内存的分代情况</p><p><img src="/assets/pic/2019-06-19-7-2.png" alt="Visual GC(监控垃圾回收器)2"></p><p>被监控的程序运行一段时间后Visual GC显示如下</p><p><img src="/assets/pic/2019-06-19-7-3.png" alt="Visual GC(监控垃圾回收器)3"></p><p>要看懂上面的图必须理解Java虚拟机的一些基本概念：</p><font color="blue" size="3"><b>堆(Heap)</b></font>：<font size="3">JVM管理的内存叫堆</font><p><strong>分代</strong>：根据对象的生命周期长短，把堆分为3个代：Young，Old和Permanent，根据不同代的特点采用不同的收集算法，扬长避短也。</p><ul><li><font color="blue">Young（年轻代）</font>年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor复制过来的对象。而且，Survivor区总有一个是空的。</li><li><font color="blue">Tenured（年老代）</font>年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。</li><li><font color="blue">Perm（持久代）</font>用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。</li></ul><font color="blue" size="3"><b>GC的基本概念</b></font><p>gc分为full gc 跟 minor gc，当每一块区满的时候都会引发gc。</p><ul><li><font color="blue">Scavenge GC</font><p>一般情况下，当新对象生成，并且在Eden申请空间失败时，就触发了Scavenge GC，堆Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。</p></li><li><font color="blue">Full GC</font><p>对整个堆进行整理，包括Young、Tenured和Perm。Full GC比Scavenge GC要慢，因此应该尽可能减少Full GC。有如下原因可能导致Full GC:</p><ul><li>上一次GC之后Heap的各域分配策略动态变化</li><li>System.gc()被显示调用</li><li>Perm域被写满</li><li>Tenured被写满</li></ul></li></ul><font color="blue" size="3"><b>内存溢出 out of memory</b></font><p>是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。</p><font color="blue" size="3"><b>内存泄露 memory leak</b></font><p>是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。<strong>其实说白了就是该内存空间使用完毕之后未回收。</strong></p><h3 id="Java-VisualVM的其他功能"><a href="#Java-VisualVM的其他功能" class="headerlink" title="Java VisualVM的其他功能"></a>Java VisualVM的其他功能</h3><ol><li><p>监视界面（cpu，类，堆，线程）</p><p><img src="/assets/pic/2019-06-19-7-4.png" alt="监视界面"></p></li><li><p>线程界面</p><p><img src="/assets/pic/2019-06-19-7-5.png" alt="线程界面"></p></li></ol><ol start="3"><li><p>Profile界面（性能剖析）</p><p>点击CPU按钮执行cpu分析查看方法</p><p><img src="/assets/pic/2019-06-19-7-6.png" alt="Profile界面"></p><p>点击内存按钮执行内存分析查看类</p><p><img src="/assets/pic/2019-06-19-7-7.png" alt="Profile界面"></p></li><li><p>堆dump和线程dump操作</p><p>Dump文件是进程的内存镜像，可以把程序的执行状态通过调试器保存到dump文件中，堆dump的dump文件内容如下图所示</p><p><img src="/assets/pic/2019-06-19-7-8.png" alt="Dump"></p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。&lt;/p&gt;&lt;p&gt;在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。&lt;/p&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之八: GC插件&amp;错误not_supported_for_this_jvm&amp;命令jstatd</title>
    <link href="http://yoursite.com/2019/06/19/JVM_8/"/>
    <id>http://yoursite.com/2019/06/19/JVM_8/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:54:00.202Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a> <font size="4"><b>1.插件安装</b></font><p>tools-&gt;plugin-&gt;Available Plugin 会有值得安装的插件，如：VisualGC</p><p><img src="/assets/pic/2019-06-19-8-1.png" alt="插件安装"></p><p>插件列表: <a href="https://visualvm.dev.java.net/plugins.html" target="_blank" rel="noopener">https://visualvm.dev.java.net/plugins.html</a></p><p>注意：上面提供的端口配置有些麻烦，不如直接这样做:</p><font size="4"><b>2.要使用 VisualGC 必须在远程机上启动jstatd代理程序，否则会显示<font color="#FF4500">“not supported for this jvm” </font>错误</b></font> <font color="blue">而启动 jstatd 时会有一个权限问题，需要做如下修改：</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@xxx-01 ~]# java -version</span><br><span class="line">java version &quot;1.7.0_55&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_55-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)</span><br><span class="line">[root@xxx-01 ~]# jstatd </span><br><span class="line">Could not create remote object</span><br><span class="line">access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">java.security.AccessControlException: access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)</span><br><span class="line">        at java.security.AccessController.checkPermission(AccessController.java:559)</span><br><span class="line">        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)</span><br><span class="line">        at java.lang.System.setProperty(System.java:783)</span><br><span class="line">        at sun.tools.jstatd.Jstatd.main(Jstatd.java:139)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@xxx-01 ~]# cd  /usr/java/jdk1.7.0_55</span><br><span class="line">[root@xxx-01 ~]# vi /usr/java/jdk1.7.0_55/jstatd.all.policy</span><br><span class="line">    grant codebase &quot;file:$&#123;JAVA_HOME&#125;/lib/tools.jar&quot; &#123;  </span><br><span class="line">     permission java.security.AllPermission;  </span><br><span class="line">    &#125;;  </span><br><span class="line">[root@xxx-01 jdk1.7.0_55]# jstatd -J-Djava.security.policy=/usr/java/jdk1.7.0_55/jstatd.all.policy  &amp;</span><br></pre></td></tr></table></figure><font color="blue">然后后台模式启动 jstatd命令</font> <font color="blue">主机面GC:</font><p><img src="/assets/pic/2019-06-19-8-2.png" alt="主机面GC"></p><font color="blue">Threads:</font><p><img src="/assets/pic/2019-06-19-8-3.png" alt="Threads"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之五: ParNew收集器+CMS收集器的产品案例分析(响应时间优先)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%94_ParNew%E6%94%B6%E9%9B%86%E5%99%A8+CMS%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BA%A7%E5%93%81%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90(%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之五_ParNew收集器+CMS收集器的产品案例分析(响应时间优先)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:55.585Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><font color="green" size="3"><b>双核,4个cores; 16G memory</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@alish2-cassandra-01 ~]# cat /proc/cpuinfo | grep &quot;cpu cores&quot;</span><br><span class="line">cpu cores       : 2</span><br><span class="line">cpu cores       : 2</span><br></pre></td></tr></table></figure><h3 id="公式简述"><a href="#公式简述" class="headerlink" title="公式简述"></a>公式简述</h3><p>响应时间优先的并发收集器，主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。</p><ol><li><font size="3" color="red">ParNew收集器</font><p>ParNew收集器是Serial收集器的多线程版本，许多运行在Server模式下的虚拟机中首选的新生代收集器，除Serial外，<font color="blue">只有它能与CMS收集器配合工作。</font></p></li><li><font size="3" color="red">CMS收集器</font><p>CMS， 全称Concurrent Low Pause Collector，是jdk1.4后期版本开始引入的新gc算法，在jdk5和jdk6中得到了进一步改进，它的主要适合场景是对响应时间的重要性需求 大于对吞吐量的要求，能够承受垃圾回收线程和应用线程共享处理器资源，并且应用中存在比较多的长生命周期的对象的应用。CMS是用于对tenured generation的回收，也就是年老代的回收，目标是尽量减少应用的暂停时间，减少FullGC发生的几率，利用和应用程序线程并发的垃圾回收线程来 标记清除年老代。<br>CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期是这样：<br><br><br><font size="3" color="blue">初始标记(CMS-initial-mark) -&gt; 并发标记(CMS-concurrent-mark) -&gt; 重新标记(CMS-remark) -&gt; 并发清除(CMS-concurrent-sweep) -&gt;并发重设状态等待下次CMS的触发(CMS-concurrent-reset)</font><br><br><br>其中的1，3两个步骤需要暂停所有的应用程序线程的。第一次暂停从root对象开始标记存活的对象，这个阶段称为初始标记；第二次暂停是在并发标记之后，暂停所有应用程序线程，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致）。第一次暂停会比较短，第二次暂停通常会比较长，并且remark这个阶段可以并行标记。<br><br><br>而并发标记、并发清除、并发重设阶段的所谓并发，是指一个或者多个垃圾回收线程和应用程序线程并发地运行，垃圾回收线程不会暂停应用程序的执行，如果你有多于一个处理器，那么并发收集线程将与应用线程在不同的处理器上运行，显然，这样的开销就是会降低应用的吞吐量。Remark阶段的并行，是指暂停了所有应用程序后，启动一定数目的垃圾回收进程进行并行标记，此时的应用线程是暂停的。</p></li></ol><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>($TOMCAT_HOME/bin/catalina.sh)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_OPTS=&quot;-server -Xmx10240m -Xms10240m -Xmn3840m -XX:PermSize=256m</span><br><span class="line"></span><br><span class="line">-XX:MaxPermSize=256m -Denv=denalicnprod</span><br><span class="line"></span><br><span class="line">-XX:SurvivorRatio=8  -XX:PretenureSizeThreshold=1048576</span><br><span class="line"></span><br><span class="line">-XX:+DisableExplicitGC  </span><br><span class="line"></span><br><span class="line">-XX:+UseParNewGC  -XX:ParallelGCThreads=10</span><br><span class="line"></span><br><span class="line">-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled</span><br><span class="line"></span><br><span class="line">-XX:+CMSScavengeBeforeRemark -XX:ParallelCMSThreads=10</span><br><span class="line"></span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</span><br><span class="line"></span><br><span class="line">-XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled</span><br><span class="line"></span><br><span class="line">-XX:+UseFastAccessorMethods</span><br><span class="line"></span><br><span class="line">-XX:LargePageSizeInBytes=128M</span><br><span class="line"></span><br><span class="line">-XX:SoftRefLRUPolicyMSPerMB=0</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDateStamps -Xloggc:gc.log -verbose:gc&quot;</span><br></pre></td></tr></table></figure><h3 id="公式解析"><a href="#公式解析" class="headerlink" title="公式解析"></a>公式解析</h3><table><thead><tr><th>参 数</th><th>含 义</th></tr></thead><tbody><tr><td>-server</td><td>一定要作为第一个参数，启用JDK的server版本，在多个CPU时性能佳</td></tr><tr><td>-Xms</td><td>java Heap初始大小。 默认是物理内存的1/64。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。</td></tr><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存。</td></tr><tr><td>-Xmn</td><td>设置年轻代大小，一般设置为Xmx的2/8~3/8,等同于-XX:NewSize 和 -XX:MaxNewSize 。</td></tr><tr><td>-XX:PermSize</td><td>设定内存的永久保存区初始大小，缺省值为64M</td></tr><tr><td>-XX:MaxPermSize</td><td>设定内存的永久保存区最大大小，缺省值为64M</td></tr><tr><td>-Denv</td><td>指定tomcat运行哪个project</td></tr><tr><td>-XX:SurvivorRatio</td><td>Eden区与Survivor区的大小比值, 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10</td></tr><tr><td>-XX:PretenureSizeThreshold</td><td>晋升年老代的对象大小。默认为0，比如设为1048576(1M)，则超过1M的对象将不在eden区分配，而直接进入年老代。</td></tr><tr><td>-XX:+DisableExplicitGC</td><td>关闭System.gc()</td></tr><tr><td><font color="#1E90FF">-XX:+UseParNewGC</font></td><td><font color="#1E90FF">设置年轻代为并发收集。可与CMS收集同时使用。</font></td></tr><tr><td>-XX:ParallelGCThreads</td><td></td></tr><tr><td><font color="#1E90FF">-XX:+UseConcMarkSweepGC</font></td><td><font color="#1E90FF">设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了。所以，此时年轻代大小最好用-Xmn设置。</font></td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>开启并行remark</td></tr><tr><td>-XX:+CMSScavengeBeforeRemark</td><td>这个参数还蛮重要的，它的意思是在执行CMS remark之前进行一次youngGC，这样能有效降低remark的时间</td></tr><tr><td>-XX:ParallelCMSThreads</td><td>CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4) ，如果你需要明确设定，可以通过-XX:ParallelCMSThreads=20来设定,其中ParallelGCThreads是年轻代的并行收集线程数</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction</td><td><font color="#3CB371">使用cms作为垃圾回收使用70％后开始CMS收集</font></td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td></tr><tr><td>-XX:+UseCMSCompactAtFullCollection</td><td>打开对年老代的压缩。可能会影响性能，但是可以消除内存碎片。</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此参数设置运行次FullGC以后对内存空间进行压缩、整理。</td></tr><tr><td>-XX:+CMSPermGenSweepingEnabled</td><td>为了避免Perm区满引起的full gc，<font color="#3CB371">建议开启CMS回收Perm区选项</font></td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr><tr><td>-XX:+UseFastAccessorMethods</td><td>原始类型的快速优化</td></tr><tr><td>-XX:LargePageSizeInBytes</td><td>内存页的大小，不可设置过大， 会影响Perm的大小</td></tr><tr><td>-XX:SoftRefLRUPolicyMSPerMB</td><td>“软引用”的对象在最后一次被访问后能存活0毫秒（默认为1秒）。</td></tr><tr><td>-XX:+PrintGCDetails</td><td>记录 GC 运行时的详细数据信息，包括新生成对象的占用内存大小以及耗费时间等</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td>打印垃圾收集的时间戳</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td></tr><tr><td>-XX:+PrintGCDateStamps</td><td>之前打印gc日志的时候使用是：-XX:+PrintGCTimeStamps，这个选项记录的是jvm启动时间为起点的相对时间，可读性较差，不利于定位问题，使用PrintGCDateStamps记录的是系统时间，更humanreadable</td></tr><tr><td>-Xloggc</td><td>与上面几个配合使用，把相关日志信息记录到文件以便分析</td></tr><tr><td>-verbose:gc</td><td>记录 GC 运行以及运行时间，一般用来查看 GC 是否是应用的瓶颈</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Oct 14 2019 15:31:38 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
</feed>
