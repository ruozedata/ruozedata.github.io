<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-09T03:05:32.043Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker实践之常用命令及自定义Web首页</title>
    <link href="http://yoursite.com/2019/06/28/Docker%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89Web%E9%A6%96%E9%A1%B5/"/>
    <id>http://yoursite.com/2019/06/28/Docker实践之常用命令及自定义Web首页/</id>
    <published>2019-06-27T16:00:00.000Z</published>
    <updated>2019-07-09T03:05:32.043Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker --help</span><br><span class="line">//常用命令：</span><br><span class="line">--------------------------------------------</span><br><span class="line">  exec        Run a command in a running container</span><br><span class="line">  history     Show the history of an image</span><br><span class="line">  images      List images</span><br><span class="line">  kill        Kill one or more running containers</span><br><span class="line">  logs        Fetch the logs of a container</span><br><span class="line">  ps          List containers</span><br><span class="line">  pull        Pull an image or a repository from a registry</span><br><span class="line">  push        Push an image or a repository to a registry</span><br><span class="line">  rename      Rename a container</span><br><span class="line">  restart     Restart one or more containers</span><br><span class="line">  rm          Remove one or more containers</span><br><span class="line">  rmi         Remove one or more images</span><br><span class="line">  run         Run a command in a new container</span><br><span class="line">  search      Search the Docker Hub for images</span><br><span class="line">  start       Start one or more stopped containers</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics</span><br><span class="line">  stop        Stop one or more running containers</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span><br><span class="line">  top         Display the running processes of a container</span><br><span class="line">  version     Show the Docker version information</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker search nginx</span><br><span class="line">NAME                                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">nginx                                                  Official build of Nginx.                        10179               [OK]</span><br><span class="line">jwilder/nginx-proxy                                    Automated Nginx reverse proxy for docker con…   1454                                    [OK]</span><br><span class="line">richarvey/nginx-php-fpm                                Container running Nginx + PHP-FPM capable of…   645                                     [OK]</span><br><span class="line">jrcs/letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as p…   436                                     [OK]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker pull nginx   //拉取官方版本的nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">f17d81b4b692: Pull complete</span><br><span class="line">82dca86e04c3: Downloading  11.24MB/22.2MB</span><br><span class="line">82dca86e04c3: Pull complete</span><br><span class="line">046ccb106982: Pull complete</span><br><span class="line">Digest: sha256:d59a1aa7866258751a261bae525a1842c7ff0662d4f34a355d5f36826abc0341</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>docker相当于一个小型的linux系统，但是它又只是一个单一的进程，可以不对外暴露端口号，如果对外暴露端口号，那也只能有一个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \     //运行一个实例</span><br><span class="line">--name huluwa-niginx-v1 \            //自定义一个名字</span><br><span class="line">-d \                                 //后台运行</span><br><span class="line">-p 8080:80 \                         //对外暴露的端口号，对应linux的8080端口号</span><br><span class="line">nginx:latest                         //运行的镜像名及版本</span><br><span class="line">d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps     //查看当前活动的实例</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   2 minutes ago</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# ps -ef | grep docker</span><br><span class="line">root     23182     1  0 22:00 ?        00:00:07 /usr/bin/dockerd</span><br><span class="line">root     23189 23182  0 22:00 ?        00:00:03 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root     25014 23182  0 22:27 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8080 -container-ip 172.17.0.2 -container-port 80</span><br><span class="line">root     25021 23189  0 22:27 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d08ffca661436d9fc676355bc52940c264e7cee62c08c56e489fb9e09e1ff538 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root     25492 10525  0 22:35 pts/0    00:00:00 grep --color=auto docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker-proxy -proto tcp&lt;br&gt;</span><br><span class="line">-host-ip 0.0.0.0   //&lt;br&gt;</span><br><span class="line">-host-port 8080   //linux系统的端口号&lt;br&gt;</span><br><span class="line">-container-ip 172.17.0.2  //docker相当于一个小型的linux系统，这就是小型系统的IP地址&lt;br&gt;</span><br><span class="line">-container-port 80  //docker内部的一个端口号&lt;br&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# netstat -nlp |grep 8080</span><br><span class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      25014/docker-proxy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker images    //查看所有的镜像</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              62f816a209e6        7 days ago          109MB</span><br><span class="line">mysql               5.6                 a46c2a2722b9        2 weeks ago         256MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        2 months ago        1.84kB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker ps -a   //查看所有实例，不论什么状态</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                    NAMES</span><br><span class="line">9883abaaad85        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour               0.0.0.0:3308-&gt;3306/tcp   huluwa-mysql-v5</span><br><span class="line">34fb53521694        mysql:5.6           &quot;docker-entrypoint.s…&quot;   About an hour ago   Created                                                 huluwa-mysql-v4</span><br><span class="line">2a5c95f3c043        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v3</span><br><span class="line">84e65fd24271        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysql-v2</span><br><span class="line">b3c12bcb28eb        mysql:5.6           &quot;docker-entrypoint.s…&quot;   2 hours ago         Exited (0) About an hour ago                            huluwa-mysqlv1</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   12 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   13 hours ago        Exited (0) 12 hours ago                                 huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 13 hours ago        Exited (0) 13 hours ago                                 elastic_ritchie</span><br></pre></td></tr></table></figure><p>正在运行的status就是Up，已经关闭的status就是Exited</p><h3 id="自定义首页"><a href="#自定义首页" class="headerlink" title="自定义首页"></a>自定义首页</h3><ol><li>登录初始的nginx Web页面</li></ol><p><img src="/assets/blogImg/2019-06-28-1.png" alt="enter description here"></p><ol start="2"><li><p>通过index.html配置一个自定义的首页</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 html]# pwd</span><br><span class="line">/root/docker/nginx/html</span><br><span class="line">[root@hadoop004 html]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 92 Nov 13 23:09 index.html</span><br></pre></td></tr></table></figure><p>在windows中打开index.html页面是这样的：<br><img src="/assets/blogImg/2019-06-28-2.png" alt="enter description here"></p></li><li><p>将本地的html文件挂载到container中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop004 ~]# docker run \</span><br><span class="line">--name huluwa-niginx-v2 \</span><br><span class="line">-v /root/docker/nginx/html:/usr/share/nginx/html:ro \ //本地的/root/docker/nginx/html和容器里的/usr/share/nginx/html建立一个映射，将本地的文件夹挂载到容器里</span><br><span class="line">-d \</span><br><span class="line">-p 8082:80 \</span><br><span class="line">nginx:latest</span><br><span class="line">c7937fd855963c7cca831d495436881a16e7e9befa61288cb28e2ab8b986decf</span><br><span class="line">[root@hadoop004 ~]# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                         PORTS                  NAMES</span><br><span class="line">c7937fd85596        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About a minute ago   Up About a minute              0.0.0.0:8082-&gt;80/tcp   huluwa-niginx-v2</span><br><span class="line">d08ffca66143        nginx:latest        &quot;nginx -g &apos;daemon of…&quot;   About an hour ago    Up About an hour               0.0.0.0:8080-&gt;80/tcp   huluwa-niginx-v1</span><br><span class="line">77a890ae6b8c        hello-world         &quot;/hello&quot;                 About an hour ago    Exited (0) About an hour ago                          elastic_ritchie</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>打开ip:8082页面查看</li></ol><p><img src="/assets/blogImg/2019-06-28-3.png" alt="enter description here"></p><p>发现首页已经被置换为本地文件中的index.html文件<br><br>-v 把本地文件或文件夹挂载到容器中<br><br>挂载的目的，就是把容器中的数据保存在本地，容器进程移除后之后，数据不会丢失，如果不挂载的话，容器进程挂掉之后，数据就全没有了<br><br>ro：可读<br><br>rw：可读写<br></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>不得不会的Spark SQL常见4种数据源</title>
    <link href="http://yoursite.com/2019/06/20/%E4%B8%8D%E5%BE%97%E4%B8%8D%E4%BC%9A%E7%9A%84Spark%20SQL%E5%B8%B8%E8%A7%814%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>http://yoursite.com/2019/06/20/不得不会的Spark SQL常见4种数据源/</id>
    <published>2019-06-19T16:00:00.000Z</published>
    <updated>2019-06-20T02:02:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h3><h4 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h4><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。</p><p>修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          </span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><h4 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h3><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Encoders for most common types are automatically provided by importing spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// DataFrames can be saved as Parquet files, maintaining the schema information</span><br><span class="line">peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Read in the parquet file created above</span><br><span class="line">// Parquet files are self-describing so the schema is preserved</span><br><span class="line">// The result of loading a Parquet file is also a DataFrame</span><br><span class="line">val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet files can also be used to create a temporary view and then used in SQL statements</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">// +------------+</span><br><span class="line">// |       value|</span><br><span class="line">// +------------+</span><br><span class="line">// |Name: Justin|</span><br><span class="line">// +------------+</span><br></pre></td></tr></table></figure><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet</p><p>或SQLContext.read.load，Spark SQL将自动解析分区信息。</p><p>返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项：</p><p><code>spark.sql.parquet.mergeSchema</code>为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sqlContext from the previous example is used in this example.</span><br><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, stored into a partition directory</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths.</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">.config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><h4 id="内嵌Hive应用"><a href="#内嵌Hive应用" class="headerlink" title="内嵌Hive应用"></a>内嵌Hive应用</h4><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.warehouse.dir=</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive应用"><a href="#外部Hive应用" class="headerlink" title="外部Hive应用"></a>外部Hive应用</h4><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;rddtable2&quot;)</span><br><span class="line">.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;, &quot;hive&quot;)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark SQL" scheme="http://yoursite.com/categories/Spark-SQL/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之一: 内存结构(堆内存和非堆内存)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%80_%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84(%E5%A0%86%E5%86%85%E5%AD%98%E5%92%8C%E9%9D%9E%E5%A0%86%E5%86%85%E5%AD%98)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之一_内存结构(堆内存和非堆内存)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:47.558Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p><strong>图为Java虚拟机运行时的数据区:</strong></p><p><img src="/assets/pic/2019-06-19-1-1.png" alt="数据区"></p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>也称”永久代” 、“非堆”， 它用于存储虚拟机加载的类信息、常量、静态变量、是<font color="red">各个线程共享的内存区域</font>。<font color="blue">默认最小值为16MB，最大值为64MB（未验证）</font>，可以通过-XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。<br><br><br>运行时常量池：是方法区的一部分，Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种符号引用，这部分内容将在类加载后放到方法区的运行时常量池中。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>描述的是java 方法执行的内存模型：每个方法被执行的时候 都会创建一个“栈帧”用于存储局部变量表(包括参数)、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。声明周期与线程相同，是<font color="red">线程私有</font>的。<br><br><br>局部变量表存放了编译器可知的各种基本数据类型(boolean、byte、char、short、int、float、long、 double)、对象引用(引用指针，并非对象本身)，其中64位长度的long和double类型的数据会占用2个局部变量的空间，其余数据类型只占1 个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量是完全确定的，在运行期间栈帧不会改变局部 变量表的大小空间。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>与虚拟机栈基本类似，区别在于虚拟机栈为虚拟机执行的java方法服务，而本地方法栈则是为Native方法服务。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>也叫做java 堆、GC堆，是java虚拟机所管理的内存中最大的一块内存区域，也是<font color="red">被各个线程共享的内存区域</font>，在JVM启动时创建。该内存区域存放了对象实例及数组(所有new的对象)。其大小通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，-Xmx为JVM可申请的最大内存。在JVM启动时，最大内存会被保留下来。为对象内存而保留的地址空间可以被分成年轻代和老年代。<br><br><br>默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过-XX:MinHeapFreeRation=来指定这个比列；当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过XX:MaxHeapFreeRation=来指定这个比列，对于运行系统，为避免在运行时频繁调整Heap的大小，通常-Xms与-Xmx的值设成一样。</p><table><thead><tr><th style="text-align:center">Parameter</th><th style="text-align:center">Default Value</th></tr></thead><tbody><tr><td style="text-align:center">MinHeapFreeRatio</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">MaxHeapFreeRatio</td><td style="text-align:center">70</td></tr><tr><td style="text-align:center">-Xms</td><td style="text-align:center">3670k</td></tr><tr><td style="text-align:center">-Xmx</td><td style="text-align:center">64m</td></tr></tbody></table><font color="red">注：如果是64位系统，这些值一般需要扩张30％，来容纳在64位系统下变大的对象。</font><p>从J2SE 1.2开始，JVM使用分代收集算法，在不同年代的区域里使用不同的算法。堆被划分为新生代和老年代。新生代主要存储新创建的对象和尚未进入老年代的对象。老年代存储经过多次新生代GC(MinorGC)任然存活的对象。</p><p><img src="/assets/pic/2019-06-19-1-2.png" alt="堆"></p><font color="red"><b><br>注1：图中的Perm不是堆内存，是永久代<br><br>注2：图中的Virtaul则是各区域还未被分配的内存，即最大内存-当前分配的内存<br></b></font><p><strong>新生代：</strong></p><p>新生代包括一块eden（伊甸园）和2块survivor(通常又称S0和S1或From和To)。大多数对象都是在eden中初始化。而对于2块survivor来说，总有一块是空的，它会在下一个复制收集过程中作为eden中的活跃对象和另一块survivor的目的地。在对象衰老之前（也就是被复制到tenured之前），它们会在两块survivor区域之间以这样的方式复制。可通过-Xmn参数来指定新生代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及Survivor Space的大小。</p><p><strong>老年代：</strong></p><p>用于存放经过多次新生代Minor GC依然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：</p><ol><li>大对象，可通过启动参数设置-XX:PretenureSizeThreshold=1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。</li><li>大的数组对象，即数组中无引用外部对象。</li></ol><p>老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。</p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>是最小的一块内存区域，它的作用是当前线程所执行的字节码的行号指示器，在虚拟机的模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="内存结构" scheme="http://yoursite.com/tags/%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之二: 常见的垃圾收集器</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%8C_%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之二_常见的垃圾收集器/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:25:25.799Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<br><br><br>Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。</p><p><img src="/assets/pic/2019-06-19-2-1.png" alt="收集器"></p><font size="3"><b>HotSpot虚拟机的垃圾回收器</b></font><p>图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。</p><font size="3"><b>概念理解</b></font><ul><li><font size="3"><b>并发和并行</b></font><p>这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，它们可以解释如下</p><ul><li><strong>并行（Parallel）</strong>：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</li><li><strong>并发（Concurrent）</strong>：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。</li></ul></li><li><font size="3"><b>Minor GC 和 Full GC</b></font><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。</li><li><strong>老年代GC（Major GC / Full GC）</strong>：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。</li></ul></li><li><font size="3"><b>吞吐量</b></font><p>吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。<br><br><br>虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。</p></li></ul><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>Serial收集器是最基本、发展历史最悠久的收集器，曾经（在JDK 1.3.1之前）是虚拟机新生代收集的唯一选择。</p><p><img src="/assets/pic/2019-06-19-2-2.png" alt="Serial收集器"></p><ul><li><p><strong>特性</strong></p><p>这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。Stop The World</p></li><li><p><strong>应用场景</strong></p><p>Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。</p></li><li><p><strong>优势</strong></p><p>简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。</p></li></ul><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p><img src="/assets/pic/2019-06-19-2-3.png" alt="ParNew收集器"></p><ul><li><p><strong>特性</strong></p><p>ParNew收集器其实就是Serial收集器的<strong>多线程版本</strong>，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。</p></li><li><p><strong>应用场景</strong></p><p>ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器。<br><br><br>很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器配合工作。<br><br><br>在JDK 1.5时期，HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器——CMS收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。<br><br><br>不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。</p></li><li><p><strong>Serial收集器 VS ParNew收集器</strong></p><p>ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。</p><p>然而，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。</p></li></ul><h3 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h3><ul><li><p><strong>特性</strong></p><p>Parallel Scavenge收集器是一个<strong>新生代收集器</strong>，它也是使用<strong>复制算法</strong>的收集器，又是并行的多线程收集器。</p></li><li><p><strong>应用场景</strong></p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。</p></li><li><p><strong>对比分析</strong></p><ul><li><p><strong>Parallel Scavenge收集器 VS CMS等收集器</strong></p><p>Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个<strong>可控制的吞吐量（Throughput）</strong>。</p><p>由于与吞吐量关系密切，Parallel Scavenge收集器也经常称为“吞吐量优先”收集器。</p></li><li><p><strong>Parallel Scavenge收集器 VS ParNew收集器</strong></p><p>Parallel Scavenge收集器与ParNew收集器的一个重要区别是它具有自适应调节策略。</p><p><strong>GC自适应的调节策略</strong></p><p>Parallel Scavenge收集器有一个参数-XX:+UseAdaptiveSizePolicy。当这个参数打开之后，就不需要手工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。</p></li></ul></li></ul><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-4.png" alt="Serial Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Serial Old是Serial收集器的<strong>老年代版本</strong>，它同样是一个<strong>单线程收集器</strong>，使用<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><ul><li><p><strong>Client模式</strong></p><p>Serial Old收集器的主要意义也是在于给Client模式下的虚拟机使用。</p></li><li><p><strong>Server模式</strong></p><p>如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。</p></li></ul></li></ul><h3 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h3><p><img src="/assets/pic/2019-06-19-2-5.png" alt="Parallel Old收集器"></p><ul><li><p><strong>特性</strong></p><p>Parallel Old是Parallel Scavenge收集器的<strong>老年代版本</strong>，使用<strong>多线程</strong>和<strong>“标记－整理”算法</strong>。</p></li><li><p><strong>应用场景</strong></p><p>在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。</p><p>这个收集器是在JDK 1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择（Parallel Scavenge收集器无法与CMS收集器配合工作）。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合。</p></li></ul><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p><img src="/assets/pic/2019-06-19-2-6.png" alt="CMS收集器"></p><ul><li><p><strong>特性</strong></p><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。</p><p>CMS收集器是基于<strong>“标记—清除”算法</strong>实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤:</p><ol><li><p><strong>初始标记（CMS initial mark）</strong></p><p>初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。</p></li><li><p><strong>并发标记（CMS concurrent mark）</strong></p><p>并发标记阶段就是进行GC Roots Tracing的过程。</p></li><li><p><strong>重新标记（CMS remark）</strong></p><p>重新标记阶段是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，仍然需要“Stop The World”。</p></li><li><p><strong>并发清除（CMS concurrent sweep）</strong></p><p>并发清除阶段会清除对象。</p><p>由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。</p></li></ol></li><li><p><strong>优点</strong></p><p>CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：<strong>并发收集、低停顿</strong>。</p></li><li><p><strong>缺点</strong></p><ul><li><p><strong>CMS收集器对CPU资源非常敏感</strong></p><p>其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。</p><p>CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得很大。</p></li><li><p><strong>CMS收集器无法处理浮动垃圾</strong></p><p>CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。</p><p>由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。<br>也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。</p></li><li><p><strong>CMS收集器会产生大量空间碎片</strong></p><p>CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。</p><p>空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。</p></li></ul></li></ul><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p><img src="/assets/pic/2019-06-19-2-7.png" alt="G1收集器"></p><ul><li><p><strong>特性</strong></p><p>G1（Garbage-First）是一款<strong>面向服务端应用</strong>的垃圾收集器。HotSpot开发团队赋予它的使命是未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。</p><p>在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。</p><p>G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。</p><ul><li><p><strong>并行与并发</strong></p><p>G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。</p></li><li><p><strong>分代收集</strong></p><p>与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。</p></li><li><p><strong>空间整合</strong></p><p>与CMS的“标记—清理”算法不同，G1从<strong>整体来看是基于“标记—整理”算法</strong>实现的收集器，从<strong>局部（两个Region之间）上来看是基于“复制”算法</strong>实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</p></li><li><p><strong>可预测的停顿</strong></p><p>这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。</p></li></ul></li><li><p><strong>执行过程</strong></p><p>G1收集器的运作大致可划分为以下几个步骤：</p><ul><li><p><strong>初始标记（Initial Marking）</strong></p><p>初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。</p></li><li><p><strong>并发标记（Concurrent Marking）</strong></p><p>并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。</p></li><li><p><strong>最终标记（Final Marking）</strong></p><p>最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。</p></li><li><p><strong>筛选回收（Live Data Counting and Evacuation）</strong></p><p>筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</p></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>虽然我们是在对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的收集器出现，更加没有万能的收集器，所以我们选择的只是对具体应用最合适的收集器。这点不需要多加解释就能证明：如果有一种放之四海皆准、任何场景下都适用的完美收集器存在，那HotSpot虚拟机就没必要实现那么多不同的收集器了。</p><p><img src="/assets/pic/2019-06-19-2-8.png" alt="总结"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="垃圾收集器" scheme="http://yoursite.com/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之八: GC插件&amp;错误not_supported_for_this_jvm&amp;命令jstatd</title>
    <link href="http://yoursite.com/2019/06/19/JVM_8/"/>
    <id>http://yoursite.com/2019/06/19/JVM_8/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:54:00.202Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a> <font size="4"><b>1.插件安装</b></font><p>tools-&gt;plugin-&gt;Available Plugin 会有值得安装的插件，如：VisualGC</p><p><img src="/assets/pic/2019-06-19-8-1.png" alt="插件安装"></p><p>插件列表: <a href="https://visualvm.dev.java.net/plugins.html" target="_blank" rel="noopener">https://visualvm.dev.java.net/plugins.html</a></p><p>注意：上面提供的端口配置有些麻烦，不如直接这样做:</p><font size="4"><b>2.要使用 VisualGC 必须在远程机上启动jstatd代理程序，否则会显示<font color="#FF4500">“not supported for this jvm” </font>错误</b></font> <font color="blue">而启动 jstatd 时会有一个权限问题，需要做如下修改：</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@xxx-01 ~]# java -version</span><br><span class="line">java version &quot;1.7.0_55&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_55-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)</span><br><span class="line">[root@xxx-01 ~]# jstatd </span><br><span class="line">Could not create remote object</span><br><span class="line">access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">java.security.AccessControlException: access denied (&quot;java.util.PropertyPermission&quot; &quot;java.rmi.server.ignoreSubClasses&quot; &quot;write&quot;)</span><br><span class="line">        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)</span><br><span class="line">        at java.security.AccessController.checkPermission(AccessController.java:559)</span><br><span class="line">        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)</span><br><span class="line">        at java.lang.System.setProperty(System.java:783)</span><br><span class="line">        at sun.tools.jstatd.Jstatd.main(Jstatd.java:139)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@xxx-01 ~]# cd  /usr/java/jdk1.7.0_55</span><br><span class="line">[root@xxx-01 ~]# vi /usr/java/jdk1.7.0_55/jstatd.all.policy</span><br><span class="line">    grant codebase &quot;file:$&#123;JAVA_HOME&#125;/lib/tools.jar&quot; &#123;  </span><br><span class="line">     permission java.security.AllPermission;  </span><br><span class="line">    &#125;;  </span><br><span class="line">[root@xxx-01 jdk1.7.0_55]# jstatd -J-Djava.security.policy=/usr/java/jdk1.7.0_55/jstatd.all.policy  &amp;</span><br></pre></td></tr></table></figure><font color="blue">然后后台模式启动 jstatd命令</font> <font color="blue">主机面GC:</font><p><img src="/assets/pic/2019-06-19-8-2.png" alt="主机面GC"></p><font color="blue">Threads:</font><p><img src="/assets/pic/2019-06-19-8-3.png" alt="Threads"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之六: JVM参数设置及分析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%85%AD_JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之六_JVM参数设置及分析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:28.434Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p>不管是YGC还是Full GC,GC过程中都会对导致程序运行中中断,正确的选择不同的GC策略,调整JVM、GC的参数，可以极大的减少由于GC工作，而导致的程序运行中断方面的问题，进而适当的提高Java程序的工作效率。但是调整GC是以个极为复杂的过程，由于各个程序具备不同的特点，如：web和GUI程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要cup个数，内存不同），所以使用的GC种类也会不同(如何选择见GC种类及如何选择)。本文将注重介绍JVM、GC的一些重要参数的设置来提高系统的性能。</p><p>JVM内存组成及GC相关内容请见之前的文章:JVM内存组成 GC策略&amp;内存申请</p><font size="3"><strong>JVM参数的含义</strong></font>实例见实例分析<br><br>参数名称|含义|默认值| |<br>——|—–|—–|—<br>-Xms|初始堆大小|物理内存的1/64(&lt;1GB)|默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.<br>-Xmx|最大堆大小|物理内存的1/4(&lt;1GB)|默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制<br>-Xmn|年轻代大小(1.4or lator)||注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。<br>整个堆大小=年轻代大小 + 年老代大小 + 持久代大小.<br>增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8<br>-XX:NewSize|设置年轻代大小(for 1.3/1.4)|<br>-XX:MaxNewSize|年轻代最大值(for 1.3/1.4)|<br>-XX:PermSize|设置持久代(perm gen)初始值|物理内存的1/64<br>-XX:MaxPermSize|设置持久代最大值|物理内存的1/4<br>-Xss|每个线程的堆栈大小||JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右。<br>一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。<br>-XX:ThreadStackSize|Thread Stack Size||(0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.]<br>-XX:NewRatio|年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)||-XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5<br>Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。<br>-XX:SurvivorRatio|Eden区与Survivor区的大小比值||设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10<br>-XX:LargePageSizeInBytes|内存页的大小不可设置过大， 会影响Perm的大小||=128m<br>-XX:+UseFastAccessorMethods|原始类型的快速优化<br>-XX:+DisableExplicitGC|关闭System.gc()||这个参数需要严格的测试<br>-XX:MaxTenuringThreshold|垃圾最大年龄||如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率<br>该参数只有在串行GC时才有效.<br>-XX:+AggressiveOpts|加快编译<br>-XX:+UseBiasedLocking|锁机制的性能改善<br>-Xnoclassgc|禁用垃圾回收<br>-XX:SoftRefLRUPolicyMSPerMB|每兆堆空闲空间中SoftReference的存活时间|1s|softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap<br>-XX:PretenureSizeThreshold|对象超过多大是直接在旧生代分配|0|单位字节 新生代采用Parallel Scavenge GC时无效<br>另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.<br>-XX:TLABWasteTargetPercent|TLAB占eden区的百分比|1%<br>-XX:+CollectGen0First|FullGC时是否先YGC|false<br><br><font size="3"><strong>并行收集器相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseParallelGC</td><td>Full GC采用parallel MSC<br>(此项待验证)</td><td></td><td>选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证)</td></tr><tr><td>-XX:+UseParNewGC</td><td>设置年轻代为并行收集</td><td></td><td>可与CMS收集同时使用<br>JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值</td></tr><tr><td>-XX:ParallelGCThreads</td><td>并行收集器的线程数</td><td></td><td>此值最好配置与处理器数目相等 同样适用于CMS</td></tr><tr><td>-XX:+UseParallelOldGC</td><td>年老代垃圾收集方式为并行收集(Parallel Compacting)</td><td></td><td>这个是JAVA 6出现的参数选项</td></tr><tr><td>-XX:MaxGCPauseMillis</td><td>每次年轻代垃圾回收的最长时间(最大暂停时间)</td><td></td><td>如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.</td></tr></tbody></table><p>-XX:+UseAdaptiveSizePolicy 自动选择年轻代区大小和相应的Survivor区比例<br>设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开.<br>-XX:GCTimeRatio|设置垃圾回收时间占程序运行时间的百分比||公式为1/(1+n)<br>-XX:+ScavengeBeforeFullGC|Full GC前调用YGC|true|Do young generation GC prior to a full GC. (Introduced in 1.4.1.)</p><font size="3"><strong>CMS相关参数</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+UseConcMarkSweepGC</td><td>使用CMS内存收集</td><td></td><td>测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.???</td></tr><tr><td>-XX:+AggressiveHeap</td><td></td><td></td><td>试图是使用大量的物理内存，长时间大内存使用的优化，能检查计算资源（内存， 处理器数量），至少需要256MB内存，大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升）</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>多少次后进行内存压缩</td><td>由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生”碎片”,使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理.</td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>降低标记停顿</td></tr><tr><td>-XX+UseCMSCompactAtFullCollection</td><td>在FULL GC的时候， 对年老代的压缩</td><td></td><td>CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。可能会影响性能,但是可以消除碎片</td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td><td>禁止hostspot自行触发CMS GC</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction=70</td><td>使用cms作为垃圾回收，使用70％后开始CMS收集</td><td>92</td><td>为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式<strong>CMSInitiatingOccupancyFraction</strong>计算公式</td></tr><tr><td>-XX:CMSInitiatingPermOccupancyFraction</td><td>设置Perm Gen使用到达多少比率时触发</td><td>92</td></tr><tr><td>-XX:+CMSIncrementalMode</td><td>设置为增量模式</td><td></td><td>用于单CPU情况</td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr></tbody></table><font size="3"><strong>辅助信息</strong></font><table><thead><tr><th>参数名称</th><th>含义</th><th>默认值</th><th></th></tr></thead><tbody><tr><td>-XX:+PrintGC</td><td></td><td></td><td>输出形式:<br>[GC 118250K-&gt;113543K(130112K), 0.0094143 secs]<br>[Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs]</td></tr><tr><td>-XX:+PrintGCDetails</td><td></td><td></td><td>输出形式:<br>[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs]<br>[GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs]</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td></td></tr><tr><td>-XX:+PrintGC:PrintGCTimeStamps</td><td></td><td></td><td>可与-XX:+PrintGC -XX:+PrintGCDetails混合使用<br>输出形式:11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs]</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td><td></td><td>输出形式:Total time for which application threads were stopped: 0.0468229 seconds</td></tr><tr><td>-XX:+PrintGCApplicationConcurrentTime</td><td>打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用</td><td></td><td>输出形式:Application time: 0.5291524 seconds</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td><td></td></tr><tr><td>-Xloggc:filename</td><td>把相关日志信息记录到文件以便分析.<br>与上面几个配合使用</td><td></td></tr><tr><td>-XX:+PrintClassHistogram</td><td>garbage collects before printing the histogram.</td><td></td></tr><tr><td>-XX:+PrintTLAB</td><td>查看TLAB空间的使用情况</td><td></td></tr><tr><td>XX:+PrintTenuringDistribution</td><td>查看每次minor GC后新的存活周期的阈值</td><td></td><td>Desired survivor size 1048576 bytes, new threshold 7 (max 15)</td></tr></tbody></table><p>new threshold 7即标识新的存活周期的阈值为7。</p><font size="3" color="#FF4500"><strong>GC性能方面的考虑</strong></font><p>对于GC的性能主要有2个方面的指标：吞吐量throughput（工作时间不算gc的时间占总的时间比）和暂停pause（gc发生时app对外显示的无法响应）</p><ol><li><p>Total Heap</p><p>默认情况下，vm会增加/减少heap大小以维持free space在整个vm中占的比例，这个比例由MinHeapFreeRatio和MaxHeapFreeRatio指定。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>对vm分配尽可能多的memory；</li><li>将Xms和Xmx设为一样的值。如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。</li><li>处理器核数增加，内存也跟着增大。</li></ul></li><li><p>The Young Generation</p><p>另外一个对于app流畅性运行影响的因素是young generation的大小。young generation越大，minor collection越少；但是在固定heap size情况下，更大的young generation就意味着小的tenured generation，就意味着更多的major collection(major collection会引发minor collection)。<br><br>NewRatio反映的是young和tenured generation的大小比例。NewSize和MaxNewSize反映的是young generation大小的下限和上限，将这两个值设为一样就固定了young generation的大小（同Xms和Xmx设为一样）。<br><br>如果希望，SurvivorRatio也可以优化survivor的大小，不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。<br><br>一般而言，server端的app会有以下规则：</p><ul><li>首先决定能分配给vm的最大的heap size，然后设定最佳的young generation的大小；</li><li>如果heap size固定后，增加young generation的大小意味着减小tenured generation大小。让tenured generation在任何时候够大，能够容纳所有live的data（留10%-20%的空余）。</li></ul></li></ol><font size="3" color="#FF4500"><strong>经验&amp;&amp;规则</strong></font><ul><li><p>年轻代大小选择</p><ul><li>响应时间优先的应用:尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象.</li><li>吞吐量优先的应用:尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用.</li><li>避免设置过小.当新生代设置过小时会导致:1.YGC次数更加频繁 2.可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC.</li></ul></li><li><p>年老代大小选择</p><ul><li>响应时间优先的应用:年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎 片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得:<br>并发垃圾收集信息、持久代并发收集次数、传统GC信息、花在年轻代和年老代回收上的时间比例。</li><li>吞吐量优先的应用:一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.原因是,这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象.</li></ul></li><li><p>较小堆引起的碎片问题</p><p>因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置:<br><br>-XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩.<br><br>-XX:CMSFullGCsBeforeCompaction=0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩</p></li><li><p>用64位操作系统，Linux下64位的jdk比32位jdk要慢一些，但是吃得内存更多，吞吐量更大</p></li><li>XMX和XMS设置一样大，MaxPermSize和MinPermSize设置一样大，这样可以减轻伸缩堆大小带来的压力</li><li>使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 然后老生代利用CMS并行收集， 这样能保证系统低延迟的吞吐效率。 实际上cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间</li><li>系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。(相关工具的使用方法将在后面的blog中介绍)</li><li>仔细了解自己的应用，如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。</li><li>采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿</li><li>JVM参数的设置(特别是 –Xmx –Xms –Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。</li></ul><p><strong>promotion failed</strong></p><p>垃圾回收时promotion failed是个很头痛的问题，一般可能是两种原因产生，第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间；第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。</p><p><strong>解决方方案一</strong></p><p>第一个原因我的最终解决办法是去掉救助空间，设置-XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0即可，第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。</p><p><strong>解决方案一的改进方案</strong></p><p>又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。</p><p>-Xmx4000M -Xms4000M -Xmn600M -XX:PermSize=500M -XX:MaxPermSize=500M -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log</p><font size="3" color="#FF4500">CMSInitiatingOccupancyFraction值与Xmn的关系公式</font><p>上面介绍了promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即：<br>(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2)) 进而推断出：</p><pre><code>CMSInitiatingOccupancyFraction &lt;=((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100 </code></pre><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当xmx=128 xmn=36 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-36)-(36-36/(1+2)))/(128-36)*100 =73.913 </span><br><span class="line"></span><br><span class="line">当xmx=128 xmn=24 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-24)-(24-24/(1+2)))/(128-24)*100=84.615… </span><br><span class="line"></span><br><span class="line">当xmx=3000 xmn=600 SurvivorRatior=1时  CMSInitiatingOccupancyFraction&lt;=((3000.0-600)-(600-600/(1+2)))/(3000-600)*100=83.33</span><br></pre></td></tr></table></figure><p>CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。</p><p>令：</p><p>网上一童鞋推断出的公式是：:(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn 这个公式个人认为不是很严谨，在内存小的时候会影响xmn的计算。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之七: Java程序性能分析工具Java VisualVM(Visual GC)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%83_Java%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7Java%20VisualVM(Visual%20GC)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之七_Java程序性能分析工具Java VisualVM(Visual GC)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:24.927Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><p>VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。</p><p>在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。</p><a id="more"></a><h3 id="Visual-GC-监控垃圾回收器"><a href="#Visual-GC-监控垃圾回收器" class="headerlink" title="Visual GC(监控垃圾回收器)"></a>Visual GC(监控垃圾回收器)</h3><p>Java VisualVM默认没有安装Visual GC插件，需要手动安装，JDK的安装目录的bin目露下双击jvisualvm.exe，即可打开Java VisualVM，点击菜单栏 工具-&gt;插件 安装Visual GC</p><p><img src="/assets/pic/2019-06-19-7-1.png" alt="Visual GC(监控垃圾回收器)1"></p><p>安装完成后重启Java VisualVM，Visual GC界面自动打开，即可看到JVM中堆内存的分代情况</p><p><img src="/assets/pic/2019-06-19-7-2.png" alt="Visual GC(监控垃圾回收器)2"></p><p>被监控的程序运行一段时间后Visual GC显示如下</p><p><img src="/assets/pic/2019-06-19-7-3.png" alt="Visual GC(监控垃圾回收器)3"></p><p>要看懂上面的图必须理解Java虚拟机的一些基本概念：</p><font color="blue" size="3"><b>堆(Heap)</b></font>：<font size="3">JVM管理的内存叫堆</font><p><strong>分代</strong>：根据对象的生命周期长短，把堆分为3个代：Young，Old和Permanent，根据不同代的特点采用不同的收集算法，扬长避短也。</p><ul><li><font color="blue">Young（年轻代）</font>年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor复制过来的对象。而且，Survivor区总有一个是空的。</li><li><font color="blue">Tenured（年老代）</font>年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。</li><li><font color="blue">Perm（持久代）</font>用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。</li></ul><font color="blue" size="3"><b>GC的基本概念</b></font><p>gc分为full gc 跟 minor gc，当每一块区满的时候都会引发gc。</p><ul><li><font color="blue">Scavenge GC</font><p>一般情况下，当新对象生成，并且在Eden申请空间失败时，就触发了Scavenge GC，堆Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。</p></li><li><font color="blue">Full GC</font><p>对整个堆进行整理，包括Young、Tenured和Perm。Full GC比Scavenge GC要慢，因此应该尽可能减少Full GC。有如下原因可能导致Full GC:</p><ul><li>上一次GC之后Heap的各域分配策略动态变化</li><li>System.gc()被显示调用</li><li>Perm域被写满</li><li>Tenured被写满</li></ul></li></ul><font color="blue" size="3"><b>内存溢出 out of memory</b></font><p>是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。</p><font color="blue" size="3"><b>内存泄露 memory leak</b></font><p>是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。<strong>其实说白了就是该内存空间使用完毕之后未回收。</strong></p><h3 id="Java-VisualVM的其他功能"><a href="#Java-VisualVM的其他功能" class="headerlink" title="Java VisualVM的其他功能"></a>Java VisualVM的其他功能</h3><ol><li><p>监视界面（cpu，类，堆，线程）</p><p><img src="/assets/pic/2019-06-19-7-4.png" alt="监视界面"></p></li><li><p>线程界面</p><p><img src="/assets/pic/2019-06-19-7-5.png" alt="线程界面"></p></li></ol><ol start="3"><li><p>Profile界面（性能剖析）</p><p>点击CPU按钮执行cpu分析查看方法</p><p><img src="/assets/pic/2019-06-19-7-6.png" alt="Profile界面"></p><p>点击内存按钮执行内存分析查看类</p><p><img src="/assets/pic/2019-06-19-7-7.png" alt="Profile界面"></p></li><li><p>堆dump和线程dump操作</p><p>Dump文件是进程的内存镜像，可以把程序的执行状态通过调试器保存到dump文件中，堆dump的dump文件内容如下图所示</p><p><img src="/assets/pic/2019-06-19-7-8.png" alt="Dump"></p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;VisualVM 是一款免费的\集成了多个JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作。&lt;/p&gt;&lt;p&gt;在内存分析上，Java VisualVM的最大好处是可通过安装Visual GC插件来分析GC（Gabage Collection）趋势、内存消耗详细状况。&lt;/p&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之三: 内存分配策略</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%B8%89_%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之三_内存分配策略/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:31.567Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><p><img src="/assets/pic/2019-06-19-3-1.png" alt="内存分配策略"></p><p>了解GC其中很重要一点就是了解JVM的内存分配策略：<font color="red">即对象在哪里分配和对象什么时候回收。</font><br><br><br>Java技术体系中所提倡的自动内存管理可以归结于两个部分：给对象分配内存以及回收分配给对象的内存。<br><br><br>我们都知道，Java对象分配，都是在Java堆上进行分配的，虽然存在JIT编译后被拆分为标量类型并简介地在栈上进行分配。如果采用分代算法，那么新生的对象是分配在新生代的Eden区上的。如果启动了本地线程分配缓冲，将按线程优先在TLAB上进行分配。<br><br><br>事实上，Java的分配规则不是百分百固定的，其取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。<br><br><br>简单来说，对象内存分配主要是在堆中分配。但是分配的规则并不是固定的，取决于使用的收集器组合以及JVM内存相关参数的设定。<br><br><br>下面Serial和Serial Old收集器做一个内存分配和回收的策略总结。</p><h3 id="对象优先在新生代Eden分配"><a href="#对象优先在新生代Eden分配" class="headerlink" title="对象优先在新生代Eden分配"></a>对象优先在新生代Eden分配</h3><p>首先，让我们来看一下新生代的内存分配情况<br><br><br>内存分配情况：将JVM内存划分为一块较大的Eden空间（80%）和两块小的Servivor（各占10%）。当回收时，将Eden和Survivor中还存活的对象一次性采用复制算法直接复制到另外一块Servivor空间上，最后清理到院Eden空间和原先的Survivor空间中的数据。<br><br><br>大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，JVM将发起一次Minor GC。<br><br><br>在这里先说明两个概念：</p><ul><li><strong>新生代GC（Minor GC）</strong>：指发生在新生代的垃圾收集动作，因为Java对象大多是具有朝生夕灭的特性，所以Minor GC非常频繁，而且该速度也比较快。</li><li><strong>老年代GC（Major GC/Full GC）</strong>：指发生在老年代的GC，出现了Major GC，一般可能也会伴随着一次Minor GC，但是与Minor GC不同的是，Major GC的速度慢十倍以上。</li></ul><h3 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h3><p>我们先对所谓的大对象做一个定义：大对象，这里指的是需要大量连续内存空间的Java对象。最典型的大对象可以是很长的字符串和数组。<br><br><br>JVM对大对象的态度：大对象对于JVM的内存分配来说是十分麻烦的，如果我们将大对象分配在新生代中，那样子的话很容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。<br><br><br>为了避免上述情况的经常发生而导致不需要的GC活动所浪费的资源和时间，可采用的分配策略是将大对象直接分配到老年代中去，虚拟机中也提供了<strong>-XX:PretenureSizeThreshold</strong>参数，令大于这个设置值的对象直接在老年代里面分配内容。</p><p><code>-XX:PretenureSizeThreshold只对Serial和ParNew收集器有效。</code></p><h3 id="长期存活的对象将进入老年代"><a href="#长期存活的对象将进入老年代" class="headerlink" title="长期存活的对象将进入老年代"></a>长期存活的对象将进入老年代</h3><p>当JVM采用分代收集的思想来管理内存时，为了识别哪些对象应该放在新生代、哪些对象应该放在老年代，JVM给每个对象定义了一个对象年龄计数器。<br><br><br>对象年龄计数器：如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，便可以被移动到Survivor空间中，年龄计数器将设置该对象的年龄为1.对于对象在Survivor区每经过一次Minor GC，年龄便增加1岁，当它的年龄增加到一定程度（可通过参数-XX:MaxTenuringThreshold设置）默认15，该对象便会进入到老年代中。成为老年代的对象。</p><h3 id="动态对象年龄判定"><a href="#动态对象年龄判定" class="headerlink" title="动态对象年龄判定"></a>动态对象年龄判定</h3><p>事实上，有的虚拟机并不永远地要求对象的年龄必须达到MaxTeruringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Surivior空间的一半，年龄大于或等于该年龄的对象就可以直接进行老年代，无须等到MaxTeruringThreshold中所要求的年龄。</p><h3 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h3><p>在发生Minor GC之前，虚拟机会先检查老年代中最大的可用的连续空间是否大于新生代中所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的，如果不成立，则虚拟机会查看HandlePromotionFaiure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次GC是有风险的；如果小于，或者HandlePromotionFaiure设置不允许冒险，那么这时就要改为进行一次Full GC。</p><p>所谓冒险：也就是说当用来轮转的Survivor区无法承受新生代中所存活的对象内存时，需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代中，前提是老年代中。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="内存分配策略" scheme="http://yoursite.com/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之五: ParNew收集器+CMS收集器的产品案例分析(响应时间优先)</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E4%BA%94_ParNew%E6%94%B6%E9%9B%86%E5%99%A8+CMS%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BA%A7%E5%93%81%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90(%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88)/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之五_ParNew收集器+CMS收集器的产品案例分析(响应时间优先)/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T11:52:55.585Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><font color="green" size="3"><b>双核,4个cores; 16G memory</b></font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@alish2-cassandra-01 ~]# cat /proc/cpuinfo | grep &quot;cpu cores&quot;</span><br><span class="line">cpu cores       : 2</span><br><span class="line">cpu cores       : 2</span><br></pre></td></tr></table></figure><h3 id="公式简述"><a href="#公式简述" class="headerlink" title="公式简述"></a>公式简述</h3><p>响应时间优先的并发收集器，主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。</p><ol><li><font size="3" color="red">ParNew收集器</font><p>ParNew收集器是Serial收集器的多线程版本，许多运行在Server模式下的虚拟机中首选的新生代收集器，除Serial外，<font color="blue">只有它能与CMS收集器配合工作。</font></p></li><li><font size="3" color="red">CMS收集器</font><p>CMS， 全称Concurrent Low Pause Collector，是jdk1.4后期版本开始引入的新gc算法，在jdk5和jdk6中得到了进一步改进，它的主要适合场景是对响应时间的重要性需求 大于对吞吐量的要求，能够承受垃圾回收线程和应用线程共享处理器资源，并且应用中存在比较多的长生命周期的对象的应用。CMS是用于对tenured generation的回收，也就是年老代的回收，目标是尽量减少应用的暂停时间，减少FullGC发生的几率，利用和应用程序线程并发的垃圾回收线程来 标记清除年老代。<br>CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期是这样：<br><br><br><font size="3" color="blue">初始标记(CMS-initial-mark) -&gt; 并发标记(CMS-concurrent-mark) -&gt; 重新标记(CMS-remark) -&gt; 并发清除(CMS-concurrent-sweep) -&gt;并发重设状态等待下次CMS的触发(CMS-concurrent-reset)</font><br><br><br>其中的1，3两个步骤需要暂停所有的应用程序线程的。第一次暂停从root对象开始标记存活的对象，这个阶段称为初始标记；第二次暂停是在并发标记之后，暂停所有应用程序线程，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致）。第一次暂停会比较短，第二次暂停通常会比较长，并且remark这个阶段可以并行标记。<br><br><br>而并发标记、并发清除、并发重设阶段的所谓并发，是指一个或者多个垃圾回收线程和应用程序线程并发地运行，垃圾回收线程不会暂停应用程序的执行，如果你有多于一个处理器，那么并发收集线程将与应用线程在不同的处理器上运行，显然，这样的开销就是会降低应用的吞吐量。Remark阶段的并行，是指暂停了所有应用程序后，启动一定数目的垃圾回收进程进行并行标记，此时的应用线程是暂停的。</p></li></ol><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>($TOMCAT_HOME/bin/catalina.sh)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_OPTS=&quot;-server -Xmx10240m -Xms10240m -Xmn3840m -XX:PermSize=256m</span><br><span class="line"></span><br><span class="line">-XX:MaxPermSize=256m -Denv=denalicnprod</span><br><span class="line"></span><br><span class="line">-XX:SurvivorRatio=8  -XX:PretenureSizeThreshold=1048576</span><br><span class="line"></span><br><span class="line">-XX:+DisableExplicitGC  </span><br><span class="line"></span><br><span class="line">-XX:+UseParNewGC  -XX:ParallelGCThreads=10</span><br><span class="line"></span><br><span class="line">-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled</span><br><span class="line"></span><br><span class="line">-XX:+CMSScavengeBeforeRemark -XX:ParallelCMSThreads=10</span><br><span class="line"></span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly</span><br><span class="line"></span><br><span class="line">-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</span><br><span class="line"></span><br><span class="line">-XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled</span><br><span class="line"></span><br><span class="line">-XX:+UseFastAccessorMethods</span><br><span class="line"></span><br><span class="line">-XX:LargePageSizeInBytes=128M</span><br><span class="line"></span><br><span class="line">-XX:SoftRefLRUPolicyMSPerMB=0</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC</span><br><span class="line"></span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line"></span><br><span class="line">-XX:+PrintGCDateStamps -Xloggc:gc.log -verbose:gc&quot;</span><br></pre></td></tr></table></figure><h3 id="公式解析"><a href="#公式解析" class="headerlink" title="公式解析"></a>公式解析</h3><table><thead><tr><th>参 数</th><th>含 义</th></tr></thead><tbody><tr><td>-server</td><td>一定要作为第一个参数，启用JDK的server版本，在多个CPU时性能佳</td></tr><tr><td>-Xms</td><td>java Heap初始大小。 默认是物理内存的1/64。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。</td></tr><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存。</td></tr><tr><td>-Xmn</td><td>设置年轻代大小，一般设置为Xmx的2/8~3/8,等同于-XX:NewSize 和 -XX:MaxNewSize 。</td></tr><tr><td>-XX:PermSize</td><td>设定内存的永久保存区初始大小，缺省值为64M</td></tr><tr><td>-XX:MaxPermSize</td><td>设定内存的永久保存区最大大小，缺省值为64M</td></tr><tr><td>-Denv</td><td>指定tomcat运行哪个project</td></tr><tr><td>-XX:SurvivorRatio</td><td>Eden区与Survivor区的大小比值, 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10</td></tr><tr><td>-XX:PretenureSizeThreshold</td><td>晋升年老代的对象大小。默认为0，比如设为1048576(1M)，则超过1M的对象将不在eden区分配，而直接进入年老代。</td></tr><tr><td>-XX:+DisableExplicitGC</td><td>关闭System.gc()</td></tr><tr><td><font color="#1E90FF">-XX:+UseParNewGC</font></td><td><font color="#1E90FF">设置年轻代为并发收集。可与CMS收集同时使用。</font></td></tr><tr><td>-XX:ParallelGCThreads</td><td></td></tr><tr><td><font color="#1E90FF">-XX:+UseConcMarkSweepGC</font></td><td><font color="#1E90FF">设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了。所以，此时年轻代大小最好用-Xmn设置。</font></td></tr><tr><td>-XX:+CMSParallelRemarkEnabled</td><td>开启并行remark</td></tr><tr><td>-XX:+CMSScavengeBeforeRemark</td><td>这个参数还蛮重要的，它的意思是在执行CMS remark之前进行一次youngGC，这样能有效降低remark的时间</td></tr><tr><td>-XX:ParallelCMSThreads</td><td>CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4) ，如果你需要明确设定，可以通过-XX:ParallelCMSThreads=20来设定,其中ParallelGCThreads是年轻代的并行收集线程数</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction</td><td><font color="#3CB371">使用cms作为垃圾回收使用70％后开始CMS收集</font></td></tr><tr><td>-XX:+UseCMSInitiatingOccupancyOnly</td><td>使用手动定义初始化定义开始CMS收集</td></tr><tr><td>-XX:+UseCMSCompactAtFullCollection</td><td>打开对年老代的压缩。可能会影响性能，但是可以消除内存碎片。</td></tr><tr><td>-XX:CMSFullGCsBeforeCompaction</td><td>由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此参数设置运行次FullGC以后对内存空间进行压缩、整理。</td></tr><tr><td>-XX:+CMSPermGenSweepingEnabled</td><td>为了避免Perm区满引起的full gc，<font color="#3CB371">建议开启CMS回收Perm区选项</font></td></tr><tr><td>-XX:+CMSClassUnloadingEnabled</td><td></td></tr><tr><td>-XX:+UseFastAccessorMethods</td><td>原始类型的快速优化</td></tr><tr><td>-XX:LargePageSizeInBytes</td><td>内存页的大小，不可设置过大， 会影响Perm的大小</td></tr><tr><td>-XX:SoftRefLRUPolicyMSPerMB</td><td>“软引用”的对象在最后一次被访问后能存活0毫秒（默认为1秒）。</td></tr><tr><td>-XX:+PrintGCDetails</td><td>记录 GC 运行时的详细数据信息，包括新生成对象的占用内存大小以及耗费时间等</td></tr><tr><td>-XX:+PrintGCTimeStamps</td><td>打印垃圾收集的时间戳</td></tr><tr><td>-XX:+PrintHeapAtGC</td><td>打印GC前后的详细堆栈信息</td></tr><tr><td>-XX:+PrintGCApplicationStoppedTime</td><td>打印垃圾回收期间程序暂停的时间.可与上面混合使用</td></tr><tr><td>-XX:+PrintGCDateStamps</td><td>之前打印gc日志的时候使用是：-XX:+PrintGCTimeStamps，这个选项记录的是jvm启动时间为起点的相对时间，可读性较差，不利于定位问题，使用PrintGCDateStamps记录的是系统时间，更humanreadable</td></tr><tr><td>-Xloggc</td><td>与上面几个配合使用，把相关日志信息记录到文件以便分析</td></tr><tr><td>-verbose:gc</td><td>记录 GC 运行以及运行时间，一般用来查看 GC 是否是应用的瓶颈</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>JVM快速调优手册之四: 堆内存分配的CMS公式解析</title>
    <link href="http://yoursite.com/2019/06/19/JVM%E5%BF%AB%E9%80%9F%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%E4%B9%8B%E5%9B%9B_%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/19/JVM快速调优手册之四_堆内存分配的CMS公式解析/</id>
    <published>2019-06-18T16:00:00.000Z</published>
    <updated>2019-06-20T09:18:36.520Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="JVM-堆内存组成"><a href="#JVM-堆内存组成" class="headerlink" title="JVM 堆内存组成"></a>JVM 堆内存组成</h3><p>Java堆由Perm区和Heap区组成，Heap区由Old区和New区（也叫Young区）组成，New区由Eden区、From区和To区（Survivor）组成。</p><p><img src="/assets/pic/2019-06-19-4-1.png" alt="JVM 堆内存组成"></p><p>Eden区用于存放新生成的对象。Eden中的对象生命不会超过一次Minor GC。Survivor Space 有两个，存放每次垃圾回收后存活的对象，即图的S0和S1。Old Generation Old区，也称老生代，主要存放应用程序中生命周期长的存活对象</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：</p><p>eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况。</p><p>即：</p><font color="blue">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2))</font><p>进而推断出：</p><font color="blue">CMSInitiatingOccupancyFraction &lt;= ((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100</font><table><thead><tr><th style="text-align:left">参数</th><th>含义</th></tr></thead><tbody><tr><td style="text-align:left">Xmx-Xmn</td><td>Old区大小</td></tr><tr><td style="text-align:left">CMSInitiatingOccupancyFraction/100</td><td>Old区百分之多少时,cms开始gc</td></tr><tr><td style="text-align:left">1-CMSInitiatingOccupancyFraction/100</td><td>Old区开始gc回收时剩余空间百分比</td></tr><tr><td style="text-align:left">(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)</td><td>Old区开始gc回收时剩余空间大小</td></tr><tr><td style="text-align:left">(Xmn-Xmn/(SurvivorRatior+2))</td><td>eden+from survivor区的大小</td></tr></tbody></table><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>-Xmx</td><td>java heap最大值。建议均设为物理内存的80%。不可超过物理内存</td></tr><tr><td>-Xmn</td><td>java heap最小值，一般设置为Xmx的3、4分之一,等同于-XX:NewSize 和 -XX:MaxNewSize ,其实为<font color="blue">young区大小</font></td></tr><tr><td>-XX</td><td>CMSInitiatingOccupancyFraction=70 :使用cms作为垃圾回收使用70％后开始CMS收集</td></tr><tr><td>-XX</td><td>SurvivorRatio=2: 生还者池的大小，默认是2</td></tr></tbody></table><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
    
      <category term="JVM" scheme="http://yoursite.com/tags/JVM/"/>
    
      <category term="调优" scheme="http://yoursite.com/tags/%E8%B0%83%E4%BC%98/"/>
    
      <category term="堆内存分配的CMS公式解析" scheme="http://yoursite.com/tags/%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84CMS%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>捷报:连续5周若泽数据第20-21名学员喜捷offer(含面试题)</title>
    <link href="http://yoursite.com/2019/06/18/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD5%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC20-21%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/"/>
    <id>http://yoursite.com/2019/06/18/捷报_连续5周若泽数据第20-21名学员喜捷offer(含面试题)/</id>
    <published>2019-06-17T16:00:00.000Z</published>
    <updated>2019-06-18T11:02:01.857Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第20个小伙伴，<font color="red">25K</font></b></font><p><img src="/assets/blogImg/2019-06-18-1.png" alt="enter description here"></p><font color="#00CD00" size="4"><b>2.第21个小伙伴，<font color="red">2.4W*14</font></b></font><p><img src="/assets/blogImg/2019-06-18-2.png" alt="就业2"></p><font color="blue" size="4"><b>接下来是现场技术面试题:</b></font><ol><li>谈谈Spark RDD 的几大特性，并深入讲讲体现在哪</li><li>说说你参与过的项目，和一些业务场景</li><li>请说说Spark的宽窄依赖</li><li>Spark的stage划分，task跟分区的关系</li><li>详细讲讲Spark的内存管理，计算与存储是如何协调的</li><li>rdd df ds 之间的区别 ，什么时候使用ds</li><li>聊聊kafka消费如何保证不会重复消费</li><li>你项目里说到了数据延迟和数据重跑，请你说说当时是怎么解决的，如何保障幂等性！</li></ol><font color="#00CD00">（ps: 这面试题，若泽数据高级班&amp;线下班的小伙伴，so easy!）</font><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#63B8FF&quot; size=&quot;4&quot;&gt;&lt;br&gt;我们不做过多宣传，因为我们是若泽数据，企业在职。&lt;br&gt;&lt;br&gt;（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）&lt;br&gt;&lt;/font&gt;
    
    </summary>
    
      <category term="高薪就业" scheme="http://yoursite.com/categories/%E9%AB%98%E8%96%AA%E5%B0%B1%E4%B8%9A/"/>
    
    
      <category term="高薪" scheme="http://yoursite.com/tags/%E9%AB%98%E8%96%AA/"/>
    
      <category term="就业" scheme="http://yoursite.com/tags/%E5%B0%B1%E4%B8%9A/"/>
    
      <category term="面试题" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>生产Flume源码导入IDEA方式</title>
    <link href="http://yoursite.com/2019/06/17/%E7%94%9F%E4%BA%A7Flume%E6%BA%90%E7%A0%81%E5%AF%BC%E5%85%A5IDEA%E6%96%B9%E5%BC%8F/"/>
    <id>http://yoursite.com/2019/06/17/生产Flume源码导入IDEA方式/</id>
    <published>2019-06-16T16:00:00.000Z</published>
    <updated>2019-06-20T09:37:18.312Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h4 id="下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz"><a href="#下载flume-ng-1-6-0-cdh5-7-0-src-tar-gz" class="headerlink" title="下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz"></a>下载flume-ng-1.6.0-cdh5.7.0-src.tar.gz</h4><p>下载地址:<a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0-src.tar.gz</a></p><h4 id="win安装好maven-3-3-9"><a href="#win安装好maven-3-3-9" class="headerlink" title="win安装好maven-3.3.9"></a>win安装好maven-3.3.9</h4><h4 id="解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径"><a href="#解压flume-ng-1-6-0-cdh5-7-0-src-tar-gz并进入解压路径" class="headerlink" title="解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径"></a>解压flume-ng-1.6.0-cdh5.7.0-src.tar.gz并进入解压路径</h4><h4 id="编译：mvn-clean-compile"><a href="#编译：mvn-clean-compile" class="headerlink" title="编译：mvn clean compile"></a>编译：mvn clean compile</h4><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.0:enforce (clean) on project flume-parent: Some Enforcer rules have failed. Look above for specific messages explaining</span><br><span class="line">why the rule failed. -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p>换成以下编译命令，跳过enforcer</p><p><code>mvn clean compile validate -Denforcer.skip=true</code></p><p>报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flume-ng-morphline-solr-sink: Could not resolve dependencies for project org.apache.flume.flume-ng-sinks:flume-ng-morphline-solr-sink:jar:1.6.0-cdh5.7.0: Fail</span><br><span class="line">ed to collect dependencies at org.kitesdk:kite-morphlines-all:pom:1.0.0-cdh5.7.0 -&gt; org.kitesdk:kite-morphlines-useragent:jar:1.0.0-cdh5.7.0 -&gt; ua_parser:ua-parser:jar:1.3.0: Failed to read artifact d</span><br><span class="line">escriptor for ua_parser:ua-parser:jar:1.3.0: Could not transfer artifact ua_parser:ua-parser:pom:1.3.0 from/to maven-twttr (http://maven.twttr.com): Connect to maven.twttr.com:80 [maven.twttr.com/31.1</span><br><span class="line">3.83.8] failed: Connection timed out: connect -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><code>flume-ng-morphline-solr-sink</code>我们用不到，可以直接注释掉，在<code>flume-ng-sinks</code>下的pom中找到并注释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;modules&gt;</span><br><span class="line">    &lt;module&gt;flume-hdfs-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-irc-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-hbase-sink&lt;/module&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-elasticsearch-sink&lt;/module&gt;</span><br><span class="line">    &lt;!--&lt;module&gt;flume-ng-morphline-solr-sink&lt;/module&gt; --&gt;</span><br><span class="line">    &lt;module&gt;flume-ng-kafka-sink&lt;/module&gt;</span><br><span class="line">&lt;/modules&gt;</span><br></pre></td></tr></table></figure><p>然后重新编译<code>mvn clean compile validate -Denforcer.skip=true</code>，成功</p><p><img src="/assets/pic/2019-06-17-1.png" alt="编译"></p><h4 id="导入IDEA"><a href="#导入IDEA" class="headerlink" title="导入IDEA"></a>导入IDEA</h4><p><img src="/assets/pic/2019-06-17-2.png" alt="1"></p><p><img src="/assets/pic/2019-06-17-3.png" alt="2"></p><p><img src="/assets/pic/2019-06-17-4.png" alt="3"></p><p><img src="/assets/pic/2019-06-17-5.png" alt="4"></p><p><img src="/assets/pic/2019-06-17-6.png" alt="5"></p><p><img src="/assets/pic/2019-06-17-7.png" alt="6"></p><p>然后等到导入完毕！</p><p><img src="/assets/pic/2019-06-17-8.png" alt="7"></p><p>导入后没有任何报错，这时我们就可以对源码进行修改了！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://yoursite.com/categories/Flume/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Spark中Cache与Persist的巅峰对决</title>
    <link href="http://yoursite.com/2019/06/14/Spark%E4%B8%ADCache%E4%B8%8EPersist%E7%9A%84%E5%B7%85%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
    <id>http://yoursite.com/2019/06/14/Spark中Cache与Persist的巅峰对决/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-20T09:37:01.640Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="Cache的产生背景"><a href="#Cache的产生背景" class="headerlink" title="Cache的产生背景"></a>Cache的产生背景</h2><p>我们先做一个简单的测试读取一个本地文件做一次collect操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br><span class="line">val rdd=sc.textFile(&quot;file:///home/hadoop/data/input.txt&quot;)</span><br></pre></td></tr></table></figure><p>上面我们进行了两次相同的操作，观察日志我们发现这样一句话<code>Submitting ResultStage 0 (file:///home/hadoop/data/input.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25), which has no missing parents</code>，每次都要去本地读取input.txt文件，这里大家能想到存在什么问题吗? 如果我的文件很大，每次都对相同的RDD进行同一个action操作，那么每次都要到本地读取文件，得到相同的结果。不断进行这样的重复操作，耗费资源浪费时间啊。这时候我们可能想到能不能把RDD保存在内存中呢？答案是可以的，这就是我们所要学习的cache。</p><h2 id="Cache的作用"><a href="#Cache的作用" class="headerlink" title="Cache的作用"></a>Cache的作用</h2><p>通过上面的讲解我们知道, 有时候很多地方都会用到同一个RDD, 那么每个地方遇到Action操作的时候都会对同一个算子计算多次, 这样会造成效率低下的问题。通过cache操作可以把RDD持久化到内存或者磁盘。</p><p>现在我们利用上面说的例子，把rdd进行cache操作</p><p>rdd.cache这时候我们打开192.168.137.130:4040界面查看storage界面中是否有我们的刚才cache的文件，发现并没有。这时候我们进行一个action操作rdd.count。继续查看storage是不是有东西了哈</p><p><img src="/assets/pic/2019-06-14-1.png" alt="Cache"></p><p>并且给我们列出了很多信息，存储级别（后面详解），大小（会发现要比源文件大，这也是一个调优点）等等。</p><p>说到这里小伙伴能能想到什么呢？ cacha是一个Tranformation还是一个Action呢？相信大伙应该知道了。</p><p>cache这个方法也是个Tranformation,当第一次遇到Action算子的时才会进行持久化，所以说我们第一次进行了cache操作在ui中并没有看到结果，进行了count操作才有。</p><h2 id="源码详细解析"><a href="#源码详细解析" class="headerlink" title="源码详细解析"></a>源码详细解析</h2><p><strong>Spark版本：2.2.0</strong></p><p>源码分析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">  */</span><br><span class="line"> def cache(): this.type = persist()</span><br></pre></td></tr></table></figure><p>从源码中可以明显看出cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：（这里注释cache的storage level）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，是不是和上面对上了哈，这里我们能够得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><p>我相信小伙伴们肯定很好奇这个缓存级别到底有多少种呢？我们继续怼源码看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: DeveloperApi ::</span><br><span class="line"> * Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,</span><br><span class="line"> * or ExternalBlockStore, whether to drop the RDD to disk if it falls out of memory or</span><br><span class="line"> * ExternalBlockStore, whether to keep the data in memory in a serialized format, and whether</span><br><span class="line"> * to replicate the RDD partitions on multiple nodes.</span><br><span class="line"> *</span><br><span class="line"> * The [[org.apache.spark.storage.StorageLevel]] singleton object contains some static constants</span><br><span class="line"> * for commonly useful storage levels. To create your own storage level object, use the</span><br><span class="line"> * factory method of the singleton object (`StorageLevel(...)`).</span><br><span class="line"> */</span><br><span class="line">@DeveloperApi</span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure><p>我们先来看看存储类型，源码中我们可以看出有五个参数，分别代表：</p><p><code>useDisk</code>:使用硬盘（外存）;</p><p><code>useMemory</code>:使用内存;</p><p><code>useOffHeap</code>:使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。这部分内存也会被频繁的使用而且也可能导致OOM，它是通过存储在堆中的DirectByteBuffer对象进行引用，可以避免堆和堆外数据进行来回复制；</p><p><code>deserialized</code>:反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象;</p><p><code>replication</code>:备份数（在多个节点上备份，默认为1）。</p><p>我们接着看看缓存级别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Various [[org.apache.spark.storage.StorageLevel]] defined and utility functions for creating</span><br><span class="line"> * new storage levels.</span><br><span class="line"> */</span><br><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，<strong>但这些有什么区别呢？</strong>可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，和上面说的存储类型是相对应的，四个参数是因为有一个是有默认值的。</p><p>好吧这里我又想问小伙伴们一个问题了，这几种存储方式什么意思呢？该如何选择呢？</p><p>官网上进行了详细的解释。我这里介绍一个有兴趣的同学可以去官网看看哈。</p><p><strong>MEMORY_ONLY</strong></p><blockquote><p>使用反序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，某些分区将不会被缓存，并且将在需要时重新计算。这是默认级别。</p></blockquote><p><strong>MEMORY_AND_DISK</strong></p><blockquote><p>使用反序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</p></blockquote><p><strong>MEMORY_ONLY_SER（(Java and Scala)）</strong></p><blockquote><p>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，但是会加大cpu负担。</p></blockquote><p>一个简单的案例感官行的认识存储级别的差别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19M     page_views.dat</span><br><span class="line"></span><br><span class="line">val rdd1=sc.textFile(&quot;file:///home/hadoop/data/page_views.dat&quot;)</span><br><span class="line">rdd1.persist().count</span><br></pre></td></tr></table></figure><p>ui查看缓存大小：</p><p><img src="/assets/pic/2019-06-14-2.png" alt="ui查看缓存大小"></p><p>是不是明显变大了，我们先删除缓存<font color="red">rdd1.unpersist()</font></p><p>使用MEMORY_ONLY_SER级别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line">rdd1.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">rdd1.count</span><br></pre></td></tr></table></figure><p><img src="/assets/pic/2019-06-14-3.png" alt="MEMORY_ONLY_SER"></p><p>这里我就用这两种方式进行对比，大家可以试试其他方式。</p><p>那如何选择呢？哈哈官网也说了。</p><p>你可以在内存使用和CPU效率之间来做出不同的选择不同的权衡。</p><p>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</p><p>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</p><p>不要泄漏到磁盘，除非你在内存中计算需要很大的花费，或者可以过滤大量数据，保存部分相对重要的在内存中。否则存储在磁盘中计算速度会很慢，性能急剧降低。</p><p>后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p><h2 id="删除缓存中的数据"><a href="#删除缓存中的数据" class="headerlink" title="删除缓存中的数据"></a>删除缓存中的数据</h2><p>spark自动监视每个节点上的缓存使用，并以最近最少使用的（LRU）方式丢弃旧数据分区。如果您想手动删除RDD，而不是等待它从缓存中掉出来，请使用 RDD.unpersist()方法。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Cache" scheme="http://yoursite.com/tags/Cache/"/>
    
      <category term="Persist" scheme="http://yoursite.com/tags/Persist/"/>
    
  </entry>
  
  <entry>
    <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
    <link href="http://yoursite.com/2019/06/14/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
    <id>http://yoursite.com/2019/06/14/生产SparkStreaming数据零丢失最佳实践(含代码)/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-14T05:14:17.936Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use test</span><br><span class="line">mysql&gt; create table hlw_offset(</span><br><span class="line">        topic varchar(32),</span><br><span class="line">        groupid varchar(50),</span><br><span class="line">        partitions int,</span><br><span class="line">        fromoffset bigint,</span><br><span class="line">        untiloffset bigint,</span><br><span class="line">        primary key(topic,groupid,partitions)</span><br><span class="line">        );</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;</span><br><span class="line">&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;</span><br><span class="line">--------------------------------------------------</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.27&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;config&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）StreamingContext</span><br><span class="line">2）从kafka中获取数据(从外部存储获取offset--&gt;根据offset获取kafka中的数据)</span><br><span class="line">3）根据业务进行逻辑处理</span><br><span class="line">4）将处理结果存到外部存储中--保存offset</span><br><span class="line">5）启动程序，等待程序结束</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import kafka.common.TopicAndPartition</span><br><span class="line">import kafka.message.MessageAndMetadata</span><br><span class="line">import kafka.serializer.StringDecoder</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils&#125;</span><br><span class="line">import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;</span><br><span class="line">import scalikejdbc._</span><br><span class="line">import scalikejdbc.config._</span><br><span class="line">object JDBCOffsetApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建SparkStreaming入口</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)</span><br><span class="line">    val ssc = new StreamingContext(conf,Seconds(5))</span><br><span class="line">    //kafka消费主题</span><br><span class="line">    val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet</span><br><span class="line">    //kafka参数</span><br><span class="line">    //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改</span><br><span class="line">    val kafkaParams = Map[String,String](</span><br><span class="line">      &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),</span><br><span class="line">      &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),</span><br><span class="line">      &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)</span><br><span class="line">    )</span><br><span class="line">    //先使用scalikejdbc从MySQL数据库中读取offset信息</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">    //+------------+------------------+------------+------------+-------------+</span><br><span class="line">    //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来</span><br><span class="line">    //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到</span><br><span class="line">    DBs.setup()</span><br><span class="line">    val fromOffset = DB.readOnly( implicit session =&gt; &#123;</span><br><span class="line">      SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; &#123;</span><br><span class="line">        (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))</span><br><span class="line">      &#125;).list().apply()</span><br><span class="line">    &#125;).toMap</span><br><span class="line">    //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费</span><br><span class="line">      val messages = if (fromOffset.isEmpty) &#123;</span><br><span class="line">        println(&quot;从头开始消费...&quot;)</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        println(&quot;从已存在记录开始消费...&quot;)</span><br><span class="line">        val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())</span><br><span class="line">        KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)</span><br><span class="line">      &#125;</span><br><span class="line">      messages.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">        if(!rdd.isEmpty())&#123;</span><br><span class="line">          //输出rdd的数据量</span><br><span class="line">          println(&quot;数据统计记录为：&quot;+rdd.count())</span><br><span class="line">          //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组</span><br><span class="line">//          trait HasOffsetRanges &#123;</span><br><span class="line">//            def offsetRanges: Array[OffsetRange]</span><br><span class="line">//          &#125;</span><br><span class="line">          val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">          offsetRanges.foreach(x =&gt; &#123;</span><br><span class="line">            //输出每次消费的主题，分区，开始偏移量和结束偏移量</span><br><span class="line">            println(s&quot;---$&#123;x.topic&#125;,$&#123;x.partition&#125;,$&#123;x.fromOffset&#125;,$&#123;x.untilOffset&#125;---&quot;)</span><br><span class="line">           //将最新的偏移量信息保存到MySQL表中</span><br><span class="line">            DB.autoCommit( implicit session =&gt; &#123;</span><br><span class="line">              SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)</span><br><span class="line">            .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)</span><br><span class="line">              .update().apply()</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义的ValueUtils工具类如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import com.typesafe.config.ConfigFactory</span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">object ValueUtils &#123;</span><br><span class="line">val load = ConfigFactory.load()</span><br><span class="line">  def getStringValue(key:String, defaultValue:String=&quot;&quot;) = &#123;</span><br><span class="line">val value = load.getString(key)</span><br><span class="line">    if(StringUtils.isNotEmpty(value)) &#123;</span><br><span class="line">      value</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      defaultValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>application.conf内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">metadata.broker.list = &quot;192.168.137.251:9092&quot;</span><br><span class="line">auto.offset.reset = &quot;smallest&quot;</span><br><span class="line">group.id = &quot;hlw_offset_group&quot;</span><br><span class="line">kafka.topics = &quot;hlw_offset&quot;</span><br><span class="line">serializer.class = &quot;kafka.serializer.StringEncoder&quot;</span><br><span class="line">request.required.acks = &quot;1&quot;</span><br><span class="line"># JDBC settings</span><br><span class="line">db.default.driver = &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;</span><br><span class="line">db.default.user=&quot;root&quot;</span><br><span class="line">db.default.password=&quot;123456&quot;</span><br></pre></td></tr></table></figure></li><li><p>自定义kafka producer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.&#123;Date, Properties&#125;</span><br><span class="line">import kafka.producer.&#123;KeyedMessage, Producer, ProducerConfig&#125;</span><br><span class="line">object KafkaProducer &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val properties = new Properties()</span><br><span class="line">    properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))</span><br><span class="line">    properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))</span><br><span class="line">    properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))</span><br><span class="line">    val producerConfig = new ProducerConfig(properties)</span><br><span class="line">    val producer = new Producer[String,String](producerConfig)</span><br><span class="line">    val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)</span><br><span class="line">    //每次产生100条数据</span><br><span class="line">    var i = 0</span><br><span class="line">    for (i &lt;- 1 to 100) &#123;</span><br><span class="line">      val runtimes = new Date().toString</span><br><span class="line">     val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)</span><br><span class="line">      producer.send(messages)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;数据发送完毕...&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka</span><br><span class="line">[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</span><br></pre></td></tr></table></figure></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台输出结果：</span><br><span class="line">从头开始消费...</span><br><span class="line">数据统计记录为：500</span><br><span class="line">---hlw_offset,0,0,500---</span><br></pre></td></tr></table></figure></li></ol><pre><code>查看MySQL表，offset记录成功<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |          0 |         500 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></code></pre><ol start="5"><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//控制台结果输出：</span><br><span class="line">从已存在记录开始消费...</span><br><span class="line">数据统计记录为：300</span><br><span class="line">---hlw_offset,0,500,800---</span><br></pre></td></tr></table></figure></li><li><p>查看更新后的offset MySQL数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from hlw_offset;</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| topic      | groupid          | partitions | fromoffset | untiloffset |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br><span class="line">| hlw_offset | hlw_offset_group |          0 |        500 |         800 |</span><br><span class="line">+------------+------------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;MySQL创建存储offset的表格&quot;&gt;&lt;a href=&quot;#MySQL创建存储offset的表格&quot; class=&quot;headerlink&quot; title=&quot;MySQL创建存储offset的表格&quot;&gt;&lt;/a&gt;MySQL创建存储offset的表格&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mysql&amp;gt; use test&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mysql&amp;gt; create table hlw_offset(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        topic varchar(32),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        groupid varchar(50),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        partitions int,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fromoffset bigint,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        untiloffset bigint,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        primary key(topic,groupid,partitions)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        );&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Spark Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="spark streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
  </entry>
  
  <entry>
    <title>2019端午-线下项目第14期圆满结束</title>
    <link href="http://yoursite.com/2019/06/11/2019%E7%AB%AF%E5%8D%88-%E7%BA%BF%E4%B8%8B%E9%A1%B9%E7%9B%AE%E7%AC%AC14%E6%9C%9F%E5%9C%86%E6%BB%A1%E7%BB%93%E6%9D%9F/"/>
    <id>http://yoursite.com/2019/06/11/2019端午-线下项目第14期圆满结束/</id>
    <published>2019-06-10T16:00:00.000Z</published>
    <updated>2019-06-17T10:00:43.105Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><center><br>2019年端午节<br><br>4天3夜上海线下班<br><br>圆满结束<br><br>一句话，上海温度有点燥热<br><br>但，会议室空调很给力<br><br><br><br>小伙伴们来自<font color="blue">11个城市</font><br><br><b><br>北京、上海、深圳<br><br>广州、杭州、合肥、徐州<br><br>石家庄、大庆、天津、厦门<br></b><br><br><br><br>大家为了一个真实目标<br><br>学习真正企业级大数据生产项目<br><br><font color="blue">3个生产项目+2个Topic分享</font><br><br>一年我们只在节假日&amp;周末举办<br><br>错过了就是错过了<br><br>期待8月下旬线下项目班第15期<br><br></center><p><img src="/assets/blogImg/2019-06-11-1.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-06-11-2.png" alt="enter description here"></p><p><img src="/assets/blogImg/2019-06-11-3.png" alt="enter description here"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="线下实战班" scheme="http://yoursite.com/categories/%E7%BA%BF%E4%B8%8B%E5%AE%9E%E6%88%98%E7%8F%AD/"/>
    
    
      <category term="线下实战班" scheme="http://yoursite.com/tags/%E7%BA%BF%E4%B8%8B%E5%AE%9E%E6%88%98%E7%8F%AD/"/>
    
  </entry>
  
  <entry>
    <title>生产HDFS Block损坏恢复最佳实践(含思考题)</title>
    <link href="http://yoursite.com/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/"/>
    <id>http://yoursite.com/2019/06/06/生产HDFS Block损坏恢复最佳实践(含思考题)/</id>
    <published>2019-06-05T16:00:00.000Z</published>
    <updated>2019-06-20T03:26:26.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h3 id="文件ruozedata-md"><a href="#文件ruozedata-md" class="headerlink" title="文件ruozedata.md"></a>文件ruozedata.md</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure><h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure><p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p><p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p><p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p><p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul><li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li><li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li></ul><p>块扫描: <a href="https://blog.cloudera.com/blog/2016/12/hdfs-datanode-scanners-and-disk-checker-explained/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2016/12/hdfs-datanode-scanners-and-disk-checker-explained/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="故障案例" scheme="http://yoursite.com/categories/%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/"/>
    
    
      <category term="高级" scheme="http://yoursite.com/tags/%E9%AB%98%E7%BA%A7/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
      <category term="Block损坏恢复" scheme="http://yoursite.com/tags/Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D/"/>
    
      <category term="实践" scheme="http://yoursite.com/tags/%E5%AE%9E%E8%B7%B5/"/>
    
  </entry>
  
  <entry>
    <title>你真的了解volatile关键字吗？</title>
    <link href="http://yoursite.com/2019/06/05/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3volatile%E5%85%B3%E9%94%AE%E5%AD%97/"/>
    <id>http://yoursite.com/2019/06/05/你真的了解volatile关键字/</id>
    <published>2019-06-04T16:00:00.000Z</published>
    <updated>2019-06-20T11:47:39.842Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><a id="more"></a><h2 id="volatile的语义"><a href="#volatile的语义" class="headerlink" title="volatile的语义"></a>volatile的语义</h2><p>一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：</p><ul><li><p>保证了不同线程对这个变量进行操作时的可见性</p><p>即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的</p></li><li><p>禁止进行指令重排序</p><p>举例，线程1先执行，线程2后执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1//线程1</span><br><span class="line">2boolean stop = false;</span><br><span class="line">3while(!stop)&#123;</span><br><span class="line">4    doSomething();</span><br><span class="line">5&#125;</span><br><span class="line">6//线程2</span><br><span class="line">7stop = true;</span><br></pre></td></tr></table></figure></li></ul><p>这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法</p><p>但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？</p><p>不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）</p><p>无法中断，导致死循环的原因：</p><ul><li>每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中</li><li>那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去</li></ul><p>当我们使用volatile修饰了flag之后就不一样了，使用volatile关键字会强制将修改的值立即写入主存：</p><ul><li>使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存值无效</li><li>由于线程1的工作内存中缓存变量stop的缓存值无效，所以线程1再次读取变量stop的值时会去主存读取stop的值</li><li>那么在线程2修改stop值时（这里包括2个操作：修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存值无效，然后线程1读取时，发现自己的缓存值无效，它就会等待缓存值对应的主存地址被更新之后，然后去对应的主存读取最新的值</li><li>那么，线程1读取到的就是最新的正确的值</li></ul><h2 id="volatile与原子性（无法保证所有的操作都具有原子性）"><a href="#volatile与原子性（无法保证所有的操作都具有原子性）" class="headerlink" title="volatile与原子性（无法保证所有的操作都具有原子性）"></a>volatile与原子性（无法保证所有的操作都具有原子性）</h2><p>从上面知道volatile关键保证了操作的可见性，但是volatile能保证对变量操作的原子性吗？</p><h3 id="看如下的一个例子"><a href="#看如下的一个例子" class="headerlink" title="看如下的一个例子"></a>看如下的一个例子</h3><p>有个被volatile修饰的int类型的变量inc初始值为0，此时有10个线程对这个变量去进行增加的操作，每个变量增加到1000，那么最终结果按道理来说是1000*10=10000的，但是并不能，运行出来可能是一个小于10000的数字（调用了，Thread.yield()方法，暂停当前正在执行的线程对象,并执行其他线程）</p><h3 id="存在的误区"><a href="#存在的误区" class="headerlink" title="存在的误区"></a>存在的误区</h3><p>可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000</p><h3 id="事实"><a href="#事实" class="headerlink" title="事实"></a>事实</h3><p>在前面已经提到过，自增操作是不具备原子性的，它的步骤包括：</p><ul><li>读取变量的原始值</li><li>进行加1操作</li><li>写入工作内存，刷到主存中去</li></ul><p>那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：</p><ul><li>假如某个时刻变量inc的值为10，此时线程1对变量进行自增操作：线程1先读取了变量inc的原始值，然后线程1被阻塞了</li><li>然后线程2对变量进行自增操作：线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存值无效，同时线程2从主存中读取到的值也是没有任何修改的10</li><li>因此线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存</li><li>然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。</li><li>那么两个线程分别进行了一次自增操作后，inc却只增加1</li></ul><h3 id="存在的疑问"><a href="#存在的疑问" class="headerlink" title="存在的疑问"></a>存在的疑问</h3><p>解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存值无效吗？然后其他线程去读就会读到新的值，对，这个没错。</p><p>这个就是上面的happens-before规则中的volatile变量规则，但是要注意：</p><ul><li>线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改</li><li><p>然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值</p><p>根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的</p></li></ul><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>解决的方法也就是提供原子性的自增操作即可：</p><p>在Java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增，自减、以及加法操作，减法操作进行了封装，保证这些操作是原子性操作；atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作</p><p>针对本案例，可以使用AtomicInteger来替换int，它利用了CAS算法来保证了原子性</p><h2 id="volatile与有序性（防止指令重排）"><a href="#volatile与有序性（防止指令重排）" class="headerlink" title="volatile与有序性（防止指令重排）"></a>volatile与有序性（防止指令重排）</h2><p>在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性；volatile关键字禁止指令重排序有两层意思：</p><ul><li><p>当程序执行到volatile变量的读操作或者写操作时</p><p>在volatile这个操作前面的操作的更改肯定全部已经进行</p><p>且结果已经对后面的操作可见</p></li><li><p>在volatile这个操作后面的操作肯定还没有进行</p><p>在进行指令优化时</p><p>不能将在对volatile变量访问的语句放在其后面执行</p><p>也不能把volatile变量后面的语句放到其前面执行</p></li></ul><p><strong>例子</strong></p><p>可能上面说的比较绕，举个简单的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1//x、y为非volatile变量</span><br><span class="line">2//flag为volatile变量</span><br><span class="line">3x = 2;        //语句1</span><br><span class="line">4y = 0;        //语句2</span><br><span class="line">5flag = true;    //语句3</span><br><span class="line">6x = 4;         //语句4</span><br><span class="line">7y = -1;        //语句5</span><br></pre></td></tr></table></figure><p>由于flag变量为volatile变量，那么在进行指令重排序的过程的时候：</p><ul><li>不会将语句3放到语句1、语句2前面</li><li>也不会将语句3放到语句4、语句5后面</li><li>但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的</li></ul><p>并且volatile关键字能保证，执行到语句3时：</p><ul><li>语句1和语句2必定是执行完毕了的</li><li>且语句1和语句2的执行结果对语句3、语句4、语句5是可见的</li></ul><h2 id="指令重排的应用（双重懒加载的单例模式）"><a href="#指令重排的应用（双重懒加载的单例模式）" class="headerlink" title="指令重排的应用（双重懒加载的单例模式）"></a>指令重排的应用（双重懒加载的单例模式）</h2><p>一个最经典的使用场景就是双重懒加载的单例模式了：</p><p><img src="/assets/pic/2019-06-12.png" alt="单例模式"></p><p>这里的volatile关键字主要是为了防止指令重排</p><p>singleton = new Singleton()这段代码，其实是分三步走的：</p><ul><li>分配内存空间</li><li>初始化对象</li><li><p>将singleton对象指向分配的内存地址</p><p>加上 volatile 是为了让以上的三步操作顺序执行，反之有可能第三步在第二步之前被执行，那么就有可能某个线程拿到的单例对象是还没有初始化的，以致于报错</p></li></ul><h2 id="volatile的实现机制"><a href="#volatile的实现机制" class="headerlink" title="volatile的实现机制"></a>volatile的实现机制</h2><p>前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的；下面这段话摘自《深入理解Java虚拟机》 ：<br>观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令：</p><ul><li>lock前缀指令实际上相当于一个 内存屏障（也成内存栅栏），内存屏障会提供3个功能：</li><li><p>它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置</p><p>也不会把前面的指令排到内存屏障的后面</p><p>即在执行到内存屏障这句指令时，在它前面的操作已经全部完成</p></li><li><p>它会强制将对缓存的修改操作立即写入主存</p></li><li>如果是写操作（即修改操作），它会导致其他CPU中对应的缓存值无效</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>捷报:连续4周若泽数据第16-19名学员喜捷offer(含面试题)</title>
    <link href="http://yoursite.com/2019/06/03/%E6%8D%B7%E6%8A%A5_%E8%BF%9E%E7%BB%AD4%E5%91%A8%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E7%AC%AC16-19%E5%90%8D%E5%AD%A6%E5%91%98%E5%96%9C%E6%8D%B7offer(%E5%90%AB%E9%9D%A2%E8%AF%95%E9%A2%98)/"/>
    <id>http://yoursite.com/2019/06/03/捷报_连续4周若泽数据第16-19名学员喜捷offer(含面试题)/</id>
    <published>2019-06-02T16:00:00.000Z</published>
    <updated>2019-06-18T11:02:49.977Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><font color="#63B8FF" size="4"><br>我们不做过多宣传，因为我们是若泽数据，企业在职。<br><br>（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）<br></font><a id="more"></a> <font color="red" size="4"><br>由于不太方便透露姓名，公司名称和offer邮件截图，但是<font color="#6495ED">若泽数据</font>所有对外信息都是真实，禁得住考验，当然也包括课程内容及老师水平！<br></font><font color="#00CD00" size="4"><b>1.第16个小伙伴，<font color="red">20K*14</font></b></font><p><img src="/assets/blogImg/2019-06-03-1.png" alt="就业1"></p><font color="#00CD00" size="4"><b>2.第17个小伙伴，<font color="red">22K</font></b></font><p><img src="/assets/blogImg/2019-06-03-2.png" alt="就业2"></p><font color="#00CD00" size="4"><b>3.第18个小伙伴，<font color="red">15K</font>(学生)</b></font><p><img src="/assets/blogImg/2019-06-03-3.png" alt="就业3"></p><font color="#00CD00" size="4"><b>4.第19个小伙伴，<font color="red">25K</font></b></font><p><img src="/assets/blogImg/2019-06-03-4.png" alt="就业4"></p><font color="blue" size="4"><b>接下来是面试题:</b></font><p><img src="/assets/blogImg/2019-06-03-5.png" alt="面试1"></p><p><img src="/assets/blogImg/2019-06-03-6.png" alt="面试2"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;font color=&quot;#63B8FF&quot; size=&quot;4&quot;&gt;&lt;br&gt;我们不做过多宣传，因为我们是若泽数据，企业在职。&lt;br&gt;&lt;br&gt;（现在其他机构也效仿我们说，企业在职，哎，很无语了，擦亮眼睛很重要！）&lt;br&gt;&lt;/font&gt;
    
    </summary>
    
      <category term="高薪就业" scheme="http://yoursite.com/categories/%E9%AB%98%E8%96%AA%E5%B0%B1%E4%B8%9A/"/>
    
    
      <category term="高薪" scheme="http://yoursite.com/tags/%E9%AB%98%E8%96%AA/"/>
    
      <category term="就业" scheme="http://yoursite.com/tags/%E5%B0%B1%E4%B8%9A/"/>
    
      <category term="面试题" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>生产常用Spark累加器剖析之四</title>
    <link href="http://yoursite.com/2019/05/31/%E7%94%9F%E4%BA%A7%E5%B8%B8%E7%94%A8Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%89%96%E6%9E%90%E4%B9%8B%E5%9B%9B/"/>
    <id>http://yoursite.com/2019/05/31/生产常用Spark累加器剖析之四/</id>
    <published>2019-05-30T16:00:00.000Z</published>
    <updated>2019-06-14T06:55:51.706Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h2 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val acc = sc.accumulator(0, “Error Accumulator”)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">val newData = data.map(x =&gt; &#123;</span><br><span class="line">  if (x % 2 == 0) &#123;</span><br><span class="line"> accum += 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">newData.count</span><br><span class="line">acc.value</span><br><span class="line">newData.foreach(println)</span><br><span class="line">acc.value</span><br></pre></td></tr></table></figure><p>上述现象，会造成acc.value的最终值变为10</p><a id="more"></a><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>Spark中的一系列transform操作都会构造成一长串的任务链，此时就需要通过一个action操作来触发（lazy的特性），accumulator也是如此。</p><ul><li>因此在一个action操作之后，调用value方法查看，是没有任何变化</li><li>第一次action操作之后，调用value方法查看，变成了5</li><li>第二次action操作之后，调用value方法查看，变成了10</li></ul><p>原因就在于第二次action操作的时候，又执行了一次累加器的操作，同个累加器，在原有的基础上又加了5，从而变成了10</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过上述的现象描述，我们可以很快知道解决的方法：只进行一次action操作。基于此，我们只要切断任务之间的依赖关系就可以了，即使用cache、persist。这样操作之后，那么后续的累加器操作就不会受前面的transform操作影响了</p><h2 id="案例地址"><a href="#案例地址" class="headerlink" title="案例地址"></a>案例地址</h2><p>相关的工程案例地址在Github上：<a href="https://github.com/lemonahit/spark-train/tree/master/01-Accumulator" target="_blank" rel="noopener">https://github.com/lemonahit/spark-train/tree/master/01-Accumulator</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">/**</span><br><span class="line">  * 使用Spark Accumulators完成Job的数据量处理</span><br><span class="line">  * 统计emp表中NULL出现的次数以及正常数据的条数 &amp; 打印正常数据的信息</span><br><span class="line">  *</span><br><span class="line">  * 若泽数据学员-呼呼呼 on 2017/11/9.</span><br><span class="line">  */</span><br><span class="line">object AccumulatorsApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;AccumulatorsApp&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val lines = sc.textFile(&quot;E:/emp.txt&quot;)</span><br><span class="line">    // long类型的累加器值</span><br><span class="line">    val nullNum = sc.longAccumulator(&quot;NullNumber&quot;)</span><br><span class="line">    val normalData = lines.filter(line =&gt; &#123;</span><br><span class="line">      var flag = true</span><br><span class="line">      val splitLines = line.split(&quot;\t&quot;)</span><br><span class="line">      for (splitLine &lt;- splitLines)&#123;</span><br><span class="line">        if (&quot;&quot;.equals(splitLine))&#123;</span><br><span class="line">          flag = false</span><br><span class="line">          nullNum.add(1)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line">    // 使用cache方法，将RDD的第一次计算结果进行缓存；防止后面RDD进行重复计算，导致累加器的值不准确</span><br><span class="line">    normalData.cache()</span><br><span class="line">    // 打印每一条正常数据</span><br><span class="line">    normalData.foreach(println)</span><br><span class="line">    // 打印正常数据的条数</span><br><span class="line">    println(&quot;NORMAL DATA NUMBER: &quot; + normalData.count())</span><br><span class="line">    // 打印emp表中NULL出现的次数</span><br><span class="line">    println(&quot;NULL: &quot; + nullNum.value)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;现象描述&quot;&gt;&lt;a href=&quot;#现象描述&quot; class=&quot;headerlink&quot; title=&quot;现象描述&quot;&gt;&lt;/a&gt;现象描述&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;val acc = sc.accumulator(0, “Error Accumulator”)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;val data = sc.parallelize(1 to 10)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;val newData = data.map(x =&amp;gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  if (x % 2 == 0) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; accum += 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;newData.count&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acc.value&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;newData.foreach(println)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acc.value&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上述现象，会造成acc.value的最终值变为10&lt;/p&gt;
    
    </summary>
    
      <category term="Spark Other" scheme="http://yoursite.com/categories/Spark-Other/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="累加器" scheme="http://yoursite.com/tags/%E7%B4%AF%E5%8A%A0%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>上海某公司的生产MySQL灾难性挽救</title>
    <link href="http://yoursite.com/2019/05/30/%E4%B8%8A%E6%B5%B7%E6%9F%90%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7MySQL%E7%81%BE%E9%9A%BE%E6%80%A7%E6%8C%BD%E6%95%91/"/>
    <id>http://yoursite.com/2019/05/30/上海某公司的生产MySQL灾难性挽救/</id>
    <published>2019-05-29T16:00:00.000Z</published>
    <updated>2019-06-11T05:41:52.743Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h3><p>本人(<a href="www.ruozedata.com">若泽数据</a>J哥)的媳妇，是个漂亮的妹子，同时也是一枚爬虫&amp;Spark开发工程师。</p><p>前天，她的公司MySQL(阿里云ECS服务器)，由于磁盘爆了加上人为的修复，导致各种问题，然后经过2天的折腾，终于公司的大神修复不了了。于是就丢给她了，顺理成章的就丢给我了。我想说，难道J哥这么出名吗？那为了在妹子面前不能丢我们真正大佬的神技，于是乎我就很爽快接了这个MySQL故障恢复，此次故障的是一个数据盘，1T。<br>这时的我，说真的并没有意识到，此事是如此的繁杂，特此写此博文记录一下，毕竟J哥我年纪也大了。</p><p>PS:<br>这里吐槽一下，并没有周日全备+周1~周6增量备份机制哟，不然恢复就爽歪歪了。<br><a id="more"></a></p><h3 id="2-故障现象"><a href="#2-故障现象" class="headerlink" title="2.故障现象"></a>2.故障现象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看表结构、查询表数据都如下抛错:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p><img src="/assets/blogImg/2019530_1.png" alt="enter description here"></p><h3 id="3-尝试修复第一次，失败"><a href="#3-尝试修复第一次，失败" class="headerlink" title="3.尝试修复第一次，失败"></a>3.尝试修复第一次，失败</h3><p>3.1 使用repair命令修复表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; repair table wenshu.wenshu2018;  </span><br><span class="line">错误依旧:</span><br><span class="line">ERROR 1030 (HY000): Got error 122 from storage engine</span><br></pre></td></tr></table></figure><p>3.2 谷歌一篇有指导意义的<br><a href="https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine" target="_blank" rel="noopener">https://stackoverflow.com/questions/68029/got-error-122-from-storage-engine</a></p><ul><li>3.2.1 让其扩容数据磁盘为1.5T，试试，依旧这个错误；</li><li>3.2.2 临时目录修改为大的磁盘空间，试试，依旧这个错误；</li><li>3.2.3 取消磁盘限额，试试，依旧这个错误；</li><li>3.2.4 就是一开始的repair命令修复，试试，依旧这个错误；</li></ul><p>这时的我，也无语了，什么鬼！谷歌一页页搜索验证，没有用！</p><h3 id="4-先部署相同系统的相同版本的机器和MySQL"><a href="#4-先部署相同系统的相同版本的机器和MySQL" class="headerlink" title="4.先部署相同系统的相同版本的机器和MySQL"></a>4.先部署相同系统的相同版本的机器和MySQL</h3><p>于是J哥，快速在【若泽数据】的阿里云账号上买了1台Ubuntu 16.04.6的按量付费机器<br>迅速部署MySQL5.7.26。</p><ul><li>4.1 购买按量付费机器(假如不会购买，找J哥)</li><li>4.2 部署MySQL</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.更新apt-get</span><br><span class="line">$ apt-get update</span><br><span class="line"></span><br><span class="line">b.安装MySQL-Server</span><br><span class="line">$ apt-get install mysql-server</span><br><span class="line"></span><br><span class="line">之后会问你，是否要下载文件， 输入 y 就好了</span><br><span class="line">然后会出现让你设置 root 密码的界面</span><br><span class="line">输入密码: ruozedata123</span><br><span class="line">然后再重复一下，</span><br><span class="line">再次输入密码: ruozedata123</span><br><span class="line"></span><br><span class="line">c.安装MySQL-Client</span><br><span class="line">$ apt install mysql-client</span><br><span class="line"></span><br><span class="line">d.我们可以使用</span><br><span class="line">$ mysql -uroot -pruozedata123</span><br><span class="line">来连接服务器本地的 MySQL</span><br></pre></td></tr></table></figure><h3 id="5-尝试先通过frm文件恢复表结构，失败"><a href="#5-尝试先通过frm文件恢复表结构，失败" class="headerlink" title="5.尝试先通过frm文件恢复表结构，失败"></a>5.尝试先通过frm文件恢复表结构，失败</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a. 建立一个数据库，比如wenshu.</span><br><span class="line"></span><br><span class="line">b. 在ruozedata数据库下建立同名的数据表wenshu2018，表结构随意，这里只有一个id字段，操作过程片段如下：</span><br><span class="line"></span><br><span class="line">mysql&gt; create table wenshu2018 (id bigint) engine=InnoDB;</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------+</span><br><span class="line">| Tables_in_aa |</span><br><span class="line">+--------------+</span><br><span class="line">| wenshu2018   |</span><br><span class="line">+--------------+</span><br><span class="line">1 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; desc wenshu2018;</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| Field | Type       | Null | Key | Default | Extra |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">| id    | bigint(20) | NO   |     | NULL    |       |</span><br><span class="line">+-------+------------+------+-----+---------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">c.停止mysql服务器，将wenshu2018.frm文件scp远程拷贝到新的正常数据库的数据目录wenshu下，覆盖掉下边同名的frm文件：</span><br><span class="line"></span><br><span class="line">d.重新启动MYSQL服务</span><br><span class="line"></span><br><span class="line">e.测试下是否恢复成功，进入wenshu数据库，用desc命令测试下，错误为:</span><br><span class="line">mysql Tablespace is missing for table `wenshu`.`wenshu2018`.</span><br></pre></td></tr></table></figure><h3 id="6-尝试有没有备份的表结构恢复数据，失败"><a href="#6-尝试有没有备份的表结构恢复数据，失败" class="headerlink" title="6.尝试有没有备份的表结构恢复数据，失败"></a>6.尝试有没有备份的表结构恢复数据，失败</h3><p>媳妇公司给出一个表结构,如下，经过测试无法恢复，原因就是无法和ibd文件匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS cpws_batch;</span><br><span class="line">CREATE TABLE cpws_batch  (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,</span><br><span class="line">  source text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  error_msg text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,</span><br><span class="line">  crawl_time datetime NULL DEFAULT NULL,</span><br><span class="line">  status tinyint(4) NULL DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id) USING BTREE,</span><br><span class="line">  INDEX ix_status(status) USING BTREE,</span><br><span class="line">  INDEX ix_doc_id(doc_id) USING BTREE</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;</span><br></pre></td></tr></table></figure><h3 id="7-如何获取正确的表结构，这是【成功的第一步】"><a href="#7-如何获取正确的表结构，这是【成功的第一步】" class="headerlink" title="7.如何获取正确的表结构，这是【成功的第一步】"></a>7.如何获取正确的表结构，这是【成功的第一步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s get.dbsake.net &gt; /tmp/dbsake</span><br><span class="line">$ chmod u+x /tmp/dbsake</span><br><span class="line">$ /tmp/dbsake frmdump /mnt/mysql_data/wenshu/wenshu2018.frm </span><br><span class="line">--</span><br><span class="line">-- Table structure for table wenshu_0_1000</span><br><span class="line">-- Created with MySQL Version 5.7.25</span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br></pre></td></tr></table></figure><p>对比Step6的表结构，感觉就差分区设置而已，坑！<br>这时，J哥有种信心，恢复应该小菜了。</p><h3 id="8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"><a href="#8-由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！" class="headerlink" title="8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！"></a>8.由于恢复ECS机器是若泽数据账号购买，这时需要从媳妇公司账号的机器传输这张表ibd文件，差不多300G，尽管我们是阿里云的同一个区域同一个可用区，加上调大外网带宽传输，依然不能等待这么久传输！</h3><h3 id="9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"><a href="#9-要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】" class="headerlink" title="9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】"></a>9.要求媳妇公司购买同账户下同区域的可用区域的云主机，系统盘300G，没有买数据盘，先尝试做恢复看看，能不能成功恢复第一个表哟？【成功的第二步】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">9.1首先需要一个跟要恢复的表结构完全一致的表，至关重要</span><br><span class="line">mysql&gt; CREATE DATABASE wenshu /*!40100 DEFAULT CHARACTER SET utf8mb4 */;</span><br><span class="line">USE wenshu;</span><br><span class="line">CREATE TABLE wenshu2018 (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  doc_id varchar(255) DEFAULT NULL,</span><br><span class="line">  source text,</span><br><span class="line">  error_msg text,</span><br><span class="line">  crawl_time datetime DEFAULT NULL,</span><br><span class="line">  status tinyint(4) DEFAULT NULL COMMENT &apos;0/1 成功/失败&apos;,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  KEY ix_status (status),</span><br><span class="line">  KEY ix_doc_id (doc_id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8 </span><br><span class="line">/*!50100  PARTITION BY RANGE (id)</span><br><span class="line">(PARTITION p0 VALUES LESS THAN (4000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p1 VALUES LESS THAN (8000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p2 VALUES LESS THAN (12000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p3 VALUES LESS THAN (16000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p4 VALUES LESS THAN (20000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p5 VALUES LESS THAN (24000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p6 VALUES LESS THAN (28000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p7 VALUES LESS THAN (32000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p8 VALUES LESS THAN (36000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p9 VALUES LESS THAN (40000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p10 VALUES LESS THAN (44000000) ENGINE = InnoDB,</span><br><span class="line"> PARTITION p11 VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */;</span><br><span class="line"></span><br><span class="line">9.2然后DISCARD TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 DISCARD TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.3把要恢复的ibd文件复制到mysql的data文件夹下，修改用户和用户组为mysql</span><br><span class="line">$ scp wenshu2018#P#p*.ibd  新建机器IP:/mnt/mysql_data/wenshu/</span><br><span class="line">$ chown -R mysql:mysql /mnt/mysql_data/wenshu/wenshu2018#P#p*.ibd</span><br><span class="line"></span><br><span class="line">9.4然后执行IMPORT TABLESPACE</span><br><span class="line">mysql&gt; ALTER TABLE wenshu.wenshu2018 IMPORT TABLESPACE;</span><br><span class="line"></span><br><span class="line">9.5等待，有戏，耗时3h，这时我相信应该么问题的</span><br><span class="line"></span><br><span class="line">9.6查询数据，果然恢复有结果，心里暗暗自喜</span><br><span class="line">mysql&gt; select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"><a href="#10-给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择" class="headerlink" title="10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择"></a>10.给媳妇公司两个选择，这个很重要，在自己公司给领导做选择时，也要应该这样，多项选择，利弊说明，供对方选择</h3><ul><li>10.1 重新购买一台新的服务器，在初始化配置时，就加上1块1.5T的大磁盘。好处是无需挂盘操作，坏处是需要重新做第一个表，浪费3h；</li><li>10.2 购买1.5T的大磁盘，挂载这个机器上。好处是无需再做一次第一个表，坏处是需要修改mysql的数据目录指向为这个大磁盘。系统盘扩容最大也就500G，所以必须外加一个数据盘1.5T容量。</li></ul><p>所以J哥是职场老手了！贼笑！</p><h3 id="11-服务器加数据磁盘，1-5T，购买、挂载、格式化"><a href="#11-服务器加数据磁盘，1-5T，购买、挂载、格式化" class="headerlink" title="11.服务器加数据磁盘，1.5T，购买、挂载、格式化"></a>11.服务器加数据磁盘，1.5T，购买、挂载、格式化</h3><p>接下来的操作是我媳妇独立完成的，这里表扬一下:</p><ul><li>11.1 先买云盘 <a href="https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25445.html?spm=a2c4g.11186623.6.753.40132c30MbE8n8</a></li><li>11.2 再挂载云盘 到对应机器 <a href="https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/25446.html?spm=a2c4g.11186623.6.756.30874f291pXOwB</a></li><li>11.3 最后Linux格式化数据盘 <a href="https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/116650.html?spm=a2c4g.11186623.6.759.11f67d562yD9Lr</a></li></ul><p>图2所示，df -h命令查看，大磁盘/dev/vdb1<br><img src="/assets/blogImg/2019530_2.png" alt="enter description here"></p><h3 id="12-MySQL修改数据目录为大磁盘，重新启动失败，解决"><a href="#12-MySQL修改数据目录为大磁盘，重新启动失败，解决" class="headerlink" title="12.MySQL修改数据目录为大磁盘，重新启动失败，解决"></a>12.MySQL修改数据目录为大磁盘，重新启动失败，解决</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">12.1 修改数据目录为大磁盘</span><br><span class="line">$ mkdir -p /mnt/mysql_data</span><br><span class="line">$ chown mysql:mysql /mnt/mysql_data</span><br><span class="line">$ vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br><span class="line">datadir         = /mnt/mysql_data</span><br><span class="line"></span><br><span class="line">12.2 无法启动mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">无法启动成功，查看日志</span><br><span class="line">2019-05-28T03:41:31.181777Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().</span><br><span class="line">2019-05-28T03:41:31.191805Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192055Z 0 [ERROR] InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable</span><br><span class="line">2019-05-28T03:41:31.192119Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error</span><br><span class="line"></span><br><span class="line">12.3 百思不得其解，CentOS也没有这么麻烦，Ubuntu难道这么搞事吗？</span><br><span class="line">12.4 新增mysqld内容</span><br><span class="line">$ vi /etc/apparmor.d/local/usr.sbin.mysqld</span><br><span class="line"># Site-specific additions and overrides for usr.sbin.mysqld.</span><br><span class="line"># For more details, please see /etc/apparmor.d/local/README.</span><br><span class="line">/mnt/mysql_data/ r,</span><br><span class="line">/mnt/mysql_data/** rwk,</span><br><span class="line"></span><br><span class="line">12.5 reload apparmor的配置并重启</span><br><span class="line">$ service apparmor reload </span><br><span class="line">$ service apparmor restart </span><br><span class="line"> </span><br><span class="line">12.6 重启mysql</span><br><span class="line">$ service mysql restart</span><br><span class="line">如果启动不了，查看/var/log/mysql/error.log</span><br><span class="line">如果出现：InnoDB: The innodb_system data file &apos;ibdata1&apos; must be writable 仔细核对目录权限</span><br><span class="line"></span><br><span class="line">12.7 进mysql查询数据验证，成功</span><br><span class="line">select * from wenshu.wenshu2018 limit 1\G;</span><br></pre></td></tr></table></figure><h3 id="13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"><a href="#13-开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。" class="headerlink" title="13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。"></a>13.开始指导我媳妇做第二个、第三个表，批量恢复，耗时共计16小时，全部恢复完成。</h3><h2 id="最后-若泽数据J哥总结一下"><a href="#最后-若泽数据J哥总结一下" class="headerlink" title="最后@若泽数据J哥总结一下:"></a>最后@若泽数据J哥总结一下:</h2><ul><li>表结构正确的获取；</li><li>机器磁盘规划提前思考；</li><li>ibd数据文件恢复；</li><li>最后加上一个聪明的媳妇！(PS:老板会给媳妇涨薪水不🙅‍♂️)</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jul 09 2019 11:25:10 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1.背景&quot;&gt;&lt;/a&gt;1.背景&lt;/h3&gt;&lt;p&gt;本人(&lt;a href=&quot;www.ruozedata.com&quot;&gt;若泽数据&lt;/a&gt;J哥)的媳妇，是个漂亮的妹子，同时也是一枚爬虫&amp;amp;Spark开发工程师。&lt;/p&gt;&lt;p&gt;前天，她的公司MySQL(阿里云ECS服务器)，由于磁盘爆了加上人为的修复，导致各种问题，然后经过2天的折腾，终于公司的大神修复不了了。于是就丢给她了，顺理成章的就丢给我了。我想说，难道J哥这么出名吗？那为了在妹子面前不能丢我们真正大佬的神技，于是乎我就很爽快接了这个MySQL故障恢复，此次故障的是一个数据盘，1T。&lt;br&gt;这时的我，说真的并没有意识到，此事是如此的繁杂，特此写此博文记录一下，毕竟J哥我年纪也大了。&lt;/p&gt;&lt;p&gt;PS:&lt;br&gt;这里吐槽一下，并没有周日全备+周1~周6增量备份机制哟，不然恢复就爽歪歪了。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="其他组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/"/>
    
      <category term="故障案例" scheme="http://yoursite.com/categories/%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/"/>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="案例" scheme="http://yoursite.com/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
</feed>
